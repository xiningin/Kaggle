{"cell_type":{"d059b064":"code","593efd5c":"code","425f90fa":"code","d2226a79":"code","976c36ac":"code","53d467b4":"code","1833384b":"code","eea6b3e2":"code","64396074":"code","31bd514c":"code","a5e9646c":"code","b97b5b38":"code","0ce18107":"code","1097f88d":"code","1ee22363":"code","b34a8f95":"code","6d385e14":"code","d4164eda":"code","10c964aa":"code","bbc6552c":"code","bb62e7ec":"code","be62b9c5":"code","1e31e4bb":"code","f1214e65":"code","c9d0af38":"code","2677bbfd":"code","ef7e1e21":"code","3a30c8b3":"code","745e6834":"code","c51efdaf":"code","39c44f92":"code","8ff3238e":"code","2df05fdd":"code","633be518":"code","c8914feb":"code","e40b6f57":"markdown","0bcf9fd8":"markdown","9361d76b":"markdown","58091f26":"markdown","ced230da":"markdown","6fcd71fc":"markdown","b6791e2b":"markdown","d8b359c5":"markdown","1896dc74":"markdown","d7e469bf":"markdown","6aefd584":"markdown","ccb45861":"markdown","d393898d":"markdown","93b0efca":"markdown","f03689d8":"markdown","e3134ef1":"markdown","9205d4a9":"markdown","1dea5d4f":"markdown","e4f4f8b6":"markdown","b8194755":"markdown","b4aba6ab":"markdown","d063b122":"markdown","40c2960e":"markdown","5506814f":"markdown","536dba4d":"markdown","46c75d11":"markdown","73713c2c":"markdown"},"source":{"d059b064":"import pandas as pd\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as torch_optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\nfrom datetime import datetime","593efd5c":"train = pd.read_csv('..\/input\/shelter-animal-outcomes\/train.csv.gz')\nprint(\"Shape:\", train.shape)\ntrain.head()","425f90fa":"test = pd.read_csv('..\/input\/shelter-animal-outcomes\/test.csv.gz')\nprint(\"Shape:\", test.shape)\ntest.head()","d2226a79":"sample = pd.read_csv('..\/input\/shelter-animal-outcomes\/sample_submission.csv.gz')\nsample.head()","976c36ac":"Counter(train['OutcomeType'])","53d467b4":"Counter(train['Name']).most_common(5)","1833384b":"train_X = train.drop(columns= ['OutcomeType', 'OutcomeSubtype', 'AnimalID'])\nY = train['OutcomeType']\ntest_X = test","eea6b3e2":"stacked_df = train_X.append(test_X.drop(columns=['ID']))","64396074":"# stacked_df['DateTime'] = pd.to_datetime(stacked_df['DateTime'])\n# stacked_df['year'] = stacked_df['DateTime'].dt.year\n# stacked_df['month'] = stacked_df['DateTime'].dt.month\nstacked_df = stacked_df.drop(columns=['DateTime'])\nstacked_df.head()","31bd514c":"for col in stacked_df.columns:\n    if stacked_df[col].isnull().sum() > 10000:\n        print(\"dropping\", col, stacked_df[col].isnull().sum())\n        stacked_df = stacked_df.drop(columns = [col])","a5e9646c":"stacked_df.head()","b97b5b38":"for col in stacked_df.columns:\n    if stacked_df.dtypes[col] == \"object\":\n        stacked_df[col] = stacked_df[col].fillna(\"NA\")\n    else:\n        stacked_df[col] = stacked_df[col].fillna(0)\n    stacked_df[col] = LabelEncoder().fit_transform(stacked_df[col])","0ce18107":"stacked_df.head()","1097f88d":"# making all variables categorical\nfor col in stacked_df.columns:\n    stacked_df[col] = stacked_df[col].astype('category')","1ee22363":"X = stacked_df[0:26729]\ntest_processed = stacked_df[26729:]\n\n#check if shape[0] matches original\nprint(\"train shape: \", X.shape, \"orignal: \", train.shape)\nprint(\"test shape: \", test_processed.shape, \"original: \", test.shape)","b34a8f95":"Y = LabelEncoder().fit_transform(Y)\n\n#sanity check to see numbers match and matching with previous counter to create target dictionary\nprint(Counter(train['OutcomeType']))\nprint(Counter(Y))\ntarget_dict = {\n    'Return_to_owner' : 3,\n    'Euthanasia': 2,\n    'Adoption': 0,\n    'Transfer': 4,\n    'Died': 1\n}","6d385e14":"X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.10, random_state=0)\nX_train.head()","d4164eda":"#categorical embedding for columns having more than two values\nembedded_cols = {n: len(col.cat.categories) for n,col in X.items() if len(col.cat.categories) > 2}\nembedded_cols","10c964aa":"embedded_col_names = embedded_cols.keys()\nlen(X.columns) - len(embedded_cols) #number of numerical columns","bbc6552c":"embedding_sizes = [(n_categories, min(50, (n_categories+1)\/\/2)) for _,n_categories in embedded_cols.items()]\nembedding_sizes","bb62e7ec":"class ShelterOutcomeDataset(Dataset):\n    def __init__(self, X, Y, embedded_col_names):\n        X = X.copy()\n        self.X1 = X.loc[:,embedded_col_names].copy().values.astype(np.int64) #categorical columns\n        self.X2 = X.drop(columns=embedded_col_names).copy().values.astype(np.float32) #numerical columns\n        self.y = Y\n        \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        return self.X1[idx], self.X2[idx], self.y[idx]","be62b9c5":"#creating train and valid datasets\ntrain_ds = ShelterOutcomeDataset(X_train, y_train, embedded_col_names)\nvalid_ds = ShelterOutcomeDataset(X_val, y_val, embedded_col_names)","1e31e4bb":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')","f1214e65":"def to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)","c9d0af38":"class DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","2677bbfd":"device = get_default_device()\ndevice","ef7e1e21":"class ShelterOutcomeModel(nn.Module):\n    def __init__(self, embedding_sizes, n_cont):\n        super().__init__()\n        self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for categories,size in embedding_sizes])\n        n_emb = sum(e.embedding_dim for e in self.embeddings) #length of all embeddings combined\n        self.n_emb, self.n_cont = n_emb, n_cont\n        self.lin1 = nn.Linear(self.n_emb + self.n_cont, 200)\n        self.lin2 = nn.Linear(200, 70)\n        self.lin3 = nn.Linear(70, 5)\n        self.bn1 = nn.BatchNorm1d(self.n_cont)\n        self.bn2 = nn.BatchNorm1d(200)\n        self.bn3 = nn.BatchNorm1d(70)\n        self.emb_drop = nn.Dropout(0.6)\n        self.drops = nn.Dropout(0.3)\n        \n\n    def forward(self, x_cat, x_cont):\n        x = [e(x_cat[:,i]) for i,e in enumerate(self.embeddings)]\n        x = torch.cat(x, 1)\n        x = self.emb_drop(x)\n        x2 = self.bn1(x_cont)\n        x = torch.cat([x, x2], 1)\n        x = F.relu(self.lin1(x))\n        x = self.drops(x)\n        x = self.bn2(x)\n        x = F.relu(self.lin2(x))\n        x = self.drops(x)\n        x = self.bn3(x)\n        x = self.lin3(x)\n        return x","3a30c8b3":"model = ShelterOutcomeModel(embedding_sizes, 1)\nto_device(model, device)","745e6834":"def get_optimizer(model, lr = 0.001, wd = 0.0):\n    parameters = filter(lambda p: p.requires_grad, model.parameters())\n    optim = torch_optim.Adam(parameters, lr=lr, weight_decay=wd)\n    return optim","c51efdaf":"def train_model(model, optim, train_dl):\n    model.train()\n    total = 0\n    sum_loss = 0\n    for x1, x2, y in train_dl:\n        batch = y.shape[0]\n        output = model(x1, x2)\n        loss = F.cross_entropy(output, y)   \n        optim.zero_grad()\n        loss.backward()\n        optim.step()\n        total += batch\n        sum_loss += batch*(loss.item())\n    return sum_loss\/total","39c44f92":"def val_loss(model, valid_dl):\n    model.eval()\n    total = 0\n    sum_loss = 0\n    correct = 0\n    for x1, x2, y in valid_dl:\n        current_batch_size = y.shape[0]\n        out = model(x1, x2)\n        loss = F.cross_entropy(out, y)\n        sum_loss += current_batch_size*(loss.item())\n        total += current_batch_size\n        pred = torch.max(out, 1)[1]\n        correct += (pred == y).float().sum().item()\n    print(\"valid loss %.3f and accuracy %.3f\" % (sum_loss\/total, correct\/total))\n    return sum_loss\/total, correct\/total","8ff3238e":"def train_loop(model, epochs, lr=0.01, wd=0.0):\n    optim = get_optimizer(model, lr = lr, wd = wd)\n    for i in range(epochs): \n        loss = train_model(model, optim, train_dl)\n        print(\"training loss: \", loss)\n        val_loss(model, valid_dl)","2df05fdd":"batch_size = 1000\ntrain_dl = DataLoader(train_ds, batch_size=batch_size,shuffle=True)\nvalid_dl = DataLoader(valid_ds, batch_size=batch_size,shuffle=True)","633be518":"train_dl = DeviceDataLoader(train_dl, device)\nvalid_dl = DeviceDataLoader(valid_dl, device)","c8914feb":"train_loop(model, epochs=8, lr=0.05, wd=0.00001)","e40b6f57":"#### Training set ","0bcf9fd8":"#### Evaluation function","9361d76b":"#### Optimizer","58091f26":"#### Choosing columns for embedding","ced230da":"#### Training function","6fcd71fc":"#### dropping columns with too many nulls","b6791e2b":"#### Encoding target","d8b359c5":"#### Sample submission file\n\nFor each row, each outcome's probability needs to be filled into the columns","1896dc74":"#### train-valid split","d7e469bf":"## Load Data","6aefd584":"## Making device (GPU\/CPU) compatible \n(borrowed from https:\/\/jovian.ml\/aakashns\/04-feedforward-nn)\n\nIn order to make use of a GPU if available, we'll have to move our data and model to it.","ccb45861":"## Library imports","d393898d":"#### splitting back train and test","93b0efca":"# Using Embeddings for Categorical Variables","f03689d8":"#### Stacking train and test set so that they undergo the same preprocessing ","e3134ef1":"#### How balanced is the dataset?\n\nAdoption and Transfer seem to occur a lot more than the rest","9205d4a9":"## Data preprocessing\n\nOutcomeSubtype column seems to be of no use, so we drop it. Also, since animal ID is unique, it doesn't help in training","1dea5d4f":"## Pytorch Dataset","e4f4f8b6":"#### label encoding","b8194755":"## Model\n\n(modified from https:\/\/www.usfca.edu\/data-institute\/certificates\/fundamentals-deep-learning lesson 2)","b4aba6ab":"<b>Dataset<\/b> - https:\/\/www.kaggle.com\/c\/shelter-animal-outcomes\n\n<b>Problem Statement<\/b>: Given certain features about a shelter animal (like age, sex, color, breed), predict its outcome.\n\nThere are 5 possible outcomes: Return_to_owner, Euthanasia, Adoption, Transfer, Died. We are expected to find the probability of an animal's outcome belonging to each of the 5 categories.","d063b122":"#### Determining size of embedding \n(borrowed from https:\/\/www.usfca.edu\/data-institute\/certificates\/fundamentals-deep-learning lesson 2)","40c2960e":"#### Test set","5506814f":"#### splitting datetime into month and year","536dba4d":"## Very basic data exploration","46c75d11":"#### What are the most common names and how many times do they occur? \n\nThere seem to be too many Nan values. Name might not be a very important factor too","73713c2c":"## Training "}}