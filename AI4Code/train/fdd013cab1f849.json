{"cell_type":{"20ac8b40":"code","6a483400":"code","96429be7":"code","37a9c5c8":"code","23ef108e":"code","4ce9e1eb":"code","e8601b16":"code","510bd4ba":"code","feaf0bdb":"code","ee2348d4":"code","a33427e5":"code","d046a4cc":"code","7b9e4c97":"code","41ef11b5":"code","922f7b81":"code","5d483f23":"code","1c21f058":"markdown","473af59b":"markdown","fbf4eab2":"markdown","155ffe86":"markdown","3d2c4148":"markdown","c4fd39c1":"markdown"},"source":{"20ac8b40":"!mkdir -p \/tmp\/pip\/cache\/\n\nimport os\nfrom shutil import copyfile\nfrom tqdm.auto import tqdm\n\nsrc = '..\/input\/autogluon-files\/'\ndst = '\/tmp\/pip\/cache\/'\nfor filename in tqdm(os.listdir(src)):\n    if '.xyz' in filename:\n        f = filename.split('.xyz')[0]\n        copyfile(src + filename, dst + f + '.tar.gz')\n    else:\n        copyfile(src + filename, dst + filename)\n\n!pip install --no-index --find-links \/tmp\/pip\/cache\/ autogluon","6a483400":"import warnings \nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport glob, os\n\nfrom joblib import Parallel, delayed\nfrom tqdm.auto import tqdm\nfrom autogluon.tabular import TabularPredictor","96429be7":"def calculate_wap(df):\n    a1 = df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']\n    b1 = df['bid_size1'] + df['ask_size1']\n    a2 = df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']\n    b2 = df['bid_size2']+ df['ask_size2']\n    return (a1 \/ b1 + a2 \/ b2) \/ 2\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return ** 2))","37a9c5c8":"from autogluon.core.metrics import make_scorer\n\ndef rmspe(targets, predictions):\n    return np.sqrt((((predictions - targets) \/ targets) ** 2).mean())\n\nrmspe_metric = make_scorer('rmspe', rmspe, optimum = 0, greater_is_better = False)","23ef108e":"def get_stock_stat(stock_id : int, dataType = 'train'):\n    \n    book_train_subset = pd.read_parquet(f'..\/input\/optiver-realized-volatility-prediction\/book_{dataType}.parquet\/stock_id={stock_id}\/')\n    book_train_subset.sort_values(by = ['time_id', 'seconds_in_bucket'])\n    book_train_subset['bas'] = (book_train_subset[['ask_price1', 'ask_price2']].min(axis = 1)\n                                \/ book_train_subset[['bid_price1', 'bid_price2']].max(axis = 1)\n                                - 1)                             \n    book_train_subset['wap'] = calculate_wap(book_train_subset)\n    book_train_subset['log_return'] = book_train_subset.groupby(by = ['time_id'])['wap'].apply(log_return).reset_index(drop = True).fillna(0)\n    stock_stat = pd.concat([\n        book_train_subset.groupby(['time_id'])['log_return'].agg(realized_volatility).rename('rv'),\n        book_train_subset.groupby(['time_id'])['bas'].mean().rename('bas_mean'), \n        book_train_subset.groupby(['time_id'])['bas'].std().rename('bas_std'), \n        ], \n        axis = 1, \n    ).reset_index()\n    stock_stat['stock_id'] = stock_id\n    return stock_stat","4ce9e1eb":"def get_dataSet(stock_ids : list, dataType = 'train'):\n\n    stock_stat = Parallel(n_jobs = -1)(\n        delayed(get_stock_stat)(stock_id, dataType) \n        for stock_id in tqdm(stock_ids, total = len(stock_ids))\n    )\n    stock_stat_df = pd.concat(stock_stat, ignore_index = True)\n    return stock_stat_df","e8601b16":"train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\ntrain.head()","510bd4ba":"train_stock_stat_df = get_dataSet(stock_ids = train['stock_id'].unique(), dataType = 'train')\ntrain_dataSet = pd.merge(train, train_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\ntrain_dataSet['stock_id'] = train_dataSet['stock_id'].astype('category')","feaf0bdb":"train_dataSet.drop('time_id', axis = 1, inplace = True)\ntrain_dataSet['sample_weight'] = 1 \/ np.square(train_dataSet['target'])\ntrain_dataSet.head()","ee2348d4":"predictor = TabularPredictor(\n    label = 'target', \n    problem_type = 'regression', \n    eval_metric = rmspe_metric, \n    sample_weight = 'sample_weight', \n    path = 'autogluon', \n    verbosity = 3, \n)","a33427e5":"predictor.fit(\n    train_data = train_dataSet, \n    time_limit = 1 * 3600, \n    presets = 'medium_quality_faster_train',#'best_quality', \n#     ag_args_fit = {'num_gpus': 1}, \n#     excluded_model_types = ['KNN', 'RF', 'NN', 'FASTAI'], \n    keep_only_best = True, \n    save_space = True, \n    verbosity = 3, \n)","d046a4cc":"predictor.leaderboard()","7b9e4c97":"test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n\ntest_stock_stat_df = get_dataSet(stock_ids = test['stock_id'].unique(), dataType = 'test')\ntest_dataSet = pd.merge(test, test_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\ntest_dataSet['stock_id'] = test_dataSet['stock_id'].astype('category')\ntest_dataSet = test_dataSet.drop(\n    [\n        'time_id', \n    ], axis = 1)\n\ny_pred = test_dataSet[['row_id']]\nX_test = test_dataSet.drop(['row_id'], axis = 1)\nX_test.head()","41ef11b5":"predictor = predictor.load('autogluon')","922f7b81":"from time import time\n\nstart = time()\npreds = predictor.predict(X_test)\nprint('Time Cost:', time() - start)","5d483f23":"y_pred = y_pred.assign(target = preds)\ny_pred.to_csv('submission.csv',index = False)\ny_pred.head()","1c21f058":"# Functions","473af59b":"# Train","fbf4eab2":"# Libraries","155ffe86":"# Inference","3d2c4148":"# Install Autogluon","c4fd39c1":"# [AutoML] Optiver Autogluon Notebook\n\nIn this notebook, I try training an AutoML model from Autogluon, which implements automated bagging, stacking, ensembling and deep learning. More can be found in https:\/\/auto.gluon.ai\/dev\/index.html\n\nI have also uploaded the [autogluon files][1] as a dataset for further inference without internet connection.\n\n**Reference:**\nhttps:\/\/www.kaggle.com\/mayunnan\/realized-volatility-prediction-code-template\n\n[1]: https:\/\/www.kaggle.com\/gogo827jz\/autogluon-files"}}