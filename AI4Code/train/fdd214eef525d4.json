{"cell_type":{"5e94268f":"code","c290ad6c":"code","89481cf6":"code","e2559354":"code","aced3d8d":"code","4ed7e16a":"code","d04827f4":"code","6c348ca2":"code","3e0807fb":"code","e35cf3d9":"code","53ccb564":"code","4437dfe2":"code","af185467":"code","3e779065":"code","175868e8":"code","d5add0a7":"code","f7484a9b":"code","5c76d7eb":"code","837f02cb":"code","08b9ef80":"code","23c514ae":"code","6984771f":"code","7bceb1d6":"code","c84c953b":"code","e975d24e":"code","865b57d8":"code","d046934e":"code","d591e7e1":"code","ef74bbe3":"code","d124166a":"code","4c9d6ac4":"code","04ea8b07":"code","648cf402":"code","02d05f0e":"code","9ccb6891":"code","6b98f877":"code","c2ea666b":"code","52a2d1ff":"code","1c01efbb":"code","28fe8bdb":"markdown","dc4316b5":"markdown","0522789d":"markdown","7d952499":"markdown","53063d8f":"markdown","7b34d1ed":"markdown","e433259b":"markdown","f33bc7c3":"markdown","4203aa5b":"markdown","365b044e":"markdown","95a04256":"markdown","d54fb26f":"markdown","2e22ae8c":"markdown","0eb18ac3":"markdown","592e1b9e":"markdown","8aa4ba03":"markdown","212959be":"markdown","f3870ad9":"markdown","bc45a0cc":"markdown","e24d1a21":"markdown"},"source":{"5e94268f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm,skew","c290ad6c":"train= pd.read_csv(\"..\/input\/train.csv\")\ntest =pd.read_csv(\"..\/input\/test.csv\")","89481cf6":"train.columns","e2559354":"train.describe()","aced3d8d":"test.describe()","4ed7e16a":"print(\"Train : \"+str(train.shape))\n\n#checking for duplicates\nidUn = len(set(train.Id))\nidTo = train.shape[0]\nidDup = idTo - idUn\nprint(str(idDup)+\" duplicates available in this dataset\")","d04827f4":"train_ID = train['Id']\ntest_ID = test['Id']\n\n#Delete the ID Column\ntrain.drop('Id',axis=1,inplace = True)\ntest.drop('Id', axis=1, inplace = True)\n\n#After dropping Id Column\nprint(\"Train Data: \"+str(train.shape))\nprint(\"Test Data: \"+str(test.shape))","6c348ca2":"#Select the Numerical & Categorical Features\n\nnumerical_features = train.select_dtypes(exclude = ['object']).columns\ncategorical_features = train.select_dtypes(include = ['object']).columns","3e0807fb":"# Plotting the numerical columns\nfig = plt.figure(figsize = (15,15))\nax = fig.gca()\ntrain[numerical_features].hist(ax=ax)\nfig.tight_layout()\nfig.show()\n","e35cf3d9":"#plot the Numeric columns against SalePrice Using ScatterPlot\n\nfig = plt.figure(figsize=(15,30))\nfor i,col in enumerate(numerical_features[1:]):\n    fig.add_subplot(12,3,1+i)\n    plt.scatter(train[col], train['SalePrice'])\n    plt.xlabel(col)\n    plt.ylabel('SalePrice')\nfig.tight_layout()\nfig.show()","53ccb564":"fig = plt.figure(figsize=(15,50))\nfor i, col in enumerate(categorical_features):\n    fig.add_subplot(11,4,1+i)\n    train.groupby(col).mean()['SalePrice'].plot.bar(yerr = train.groupby(col).std())\nfig.tight_layout()\nfig.show()","4437dfe2":"sns.set_style('darkgrid')\nfig, ax = plt.subplots()\nsns.regplot(train['GrLivArea'], train['SalePrice'])\n#ax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()\n","af185467":"#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n","3e779065":"fig, ax = plt.subplots()\nsns.regplot(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","175868e8":"train.SalePrice.describe()","d5add0a7":"#PLot Histogram for 'SalePrice'\nsns.distplot(train['SalePrice'])","f7484a9b":"#Skewness & Kurtosis\nprint(\"Skewness : %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","5c76d7eb":"#PLot Histogram for 'SalePrice'\nstats.probplot(train['SalePrice'], plot=plt)","837f02cb":"train['SalePrice'] = np.log1p(train['SalePrice'])\n\n#Normal Distribution of New Sales Price\nmu, sigma = norm.fit(train['SalePrice'])\nprint(\"Mu : {:.2f}\\nSigma : {:.2f}\".format(mu,sigma))\n\n#Visualization\nsns.distplot(train['SalePrice'],fit=norm);\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\Sigma=$ {:.2f})'.format(mu,sigma)],loc = 'best')\nplt.xlabel('SalePrice Distribution')\nplt.ylabel('Frequency')\n\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'],plot=plt)\nplt.show()","08b9ef80":"train_n = train.shape[0]\ntest_n = test.shape[0]\ny_train = train.SalePrice.values\ny_test = train['SalePrice']\nall_data = pd.concat((train,test),sort=False).reset_index(drop = True)\nall_data.drop(['SalePrice'], axis=1, inplace = True)\nprint(\"all_data size is : {}\".format(all_data.shape))","23c514ae":"all_data.isnull().sum().sort_values(ascending=False)\n\n","6984771f":"all_data_na_values = all_data.isnull().sum()\nall_data_na_values = all_data_na_values.drop(all_data_na_values[all_data_na_values == 0].index).sort_values(ascending=False)[:30]\nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na,'Missing Values' :all_data_na_values})\nmissing_data.head(20)","7bceb1d6":"plt.subplots(figsize = (15,12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index,y=all_data_na)\nplt.xlabel('Features',fontsize=15)\nplt.ylabel('Percent of Missing Values', fontsize=15)\nplt.title('% of Misssing data by Features', fontsize=15)\n\n","c84c953b":"#Correlation map to see how features are correlated with SalePrice\ncorrmat = train.corr()\nplt.subplots(figsize=(25,15))\nsns.heatmap(corrmat, vmax=0.9, square=True, annot=True, fmt=\".2f\")","e975d24e":"# Fill the Missing Values\nall_data['PoolQC'] = all_data['PoolQC'].fillna(\"None\")\nall_data['MiscFeature'] = all_data['MiscFeature'].fillna(\"None\")\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\n\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\n    \nfor col in ('BsmtCond', 'BsmtExposure', 'BsmtQual', 'BsmtFinType2', 'BsmtFinType1'):\n    all_data[col] = all_data[col].fillna('None')\n\nfor col in ('BsmtHalfBath', 'BsmtFullBath', 'TotalBsmtSF', 'BsmtUnfSF', 'BsmtFinSF2', 'BsmtFinSF1'):\n    all_data[col] = all_data[col].fillna(0)\n\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\nall_data = all_data.drop(['Utilities'], axis=1)\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n","865b57d8":"all_data_na_values = all_data.isnull().sum()\nall_data_na_values = all_data_na_values.drop(all_data_na_values[all_data_na_values == 0].index).sort_values(ascending=False)[:30]\nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na,'Missing Values' :all_data_na_values,'Data_type':all_data_na.dtype})\nmissing_data.head()\n","d046934e":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","d591e7e1":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","ef74bbe3":"#Adding Total sqfoot feature\nall_data['TotalSF'] = all_data['TotalBsmtSF']+all_data['1stFlrSF']+all_data['2ndFlrSF']\n","d124166a":"from scipy.stats import skew\nnum = all_data.dtypes[all_data.dtypes != 'object'].index\n\n#Skew all the Numerical Features\nskew_feat = all_data[num].apply(lambda x: skew(x.dropna())).sort_values(ascending = False)\n\nsk = pd.DataFrame({'Skewness' :skew_feat})\nsk.head(10)","4c9d6ac4":"#should\/need to define categorical columns list\nall_data = pd.get_dummies(all_data)\nprint(all_data.shape)","04ea8b07":"train_new = all_data[:train_n]\ntest_new = all_data[train_n:]\nprint(train_new.shape)\nprint(test_new.shape)","648cf402":" \nimport xgboost as xgb\n\nregr = xgb.XGBRegressor(colsample_bytree=0.2,\n                 gamma=0.0,\n                 learning_rate=0.1,\n                 max_depth=4,\n                 min_child_weight=1.5,\n                 n_estimators=7200,                                                                  \n                 reg_alpha=0.9,\n                 reg_lambda=0.6,\n                 subsample=0.2,\n                 seed=42,\n                 silent=1)\n\nregr.fit(train_new,y_train)\n\ny_pred = regr.predict(train_new)\ny_test = train['SalePrice']\n\nfrom sklearn.metrics import mean_squared_error\nprint(\"XGB Score :\",(np.sqrt(mean_squared_error(y_test, y_pred))))\n\ny_pred_xgb = regr.predict(test_new)\n\ny_pred_xgb = np.exp(y_pred_xgb)\n\npred_df = pd.DataFrame(y_pred_xgb, index=test_ID, columns=[\"SalePrice\"])\npred_df.to_csv('submission1.csv', header=True, index_label='Id')","02d05f0e":"from sklearn.linear_model import ElasticNet, Lasso,Ridge, LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.neural_network import MLPRegressor\nmodel_lasso =Lasso(alpha=0.0005,normalize=True, max_iter=1e5)\nmodel_lasso.fit(train_new,y_train)\ny_pred_lasso = model_lasso.predict(train_new)\nscore_lasso = np.sqrt(mean_squared_error(y_train, y_pred_lasso))\nprint(\"Lasso Score :\",score_lasso)\n\ny_pred_lasso_test = model_lasso.predict(test_new)\ny_pred_lasso_test = np.exp(y_pred_lasso_test)","9ccb6891":"model_rd = Ridge(alpha = 4.84)\nmodel_rd.fit(train_new,y_train)\ny_pred_rd = model_rd.predict(train_new)\nscore_rd = np.sqrt(mean_squared_error(y_train, y_pred_rd))\nprint(\"Ridge Score :\",score_rd)\n\ny_pred_rd_test = model_rd.predict(test_new)\ny_pred_rd_test = np.exp(y_pred_rd_test)","6b98f877":"model_enet = ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3)\nmodel_enet.fit(train_new,y_train)\ny_pred_enet = model_enet.predict(train_new)\nscore_enet = np.sqrt(mean_squared_error(y_train, y_pred_enet))\nprint(\"ElasticNet Score :\",score_enet)\n\ny_pred_enet_test = model_enet.predict(test_new)\ny_pred_enet_test = np.exp(y_pred_enet_test)","c2ea666b":"model_rf = RandomForestRegressor(n_estimators = 12,max_depth = 3,n_jobs = -1)\nmodel_rf.fit(train_new,y_train)\ny_pred_rf = model_rf.predict(train_new)\nscore_rf = np.sqrt(mean_squared_error(y_train, y_pred_rf))\nprint(\"RandomForest Score :\",score_rf)\n\ny_pred_rf_test = model_rf.predict(test_new)\ny_pred_rf_test = np.exp(y_pred_rf_test)","52a2d1ff":"model_gb = GradientBoostingRegressor(n_estimators = 40,max_depth = 2)\nmodel_gb.fit(train_new,y_train)\ny_pred_gb = model_gb.predict(train_new)\nscore_gb = np.sqrt(mean_squared_error(y_train, y_pred_gb))\nprint(\"GradientBoosting Score :\",score_gb)\n\ny_pred_gb_test = model_gb.predict(test_new)\ny_pred_gb_test = np.exp(y_pred_gb_test)","1c01efbb":"from sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor(max_depth=4)\nmodel_ad = AdaBoostRegressor(dt, learning_rate = 0.1, n_estimators=300,random_state= None)\nmodel_ad.fit(train_new,y_train)\ny_pred_ad = model_ad.predict(train_new)\nscore_ad = np.sqrt(mean_squared_error(y_train, y_pred_ad))\nprint(\"AdaBoost Model :\",score_ad)\n\ny_pred_ad_test = model_ad.predict(test_new)\ny_pred_ad_test = np.exp(y_pred_ad_test)","28fe8bdb":"The target variable is right skewed. As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.","dc4316b5":"Label Encoding some categorical variables that may contain information in their ordering set","0522789d":"#### Adding one more important feature\n\nSince area related features are very important to determine house prices, we add one more feature which is the total area of basement, first and second floor areas of each house","7d952499":"Checking for any Remaining Missing Data","53063d8f":"Since there is no missing data ","7b34d1ed":"### More Feature Engineering","e433259b":"#### Transforming some numerical variables that are really categorical","f33bc7c3":"We can see at the bottom right two with extremely large GrLivArea that are of a low price. These values are huge oultliers. Therefore, we can safely delete them","4203aa5b":"## Use bar plots to plot categorical features against SalePrice","365b044e":"Let's have a look at the distribution of 'SalePrice' by plotting a simple histogram","95a04256":"#### Misssing Data","d54fb26f":"### Correlation between Columns","2e22ae8c":"### Fill The Missing Data","0eb18ac3":"Let's Concatenate train & test data\n\n","592e1b9e":"The skew seems now corrected and the data appears more normally distributed.","8aa4ba03":"outliers has been removed","212959be":"#### Skewness: -\nThe term \u2018skewness\u2019 is used to mean the absence of symmetry from the mean of the dataset. \nSkewness is used to indicate the shape of the distribution of data.\nIn a skewed distribution, the curve is extended to either left or right side.\nSo, when the plot is extended towards the right side more, it denotes positive skewness.\nOn the other hand, when the plot is stretched more towards the left direction, then it is called as negative skewness.\n\n#### Kurtosis:-\nIn statistics, kurtosis is defined as the parameter of relative sharpness of the peak of the probability distribution curve.\nIt is used to indicate the flatness or peakedness of the frequency distribution curve and measures the tails or outliers of the distribution.\nPositive kurtosis represents that the distribution is more peaked than the normal distribution, \nwhereas negative kurtosis shows that the distribution is less peaked than the normal distribution.","f3870ad9":"## Data Preprocessing","bc45a0cc":"## Sales Price Tragert Variable","e24d1a21":"### Getting dummy categorical features"}}