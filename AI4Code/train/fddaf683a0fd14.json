{"cell_type":{"3cbd8976":"code","c7d48d8b":"code","1a4f8fd5":"code","d108fbc9":"code","e51b6e04":"code","bc56f317":"code","866f3f83":"code","1e7b92e5":"code","153405a0":"code","253ed01f":"code","08ba8647":"code","65a36814":"code","7b9017dc":"markdown","91d362d5":"markdown","00be753a":"markdown","faef2818":"markdown","795f3cc0":"markdown","ea5d87ce":"markdown","6a37ebd8":"markdown"},"source":{"3cbd8976":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\n\nimport os\nimport cv2\nimport random\n\nimport matplotlib.pyplot as plt","c7d48d8b":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout,Activation, Flatten, Conv2D, MaxPool2D\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import RMSprop\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\nfrom sklearn.model_selection import train_test_split","1a4f8fd5":"def extract_label(img_path,train = True):\n    filename, _ = os.path.splitext(os.path.basename(img_path))\n\n    subject_id, etc = filename.split('__')\n    \n    if train:\n        gender, lr, finger, _, _ = etc.split('_')\n    else:\n        gender, lr, finger, _ = etc.split('_')\n    \n    gender = 0 if gender == 'M' else 1\n    lr = 0 if lr == 'Left' else 1\n\n    if finger == 'thumb':\n        finger = 0\n    elif finger == 'index':\n        finger = 1\n    elif finger == 'middle':\n        finger = 2\n    elif finger == 'ring':\n        finger = 3\n    elif finger == 'little':\n        finger = 4\n        \n    return np.array([subject_id, gender, lr, finger], dtype=np.uint16)","d108fbc9":"img_size = 96\n\ndef loading_data(path,train):\n    print(\"loading data from: \",path)\n    data = []\n    for img in os.listdir(path):\n        try:\n            img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n            img_resize = cv2.resize(img_array, (img_size, img_size))\n            label = extract_label(os.path.join(path, img),train)\n            data.append([label[3], img_resize ])\n        except Exception as e:\n            pass\n    data\n    return data","e51b6e04":"Real_path = \"..\/input\/socofing\/SOCOFing\/Real\"\nEasy_path = \"..\/input\/socofing\/SOCOFing\/Altered\/Altered-Easy\"\nMedium_path = \"..\/input\/socofing\/SOCOFing\/Altered\/Altered-Medium\"\nHard_path = \"..\/input\/socofing\/SOCOFing\/Altered\/Altered-Hard\"\n\n\nEasy_data = loading_data(Easy_path, train = True)\nMedium_data = loading_data(Medium_path, train = True)\nHard_data = loading_data(Hard_path, train = True)\ntest = loading_data(Real_path, train = False)\n\ndata = np.concatenate([Easy_data, Medium_data, Hard_data], axis=0)\n\ndel Easy_data, Medium_data, Hard_data\n","bc56f317":"X, y = [], []\n\nfor label, feature in data:\n    X.append(feature)\n    y.append(label)\n    \ndel data\n\nX = np.array(X).reshape(-1, img_size, img_size, 1)\nX = X \/ 255.0\n\ny = to_categorical(y, num_classes = 5)","866f3f83":"X_test, y_test = [], []\n\nfor label, feature in test:\n    X_test.append(feature)\n    y_test.append(label)\n    \ndel test    \nX_test = np.array(X_test).reshape(-1, img_size, img_size, 1)\nX_test = X_test \/ 255.0\n\ny_test = to_categorical(y_test, num_classes = 5)","1e7b92e5":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1)","153405a0":"print(\"full data:  \",X.shape)\nprint(\"Train:      \",X_train.shape)\nprint(\"Validation: \",X_val.shape)\nprint(\"Test:       \",X_test.shape)","253ed01f":"epochs = 15\nbatch_size = 86\n\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (96,96,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(100, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(5, activation = \"softmax\"))\n\nmodel.summary()","08ba8647":"epochs = 30 # Turn epochs to 30 to get 0.9967 accuracy\nbatch_size = 86\nmodel_path = '.\/Model.h5'\n\n\nmodel.compile(optimizer = 'adam' , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n# Set a learning rate annealer\ncallbacks = [\n    EarlyStopping(monitor='val_acc', patience=20, mode='max', verbose=1),\n    ModelCheckpoint(model_path, monitor='val_acc', save_best_only=True, mode='max', verbose=1),\n    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1)\n]\n\n\n# Without data augmentation i obtained an accuracy of 0.98114\nhistory = model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, \n          validation_data = (X_val, y_val), verbose = 1, callbacks= callbacks)","65a36814":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, label='Training acc')\nplt.plot(epochs, val_acc, label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss,  label='Training loss')\nplt.plot(epochs, val_loss, label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nscore = model.evaluate([X_test], [y_test], verbose=0)\nprint(\"Score: \",score[1]*100)\n\nplt.show()","7b9017dc":"# Intro \n##### welcome to my playground :)\n- In this kernel I am tring to make finger gender classifier by the use of fingerprints \n- And that what i got so far, just a beginner, so any help or suggestion will be nice .. thank you :) :* \n- i have problem with the Memory any suggestions ?? its almost full","91d362d5":"# Imports","00be753a":"# Loading Data ","faef2818":"# Model\n* its from Yassine Ghouzam kernel on the NMIST Compition:\n* https:\/\/www.kaggle.com\/yassineghouzam\/introduction-to-cnn-keras-0-997-top-6","795f3cc0":"# Conclusion","ea5d87ce":"# labels\n\n- so the labes for the dataset is not the folder name, instead its the img name\n- therefore I searched and get this function from:\n    * https:\/\/github.com\/kairess\/fingerprint_recognition\n    * https:\/\/www.kaggle.com\/kairess\/fingerprint-recognition\n- it return an numpy array with the subject ID, the person gender, left of right hand, and the finger gender\n\n- Note:\n    - Training data labels are like this: 101__M_Right_ring_finger_Zcut\n    - Testing data labels are like this: 101__M_Right_ring_finger \n    - so the split is different \n    \n","6a37ebd8":"# Preparing Data\n\n* Create X as an array of pixels in img \n* Reshape X\n* normalize X\n\n* create y as the finger gender only and change it to onehot form \n\nrepeat for test\n\n* finally split the data into train, val"}}