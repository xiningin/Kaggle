{"cell_type":{"d9116e83":"code","49ff0f58":"code","dbdc4664":"code","957b84db":"code","21d52c68":"code","1fdb9cc7":"code","b09d10de":"code","0ef8f576":"code","3d5a663b":"code","b6f64ffe":"code","b788eb2b":"code","281574ff":"code","38c93991":"code","860d2956":"code","afe0cb14":"code","52358dfa":"code","c9d9d0b4":"code","5e027321":"code","3cc60e73":"code","8b955831":"code","481e894f":"code","42f172a2":"code","474a2708":"code","a587d36f":"code","05c775c1":"code","3bc22455":"code","35444e0e":"code","8dfdd901":"code","1819a38d":"code","0b2c0e70":"code","ab21c2d3":"markdown","0a1d66d2":"markdown","0515318e":"markdown","3fdc0d39":"markdown","6e4b1cc6":"markdown","e112af00":"markdown","13aa8495":"markdown","61a2c06f":"markdown","54d0cb52":"markdown","082af7e6":"markdown","731de5ef":"markdown","27ec0465":"markdown","341047af":"markdown","b0172fc9":"markdown","2e212dd2":"markdown","2040192a":"markdown","5f2ba6d3":"markdown","76b5aeff":"markdown","061853c2":"markdown","73b56851":"markdown","995d847c":"markdown","81fdee0c":"markdown","1ecd1eaf":"markdown","6ef3250f":"markdown","57923743":"markdown","5d19d3f9":"markdown","e03764de":"markdown","ea00f37a":"markdown","42f455d8":"markdown"},"source":{"d9116e83":"!pip install git+https:\/\/github.com\/rwightman\/pytorch-image-models\n!pip install --upgrade wandb\n!pip install torch==1.10.0+cu113 torchvision==0.11.1+cu113 torchaudio==0.10.0+cu113 -f https:\/\/download.pytorch.org\/whl\/cu113\/torch_stable.html","49ff0f58":"import os\nimport gc\nimport cv2\nimport copy\nimport time\nimport random\nfrom PIL import Image\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\n\n# Utils\nimport joblib\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n# Sklearn Imports\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nimport timm\n\n# Albumentations for augmentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nb_ = Fore.BLUE\nsr_ = Style.RESET_ALL\n\n# Suppress warning messages\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","dbdc4664":"import wandb\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=api_key)\n    anony = None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https:\/\/wandb.ai\/authorize')","957b84db":"ROOT_DIR = \"..\/input\/petfinder-pawpularity-score\"\nTRAIN_DIR = \"..\/input\/petfinder-pawpularity-score\/train\"\nTEST_DIR = \"..\/input\/petfinder-pawpularity-score\/test\"","21d52c68":"CONFIG = dict(\n    seed = 42,\n    model_name = 'tf_efficientnet_b4_ns',\n    train_batch_size = 16,\n    valid_batch_size = 32,\n    img_size = 512,\n    epochs = 5,\n    learning_rate = 1e-4,\n    scheduler = 'CosineAnnealingLR',\n    min_lr = 1e-6,\n    T_max = 100,\n    T_0 = 25,\n    warmup_epochs = 0,\n    weight_decay = 1e-6,\n    n_accumulate = 1,\n    n_fold = 5,\n    num_classes = 1,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n    competition = 'PetFinder',\n    _wandb_kernel = 'deb'\n)","1fdb9cc7":"def set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(CONFIG['seed'])","b09d10de":"def get_train_file_path(id):\n    return f\"{TRAIN_DIR}\/{id}.jpg\"","0ef8f576":"df = pd.read_csv(f\"{ROOT_DIR}\/train.csv\")\ndf['file_path'] = df['Id'].apply(get_train_file_path)","3d5a663b":"feature_cols = [col for col in df.columns if col not in ['Id', 'Pawpularity', 'file_path']]","b6f64ffe":"run = wandb.init(project='Pawpularity', \n                 config=CONFIG,\n                 job_type='Visualization',\n                 group='Public_baseline',\n                 anonymous='must')","b788eb2b":"preview_table = wandb.Table(columns=['Id', 'Image', 'Subject Focus', 'Eyes', 'Face', \n                                     'Near', 'Action', 'Accessory', 'Group', 'Collage', \n                                     'Human', 'Occlusion', 'Info', 'Blur', 'Pawpularity'])\ntmp_df = df.sample(1000, random_state=CONFIG['seed']).reset_index(drop=True)\nfor i in tqdm(range(len(tmp_df))):\n    row = tmp_df.loc[i]\n    img = Image.open(row.file_path)\n    preview_table.add_data(row['Id'],\n                           wandb.Image(img),\n                           row['Subject Focus'],\n                           row['Eyes'],\n                           row['Face'],\n                           row['Near'],\n                           row['Action'],\n                           row['Accessory'],\n                           row['Group'],\n                           row['Collage'],\n                           row['Human'],\n                           row['Occlusion'],\n                           row['Info'],\n                           row['Blur'],\n                           row['Pawpularity'])\n\nwandb.log({'Visualization': preview_table})\nrun.finish()","281574ff":"# Code taken from https:\/\/www.kaggle.com\/ayuraj\/interactive-eda-using-w-b-tables\n\n# This is just to display the W&B run page in this interactive session.\nfrom IPython import display\n\n# we create an IFrame and set the width and height\niF = display.IFrame(run.url, width=1080, height=720)\niF","38c93991":"def create_folds(df, n_s=5, n_grp=None):\n    df['kfold'] = -1\n    \n    if n_grp is None:\n        skf = KFold(n_splits=n_s, random_state=CONFIG['seed'])\n        target = df['Pawpularity']\n    else:\n        skf = StratifiedKFold(n_splits=n_s, shuffle=True, random_state=CONFIG['seed'])\n        df['grp'] = pd.cut(df['Pawpularity'], n_grp, labels=False)\n        target = df.grp\n    \n    for fold_no, (t, v) in enumerate(skf.split(target, target)):\n        df.loc[v, 'kfold'] = fold_no\n\n    df = df.drop('grp', axis=1)\n    \n    return df","860d2956":"df = create_folds(df, n_s=CONFIG['n_fold'], n_grp=14)\ndf.head()","afe0cb14":"class PawpularityDataset(Dataset):\n    def __init__(self, root_dir, df, transforms=None):\n        self.root_dir = root_dir\n        self.df = df\n        self.file_names = df['file_path'].values\n        self.targets = df['Pawpularity'].values\n        self.meta = df[feature_cols].values\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_path = self.file_names[index]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        meta = self.meta[index, :]\n        target = self.targets[index]\n        \n        if self.transforms:\n            img = self.transforms(image=img)[\"image\"]\n            \n        return img, meta, target","52358dfa":"data_transforms = {\n    \"train\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.HorizontalFlip(p=0.5),\n        A.Normalize(\n                mean=[0.485, 0.456, 0.406], \n                std=[0.229, 0.224, 0.225], \n                max_pixel_value=255.0, \n                p=1.0\n            ),\n        ToTensorV2()], p=1.),\n    \n    \"valid\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.Normalize(\n                mean=[0.485, 0.456, 0.406], \n                std=[0.229, 0.224, 0.225], \n                max_pixel_value=255.0, \n                p=1.0\n            ),\n        ToTensorV2()], p=1.)\n}","c9d9d0b4":"class PawpularityModel(nn.Module):\n    def __init__(self, model_name, pretrained=True):\n        super(PawpularityModel, self).__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained, num_classes=0)\n        self.fc = nn.LazyLinear(CONFIG['num_classes'])\n        self.dropout = nn.Dropout(p=0.3)\n\n    def forward(self, images, meta):\n        features = self.model(images)                 # features = (bs, embedding_size)\n        features = self.dropout(features)\n        features = torch.cat([features, meta], dim=1) # features = (bs, embedding_size + 12)\n        output = self.fc(features)                    # outputs  = (bs, num_classes)\n        return output\n    \nmodel = PawpularityModel(CONFIG['model_name'])\nmodel.to(CONFIG['device']);","5e027321":"# dummy run to initialize the layers \nimg = torch.randn(1, 3, CONFIG['img_size'], CONFIG['img_size']).to(CONFIG['device'])\nmeta = torch.randn(1, len(feature_cols)).to(CONFIG['device'])\nmodel(img, meta)","3cc60e73":"def criterion(outputs, targets):\n    return torch.sqrt(nn.MSELoss()(outputs.view(-1), targets.view(-1)))","8b955831":"def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.train()\n    scaler = amp.GradScaler()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, (images, meta, targets) in bar:         \n        images = images.to(device, dtype=torch.float)\n        meta = meta.to(device, dtype=torch.float)\n        targets = targets.to(device, dtype=torch.float)\n        \n        batch_size = images.size(0)\n        \n        with amp.autocast(enabled=True):\n            outputs = model(images, meta)\n            loss = criterion(outputs, targets)\n            loss = loss \/ CONFIG['n_accumulate']\n            \n        scaler.scale(loss).backward()\n    \n        if (step + 1) % CONFIG['n_accumulate'] == 0:\n            scaler.step(optimizer)\n            scaler.update()\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            if scheduler is not None:\n                scheduler.step()\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss \/ dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n    gc.collect()\n    \n    return epoch_loss","481e894f":"@torch.no_grad()\ndef valid_one_epoch(model, dataloader, device, epoch):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    TARGETS = []\n    PREDS = []\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, (images, meta, targets) in bar:        \n        images = images.to(device, dtype=torch.float)\n        meta = meta.to(device, dtype=torch.float)\n        targets = targets.to(device, dtype=torch.float)\n        \n        batch_size = images.size(0)\n        \n        outputs = model(images, meta)\n        loss = criterion(outputs, targets)\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss \/ dataset_size\n        \n        PREDS.append(outputs.view(-1).cpu().detach().numpy())\n        TARGETS.append(targets.view(-1).cpu().detach().numpy())\n        \n        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])   \n    \n    TARGETS = np.concatenate(TARGETS)\n    PREDS = np.concatenate(PREDS)\n    val_rmse = mean_squared_error(TARGETS, PREDS, squared=False)\n    gc.collect()\n    \n    return epoch_loss, val_rmse","42f172a2":"def run_training(model, optimizer, scheduler, device, num_epochs):\n    # To automatically log gradients\n    wandb.watch(model, log_freq=100)\n    \n    if torch.cuda.is_available():\n        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n    \n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_epoch_rmse = np.inf\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs + 1): \n        gc.collect()\n        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n                                           dataloader=train_loader, \n                                           device=CONFIG['device'], epoch=epoch)\n        \n        val_epoch_loss, val_epoch_rmse = valid_one_epoch(model, valid_loader, \n                                                         device=CONFIG['device'], \n                                                         epoch=epoch)\n    \n        history['Train Loss'].append(train_epoch_loss)\n        history['Valid Loss'].append(val_epoch_loss)\n        history['Valid RMSE'].append(val_epoch_rmse)\n        \n        # Log the metrics\n        wandb.log({\"Train Loss\": train_epoch_loss})\n        wandb.log({\"Valid Loss\": val_epoch_loss})\n        wandb.log({\"Valid RMSE\": val_epoch_rmse})\n        \n        print(f'Valid RMSE: {val_epoch_rmse}')\n        \n        # deep copy the model\n        if val_epoch_rmse <= best_epoch_rmse:\n            print(f\"{b_}Validation Loss Improved ({best_epoch_rmse} ---> {val_epoch_rmse})\")\n            best_epoch_rmse = val_epoch_rmse\n            run.summary[\"Best RMSE\"] = best_epoch_rmse\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = \"RMSE{:.4f}_epoch{:.0f}.bin\".format(best_epoch_rmse, epoch)\n            torch.save(model.state_dict(), PATH)\n            # Save a model file from the current directory\n            wandb.save(PATH)\n            print(f\"Model Saved{sr_}\")\n            \n        print()\n    \n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 3600, (time_elapsed % 3600) \/\/ 60, (time_elapsed % 3600) % 60))\n    print(\"Best RMSE: {:.4f}\".format(best_epoch_rmse))\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    \n    return model, history","474a2708":"def prepare_loaders(fold):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    train_dataset = PawpularityDataset(TRAIN_DIR, df_train, transforms=data_transforms['train'])\n    valid_dataset = PawpularityDataset(TRAIN_DIR, df_valid, transforms=data_transforms['valid'])\n\n    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n                              num_workers=4, shuffle=True, pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n                              num_workers=4, shuffle=False, pin_memory=True)\n    \n    return train_loader, valid_loader","a587d36f":"def fetch_scheduler(optimizer):\n    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['T_max'], \n                                                   eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CONFIG['T_0'], \n                                                             eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == None:\n        return None\n        \n    return scheduler","05c775c1":"train_loader, valid_loader = prepare_loaders(fold=0)","3bc22455":"optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\nscheduler = fetch_scheduler(optimizer)","35444e0e":"run = wandb.init(project='Pawpularity', \n                 config=CONFIG,\n                 job_type='Train',\n                 group='Public_baseline',\n                 anonymous='must')","8dfdd901":"model, history = run_training(model, optimizer, scheduler,\n                              device=CONFIG['device'],\n                              num_epochs=CONFIG['epochs'])","1819a38d":"run.finish()","0b2c0e70":"# Code taken from https:\/\/www.kaggle.com\/ayuraj\/interactive-eda-using-w-b-tables\n\n# This is just to display the W&B run page in this interactive session.\nfrom IPython import display\n\n# we create an IFrame and set the width and height\niF = display.IFrame(run.url, width=1080, height=720)\niF","ab21c2d3":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Training Function<\/h1><\/span>","0a1d66d2":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Import Required Libraries \ud83d\udcda<\/h1><\/span>","0515318e":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Dataset Class<\/h1><\/span>","3fdc0d39":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Install Required Libraries<\/h1><\/span>","6e4b1cc6":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Create Folds<\/h1><\/span>","e112af00":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Training Configuration \u2699\ufe0f<\/h1><\/span>","13aa8495":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Introduction<\/h1><span>","61a2c06f":"![](https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/Petfinder\/PetFinder%20-%20Logo.png)","54d0cb52":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Visualizations<\/h1><\/span>","082af7e6":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Visualize Images<\/h1><\/span>","731de5ef":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Validation Function<\/h1><\/span>","27ec0465":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Create Model<\/h1><\/span>","341047af":"<br>\n<h1 style = \"font-size:60px; font-family:Garamond ; font-weight : normal; background-color: #f6f5f5 ; color : #fe346e; text-align: center; border-radius: 100px 100px;\">PetFinder.my - Pawpularity Training<\/h1>\n<br>","b0172fc9":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\"><a href=\"https:\/\/wandb.ai\/dchanda\/Pawpularity\/runs\/xk03gkly\">View the Complete Dashboard Here \u2b95<\/a><\/span>","2e212dd2":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Loss Function<\/h1><\/span>","2040192a":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">Create Dataloaders<\/span>","5f2ba6d3":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">Define Optimizer and Scheduler<\/span>","76b5aeff":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">Start Training<\/span>","061853c2":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Run Training<\/h1><\/span>","73b56851":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.3em; font-weight: 300;\">Millions of stray animals suffer on the streets or are euthanized in shelters every day around the world. You might expect pets with attractive photos to generate more interest and be adopted faster. But what makes a good picture? With the help of data science, you may be able to accurately determine a pet photo\u2019s appeal and even suggest improvements to give these rescue animals a higher chance of loving homes.<\/span><br>\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.3em; font-weight: 300;\">In this competition, you\u2019ll analyze raw images and metadata to predict the \u201cPawpularity\u201d of pet photos. You'll train and test your model on PetFinder.my's thousands of pet profiles. Winning versions will offer accurate recommendations that will improve animal welfare.<\/span><br>","995d847c":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\"><a href=\"https:\/\/wandb.ai\/dchanda\/Pawpularity\/runs\/bnf1k7kk\">View the Complete Dashboard Here \u2b95<\/a><\/span>","81fdee0c":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Augmentations<\/h1><\/span>","1ecd1eaf":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">Code taken from <a href=\"https:\/\/www.kaggle.com\/tolgadincer\/continuous-target-stratification?rvi=1&scriptVersionId=52551118&cellId=6\">this notebook<\/a><\/span>","6ef3250f":"![](https:\/\/i.imgur.com\/u8jYX3J.png)","57923743":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Set Seed for Reproducibility<\/h1><\/span>","5d19d3f9":"![Upvote!](https:\/\/img.shields.io\/badge\/Upvote-If%20you%20like%20my%20work-07b3c8?style=for-the-badge&logo=kaggle)","e03764de":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Read the Data \ud83d\udcd6<\/h1><\/span>","ea00f37a":"<img src=\"https:\/\/i.imgur.com\/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" \/>\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\"> Weights & Biases (W&B) is a set of machine learning tools that helps you build better models faster. <strong>Kaggle competitions require fast-paced model development and evaluation<\/strong>. There are a lot of components: exploring the training data, training different models, combining trained models in different combinations (ensembling), and so on.<\/span>\n\n> <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">\u23f3 Lots of components = Lots of places to go wrong = Lots of time spent debugging<\/span>\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">W&B can be useful for Kaggle competition with it's lightweight and interoperable tools:<\/span>\n\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Quickly track experiments,<br><\/span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Version and iterate on datasets, <br><\/span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Evaluate model performance,<br><\/span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Reproduce models,<br><\/span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Visualize results and spot regressions,<br><\/span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Share findings with colleagues.<\/span>\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">To learn more about Weights and Biases check out this <strong><a href=\"https:\/\/www.kaggle.com\/ayuraj\/experiment-tracking-with-weights-and-biases\">kernel<\/a><\/strong>.<\/span>\n\n![img](https:\/\/i.imgur.com\/BGgfZj3.png)","42f455d8":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.3em; font-weight: 300;\"><strong>UPDATE<\/strong>: Made the <code>PawpularityModel<\/code> class architecture agnostic using <code>nn.LazyLinear<\/code> as mentioned in this <a href=\"https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/discussion\/279460\">discussion<\/a><\/span><br>"}}