{"cell_type":{"0017fc3b":"code","d7c6b1ce":"code","56181701":"code","d22f53e0":"code","4c22c9c7":"code","075e5414":"code","5f20142c":"code","b8ea0255":"code","a638a664":"code","454c2dd3":"code","5c122ef5":"code","34ac7f45":"code","75eaad28":"code","99405b5e":"code","9c772517":"code","ece34889":"code","7767d4ee":"code","ea82c7c9":"code","47709c09":"code","1435c997":"code","3ee4ae18":"code","6da68e43":"code","36602cd0":"code","40754d82":"code","3550a756":"code","c35011b5":"code","a8b4ee16":"code","da4c3352":"code","0a525322":"code","49b73a16":"code","626a949d":"code","133d981e":"code","6ee53a8f":"code","e47a7c11":"code","7a0d06c4":"code","88e885f8":"code","95a8d002":"code","8a1d1f6c":"code","0cc84fb0":"markdown","997eb027":"markdown","3d119563":"markdown","823a8f6f":"markdown","c4bdb79b":"markdown","b9f6f30e":"markdown","5a5e96b0":"markdown","25504193":"markdown","aa61715c":"markdown","4d62c07c":"markdown","01d5160a":"markdown","1f315e12":"markdown","a39b192a":"markdown","7b48a7f2":"markdown","8156a800":"markdown","a6b0d6dd":"markdown","ffeb3052":"markdown"},"source":{"0017fc3b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d7c6b1ce":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style(\"darkgrid\")\n","56181701":"data  = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\ndata  = pd.DataFrame(data)\nprint(\"DataFrame shape :\",data.shape)","d22f53e0":"# missing_values in dataframe\ndata.isnull().sum()\/len(data)*100","4c22c9c7":"data.head()","075e5414":"fraud_amount = data[data.Class == 1][\"Amount\"]\nfraud_amount = fraud_amount.astype(int)\nfraud_amount.hist(color = \"r\",alpha = 0.6)\nplt.xlabel(\"Fraud Amount\")\nplt.ylabel(\"Frequency\")\nprint(\"Highest Fruad amount was :\",max(fraud_amount))\nprint(\"Least   Fruad amount was :\",min(fraud_amount))\n# i have no idea  how they  , considered amount 0 as fruad transcation .!! Because there is no transcation at all\n","5f20142c":"# most of the fraud amount was below 1000\nfraud_amount.hist(color = \"b\",alpha = 0.3,bins = [1,1000,2000])\nplt.xlabel(\"Fraud Amount\")\nplt.ylabel(\"Frequency\")","b8ea0255":"# our Data seems imbalanced \nsns.countplot(data[\"Class\"],color = \"orange\")\nprint(data[\"Class\"].value_counts())\nprint(\"=\"*60)\nprint(\"Percentage of class values :\")\nprint(data[\"Class\"].value_counts()\/len(data)*100)","a638a664":"data[\"Amount\"].hist(color = \"y\",bins = [0,500,1500,2000])","454c2dd3":"\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\ndata[\"scaled_amount\"] = sc.fit_transform(np.array(data[\"Amount\"]).reshape(-1,1))\ndata.drop([\"Amount\",\"Time\"],axis = 1,inplace = True)\n","5c122ef5":"from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier","34ac7f45":"def train_set(ytrain,train_pred):\n    print(\"confusion matrix for train set : \")\n    cm = confusion_matrix(ytrain,train_pred)\n    print(cm)\n    print(\"--\"*40)\n    print(\"False positive rate :\",(cm[1][0]\/(cm[1][0]+cm[1][1]))*100)  # FPR = FP\/FP +TN\n    print(\"\\n\")\n    print(cm[1][0] ,\"out of\",(cm[1][0]+cm[1][1]),\"fraud transaction instances were classified as not a fraudulent transactions \\n\")\n    print(\"--\"*40)\n    print(\"False Negative rate :\",(cm[0][1]\/(cm[0][1]+cm[0][0]))*100) # FNR = FN + (FN + TP )\n    print(\"\\n\")\n    print(cm[0][1],\"out of \",(cm[0][1]+cm[0][0]),\"non fraudulent transaction instances were classified as  a fraudulent transactions\")\n    print(\"--\"*40)\n    print(classification_report(ytrain,train_pred))","75eaad28":"def test_set(ytest,test_pred):\n    print(\"confusion matrix for test set : \")\n    cm = confusion_matrix(ytest,test_pred)\n    print(cm)\n    print(\"--\"*40)\n    print(\"False positive rate :\",(cm[1][0]\/(cm[1][0]+cm[1][1]))*100)  # FPR = FP\/FP +TN\n    print(\"\\n\")\n    print(cm[1][0] ,\"out of\",(cm[1][0]+cm[1][1]),\"fraud transaction instances were classified as not a fraudulent transactions \\n\")\n    print(\"--\"*40)\n    print(\"False Negative rate :\",(cm[0][1]\/(cm[0][1]+cm[0][0]))*100) # FNR = FN + (FN + TP )\n    print(\"\\n\")\n    print(cm[0][1],\"out of \",(cm[0][1]+cm[0][0]),\"non fraudulent transaction instances were classified as  a fraudulent transactions\")\n    print(\"--\"*40)\n    print(classification_report(ytest,test_pred))","99405b5e":"def under_sample(data):\n    under_sample_zero = data[data.Class == 0].iloc[:,:]\n    under_sample_zero = under_sample_zero.sample(data.Class.value_counts()[1])\n    under_sample_one  = data[data.Class == 1].iloc[:,:]\n    under_sampled_data = pd.concat([under_sample_zero,under_sample_one],axis = 0)\n    us_x = under_sampled_data.loc[:,under_sampled_data.columns != \"Class\"]\n    us_y = under_sampled_data.loc[:,\"Class\"]\n    return us_x,us_y,under_sample_zero,under_sample_one,under_sampled_data\n\nus_x,us_y,under_sample_zero,under_sample_one,under_sampled_data = under_sample(data)","9c772517":"# Now we have equal number of 1's and 0's ,i.e we have balanced data !\n\nprint(\"under_sampled_data size      : \",under_sampled_data.shape)\nprint(\"sample size where class is 0 :\",under_sample_zero.shape)\nprint(\"sample size where class is 1 :\",under_sample_one.shape)\nprint(\"Features shape :\",x.shape)\nprint(\"Dependent Variable shape \",y.shape)\nsns.countplot(under_sampled_data.Class)\nplt.show()","ece34889":"x = data.loc[:,data.columns != \"Class\"]\ny = data.loc[:,\"Class\"]\nprint(\"Features shape :\",x.shape)\nprint(\"Dependent Variable shape \",y.shape)","7767d4ee":"\nlr_model = LogisticRegression()\nxtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size = 0.4,random_state = 1)\nlr_model.fit(xtrain,ytrain)\n#train_set\ntrain_pred = lr_model.predict(xtrain)\n#test_set\ntest_pred  = lr_model.predict(xtest)\nprint(\"Accuracy for Training set : \",accuracy_score(ytrain,train_pred))\nprint(\"Accuracy for Testing set  : \",accuracy_score(ytest,test_pred))\n","ea82c7c9":"train_set(ytrain,train_pred)","47709c09":"test_set(ytest,test_pred)","1435c997":"lr_model = LogisticRegression()\nus_x,us_y,under_sample_zero,under_sample_one,under_sampled_data = under_sample(data)\nxtrain,xtest,ytrain,ytest = train_test_split(us_x,us_y,test_size = 0.4,random_state = 1)\nlr_model.fit(xtrain,ytrain)\n#train_set\ntrain_pred = lr_model.predict(xtrain)\n#test_set\ntest_pred  = lr_model.predict(xtest)\nprint(\"Accuracy for Training set : \",accuracy_score(ytrain,train_pred))\nprint(\"Accuracy for Testing set  : \",accuracy_score(ytest,test_pred))\nroc_auc_logistic = roc_auc_score(ytest,test_pred)\n","3ee4ae18":"train_set(ytrain,train_pred)\n# on applying undersampling technique we got !\n# Decrease in False positive rate ,\n# Increase in precision,recall,f1-score ","6da68e43":"test_set(ytest,test_pred)\n# on applying undersampling technique we got !\n# Decrease in False positive rate ,\n# Increase in precision,recall,f1-score ","36602cd0":"\nrf_model = RandomForestClassifier(n_estimators=20,max_depth = 10,min_samples_split = 20)\nxtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size = 0.4,random_state = 1)\nrf_model.fit(xtrain,ytrain)\n#train_set\ntrain_pred = rf_model.predict(xtrain)\n#test_set\ntest_pred  = rf_model.predict(xtest)\n","40754d82":"print(\"Accuracy for Training set : \",accuracy_score(ytrain,train_pred))\nprint(\"Accuracy for Testing set  : \",accuracy_score(ytest,test_pred)) ","3550a756":"train_set(ytrain,train_pred)\n# 57 out of 306 fraud transaction instances were classified as non a fraudulent transactions ","c35011b5":"test_set(ytest,test_pred)\n# 51 out of 186 fraud transaction instances were classified as not a fraudulent transactions\n# 25% of fradulent transaction were classified as non fraudulent transactions","a8b4ee16":"us_x,us_y,under_sample_zero,under_sample_one,under_sampled_data = under_sample(data)\nrf_model = RandomForestClassifier(n_estimators=20,max_depth = 10,min_samples_split = 20)\nxtrain,xtest,ytrain,ytest = train_test_split(us_x,us_y,test_size = 0.4,random_state = 1)\nrf_model.fit(xtrain,ytrain)\n#train_set\ntrain_pred = rf_model.predict(xtrain)\n#test_set\ntest_pred  = rf_model.predict(xtest)\nroc_auc_random = roc_auc_score(ytest,test_pred)","da4c3352":"train_set(ytrain,train_pred)\n# on applying undersampling technique we got !\n# Decrease in False positive rate ,\n# Increase in precision,recall,f1-score ","0a525322":"test_set(ytest,test_pred)\n# on applying undersampling technique we got !\n# Decrease in False positive rate ,\n# Increase in precision,recall,f1-score ","49b73a16":"# for test_set's only (validation set)\nprint(\"ROC_AUC_SCORE FOR LOGISTIC REG IS  : \",roc_auc_logistic)\nprint(\"ROC_AUC_SCORE FOR Random Forest IS : \",roc_auc_random)\n# both algo's are working equally ,","626a949d":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state = 1)\nover_sam_x,over_sam_y = sm.fit_sample(x,y)\nprint(\"Feature size   after oversampling :\",over_sam_x.shape)\nprint(\"Dependent size after oversampling :\",over_sam_y.shape)\nsns.countplot(over_sam_y)\n","133d981e":"\nxtrain,xtest,ytrain,ytest = train_test_split(over_sam_x,over_sam_y,test_size = 0.4,random_state = 1)\nlr_model.fit(xtrain,ytrain)\n#train_set\ntrain_pred = lr_model.predict(xtrain)\n#test_set\ntest_pred  = lr_model.predict(xtest)\nprint(\"Accuracy for Training set : \",accuracy_score(ytrain,train_pred))\nprint(\"Accuracy for Testing set  : \",accuracy_score(ytest,test_pred))\nover_sam_roc_auc_logistic = roc_auc_score(ytest,test_pred)","6ee53a8f":"train_set(ytrain,train_pred)\n# on applying undersampling technique we got !\n# Decrease in False positive rate ,\n# Increase in precision,recall,f1-score ","e47a7c11":"test_set(ytest,test_pred)\n# on applying undersampling technique we got !\n# Decrease in False positive rate ,\n# Increase in precision,recall,f1-score","7a0d06c4":"rf_model = RandomForestClassifier(n_estimators=20,max_depth = 10,min_samples_split = 20)\nxtrain,xtest,ytrain,ytest = train_test_split(over_sam_x,over_sam_y,test_size = 0.4,random_state = 1)\nrf_model.fit(xtrain,ytrain)\n#train_set\ntrain_pred = rf_model.predict(xtrain)\n#test_set\ntest_pred  = rf_model.predict(xtest)\nprint(\"Accuracy for Training set : \",accuracy_score(ytrain,train_pred))\nprint(\"Accuracy for Testing set  : \",accuracy_score(ytest,test_pred))\nover_sam_roc_auc_random_forest = roc_auc_score(ytest,test_pred)","88e885f8":"train_set(ytrain,train_pred)\n# on applying undersampling technique we got !\n# Decrease in False positive rate ,\n# Increase in precision,recall,f1-score ","95a8d002":"test_set(ytest,test_pred)\n# on applying undersampling technique we got !\n# Decrease in False positive rate ,\n# Increase in precision,recall,f1-score","8a1d1f6c":"# for test_set's only \nprint(\"ROC_AUC_SCORE FOR LOGISTIC REG IS  : \",over_sam_roc_auc_logistic)\nprint(\"ROC_AUC_SCORE FOR Random Forest IS : \",over_sam_roc_auc_random_forest)\n#  Random Forest performs well compared to Logistic Regression.","0cc84fb0":"# Let's Calculate roc_auc_score in order to find out which algorithm out of (Logistic Reg v\/s Random Forest) performed well on applying Undersampling Technique . ","997eb027":"## let's analyze effect of  random forest without handling imbalanced data.\n","3d119563":"## *our goal is to reduce false positive rate* ,\nNote :  again it's entirely depends on the business requirement \n","823a8f6f":"# Random Forest Classifier ","c4bdb79b":"1. #  Let's build our model without treating imbalanced data","b9f6f30e":"# Logistic Regression","5a5e96b0":"# over sampling technique Random Forest Algorithm","25504193":"##  40% fraud transaction instances were classified as not a fraudulent transactions ,that's really bad","aa61715c":"# let's use UnderSampling technique for Random Forest","4d62c07c":"Thank you :)","01d5160a":"# over sampling technique using Logistic Regression ","1f315e12":"# Now using Logistic Regression after allowing Under_sampling","a39b192a":"# Let's use  Under Sampling technique for handling imbalanced data","7b48a7f2":"## 33% of  fraud transaction instances were classified as not a fraudulent transactions","8156a800":"# Let's See Handling Imbalanced Data with Oversampling Technique (SMOTE).\nhttps:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.over_sampling.SMOTE.html","a6b0d6dd":"Note (in this use case) : On applying Over Sampling technique in order to deal with Imbalanced Dataset provides better result compared to Under Sampling technique ( observe  Confusion Matrix of under v\/s over techniques in order to understand )","ffeb3052":"# Let's Calculate roc_auc_score in order to find out which algorithm out of (Logistic Reg v\/s Random Forest) performed well on applying OverSampling Technique . "}}