{"cell_type":{"43b36ed9":"code","600d2a78":"code","ad29dd1f":"code","6cce4a01":"code","1a6504ce":"code","d890475d":"code","18a56535":"code","0146eaa9":"code","6d361827":"code","6bca9480":"code","13860006":"code","04e35023":"code","4bc71b9d":"code","c65cb5b5":"code","c2f94f90":"code","ee5bc29d":"code","242c7d08":"code","01523518":"code","5f3f2eaa":"code","ad5fe474":"code","91bad809":"code","24747df4":"code","b8ba13d0":"code","633a0f89":"code","0d0fc36e":"code","62023181":"code","ae87466a":"markdown","ffd832db":"markdown","2ecc1b1f":"markdown","0ef4e2d9":"markdown","746517e9":"markdown","956ae80b":"markdown","6325aaaf":"markdown","fae4dd8e":"markdown","5a9901cd":"markdown","616a056d":"markdown","81230ed7":"markdown","5f996449":"markdown","532f6f33":"markdown","6f8fa853":"markdown","99f8f810":"markdown","730dabf3":"markdown"},"source":{"43b36ed9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n","600d2a78":"def plt_imgs(df, n_cols=4, n_rows=4):\n    plt.figure(figsize=(n_cols*4.2, n_rows*4.2))\n    for row in range(n_rows):\n        for col in range(n_cols):\n            index = n_cols * row + col\n            plt.subplot(n_rows, n_cols, index+1)\n            image = mpimg.imread(train_imgs_dir+df.iloc[index:index+1].image.item())\n            plt.imshow(image, interpolation='nearest')\n            plt.axis('off')\n            plt.title(df.iloc[index:index+1].label_group.item())","ad29dd1f":"root_dir = '\/kaggle\/input\/shopee-product-matching\/'\ntrain_imgs_dir = root_dir+'train_images\/'\ntest_imgs_dir = root_dir+'test_images\/'\n\ntrain = pd.read_csv(root_dir+'train.csv')\ntest = pd.read_csv(root_dir+'test.csv')\nsubmission = pd.read_csv(root_dir+'sample_submission.csv')\n","6cce4a01":"# take a look into csvs\nprint('Train shape:', train.shape)\ntrain.head()","1a6504ce":"train.nunique(axis=0)","d890475d":"# image duplicates sample\ndup_img = train[train.image.duplicated()].sample(2).image.tolist()\ntrain[train.image.isin(dup_img)]","18a56535":"tmp = train[train.image.isin(dup_img)]\nn_rows = 2\nn_cols = tmp.shape[0]\/\/n_rows\nplt_imgs(tmp, n_rows=n_rows, n_cols=n_cols)","0146eaa9":"# image_phash duplicate sample\ndup_phash = train[train.image_phash.duplicated()].sample(2).image_phash.tolist()\ntrain[train.image_phash.isin(dup_phash)]","6d361827":"tmp = train[train.image_phash.isin(dup_phash)].sort_values('label_group')\nn_rows = 2\nn_cols = tmp.shape[0]\/\/n_rows\nplt_imgs(tmp, n_rows=n_rows, n_cols=n_cols)","6bca9480":"# title duplicates sample\ndup_title = train[train.title.duplicated()].sample(5).title.tolist()\ntrain[train.title.isin(dup_title)].sort_values(['label_group'])","13860006":"tmp = train[train.title.isin(dup_title)].sort_values(['label_group'])\nn_rows = 2\nn_cols = tmp.shape[0]\/\/n_rows\nplt_imgs(tmp, n_rows=n_rows, n_cols=n_cols)","04e35023":"# label_group duplicate sample\ndup_label = train[train.label_group.duplicated()].sample(5).label_group.tolist()\ntrain[train.label_group.isin(dup_label)].sort_values(by=['label_group'])","4bc71b9d":"tmp = train[train.label_group.isin(dup_label)].sort_values(by=['label_group'])\nn_rows = 7\nn_cols = tmp.shape[0]\/\/n_rows\nplt_imgs(tmp, n_rows=n_rows, n_cols=n_cols)","c65cb5b5":"phash_labels = train.groupby('image_phash')['label_group'].nunique().reset_index()\nphash_labels[phash_labels.label_group > 1]","c2f94f90":"phash_mult_labels = phash_labels[phash_labels.label_group > 1].image_phash.tolist()\ntrain[train.image_phash.isin(phash_mult_labels[:5])].sort_values('image_phash')","ee5bc29d":"tmp = train[train.image_phash.isin(phash_mult_labels[:5])].sort_values('image_phash')\nn_rows = 3\nn_cols = tmp.shape[0]\/\/n_rows\nplt_imgs(tmp, n_rows=n_rows, n_cols=n_cols)","242c7d08":"phash_mult_labels = phash_labels[phash_labels.label_group > 1].image_phash.tolist()\ntrain[train.image_phash.isin(phash_mult_labels[120:131])]","01523518":"tmp = train[train.image_phash.isin(phash_mult_labels[120:131])].sort_values('image_phash')\nn_rows = 7\nn_cols = tmp.shape[0]\/\/n_rows\nplt_imgs(tmp, n_rows=n_rows, n_cols=n_cols)","5f3f2eaa":"test.head()","ad5fe474":"import nltk\nnltk.download('stopwords')\nnltk.download('wordnet')","91bad809":"# remove special characters\ntrain.title = train.title.str.replace('[^A-Za-z0-9]+', ' ', regex=True)","24747df4":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom time import time\nimport random\n\ndef inspect(vectoriser, X):\n    # Fit and transform\n    start = time()\n    # counts features\n    print(f\"There are {vectoriser.fit_transform(X).shape[1]} columns.\\n\")\n    end = time()\n    print(f\"Took {round((end-start),2)} seconds.\\n\")\n    \n    # Inspect tokens\n    tokens = list(vectoriser.vocabulary_.keys())\n    tokens.sort()\n    print(f\"Example tokens: {tokens[:100]}\\n\")\n    \n    # Inspect ignored tokens\n    ignored = vectoriser.stop_words_\n    if len(ignored)==0:\n        print(\"No token is ignored.\")\n    elif len(ignored)>50:\n        print(f\"Example ignored tokens: {random.sample(ignored, 50)}\")\n    else:\n        print(f\"Example ignored tokens: {ignored}\")","b8ba13d0":"vectoriser = TfidfVectorizer(token_pattern=r'[A-Za-z]+', stop_words='english', min_df=30, max_df=.7)\ninspect(vectoriser, train['title'])","633a0f89":"vectoriser.transform(train.title).shape","0d0fc36e":"import numpy as np \nfrom scipy.spatial.distance import cdist\n\nvectorised = vectoriser.transform(train.title)\nsim_matrix = cdist(vectorised, vectorised, metric='cosine')\nsim_matrix","62023181":"vectorised.shape","ae87466a":"## text preprocessing","ffd832db":"Work in progress...","2ecc1b1f":"We can see here the cases that the algorithm is supposed to solve, as some items have the same title, but completely different images.","0ef4e2d9":"Duplicate titles may have different image, image_phash but the same label_group.","746517e9":"We have 34250 rows in training data, with 5 columns.\n    1. \"posting_id\" column has unique values\n    2. \"image\" column has some duplicates\n    3. \"image_phash\" also has some duplicates\n    4. \"title\" has some duplicates\n    5. \"label\" group has some duplicates\n    \nLet's look at a sample of each column's duplicates.","956ae80b":"## Read data","6325aaaf":"So we can see that images may duplicated in multiple files with different names and that is discovered by images with duplicates pHash.","fae4dd8e":"So the final notes on this dataframe is that label_group represents an items that might have slightly different titles, with different images, but they are the same item after all. \n\nimage_phash represents fingerprints of images similar to each other, and so far the sample showed that these images represent the same items, but is it possible for an image_phash to be the same with images of totaly different items? This could be found out by searching for rows that have similar image_phash, but a different label_group. ","5a9901cd":"I think that this highlights a very important pint, and that is it will impossible to tag these images based only on their image, as there must some diffreence in their titles that deems them to be in different labels, or this is just mislabelled rows that need to be corrected.","616a056d":"1. Simplest approach is to classify similarity based on title and pHash simialrity\n2. title similarity could be calculated using cosine simialrity of tfidf vectors generated from training data\n3. pHash similarity could be calculated using a distance metric between two pairs of pHashs\n4. After generating a similarity matrix using either title or pHash, training data should be generated using similarities to \n    predict whether the two images are the same or not\n5. More complex methods should utilize the images (Not necessairly complex as the two images could be identical in shape and\n    flattened then concatenated and directly fed to a logistic regression algorithm to predict whether they are the same or not).","81230ed7":"We can see that two items with literally the same images have different label groups.","5f996449":"Further exploration may be:\n- looking into images with the same image_phash\n- images with the same label_group\n- exploring the title column separetely and in relationship with other columns","532f6f33":"## Import necessary libraries","6f8fa853":"So evidently we have 147 image_phash that doesn't have the same label_group.\nLet's look more into them.","99f8f810":"We can see in train_2018235992 and train_1810772318 that they have the same image and title but have different label groups, and I think that if the the items are going to be judged to be the same or not based on label_group, this column needs to be cleaned in order to obtain good results, as if left the way it is the model will be fed contradictory inputs, where an item with literally the same title and pHash similarity will won't be the same.","730dabf3":"We can see that this is just a duplicated row, except for posting_id which is a unique identifier."}}