{"cell_type":{"bea71289":"code","0e09638e":"code","46e3c0e1":"code","3027012c":"code","25ba9cbc":"code","f76366e1":"code","0e1aa9ad":"code","ead8d109":"code","8fd28238":"code","12f77a50":"code","a80658f1":"code","a9a87ce7":"code","11d7caf6":"code","ef85e005":"code","1107f6e4":"code","49d94f31":"code","e95af8f2":"code","c9c96f27":"code","c76aad43":"code","35b203da":"code","a368889c":"code","5cc9676b":"code","ef4085dc":"code","b9b00afc":"code","323a0a98":"code","66905f52":"code","021af8b4":"code","8fd4d7ed":"code","8941ac7d":"code","8ea7e949":"code","bc362a1e":"code","3c59d889":"code","e8182622":"code","bd55e23c":"code","3495626b":"code","8b58928a":"code","738156b8":"code","0b4ca3d1":"code","b479a091":"code","fcd505e8":"code","a12c362e":"code","b3be8620":"markdown","aee20f90":"markdown","cba9eb09":"markdown","2f96cdad":"markdown","6805df94":"markdown","eb109fda":"markdown","358d2eca":"markdown","04ae7646":"markdown","e34ef392":"markdown","ce49930b":"markdown","c69769e2":"markdown","345a419e":"markdown","d2d028ca":"markdown","983c46d6":"markdown","c00de9d0":"markdown","94649183":"markdown","b483c422":"markdown","b8b80251":"markdown","4aac487c":"markdown"},"source":{"bea71289":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0e09638e":"df=pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")\ndf.head()","46e3c0e1":"df.info()","3027012c":"df.isna().sum()","25ba9cbc":"df.sex.value_counts()","f76366e1":"df.sex[df.target==1].value_counts()","0e1aa9ad":"df.sex[df.target==1].value_counts().plot(kind='bar',figsize=(10,6),color=['green','blue'])\nplt.title(\"Count of the number of males and females with heart disease\")\nplt.xticks(rotation=0);","ead8d109":"pd.crosstab(df.target,df.sex)","8fd28238":"pd.crosstab(df.target,df.sex).plot(kind='bar',figsize=(10,6),color=[\"lightblue\",\"pink\"])\nplt.title(\"Frequency of Heart Disease vs Sex\")\nplt.xlabel(\"0= Heart Disease, 1= No disease\")\nplt.ylabel(\"Number of people with heart disease\")\nplt.legend([\"Female\",\"Male\"])\nplt.xticks(rotation=0);","12f77a50":"df.corr()","a80658f1":"cor_mat=df.corr()\nfig,ax=plt.subplots(figsize=(15,10))\nsns.heatmap(cor_mat,annot=True,linewidths=0.5,fmt=\".3f\")\n","a9a87ce7":"from sklearn.preprocessing import MinMaxScaler\nscal=MinMaxScaler()\nfeat=['age', \t'sex', \t'cp', 'trestbps', 'chol', \t'fbs', \t'restecg', \t'thalach' ,\t'exang', \t'oldpeak' ,\t'slope', \t'ca', 'thal']\ndf[feat] = scal.fit_transform(df[feat])\ndf.head()","11d7caf6":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nfeatures= ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n#df[features] = scaler.fit_transform(df[features])\ndf.head()","ef85e005":"X=df.drop(\"target\",axis=1).values\nY=df.target.values","1107f6e4":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,random_state=0,test_size=0.2)","49d94f31":"from sklearn.metrics import accuracy_score,recall_score,f1_score,precision_score,roc_auc_score,confusion_matrix\n\ndef evaluation(Y_test,Y_pred):\n  acc=accuracy_score(Y_test,Y_pred)\n  rcl=recall_score(Y_test,Y_pred)\n  f1=f1_score(Y_test,Y_pred)\n \n\n  metric_dict={'accuracy': round(acc,3),\n               'recall': round(rcl,3),\n               'F1 score': round(f1,3),\n               \n              }\n\n  return print(metric_dict)\n\n#evaluation(Y_test,SVC_Y_pred)","e95af8f2":"np.random.seed(42)\nfrom sklearn.neighbors import KNeighborsClassifier\nKnn_clf=  KNeighborsClassifier()\nKnn_clf.fit(X_train,Y_train)\nKnn_Y_pred=Knn_clf.predict(X_test)\nKnn_score=Knn_clf.score(X_test,Y_test)\n#print(Knn_score)\nevaluation(Y_test,Knn_Y_pred)","c9c96f27":"np.random.seed(42)\nfrom sklearn.linear_model import LogisticRegression\nLR_clf=LogisticRegression()\nLR_clf.fit(X_train,Y_train)\nLR_Y_pred=LR_clf.predict(X_test)\nLR_score=LR_clf.score(X_test,Y_test)\n#print(LR_score)\nevaluation(Y_test,LR_Y_pred)","c76aad43":"np.random.seed(42)\nfrom sklearn.ensemble import RandomForestClassifier\nRF_clf=RandomForestClassifier(n_estimators=450)\nRF_clf.fit(X_train,Y_train)\nRF_score=RF_clf.score(X_test,Y_test)\nRF_Y_pred=RF_clf.predict(X_test)\n#print(RF_score)\nevaluation(Y_test,RF_Y_pred)","35b203da":"np.random.seed(42)\nfrom sklearn.svm import SVC\nSVC_clf=SVC()\nSVC_clf.fit(X_train,Y_train)\nSVC_score=SVC_clf.score(X_test,Y_test)\nSVC_Y_pred=SVC_clf.predict(X_test)\n#print(SVC_score)\nevaluation(Y_test,SVC_Y_pred)","a368889c":"from xgboost import XGBClassifier\nXGB_clf=XGBClassifier()\nXGB_clf.fit(X_train,Y_train)\nXGB_score=XGB_clf.score(X_test,Y_test)\nXGB_Y_pred=XGB_clf.predict(X_test)\n#print(SVC_score)\nevaluation(Y_test,XGB_Y_pred)","5cc9676b":"model_comp = pd.DataFrame({'Model': ['Logistic Regression','Random Forest',\n                    'K-Nearest Neighbour','Support Vector Machine',\"XGBoost\"], 'Accuracy': [LR_score*100,\n                    RF_score*100,Knn_score*100,SVC_score*100,XGB_score*100]})\nmodel_comp","ef4085dc":"neighbors = range(1, 21) # 1 to 20\n\n# Setup algorithm\nknn = KNeighborsClassifier()\n\n# Loop through different neighbors values\nfor i in neighbors:\n    knn.set_params(n_neighbors = i) # set neighbors value\n    \n    # Fit the algorithm\n    print(f\"Accuracy with {i} no. of neighbors: {knn.fit(X_train, Y_train).score(X_test,Y_test)}%\")","b9b00afc":"np.random.seed(42)\nfrom sklearn.neighbors import KNeighborsClassifier\nKnn_clf=  KNeighborsClassifier(n_neighbors=7)\nKnn_clf.fit(X_train,Y_train)\nKnn_Y_pred=Knn_clf.predict(X_test)\nKnn_score=Knn_clf.score(X_test,Y_test)\nevaluation(Y_test,Knn_Y_pred)","323a0a98":"from sklearn.ensemble import RandomForestClassifier\nnp.random.seed(42)\nfor i in range(1,40,1):\n  print(f\"With {i*10} estimators:\")\n  clf2=RandomForestClassifier(n_estimators=i*10,max_depth=i,random_state=i).fit(X_train,Y_train)\n  print(f\"Accuracy: {clf2.score(X_test,Y_test)*100:2f}%\")","66905f52":"from sklearn.ensemble import RandomForestClassifier\nRF_clf2=RandomForestClassifier(n_estimators=30,max_depth=3,random_state=3)\nRF_clf2.fit(X_train,Y_train)\nRF2_acc_score=RF_clf2.score(X_test,Y_test)\nRF2_Y_pred=RF_clf2.predict(X_test)\n#print(RF2_acc_score)\nevaluation(Y_test,RF2_Y_pred)","021af8b4":"xgb = XGBClassifier(learning_rate=0.01, n_estimators=25, max_depth=15,gamma=0.6, subsample=0.52,colsample_bytree=0.6,seed=27, \n                    reg_lambda=2, booster='dart', colsample_bylevel=0.6, colsample_bynode=0.5)\n\nxgb.fit(X_train,Y_train)\nxgb_score=XGB_clf.score(X_test,Y_test)\nxgb_Y_pred=XGB_clf.predict(X_test)\n#print(SVC_score)\nevaluation(Y_test,xgb_Y_pred)","8fd4d7ed":"\nfrom sklearn.model_selection import GridSearchCV \n  \n# defining parameter range \nparam_grid = {'C': [0.1, 1,2, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf','linear']}  \n  \ngs_clf = GridSearchCV(SVC(), param_grid,cv=5, refit = True, verbose = 3) \n  \n# fitting the model for grid search \ngs_clf.fit(X_train, Y_train)\n\nprint(gs_clf.best_params_)\n\nprint(f\"Accuracy score:{gs_clf.score(X_test,Y_test)}%\")\n\n","8941ac7d":"knn_grid={'n_neighbors': np.arange(1,30,1),\n          'leaf_size': np.arange(1,50,1)}\n\ngs_knn=GridSearchCV(KNeighborsClassifier(),param_grid=knn_grid,cv=5,verbose=True)\n\ngs_knn.fit(X_train, Y_train)\n\ngs_knn.best_params_\n\nprint(f\"Accuracy score:{gs_knn.score(X_test,Y_test)*100}%\")","8ea7e949":"model_comp = pd.DataFrame({'Model': ['Logistic Regression','Random Forest',\n                    'K-Nearest Neighbour','Support Vector Machine','Extreme Gradient Boost'], 'Accuracy': [LR_score*100,\n                    RF2_acc_score*100,Knn_score*100,SVC_score*100, XGB_score*100]})\nmodel_comp","bc362a1e":"from mlxtend.classifier import StackingCVClassifier\nscv=StackingCVClassifier(classifiers=[xgb,Knn_clf,RF_clf2],meta_classifier= Knn_clf)\nscv.fit(X_train,Y_train)\nscv_score=scv.score(X_test,Y_test)\nscv_Y_pred=scv.predict(X_test)\n#print(SVC_score)\nevaluation(Y_test,scv_Y_pred)","3c59d889":"model_comp = pd.DataFrame({'Model': ['Logistic Regression','Random Forest',\n                    'K-Nearest Neighbour','Support Vector Machine','Extreme Gradient Boost', 'StackingCV Classifier'], 'Accuracy': [LR_score*100,\n                    RF2_acc_score*100,Knn_score*100,SVC_score*100, XGB_score*100, scv_score*100]})\nmodel_comp","e8182622":"print(\" Best evaluation parameters achieved with KNN:\") \nevaluation(Y_test,scv_Y_pred)","bd55e23c":"final_metrics={'Accuracy': scv.score(X_test,Y_test),\n                   'Precision': precision_score(Y_test,scv_Y_pred),\n                   'Recall': recall_score(Y_test,scv_Y_pred),\n                   'F1': f1_score(Y_test,scv_Y_pred),\n                   'AUC': roc_auc_score(Y_test,scv_Y_pred)}\n\nmetrics=pd.DataFrame(final_metrics,index=[0])\n\nmetrics.T.plot.bar(title='Final metric evaluation',legend=False);","3495626b":"from sklearn.metrics import confusion_matrix\n\nfig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(Y_test,Knn_Y_pred),annot=True,cbar=True);","8b58928a":"user_input=input(\"Enter the values one by one\")\nuser_input=user_input.split(\",\")\n\n\nfor i in range(len(user_input)):\n    # convert each item to int type\n    user_input[i] = float(user_input[i])\n\nuser_input=np.array(user_input)\nuser_input=user_input.reshape(1,-1)\nuser_input=scal.transform(user_input)\nscv_Y_pred=scv.predict(user_input)\nif(scv_Y_pred[0]==0):\n  print(\"Warning! You have chances of getting a heart disease!\")\nelse:\n  print(\"You are healthy and are less likely to get a heart disease!\")\n","738156b8":"import pickle as pkl\npkl.dump(Knn_clf,open(\"final_model.p\",\"wb\"))","0b4ca3d1":"import sklearn\nsklearn_version = sklearn.__version__\nprint(sklearn_version)","b479a091":"!pip install streamlit\n!pip install pyngrok===4.1.1\nfrom pyngrok import ngrok","fcd505e8":"%%writefile healthy-heart-app.py\nimport streamlit as st\nimport base64\nimport sklearn\nimport numpy as np\nimport pickle as pkl\nfrom sklearn.preprocessing import MinMaxScaler\nscal=MinMaxScaler()\n#Load the saved model\nmodel=pkl.load(open(\"final_model.p\",\"rb\"))\n\n\n\n\n\nst.set_page_config(page_title=\"Healthy Heart App\",page_icon=\"\u2695\ufe0f\",layout=\"centered\",initial_sidebar_state=\"expanded\")\n\n\n\ndef preprocess(age,sex,cp,trestbps,restecg,chol,fbs,thalach,exang,oldpeak,slope,ca,thal ):   \n \n    \n    # Pre-processing user input   \n    if sex==\"male\":\n        sex=1 \n    else: sex=0\n    \n    \n    if cp==\"Typical angina\":\n        cp=0\n    elif cp==\"Atypical angina\":\n        cp=1\n    elif cp==\"Non-anginal pain\":\n        cp=2\n    elif cp==\"Asymptomatic\":\n        cp=2\n    \n    if exang==\"Yes\":\n        exang=1\n    elif exang==\"No\":\n        exang=0\n \n    if fbs==\"Yes\":\n        fbs=1\n    elif fbs==\"No\":\n        fbs=0\n \n    if slope==\"Upsloping: better heart rate with excercise(uncommon)\":\n        slope=0\n    elif slope==\"Flatsloping: minimal change(typical healthy heart)\":\n          slope=1\n    elif slope==\"Downsloping: signs of unhealthy heart\":\n        slope=2  \n \n    if thal==\"fixed defect: used to be defect but ok now\":\n        thal=6\n    elif thal==\"reversable defect: no proper blood movement when excercising\":\n        thal=7\n    elif thal==\"normal\":\n        thal=2.31\n\n    if restecg==\"Nothing to note\":\n        restecg=0\n    elif restecg==\"ST-T Wave abnormality\":\n        restecg=1\n    elif restecg==\"Possible or definite left ventricular hypertrophy\":\n        restecg=2\n\n\n    user_input=[age,sex,cp,trestbps,restecg,chol,fbs,thalach,exang,oldpeak,slope,ca,thal]\n    user_input=np.array(user_input)\n    user_input=user_input.reshape(1,-1)\n    user_input=scal.fit_transform(user_input)\n    prediction = model.predict(user_input)\n\n    return prediction\n\n    \n\n       \n    # front end elements of the web page \nhtml_temp = \"\"\" \n    <div style =\"background-color:pink;padding:13px\"> \n    <h1 style =\"color:black;text-align:center;\">Healthy Heart App<\/h1> \n    <\/div> \n    \"\"\"\n      \n# display the front end aspect\nst.markdown(html_temp, unsafe_allow_html = True) \nst.subheader('by Amlan Mohanty ')\n      \n# following lines create boxes in which user can enter data required to make prediction\nage=st.selectbox (\"Age\",range(1,121,1))\nsex = st.radio(\"Select Gender: \", ('male', 'female'))\ncp = st.selectbox('Chest Pain Type',(\"Typical angina\",\"Atypical angina\",\"Non-anginal pain\",\"Asymptomatic\")) \ntrestbps=st.selectbox('Resting Blood Sugar',range(1,500,1))\nrestecg=st.selectbox('Resting Electrocardiographic Results',(\"Nothing to note\",\"ST-T Wave abnormality\",\"Possible or definite left ventricular hypertrophy\"))\nchol=st.selectbox('Serum Cholestoral in mg\/dl',range(1,1000,1))\nfbs=st.radio(\"Fasting Blood Sugar higher than 120 mg\/dl\", ['Yes','No'])\nthalach=st.selectbox('Maximum Heart Rate Achieved',range(1,300,1))\nexang=st.selectbox('Exercise Induced Angina',[\"Yes\",\"No\"])\noldpeak=st.number_input('Oldpeak')\nslope = st.selectbox('Heart Rate Slope',(\"Upsloping: better heart rate with excercise(uncommon)\",\"Flatsloping: minimal change(typical healthy heart)\",\"Downsloping: signs of unhealthy heart\"))\nca=st.selectbox('Number of Major Vessels Colored by Flourosopy',range(0,5,1))\nthal=st.selectbox('Thalium Stress Result',range(1,8,1))\n\n\n\n#user_input=preprocess(sex,cp,exang, fbs, slope, thal )\npred=preprocess(age,sex,cp,trestbps,restecg,chol,fbs,thalach,exang,oldpeak,slope,ca,thal)\n\n\n\n\nif st.button(\"Predict\"):    \n  if pred[0] == 0:\n    st.error('Warning! You have high risk of getting a heart attack!')\n    \n  else:\n    st.success('You have lower risk of getting a heart disease!')\n    \n   \n\n\n\nst.sidebar.subheader(\"About App\")\n\nst.sidebar.info(\"This web app is helps you to find out whether you are at a risk of developing a heart disease.\")\nst.sidebar.info(\"Enter the required fields and click on the 'Predict' button to check whether you have a healthy heart\")\nst.sidebar.info(\"Don't forget to rate this app\")\n\n\n\nfeedback = st.sidebar.slider('How much would you rate this app?',min_value=0,max_value=5,step=1)\n\nif feedback:\n  st.header(\"Thank you for rating the app!\")\n  st.info(\"Caution: This is just a prediction and not doctoral advice. Kindly see a doctor if you feel the symptoms persist.\") \n\n\n     \n\n\n\n\n\n\n\n\n\n\n\n","a12c362e":"!nohup streamlit run healthy-heart-app.py &\nurl = ngrok.connect(port='8501')\nurl","b3be8620":"## **Creating Features and Target variable**","aee20f90":"# **Hyper parameter tuning  SVC using GridSearchCV**","cba9eb09":"# **Write a file for creating web app**","2f96cdad":"# **Hyper parameter tuning KNN using GridSearchCV**","6805df94":"# **Looking at the evaluation metrics for our best model**\n\nAs we can see, the StackingCV Classifier gives us an accuracy of 92%. \n\nLet us evaluate the model now.","eb109fda":"# **Tuning XGBoost manually**","358d2eca":"## **Building a Correlation Matrix**","04ae7646":"## **Create a function for evaluating metrics**","e34ef392":"## **Tuning KNN**","ce49930b":"# **Import streamlit,pyngrok, and ngrok modules**","c69769e2":" ### **Count of the number of males and females who have heart disease**","345a419e":"\n# **Let's save our model using pickle**","d2d028ca":"## **Splitting the data into train and test sets**","983c46d6":"# **Tuning Random Forest**","c00de9d0":"## **Fitting and Comparing different Models**","94649183":"# **The link to the final deployed web app is attached below:**\n## https:\/\/healthy-heart-app.herokuapp.com\/\n\n# **GitHub link:**\nhttps:\/\/github.com\/amlanmohanty1\/Heart-Disease-Classification\/","b483c422":"# **Your web app is ready! To deploy the web app on Heroku, follow the following blog!**\n\n## https:\/\/www.geeksforgeeks.org\/deploy-your-machine-learning-web-app-streamlit-on-heroku\/","b8b80251":"# **Trying StackingCV Classifier**","4aac487c":"### **Number of males and females whose heart data is stored in the dataset**"}}