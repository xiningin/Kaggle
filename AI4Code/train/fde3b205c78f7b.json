{"cell_type":{"52711204":"code","41d3b6ea":"code","3de5c1d1":"code","2d3bae8d":"code","e54c4d7d":"code","37b3e3c7":"code","29412a99":"code","aa4e209c":"code","55811629":"code","124c4d8d":"code","f9b36992":"code","31984032":"code","a37610e4":"code","5d321714":"code","c08dbba5":"code","2b51dc8e":"code","3a71661b":"code","85c34da6":"code","61da359d":"code","5cf322dd":"code","9cc26697":"code","8f4f0543":"code","d063a80c":"code","d2428655":"code","73326f2b":"code","43c01a74":"code","0ecf042b":"code","9f019ddc":"code","cdccf514":"code","d67d416e":"code","3fdcc3c2":"code","5aeccc0f":"code","f473b670":"code","e220ffe8":"code","26fe238d":"code","6876bb78":"code","0a96e0f4":"code","976591e8":"code","e8ed8fbc":"code","e8f0d995":"code","d433528c":"code","f1cbfcd7":"code","f733b15e":"code","dcc48b79":"code","7a66db13":"code","e48f30e9":"code","8f503469":"code","4268ee16":"code","72738a37":"code","d132a62e":"code","3ab4ec5d":"code","2d92def6":"code","a7b0d6ba":"code","a438901b":"code","2188ca6e":"code","c700c057":"code","e987f336":"code","61639e84":"code","8b556c81":"code","cce4a25d":"code","7eadf460":"code","3d4c1b55":"code","d60c2102":"code","43181913":"code","9e14b71a":"code","b252becc":"code","86862e3b":"code","9cff4a88":"code","c1af4d72":"code","623600b2":"code","e2d24ea4":"markdown","50634921":"markdown","a9a50fc1":"markdown","d44d68ce":"markdown","e7666162":"markdown","2071f0e8":"markdown","0f379ff1":"markdown","cbd29dfa":"markdown","02a38461":"markdown","5aebfee1":"markdown","55d7dda6":"markdown","00cc0746":"markdown","665e2c26":"markdown","eb3dc96f":"markdown","beeeb8f4":"markdown","9f87dde5":"markdown","b4ef2535":"markdown","20a86d21":"markdown"},"source":{"52711204":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","41d3b6ea":"import os\n!pip install python-docx\nimport docx\n\ndef getText(filename):\n    file_ref = open(\"\/kaggle\/input\/math-word-grade-3\/\" + filename,\"rb\")\n    doc = docx.Document(file_ref)\n    fullText = []\n    for para in doc.paragraphs:\n        fullText.append(para.text)\n    return '\\n'.join(fullText)\n\nfiles = os.listdir('\/kaggle\/input\/math-word-grade-3')\nprint(files)\ntexts = []\nfor f in files:\n  if '.pkl' not in f and '.json' not in f:\n    texts.append(getText(f))","3de5c1d1":"questions = []\nequations = []\nfor text in texts:\n  for line in text.split('\\n')[1:]:\n    if 'C\u00e2u h\u1ecfi:' in line:\n      q = line[len('C\u00e2u h\u1ecfi: \\\"'):-1]\n      questions.append(q.strip())\n    if 'Ph\u01b0\u01a1ng tr\u00ecnh:' in line:\n      e = line[len('Ph\u01b0\u01a1ng tr\u00ecnh: \\\"'):-1]\n      equations.append(e.strip())","2d3bae8d":"len(questions), len(equations)","e54c4d7d":"np.random.choice(questions, 10)","37b3e3c7":"np.random.choice(equations, 10)","29412a99":"data = pd.DataFrame({'Question':questions, 'Equation':equations})\ndata","aa4e209c":"import pickle\nwith open('data_final.pkl','wb') as f:\n  pickle.dump(data, f)","55811629":"import re\nimport time\nimport random\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom sklearn.model_selection import train_test_split\nimport pickle\nimport spacy\nfrom nltk.translate.bleu_score import corpus_bleu","124c4d8d":"with open('data_final.pkl', 'rb') as f:\n  df = pickle.load(f)","f9b36992":"df.shape\ndf.head()","31984032":"input_exps = list(df['Question'].values)","a37610e4":"def convert_eqn(eqn):\n  '''\n  Add a space between every character in the equation string.\n  Eg: 'x = 23 + 88' becomes 'x =  2 3 + 8 8'\n  '''\n  elements = list(eqn)\n  return ' '.join(elements)\ntarget_exps = list(df['Equation'].apply(lambda x: convert_eqn(x)).values)","5d321714":"# Input: Word problem\ninput_exps[:5]","c08dbba5":"# Target: Equation\ntarget_exps[:5]","2b51dc8e":"len(pd.Series(input_exps)), len(pd.Series(input_exps).unique())","3a71661b":"\nlen(pd.Series(target_exps)), len(pd.Series(target_exps).unique())","85c34da6":"def preprocess_input(sentence):\n  '''\n  For the word problem, convert everything to lowercase, add spaces around all\n  punctuations and digits, and remove any extra spaces. \n  '''\n  sentence = sentence.lower().strip()\n  sentence = re.sub(r\"([?.!,\u2019])\", r\" \\1 \", sentence)\n  sentence = re.sub(r\"([0-9])\", r\" \\1 \", sentence)\n  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n  sentence = sentence.rstrip().strip()\n  return sentence\ndef preprocess_target(sentence):\n  '''\n  For the equation, convert it to lowercase and remove extra spaces\n  '''\n  sentence = sentence.lower().strip()\n  return sentence\npreprocessed_input_exps = list(map(preprocess_input, input_exps))\npreprocessed_target_exps = list(map(preprocess_target, target_exps))","61da359d":"preprocessed_input_exps[:5]","5cf322dd":"preprocessed_target_exps[:5]","9cc26697":"def tokenize(lang):\n  '''\n  Tokenize the given list of strings and return the tokenized output\n  along with the fitted tokenizer.\n  '''\n  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n  lang_tokenizer.fit_on_texts(lang)\n  tensor = lang_tokenizer.texts_to_sequences(lang)\n  return tensor, lang_tokenizer\ninput_tensor, inp_lang_tokenizer = tokenize(preprocessed_input_exps)","8f4f0543":"len(inp_lang_tokenizer.word_index)","d063a80c":"target_tensor, targ_lang_tokenizer = tokenize(preprocessed_target_exps)\nold_len = len(targ_lang_tokenizer.word_index)","d2428655":"def append_start_end(x,last_int):\n  '''\n  Add integers for start and end tokens for input\/target exps\n  '''\n  l = []\n  l.append(last_int+1)\n  l.extend(x)\n  l.append(last_int+2)\n  return l\ninput_tensor_list = [append_start_end(i,len(inp_lang_tokenizer.word_index)) for i in input_tensor]\ntarget_tensor_list = [append_start_end(i,len(targ_lang_tokenizer.word_index)) for i in target_tensor]\n","73326f2b":"# Pad all sequences such that they are of equal length\ninput_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor_list, padding='post')\ntarget_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor_list, padding='post')","43c01a74":"input_tensor","0ecf042b":"target_tensor","9f019ddc":"# Here we are increasing the vocabulary size of the target, by adding a\n# few extra vocabulary words (which will not actually be used) as otherwise the\n# small vocab size causes issues downstream in the network.\nkeys = [str(i) for i in range(10,51)]\nfor i,k in enumerate(keys):\n  targ_lang_tokenizer.word_index[k]=len(targ_lang_tokenizer.word_index)+i+4\nlen(targ_lang_tokenizer.word_index)","cdccf514":"# Creating training and validation sets\ninput_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor,\n                                                                                                target_tensor,\n                                                                                                test_size=0.05,\n                                                                                                random_state=42)","d67d416e":"len(input_tensor_train)","3fdcc3c2":"len(input_tensor_val)","5aeccc0f":"BUFFER_SIZE = len(input_tensor_train)\nBATCH_SIZE = 64\nsteps_per_epoch = len(input_tensor_train)\/\/BATCH_SIZE\ndataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\ndataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\nnum_layers = 4\nd_model = 128\ndff = 512\nnum_heads = 8\ninput_vocab_size = len(inp_lang_tokenizer.word_index)+3\ntarget_vocab_size = len(targ_lang_tokenizer.word_index)+3\ndropout_rate = 0.0","f473b670":"example_input_batch, example_target_batch = next(iter(dataset))\nexample_input_batch.shape, example_target_batch.shape","e220ffe8":"# We provide positional information about the data to the model,\n# otherwise each sentence will be treated as Bag of Words\ndef get_angles(pos, i, d_model):\n  angle_rates = 1 \/ np.power(10000, (2 * (i\/\/2)) \/ np.float32(d_model))\n  return pos * angle_rates\ndef positional_encoding(position, d_model):\n  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                          np.arange(d_model)[np.newaxis, :],\n                          d_model)\n  \n  # apply sin to even indices in the array; 2i\n  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n  \n  # apply cos to odd indices in the array; 2i+1\n  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    \n  pos_encoding = angle_rads[np.newaxis, ...]\n    \n  return tf.cast(pos_encoding, dtype=tf.float32)","26fe238d":"# mask all elements are that not words (padding) so that it is not treated as input\ndef create_padding_mask(seq):\n  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n  \n  # add extra dimensions to add the padding\n  # to the attention logits.\n  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\ndef create_look_ahead_mask(size):\n  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n  return mask\ndataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)","6876bb78":"def scaled_dot_product_attention(q, k, v, mask):\n  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n  \n  # scale matmul_qk\n  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n  scaled_attention_logits = matmul_qk \/ tf.math.sqrt(dk)\n\n  # add the mask to the scaled tensor.\n  if mask is not None:\n    scaled_attention_logits += (mask * -1e9)  \n\n  # softmax is normalized on the last axis (seq_len_k) so that the scores\n  # add up to 1.\n  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n  return output, attention_weights","0a96e0f4":"class MultiHeadAttention(tf.keras.layers.Layer):\n  def __init__(self, d_model, num_heads):\n    super(MultiHeadAttention, self).__init__()\n    self.num_heads = num_heads\n    self.d_model = d_model\n    \n    assert d_model % self.num_heads == 0\n    \n    self.depth = d_model \/\/ self.num_heads\n    \n    self.wq = tf.keras.layers.Dense(d_model)\n    self.wk = tf.keras.layers.Dense(d_model)\n    self.wv = tf.keras.layers.Dense(d_model)\n    \n    self.dense = tf.keras.layers.Dense(d_model)\n        \n  def split_heads(self, x, batch_size):\n    \"\"\"Split the last dimension into (num_heads, depth).\n    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n    \"\"\"\n    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n    return tf.transpose(x, perm=[0, 2, 1, 3])\n    \n  def call(self, v, k, q, mask):\n    batch_size = tf.shape(q)[0]\n    \n    q = self.wq(q)  # (batch_size, seq_len, d_model)\n    k = self.wk(k)  # (batch_size, seq_len, d_model)\n    v = self.wv(v)  # (batch_size, seq_len, d_model)\n    \n    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n    \n    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n    scaled_attention, attention_weights = scaled_dot_product_attention(\n        q, k, v, mask)\n    \n    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n    concat_attention = tf.reshape(scaled_attention, \n                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n        \n    return output, attention_weights","976591e8":"def point_wise_feed_forward_network(d_model, dff):\n  return tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n  ])","e8ed8fbc":"class EncoderLayer(tf.keras.layers.Layer):\n  def __init__(self, d_model, num_heads, dff, rate=0.1):\n    super(EncoderLayer, self).__init__()\n\n    self.mha = MultiHeadAttention(d_model, num_heads)\n    self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n    # normalize data per feature instead of batch\n    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    \n    self.dropout1 = tf.keras.layers.Dropout(rate)\n    self.dropout2 = tf.keras.layers.Dropout(rate)\n    \n  def call(self, x, training, mask):\n    # Multi-head attention layer\n    attn_output, _ = self.mha(x, x, x, mask) \n    attn_output = self.dropout1(attn_output, training=training)\n    # add residual connection to avoid vanishing gradient problem\n    out1 = self.layernorm1(x + attn_output)\n    \n    # Feedforward layer\n    ffn_output = self.ffn(out1)\n    ffn_output = self.dropout2(ffn_output, training=training)\n    # add residual connection to avoid vanishing gradient problem\n    out2 = self.layernorm2(out1 + ffn_output)\n    return out2","e8f0d995":"class Encoder(tf.keras.layers.Layer):\n  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n               maximum_position_encoding, rate=0.1):\n    super(Encoder, self).__init__()\n\n    self.d_model = d_model\n    self.num_layers = num_layers\n    \n    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n    self.pos_encoding = positional_encoding(maximum_position_encoding, \n                                            self.d_model)\n    \n    # Create encoder layers (count: num_layers)\n    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n                       for _ in range(num_layers)]\n  \n    self.dropout = tf.keras.layers.Dropout(rate)\n        \n  def call(self, x, training, mask):\n\n    seq_len = tf.shape(x)[1]\n\n    # adding embedding and position encoding.\n    x = self.embedding(x)  \n    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n    x += self.pos_encoding[:, :seq_len, :]\n\n    x = self.dropout(x, training=training)\n    \n    for i in range(self.num_layers):\n      x = self.enc_layers[i](x, training, mask)\n    \n    return x","d433528c":"class DecoderLayer(tf.keras.layers.Layer):\n  def __init__(self, d_model, num_heads, dff, rate=0.1):\n    super(DecoderLayer, self).__init__()\n\n    self.mha1 = MultiHeadAttention(d_model, num_heads)\n    self.mha2 = MultiHeadAttention(d_model, num_heads)\n\n    self.ffn = point_wise_feed_forward_network(d_model, dff)\n \n    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    \n    self.dropout1 = tf.keras.layers.Dropout(rate)\n    self.dropout2 = tf.keras.layers.Dropout(rate)\n    self.dropout3 = tf.keras.layers.Dropout(rate)\n    \n    \n  def call(self, x, enc_output, training, \n           look_ahead_mask, padding_mask):\n\n    # Masked multihead attention layer (padding + look-ahead)\n    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n    attn1 = self.dropout1(attn1, training=training)\n    # again add residual connection\n    out1 = self.layernorm1(attn1 + x)\n    \n    # Masked multihead attention layer (only padding)\n    # with input from encoder as Key and Value, and input from previous layer as Query\n    attn2, attn_weights_block2 = self.mha2(\n        enc_output, enc_output, out1, padding_mask)\n    attn2 = self.dropout2(attn2, training=training)\n    # again add residual connection\n    out2 = self.layernorm2(attn2 + out1)\n    \n    # Feedforward layer\n    ffn_output = self.ffn(out2)\n    ffn_output = self.dropout3(ffn_output, training=training)\n    # again add residual connection\n    out3 = self.layernorm3(ffn_output + out2)\n    return out3, attn_weights_block1, attn_weights_block2","f1cbfcd7":"class Decoder(tf.keras.layers.Layer):\n  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n               maximum_position_encoding, rate=0.1):\n    super(Decoder, self).__init__()\n\n    self.d_model = d_model\n    self.num_layers = num_layers\n     \n    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n    \n    # Create decoder layers (count: num_layers)\n    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n                       for _ in range(num_layers)]\n    self.dropout = tf.keras.layers.Dropout(rate)\n    \n  def call(self, x, enc_output, training, \n           look_ahead_mask, padding_mask):\n\n    seq_len = tf.shape(x)[1]\n    attention_weights = {}\n    \n    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n    \n    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n    \n    x += self.pos_encoding[:,:seq_len,:]\n    \n    x = self.dropout(x, training=training)\n\n    for i in range(self.num_layers):\n      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n                                             look_ahead_mask, padding_mask)\n      \n      # store attenion weights, they can be used to visualize while translating\n      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n    \n    return x, attention_weights","f733b15e":"class Transformer(tf.keras.Model):\n  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n               target_vocab_size, pe_input, pe_target, rate=0.1):\n    super(Transformer, self).__init__()\n\n    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n                           input_vocab_size, pe_input, rate)\n\n    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n                           target_vocab_size, pe_target, rate)\n\n    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n    \n  def call(self, inp, tar, training, enc_padding_mask, \n           look_ahead_mask, dec_padding_mask):\n\n    # Pass the input to the encoder\n    enc_output = self.encoder(inp, training, enc_padding_mask)\n    \n    # Pass the encoder output to the decoder\n    dec_output, attention_weights = self.decoder(\n        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n    \n    # Pass the decoder output to the last linear layer\n    final_output = self.final_layer(dec_output)\n    \n    return final_output, attention_weights","dcc48b79":"class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n  def __init__(self, d_model, warmup_steps=4000):\n    super(CustomSchedule, self).__init__()\n    \n    self.d_model = d_model\n    self.d_model = tf.cast(self.d_model, tf.float32)\n\n    self.warmup_steps = warmup_steps\n    \n  def __call__(self, step):\n    arg1 = tf.math.rsqrt(step)\n    arg2 = step * (self.warmup_steps ** -1.5)\n    \n    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)","7a66db13":"learning_rate = CustomSchedule(d_model)\n\n# Adam optimizer with a custom learning rate\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n                                     epsilon=1e-9)","e48f30e9":"loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')","8f503469":"def loss_function(real, pred):\n  # Apply a mask to paddings (0)\n  mask = tf.math.logical_not(tf.math.equal(real, 0))\n  loss_ = loss_object(real, pred)\n\n  mask = tf.cast(mask, dtype=loss_.dtype)\n  loss_ *= mask\n  \n  return tf.reduce_mean(loss_)","4268ee16":"train_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')","72738a37":"transformer = Transformer(num_layers, d_model, num_heads, dff,\n                          input_vocab_size, target_vocab_size, \n                          pe_input=input_vocab_size, \n                          pe_target=target_vocab_size,\n                          rate=dropout_rate)","d132a62e":"def create_masks(inp, tar):\n  # Encoder padding mask\n  enc_padding_mask = create_padding_mask(inp)\n  \n  # Decoder padding mask\n  dec_padding_mask = create_padding_mask(inp)\n  \n  # Look ahead mask (for hiding the rest of the sequence in the 1st decoder attention layer)\n  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n  dec_target_padding_mask = create_padding_mask(tar)\n  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n  \n  return enc_padding_mask, combined_mask, dec_padding_mask","3ab4ec5d":"checkpoint_dir = os.path.join(\"checkpoints\/phong\")\n\nprint(\"Checkpoints directory is\", checkpoint_dir)\nif os.path.exists(checkpoint_dir):\n  print(\"Checkpoints folder already exists\")\nelse:\n  print(\"Creating a checkpoints directory\")\n  os.makedirs(checkpoint_dir)\n\n\ncheckpoint = tf.train.Checkpoint(transformer=transformer,\n                           optimizer=optimizer)\n\nckpt_manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=5)","2d92def6":"latest = ckpt_manager.latest_checkpoint\nlatest","a7b0d6ba":"if latest:\n  epoch_num = int(latest.split('\/')[-1].split('-')[-1])\n  checkpoint.restore(latest)\n  print ('Latest checkpoint restored!!')\nelse:\n  epoch_num = 0","a438901b":"epoch_num","2188ca6e":"EPOCHS = 5\n\ndef train_step(inp, tar):\n  tar_inp = tar[:, :-1]\n  tar_real = tar[:, 1:]\n  \n  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n  \n  with tf.GradientTape() as tape:\n    predictions, _ = transformer(inp, tar_inp, \n                                 True, \n                                 enc_padding_mask, \n                                 combined_mask, \n                                 dec_padding_mask)\n    loss = loss_function(tar_real, predictions)\n\n  gradients = tape.gradient(loss, transformer.trainable_variables)    \n  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n  \n  train_loss(loss)\n  train_accuracy(tar_real, predictions)","c700c057":"EPOCHS = 15","e987f336":"for epoch in range(epoch_num, EPOCHS):\n  start = time.time()\n  \n  train_loss.reset_states()\n  train_accuracy.reset_states()\n  \n  # inp -> question, tar -> equation\n  for (batch, (inp, tar)) in enumerate(dataset):\n    train_step(inp, tar)\n    \n    if batch % 50 == 0:\n      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n      \n  ckpt_save_path = ckpt_manager.save()\n  print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n                                                         ckpt_save_path))\n    \n  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n                                                train_loss.result(), \n                                                train_accuracy.result()))\n\n  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))","61639e84":"def evaluate(inp_sentence):\n  start_token = [len(inp_lang_tokenizer.word_index)+1]\n  end_token = [len(inp_lang_tokenizer.word_index)+2]\n  \n  # inp sentence is the word problem, hence adding the start and end token\n  inp_sentence = start_token + [inp_lang_tokenizer.word_index[i] for i in preprocess_input(inp_sentence).split(' ')]+end_token\n  encoder_input = tf.expand_dims(inp_sentence, 0)\n  \n  # start with equation's start token\n  decoder_input = [old_len+1]\n  output = tf.expand_dims(decoder_input, 0)\n    \n  for i in range(MAX_LENGTH):\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n        encoder_input, output)\n  \n    predictions, attention_weights = transformer(encoder_input, \n                                                 output,\n                                                 False,\n                                                 enc_padding_mask,\n                                                 combined_mask,\n                                                 dec_padding_mask)\n    \n    # select the last word from the seq_len dimension\n    predictions = predictions[: ,-1:, :] \n    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n    \n    # return the result if the predicted_id is equal to the end token\n    if predicted_id == old_len+2:\n      return tf.squeeze(output, axis=0), attention_weights\n    \n    # concatentate the predicted_id to the output which is given to the decoder\n    # as its input.\n    output = tf.concat([output, predicted_id], axis=-1)\n  return tf.squeeze(output, axis=0), attention_weights","8b556c81":"def plot_attention_weights(attention, sentence, result, layer):\n  fig = plt.figure(figsize=(16, 8))\n  \n  sentence = preprocess_input(sentence)\n  \n  attention = tf.squeeze(attention[layer], axis=0)\n  \n  for head in range(attention.shape[0]):\n    ax = fig.add_subplot(2, 4, head+1)\n    \n    # plot the attention weights\n    ax.matshow(attention[head][:-1, :], cmap='viridis')\n    \n    fontdict = {'fontsize': 10}\n    \n    ax.set_xticks(range(len(sentence.split(' '))+2))\n    ax.set_yticks(range(len([targ_lang_tokenizer.index_word[i] for i in list(result.numpy()) \n                        if i < len(targ_lang_tokenizer.word_index) and i not in [0,old_len+1,old_len+2]])+3))\n    \n    \n    ax.set_ylim(len([targ_lang_tokenizer.index_word[i] for i in list(result.numpy()) \n                        if i < len(targ_lang_tokenizer.word_index) and i not in [0,old_len+1,old_len+2]]), -0.5)\n        \n    ax.set_xticklabels(\n        ['<start>']+sentence.split(' ')+['<end>'], \n        fontdict=fontdict, rotation=90)\n    \n    ax.set_yticklabels([targ_lang_tokenizer.index_word[i] for i in list(result.numpy()) \n                        if i < len(targ_lang_tokenizer.word_index) and i not in [0,old_len+1,old_len+2]], \n                       fontdict=fontdict)\n    \n    ax.set_xlabel('Head {}'.format(head+1))\n  \n  plt.tight_layout()\n  plt.show()","cce4a25d":"MAX_LENGTH = 40","7eadf460":"def translate(sentence, plot=''):\n  result, attention_weights = evaluate(sentence)\n  # print('result',list(result.numpy()))\n\n  # use the result tokens to convert prediction into a list of characters\n  # (not inclusing padding, start and end tokens)\n  predicted_sentence = [targ_lang_tokenizer.index_word[i] for i in list(result.numpy()) if (i < len(targ_lang_tokenizer.word_index) and i not in [0,46,47])]  \n\n  print('Input: {}'.format(sentence))\n  print('Predicted translation: {}'.format(' '.join(predicted_sentence)))\n  \n  if plot:\n    plot_attention_weights(attention_weights, sentence, result, plot)","3d4c1b55":"def evaluate_results(inp_sentence):\n  start_token = [len(inp_lang_tokenizer.word_index)+1]\n  end_token = [len(inp_lang_tokenizer.word_index)+2]\n  \n  # inp sentence is the word problem, hence adding the start and end token\n  inp_sentence = start_token + list(inp_sentence.numpy()[0]) + end_token\n  \n  encoder_input = tf.expand_dims(inp_sentence, 0)\n  \n  \n  decoder_input = [old_len+1]\n  output = tf.expand_dims(decoder_input, 0)\n    \n  for i in range(MAX_LENGTH):\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n        encoder_input, output)\n  \n    # predictions.shape == (batch_size, seq_len, vocab_size)\n    predictions, attention_weights = transformer(encoder_input, \n                                                 output,\n                                                 False,\n                                                 enc_padding_mask,\n                                                 combined_mask,\n                                                 dec_padding_mask)\n    \n    # select the last word from the seq_len dimension\n    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n\n    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n    \n    # return the result if the predicted_id is equal to the end token\n    if predicted_id == old_len+2:\n      return tf.squeeze(output, axis=0), attention_weights\n    \n    # concatentate the predicted_id to the output which is given to the decoder\n    # as its input.\n    output = tf.concat([output, predicted_id], axis=-1)\n\n  return tf.squeeze(output, axis=0), attention_weights","d60c2102":"dataset_val = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val)).shuffle(BUFFER_SIZE)\ndataset_val = dataset_val.batch(1, drop_remainder=True)","43181913":"y_pred = []\ny_true = []\nacc_cnt = 0\n\na = 0\nfor (inp_val_batch, target_val_batch) in iter(dataset_val):\n  a += 1\n  if a % 10 == 0:\n    print(a)\n    print(\"Accuracy count: \",acc_cnt)\n    print('------------------')\n  target_sentence = ''\n  for i in target_val_batch.numpy()[0]:\n    if i not in [0,old_len+1,old_len+2]:\n      target_sentence += (targ_lang_tokenizer.index_word[i] + ' ')\n  \n  y_true.append([target_sentence.split(' ')[:-1]])\n  \n  result, _ = evaluate_results(inp_val_batch)\n  predicted_sentence = [targ_lang_tokenizer.index_word[i] for i in list(result.numpy()) if (i < len(targ_lang_tokenizer.word_index) and i not in [0,old_len+1,old_len+2])] \n  y_pred.append(predicted_sentence)\n  \n  if target_sentence.split(' ')[:-1] == predicted_sentence:\n    acc_cnt += 1","9e14b71a":"len(y_true), len(y_pred)","b252becc":"print('Corpus BLEU score of the model: ', corpus_bleu(y_true, y_pred))","86862e3b":"print('Accuracy of the model: ', acc_cnt\/len(input_tensor_val))","9cff4a88":"check_str = ' '.join([inp_lang_tokenizer.index_word[i] for i in input_tensor_val[242] if i not in [0,\n                                                                                                  len(inp_lang_tokenizer.word_index)+1,\n                                                                                                  len(inp_lang_tokenizer.word_index)+2]])","c1af4d72":"check_str","623600b2":"translate(check_str,\n          plot='decoder_layer4_block2')","e2d24ea4":"### Create a tf.data dataset","50634921":"### Preprocessing and Tokenizing the Input and Target exps","a9a50fc1":"### Evaluate","d44d68ce":"#### Encoder","e7666162":"#### Masking","2071f0e8":"Decoder","0f379ff1":"### Transformer Model\n#### Positional Encoding","cbd29dfa":"Optimizer and Loss","02a38461":"### Checkpoints","5aebfee1":"Training","55d7dda6":"#### Encoder Layer","00cc0746":"### Creating the dataset of word problems","665e2c26":"Attention","eb3dc96f":"Transformer","beeeb8f4":"Translation","9f87dde5":"### Get Accuracy and Corpus BLEU","b4ef2535":"Decoder Layer","20a86d21":"#### Pointwise Feed forward network"}}