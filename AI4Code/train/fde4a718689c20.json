{"cell_type":{"22b735f6":"code","3b9b193f":"code","2eabc87a":"code","960ee712":"code","d097b8a7":"code","7fffa0cd":"code","cc497230":"code","e862ef6c":"code","223896b6":"code","d0d270b4":"code","0ea7816e":"code","41a433e6":"code","1b8e1206":"code","97364091":"code","a032b72a":"code","9baf4874":"code","866c95f7":"code","41c296b1":"code","bdf4bffa":"code","21c78b29":"code","ba1e4b40":"code","2a8ef2b9":"code","faecfa44":"code","24f37906":"code","a4b56210":"code","5022d853":"code","d7d0211c":"code","37924ecd":"code","6e3f8a8f":"code","85176a6a":"code","79bf8bd2":"code","4b84aeba":"code","397de3d2":"code","8d0362b1":"markdown","37941950":"markdown","a706b822":"markdown","4029eb78":"markdown","8cafa50f":"markdown","9fb57969":"markdown","a828d7da":"markdown","3a414eec":"markdown","380cbbdc":"markdown","a4b26cd9":"markdown","ebb91e4a":"markdown","4a09f1e2":"markdown","f4ee9e98":"markdown","712823b4":"markdown","00d8a8e4":"markdown","0aa02c76":"markdown","1a485a0c":"markdown","725e4845":"markdown","f5db0d72":"markdown"},"source":{"22b735f6":"## install empath libirary needed for scattertext\n!pip install empath\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport pandas as pd\nimport numpy as np \nimport seaborn as sns","3b9b193f":"train = pd.read_csv('..\/input\/train.tsv', sep=\"\\t\")\ntest = pd.read_csv('..\/input\/test.tsv', sep=\"\\t\")\nsub = pd.read_csv('..\/input\/sampleSubmission.csv', sep=\",\")","2eabc87a":"train.head(5)","960ee712":"test.head(5)","d097b8a7":"train.groupby('Sentiment').Phrase.count().plot(kind='bar')","7fffa0cd":"len(train.SentenceId.unique())","cc497230":"import scattertext as st\nimport spacy\nfrom pprint import pprint\nfrom IPython.display import IFrame\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:98% !important; }<\/style>\"))\n\ndata=train[(train['Sentiment']==0)|(train['Sentiment']==4)]\ndata['cat']=data['Sentiment'].astype(\"category\").cat.rename_categories({0:'neg',4:'pos'})","e862ef6c":"corpus = st.CorpusFromPandas(data, \n                             category_col='cat',                               \n                             text_col='Phrase',\n                             nlp=st.whitespace_nlp_with_sentences).build()","223896b6":"term_freq_df = corpus.get_term_freq_df()\nterm_freq_df['positive Score'] = corpus.get_scaled_f_scores('pos')\npprint(list(term_freq_df.sort_values(by='positive Score', \n                                      ascending=False).index[:10]))","d0d270b4":"term_freq_df['negative Score'] = corpus.get_scaled_f_scores('neg')\npprint(list(term_freq_df.sort_values(by='negative Score', \n                                      ascending=False).index[:10]))","0ea7816e":"html = st.produce_scattertext_explorer(corpus,\n         category='pos',category_name='positive',         \n        not_category_name='neg',width_in_pixels=1000,\n          metadata=data['cat'])\nopen(\"Convention-Visualization.html\", 'wb').write(html.encode('utf-8'))\nIFrame(src='Convention-Visualization.html', width = 1300, height=700)","41a433e6":"feat_builder = st.FeatsFromOnlyEmpath()\nempath_corpus = st.CorpusFromParsedDocuments(data,\n                                              category_col='cat',\n                                              feats_from_spacy_doc=feat_builder,\n                                              parsed_col='Phrase').build()\nhtml = st.produce_scattertext_explorer(empath_corpus,\n                                        category='pos',\n                                        category_name='Positive',\n                                        not_category_name='Negative',\n                                        width_in_pixels=1000,\n                                        metadata=data['cat'],\n                                        use_non_text_features=True,\n                                        use_full_doc=True,\n                                        topic_model_term_lists=feat_builder.get_top_model_term_lists())\nopen(\"Convention-Visualization-Empath.html\", 'wb').write(html.encode('utf-8'))\nIFrame(src='Convention-Visualization-Empath.html', width = 1300, height=700)","1b8e1206":"data=train[(train['Sentiment']!=1)&(train['Sentiment']!=3)]\ndata['cat']=data['Sentiment'].astype(\"category\").cat.rename_categories({0:'neg',2:'neu',4:'pos'})","97364091":"corpus = st.CorpusFromPandas(\n    data,\n    category_col='cat',\n    text_col='Phrase',\n    nlp=st.whitespace_nlp_with_sentences\n).build().get_unigram_corpus()\n\nsemiotic_square = st.SemioticSquare(\n    corpus,\n    category_a='pos',\n    category_b='neg',\n    neutral_categories=['neu'],\n    scorer=st.RankDifference(),\n    labels={'not_a_and_not_b': 'Neutral', 'a_and_b': 'Reviews'})\n\nhtml = st.produce_semiotic_square_explorer(semiotic_square,\n                                           category_name='Positive',\n                                           not_category_name='Negative',\n                                           x_label='pos-neg',\n                                           y_label='neu-Review',\n                                           neutral_category_name='neutral',\n                                           metadata=data['cat'])\n","a032b72a":"open(\"lexicalized_semiotic_squares.html\", 'wb').write(html.encode('utf-8'))\nIFrame(src='lexicalized_semiotic_squares.html', width = 1600, height=900)","9baf4874":"import re\ndef clean(text):\n    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n    return text","866c95f7":"train['Phrase'] = train['Phrase'].apply(lambda x: clean(x))\ntest['Phrase']=test['Phrase'].apply(lambda x: clean(x))","41c296b1":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nx_train, x_test, y_train, y_test = train_test_split(train['Phrase'], train['Sentiment'], train_size=0.8)\nvectorizer = TfidfVectorizer().fit(x_train)\nx_train_v = vectorizer.transform(x_train)\nx_test_v  = vectorizer.transform(x_test)","bdf4bffa":"from sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import MultinomialNB,GaussianNB\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nfrom time import time\nentries = []\ndef training():\n    models = {\n        \"LogisticRegression\": LogisticRegression(),\n        \"SGDClassifier\": SGDClassifier(),\n        \"Multinomial\":MultinomialNB(),\n        \"LinearSVC\": LinearSVC(),\n        \"GBClassifier\":GradientBoostingClassifier(n_estimators=20, learning_rate=1.0, max_depth=1, random_state=0)\n    }\n    for model in models:\n        print(\"training model\"+model)\n        start = time()\n        models[model].fit(x_train_v, y_train)\n        end = time()\n        print(\"trained in {} secs\".format(end-start))\n        y_pred = models[model].predict(x_test_v)\n        entries.append((model,accuracy_score(y_test, y_pred)))","21c78b29":"training()","ba1e4b40":"cv_df = pd.DataFrame(entries, columns=['model_name','accuracy'])\nsns.stripplot(x='model_name', y='accuracy', data=cv_df, \n              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\nplt.show()\n\n","2a8ef2b9":"corpus = st.CorpusFromScikit(\n    X=CountVectorizer(vocabulary=vectorizer.vocabulary_).fit_transform(x_test[0:1000]),\n    y=y_test[0:1000].values,\n    feature_vocabulary=vectorizer.vocabulary_,\n    category_names=['neg','som_neg','neu','som_pos','pos'],\n    raw_texts=x_test[0:1000].values\n).build()\n\nclf=LinearSVC()\nclf.fit(x_test_v,y_test)\nhtml = st.produce_frequency_explorer(\n    corpus,\n    'neg',\n    scores=clf.coef_[0],\n    use_term_significance=False,\n    terms_to_include=st.AutoTermSelector.get_selected_terms(corpus, clf.coef_[0])\n)\nfile_name = \"test_sklearn.html\"\nopen(file_name, 'wb').write(html.encode('utf-8'))\nIFrame(src=file_name, width = 1300, height=700)","faecfa44":"corpus = st.CorpusFromScikit(\n    X=CountVectorizer(vocabulary=vectorizer.vocabulary_).fit_transform(x_train[0:1000]),\n    y=y_train[0:1000].values,\n    feature_vocabulary=vectorizer.vocabulary_,\n    category_names=['neg','som_neg','neu','som_pos','pos'],\n    raw_texts=x_train[0:1000].values\n).build()\n\nclf=LinearSVC()\nclf.fit(x_train_v,y_train)\nhtml = st.produce_frequency_explorer(\n    corpus,\n    'neg',\n    scores=clf.coef_[0],\n    use_term_significance=False,\n    terms_to_include=st.AutoTermSelector.get_selected_terms(corpus, clf.coef_[0])\n)\nfile_name = \"train_sklearn.html\"\nopen(file_name, 'wb').write(html.encode('utf-8'))\nIFrame(src=file_name, width = 1300, height=700)","24f37906":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\ntokenizer = Tokenizer()\nfull_text=list(train['Phrase'].values) + list(test['Phrase'].values)\ntokenizer.fit_on_texts(full_text)\ntrain_seq = tokenizer.texts_to_sequences(train['Phrase'])\ntest_seq=tokenizer.texts_to_sequences(test['Phrase'])","a4b56210":"voc_size=len(tokenizer.word_counts)","5022d853":"m=len(max(full_text, key=len))\nX_train = pad_sequences(train_seq, maxlen = m)\nX_test = pad_sequences(test_seq, maxlen = m)","d7d0211c":"y_train=train['Sentiment'].values\nfrom sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse=False)\ny_ohe = ohe.fit_transform(y_train.reshape(-1, 1))","37924ecd":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam\n\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\n\nfile_path = \"best_model.hdf5\"\ncheck_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                              save_best_only = True, mode = \"min\")\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n\ndef build_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n    inp = Input(shape = (m,))\n    x = Embedding(19479, 300)(inp)\n    x1 = SpatialDropout1D(dr)(x)\n\n    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n    x1 = Conv1D(32, kernel_size=3, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n    max_pool1_gru = GlobalMaxPooling1D()(x1)\n    \n    x3 = Conv1D(32, kernel_size=2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool3_gru = GlobalAveragePooling1D()(x3)\n    max_pool3_gru = GlobalMaxPooling1D()(x3)\n    \n    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n    x1 = Conv1D(32, kernel_size=3, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n    \n    x3 = Conv1D(32, kernel_size=2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n    max_pool3_lstm = GlobalMaxPooling1D()(x3)\n    \n    \n    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n                    avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(Dense(128,activation='relu') (x))\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(Dense(100,activation='relu') (x))\n    x = Dense(5, activation = \"sigmoid\")(x)\n    model = Model(inputs = inp, outputs = x)\n    print(model.summary())\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 10, validation_split=0.1, \n                        verbose = 1, callbacks = [check_point, early_stop])\n    model = load_model(file_path)\n    return model,history","6e3f8a8f":"model,history = build_model(lr = 1e-4, lr_d = 0, units = 128, dr = 0.5)","85176a6a":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","79bf8bd2":"\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper right')\nplt.show()\n\n","4b84aeba":"pred = model.predict(X_test, batch_size = 1024)","397de3d2":"predictions = np.round(np.argmax(pred, axis=1)).astype(int)\nsub['Sentiment'] = predictions\nsub.to_csv(\"blend.csv\", index=False)\n","8d0362b1":"Here Linear SVC gives us 64% \n\nLets visualzie this weights as bag of words features.","37941950":"Train data","a706b822":"# IMPORTANT\n**This kernel has heavy html pages generated from ScatterText libirary, might cause the browser to take time to load please wait for the page to finish loading **","4029eb78":"i want to see how many sentence we have ","8cafa50f":"Lets look at the most positive words and most negative with respect to the scaled F score ","9fb57969":"## ML Approaches\nthere are two common ways for using text as input to machine learning algorthim Bag of words, TF-idf, here i'm goin to use TFIDF and i will try some different algorthims to see which one will gove me better accuracy.","a828d7da":"## Term Importance metrics\n**tf.idf difference** (not recommended)\n$$ \\mbox{Term Frquency}(\\mbox{term}, \\mbox{category}) = \\#(\\mbox{term}\\in\\mbox{category}) $$$$ \\mbox{Inverse Document Frquency}(\\mbox{term}) = \\log \\frac{\\mbox{# of categories}}{\\mbox{# of categories containing term}} $$$$ \\mbox{tfidf}(\\mbox{term}, \\mbox{category}) = \\mbox{Term Frquency}(\\mbox{term}, \\mbox{category}) \\times \\mbox{Inverse Document Frquency}(\\mbox{term}) $$$$ \\mbox{tfidf-difference}(\\mbox{term}, \\mbox{category}) = \\mbox{tf.idf}(\\mbox{term}, \\mbox{category}_a) - \\mbox{tf.idf}(\\mbox{term}, \\mbox{category}_b) $$\n\nTf.idf ignores terms used in each category. Since we only consider two categories (positive, negative), a large number of terms have zero (log 1) scores. The problem is Tf.idf doesn't weight how often a term is used in another category. This causes eccentric, brittle, low-frequency terms to be favored.\n\nThis formulation does take into account data from a background corpus.\n$$ \\#(\\mbox{term}, \\mbox{category}) \\times \\log \\frac{\\mbox{# of categories}}{\\mbox{# of categories containing term}} $$","3a414eec":"### Visualizing scikit-learn text classification weights\n\nlets look at what this weights interpret our test and train data","380cbbdc":"### DL Approach\nSo i have tried different approaches like LSTM,GRU,bidirectional but none of them was good enough so i will use the model mentioned in this  [kernenl](https:\/\/www.kaggle.com\/parth05rohilla\/bi-lstm-and-cnn-model-top-10#) which gives an very good results BTW ","a4b26cd9":"Test data","ebb91e4a":"### Visualizing term associations\nhere i'm making a scatter plot for the terms that associated with the (positive,negative) classes .\nEach dot corresponds to a word or phrase in the two classes. The closer a dot is to the top of the plot, the more frequently it was used in positive reviews. The further right a dot, the more that word or phrase was used in negative reviews. Words frequently used by both parties, like \"cast\" and \"film\" and even \"movie\" tend to occur in the upper-right-hand corner. Although very low frequency words have been hidden to preserve computing resources, a word that neither party used, like \"implausible\" would be in the bottom-left-hand corner.","4a09f1e2":"### Keras API \ni can make custom function that transform out text to sequence of integers but here i will use keras tokenizer api will make it much easier ","f4ee9e98":"There are some problem with harmonic means, they are dominated by the precision.\na solution for that is to take the normal CDF of precision and frequency percentage scores, which will fall between 0 and 1, which scales and standardizes both scores.\nbut i'm not going to try this approach here.","712823b4":"lets see the imbalance between classes","00d8a8e4":"## Introduction\nin this kernel i'm going to explore the movies reviews data and try some new visualization for text , i'm also going to try different machine learning and Deep learning approches for prediction","0aa02c76":"### Visualizing Empath topics and categories\n\nOften the terms of most interest are ones that are characteristic to the corpus as a whole. These are terms which occur frequently in all sets of documents being studied, but relatively infrequent compared to general term frequencies.","1a485a0c":"\n**Scaled F-Score**\n\nAssociatied terms have a relatively high category-specific precision and category-specific term frequency (i.e., % of terms in category are term)\nTake the harmonic mean of precision and frequency (both have to be high)\nWe will make two adjustments to this method in order to come up with the final formulation of Scaled F-Score\n\nGiven a word $w_i \\in W$ and a category $c_j \\in C$, define the precision of the word $w_i$ wrt to a category as: $$ \\mbox{prec}(i,j) = \\frac{\\#(w_i, c_j)}{\\sum_{c \\in C} \\#(w_i, c)}. $$\n\nThe function $\\#(w_i, c_j)$ represents either the number of times $w_i$ occurs in a document labeled with the category $c_j$ or the number of documents labeled $c_j$ which contain $w_i$.\n\nSimilarly, define the frequency a word occurs in the category as:\n$$ \\mbox{freq}(i, j) = \\frac{\\#(w_i, c_j)}{\\sum_{w \\in W} \\#(w, c_j)}. $$\n\nThe harmonic mean of these two values of these two values is defined as:\n$$ \\mathcal{H}_\\beta(i,j) = (1 + \\beta^2) \\frac{\\mbox{prec}(i,j) \\cdot \\mbox{freq}(i,j)}{\\beta^2 \\cdot \\mbox{prec}(i,j) + \\mbox{freq}(i,j)}. $$\n\n$\\beta \\in \\mathcal{R}^+$ is a scaling factor where frequency is favored if $\\beta <1$, precision if $\\beta >1$, and both are equally weighted if $\\beta = 1$. F-Score is equivalent to the harmonic mean where $\\beta = 1$.\n","725e4845":"## Resources\n[ScatterText doc](https:\/\/github.com\/JasonKessler\/scattertext)\n\n[Amazing slides for explansion to F-scaled and other ideas](https:\/\/www.slideshare.net\/JasonKessler\/turning-unstructured-content-into-kernels-of-ideas\/)","f5db0d72":"### lexicalized semiotic squares \nThe idea behind the semiotic square is to express the relationship between two opposing concepts and concepts things within a larger domain of a discourse. Examples of opposed concepts life or death, male or female, or, in our example, positive or negative sentiment. Semiotics squares are comprised of four \"corners\": the upper two corners are the opposing concepts, while the bottom corners are the negation of the concepts.\n\nCircumscribing the negation of a concept involves finding everything in the domain of discourse that isn't associated with the concept. For example, in the life-death opposition, one can consider the universe of discourse to be all animate beings, real and hypothetical. The not-alive category will cover dead things, but also hypothetical entities like fictional characters or sentient AIs."}}