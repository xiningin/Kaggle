{"cell_type":{"74c76c56":"code","ce301173":"code","c23171a9":"code","78ae7ea3":"code","590da2c9":"code","48c10f2e":"code","c7546a76":"code","723298be":"code","9aa8637f":"code","8386cad4":"code","d29ff86b":"code","71f97391":"code","e836d9c4":"code","2dafb03a":"code","f156d761":"code","a010b2f6":"code","534a5340":"code","d908b7ed":"markdown","264238ec":"markdown","664e80f0":"markdown","1717f2af":"markdown","be40b510":"markdown","ae10325d":"markdown","d488b564":"markdown","a55830ef":"markdown","60daba52":"markdown","9450f0c8":"markdown","90c66906":"markdown","b115dc4b":"markdown","99231bde":"markdown","6518b863":"markdown","5c1a0fc8":"markdown","212640c7":"markdown","259d1788":"markdown"},"source":{"74c76c56":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom bs4 import BeautifulSoup # for removing HTML tags\nimport re # removing punctuation and numbers\nimport nltk # removing stop words\nimport gensim # word vectors (word2vec)\nimport cython # speeding up training\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ce301173":"# reading in the train and test data\ntrain_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nprint(train_data.shape, test_data.shape)\nprint(train_data[\"excerpt\"][0])\ntrain_data.head()","c23171a9":"# splitting data (not using train_test_split since that would mess up the index)\nX_train = train_data[\"excerpt\"].iloc[:(train_data[\"excerpt\"].size-200)]\nX_val = train_data[\"excerpt\"].iloc[(train_data[\"excerpt\"].size-200):]\ny_train = train_data[\"target\"].iloc[:(train_data[\"target\"].size-200)]\ny_val = train_data[\"target\"].iloc[(train_data[\"target\"].size-200):]","78ae7ea3":"from nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\n\ndef excerpt_to_wordlist(excerpt, remove_stop_words=False):\n    '''Creates a clean excerpt by removing\n    HTML tags, punctuation, numbers, and stop words'''\n    # remove HTML tags\n    excerpt1 = BeautifulSoup(excerpt).get_text()\n    # remove punctuation\n    excerpt2 = re.sub(\"[^a-zA-Z0-9]\", \" \", excerpt1)\n    # removing unneccesary newlines\/spaces + lower case\n    excerpt3 = excerpt2.lower().split()\n    if remove_stop_words:\n        # removing stop words\n        stops = set(stopwords.words(\"english\")) # faster search through set than list\n        excerpt4 = [word for word in excerpt3 if word not in stops]\n        # lemmatization and stemming\n        lemmatizer = WordNetLemmatizer()\n        porterStemmer = PorterStemmer()\n        excerpt5 = [lemmatizer.lemmatize(porterStemmer.stem(word)) for \\\n                   word in excerpt4]\n        # return final review (joined by spaces)\n        return \" \".join(excerpt5)\n    else:\n        return excerpt3","590da2c9":"clean_train_excerpts = []\nclean_valid_excerpts = []\nclean_test_excerpts = []\nfor i in range(0, X_train.size):\n    clean_train_excerpts.append(excerpt_to_wordlist(X_train[i], True))\n    if (i+1)%500 == 0:\n        print(f\"{i+1} finished for train data\")\nfor i in range(X_train.size, X_train.size+X_val.size):\n    clean_valid_excerpts.append(excerpt_to_wordlist(X_val[i], True))\nfor i in range(0, test_data[\"excerpt\"].size):\n    clean_test_excerpts.append(excerpt_to_wordlist(test_data[\"excerpt\"][i], True))","48c10f2e":"from sklearn.feature_extraction.text import CountVectorizer\n# creating the vectorizer for creating the bag of words\nvectorizer = CountVectorizer(analyzer = \"word\", \\\n                             tokenizer = None, \\\n                             preprocessor = None, \\\n                             stop_words = None, \\\n                             max_features = 5000)\n\ntrain_data_features = vectorizer.fit_transform(clean_train_excerpts)\ntrain_data_features = train_data_features.toarray()\nvalid_data_features = vectorizer.transform(clean_valid_excerpts)\nvalid_data_features = valid_data_features.toarray()\ntest_data_features = vectorizer.transform(clean_test_excerpts)\ntest_data_features = test_data_features.toarray()","c7546a76":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nmodelbow1 = RandomForestRegressor(n_estimators=100, random_state=1)\nmodelbow1.fit(train_data_features, y_train)\nval_predictions_bow1 = modelbow1.predict(valid_data_features)\nprint(mean_squared_error(y_val,val_predictions_bow1))","723298be":"from xgboost import XGBRegressor\n\nmodelbow2 = XGBRegressor(n_estimators=100, learning_rate=0.005, n_jobs=3, random_state=1)\nmodelbow2.fit(train_data_features, y_train)\nval_predictions_bow2 = modelbow2.predict(valid_data_features)\nprint(mean_squared_error(y_val,val_predictions_bow2))","9aa8637f":"# Load punkt tokenizer\ntokenizer = nltk.data.load('tokenizers\/punkt\/english.pickle')\n\n# Function to split review into sentences\ndef excerpt_to_sentences(excerpt, tokenizer, remove_stopwords=False):\n    '''Creates a clean text split into sentences'''\n    # Using tokenizer to split paragraph into sentences\n    sentences1 = tokenizer.tokenize(excerpt.strip())\n    # go over each sentence\n    sentences = []\n    for sentence in sentences1:\n        if len(sentence) > 0:\n            # call review_to_wordlist\n            sentences.append(excerpt_to_wordlist(sentence, remove_stopwords))\n    return sentences","8386cad4":"sentences = []\n\n# adding train sentences in a list for training the word2vec model (not removing stop words for better model training)\nfor i in range(0, X_train.size):\n    sentences += excerpt_to_sentences(X_train[i], tokenizer)\n    if (i+1)%500 == 0:\n        print(f\"{i+1} sentences finished for train data\")\n    if (i+1) == X_train.size:\n        print(\"All done\")","d29ff86b":"# Import the built-in logging module; configure it for Word2Vec to create nice output messages (next three lines from Bag of Words meets Bags of Popcorn tutorial)\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n                    level=logging.INFO)\n\n# Setting values for parameters\nnum_features = 300     # Word vector dimensionality                      \nmin_word_count = 20    # Minimum word count                        \nnum_workers = 4        # Number of threads to run in parallel\ncontext = 20           # Context window size                                                                                    \ndownsampling = 0.0001  # Downsample setting for frequent words\n\n# initializing and training the word2vec model (number 1 from the list)\nfrom gensim.models import word2vec\nwv = word2vec.Word2Vec(sentences, workers=num_workers, vector_size=num_features, min_count=min_word_count, \\\n                        window=context, sample=downsampling)\nwv = wv.wv","71f97391":"def excerpt_to_vector(excerpt, model, num_features):\n    '''Function to average all word vectors in an excerpt'''\n    # initializing an empty np array\n    feature_vector = np.zeros((num_features,),dtype=\"float32\")\n    num_words = 0\n    # set of words in model's vocabulary\n    set_of_words = set(model.index_to_key)\n    for word in excerpt:\n        if word in set_of_words:\n            num_words += 1\n            feature_vector = np.add(feature_vector,model[word])\n    feature_vector = np.divide(feature_vector, num_words)\n    return feature_vector\n\ndef get_feature_vectors(excerpts, model, num_features):\n    '''Function to create feature vectors for all excerpts'''\n    count = 0\n    excerpt_vectors = np.zeros((len(excerpts),num_features),dtype=\"float32\")\n    for excerpt in excerpts:\n        excerpt_vectors[count] = excerpt_to_vector(excerpt, model, num_features)\n        count += 1\n        if count%1000 == 0:\n            print(f\"{count} reviews converted\")\n    return excerpt_vectors","e836d9c4":"clean_train_excerpts_vec = []\nclean_valid_excerpts_vec = []\nclean_test_excerpts_vec = []\nfor i in range(0, X_train.size):\n    clean_train_excerpts_vec.append(excerpt_to_wordlist(X_train[i]))\n    if (i+1)%500 == 0:\n        print(f\"{i+1} finished for train data\")\nfor i in range(X_train.size, X_train.size+X_val.size):\n    clean_valid_excerpts_vec.append(excerpt_to_wordlist(X_val[i]))\nfor i in range(0, test_data[\"excerpt\"].size):\n    clean_test_excerpts_vec.append(excerpt_to_wordlist(test_data[\"excerpt\"][i]))\n\ntrain_data_vectors = get_feature_vectors(clean_train_excerpts_vec, wv, num_features)\nvalid_data_vectors = get_feature_vectors(clean_valid_excerpts_vec, wv, num_features)\ntest_data_vectors = get_feature_vectors(clean_test_excerpts_vec, wv, num_features)","2dafb03a":"modelvec1 = RandomForestRegressor(n_estimators=100, random_state=1)\nmodelvec1.fit(train_data_vectors, y_train)\nval_predictions_vec1 = modelvec1.predict(valid_data_vectors)\nprint(mean_squared_error(y_val,val_predictions_vec1))","f156d761":"modelvec2 = XGBRegressor(n_estimators=100, learning_rate=0.005, n_jobs=3, random_state=1)\nmodelvec2.fit(train_data_vectors, y_train)\nval_predictions_vec2 = modelvec2.predict(valid_data_vectors)\nprint(mean_squared_error(y_val,val_predictions_vec2))","a010b2f6":"# creating final X and y\nfinal_train_features = np.concatenate([train_data_features, valid_data_features], axis=0)\nfinal_y_train = pd.concat([y_train, y_val], axis=0)","534a5340":"# final training\nfinal_model = RandomForestRegressor(n_estimators=100, random_state=1)\nfinal_model.fit(final_train_features, final_y_train)\n\n# prediction and output to csv file\npredictions = final_model.predict(test_data_features)\noutput = pd.DataFrame(data={\"id\":test_data[\"id\"], \"target\": predictions})\noutput.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file created!\")","d908b7ed":"## Models for Bag of Words\nNow, I will create the following models to test on the Bag of Words:\n1. Random Forest Regressor\n2. XGBoost Regressor","264238ec":"## Data Preprocessing for Bag of Words\nNext, I'll do the data preprocessing to get the sentences ready for a Bag of Words (also partially used later for Word2Vec model).","664e80f0":"### Random Forest Regressor","1717f2af":"## Training Models\nNow, I will train the following models for Word Embeddings and try to find the best one:\n1. Random Forest Regressor\n2. XGBoost Regressor","be40b510":"## Thank you for reading! Any feedback on the notebook would be appreciated.","ae10325d":"### XGBoost Regressor","d488b564":"## Word Embedding Model\nNow, I will be using a Word2Vec model to create the word embeddings.\n\nWord embeddings are a way of coding words into a list of numbers, and this vector effectively stores the word's information. This can help for operations such as the following one:\n> king - man + woman = queen\n\nThis should work because the relation between a king and a man is very similar to the relation between a queen and a woman.\n\nFirst, I'll create the word embedding model.","a55830ef":"### Random Forest Regressor","60daba52":"# Bag of Words and Word2Vec on CommonLit Readability Prize Competition\nIn this notebook, I will create a Bag of Words model and a Word Vectors model with Google's Word2Vec, and I will train multiple models on each.\n\nHere are the basic imports.","9450f0c8":"### XGBoost Regressor","90c66906":"## Creating Word Embeddings\nNow, I'll create the word embeddings (average over the whole excerpt).","b115dc4b":"## Data Preprocessing for Word Embeddings\nNext, I'll do the data preprocessing for word embeddings.","99231bde":"## Data Imports\nFirst, I'll import the required data.","6518b863":"It seems like RandomForestRegressor does the best for Word Embeddings. It does slightly better on the Bag of Words, so we will use the first model.","5c1a0fc8":"As seen from above, it seems that Random Forest did the best on the Bag of Words representation of the data.","212640c7":"## Final Training\n\nHere, we'll do the final training on all of the data, and we will use this for the final submission.","259d1788":"## Bag of Words Representation\nHere, I will create a Bag of Words representation for the excerpts."}}