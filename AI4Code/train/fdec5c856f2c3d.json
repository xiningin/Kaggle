{"cell_type":{"e5592110":"code","0c2a4656":"code","c278e518":"code","722bf3c5":"code","9d1e0e1e":"code","2f482487":"code","a06b8cbd":"code","0801d162":"code","1e51861c":"code","9639d763":"code","d09f39bb":"code","c9d5cb62":"code","b931f90a":"code","b971c004":"code","ede1cebd":"code","8c5a2de0":"code","151678da":"code","f9b953bb":"code","72ab4829":"code","8f6a4d9b":"code","f1b890d5":"code","acf532da":"code","8e48d6a1":"markdown","790651c3":"markdown","f1f981ee":"markdown","bd256c00":"markdown","9c1653a1":"markdown","bd7e3828":"markdown","b162fd72":"markdown","7f995c11":"markdown","a5fa33c7":"markdown","3d7bbfe4":"markdown","a98e1413":"markdown","a3240ab5":"markdown","0209dba2":"markdown","f0cc0661":"markdown","cc2c45a5":"markdown","0c2ccb9d":"markdown","8abc3592":"markdown","8afe2af6":"markdown","70a2ef20":"markdown","f9f026ab":"markdown","c90d05ff":"markdown","1b91ad70":"markdown","16debd8b":"markdown","ec39ba88":"markdown","c6f895ee":"markdown","710fc450":"markdown","37d78e99":"markdown","5cf89e77":"markdown"},"source":{"e5592110":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0c2a4656":"import matplotlib.pyplot as plt \ndf = pd.read_csv('..\/input\/timeseries\/Train.csv')\ndf.head()","c278e518":"df.tail()","722bf3c5":"#Subsetting the dataset\n#Index 11856 marks the end of year 2013\ndf = pd.read_csv('..\/input\/timeseries\/Train.csv', nrows = 11856)\n\n#Creating train and test set \n#Index 10392 marks the end of October 2013 \ntrain=df[0:10392] \ntest=df[10392:]","9d1e0e1e":"#Aggregating the dataset at daily level\ndf.Timestamp = pd.to_datetime(df.Datetime,format='%d-%m-%Y %H:%M') \ndf.index = df.Timestamp \ndf = df.resample('D').mean()\ntrain.Timestamp = pd.to_datetime(train.Datetime,format='%d-%m-%Y %H:%M') \ntrain.index = train.Timestamp \ntrain = train.resample('D').mean() \ntest.Timestamp = pd.to_datetime(test.Datetime,format='%d-%m-%Y %H:%M') \ntest.index = test.Timestamp \ntest = test.resample('D').mean()","2f482487":"train.head()","a06b8cbd":"#Plotting data\ntrain.Count.plot(figsize=(15,8), title= 'Daily Ridership', fontsize=14)\ntest.Count.plot(figsize=(15,8), title= 'Daily Ridership', fontsize=14)\nplt.show()","0801d162":"dd= np.asarray(train.Count)\ny_hat = test.copy()\ny_hat['naive'] = dd[len(dd)-1]\nplt.figure(figsize=(12,8))\nplt.plot(train.index, train['Count'], label='Train')\nplt.plot(test.index,test['Count'], label='Test')\nplt.plot(y_hat.index,y_hat['naive'], label='Naive Forecast')\nplt.legend(loc='best')\nplt.title(\"Naive Forecast\")\nplt.show()","1e51861c":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nrms = sqrt(mean_squared_error(test.Count, y_hat.naive))\nprint(rms)","9639d763":"y_hat_avg = test.copy()\ny_hat_avg['avg_forecast'] = train['Count'].mean()\nplt.figure(figsize=(12,8))\nplt.plot(train['Count'], label='Train')\nplt.plot(test['Count'], label='Test')\nplt.plot(y_hat_avg['avg_forecast'], label='Average Forecast')\nplt.legend(loc='best')\nplt.show()","d09f39bb":"rms = sqrt(mean_squared_error(test.Count, y_hat_avg.avg_forecast))\nprint(rms)","c9d5cb62":"y_hat_avg = test.copy()\ny_hat_avg['moving_avg_forecast'] = train['Count'].rolling(60).mean().iloc[-1]\nplt.figure(figsize=(16,8))\nplt.plot(train['Count'], label='Train')\nplt.plot(test['Count'], label='Test')\nplt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast')\nplt.legend(loc='best')\nplt.show()","b931f90a":"rms = sqrt(mean_squared_error(test.Count, y_hat_avg.moving_avg_forecast))\nprint(rms)","b971c004":"from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\ny_hat_avg = test.copy()\nfit2 = SimpleExpSmoothing(np.asarray(train['Count'])).fit(smoothing_level=0.6,optimized=False)\ny_hat_avg['SES'] = fit2.forecast(len(test)) # Simple Exponential Smoothing.\nplt.figure(figsize=(16,8))\nplt.plot(train['Count'], label='Train')\nplt.plot(test['Count'], label='Test')\nplt.plot(y_hat_avg['SES'], label='SES')\nplt.legend(loc='best')\nplt.show()","ede1cebd":"rms = sqrt(mean_squared_error(test.Count, y_hat_avg.SES))\nprint(rms)","8c5a2de0":"import statsmodels.api as sm\nsm.tsa.seasonal_decompose(train.Count).plot()\nresult = sm.tsa.stattools.adfuller(train.Count)\nplt.show()","151678da":"y_hat_avg = test.copy()\n\nfit1 = Holt(np.asarray(train['Count'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1)\ny_hat_avg['Holt_linear'] = fit1.forecast(len(test))\n\nplt.figure(figsize=(16,8))\nplt.plot(train['Count'], label='Train')\nplt.plot(test['Count'], label='Test')\nplt.plot(y_hat_avg['Holt_linear'], label='Holt_linear')\nplt.legend(loc='best')\nplt.show()","f9b953bb":"rms = sqrt(mean_squared_error(test.Count, y_hat_avg.Holt_linear))\nprint(rms)","72ab4829":"y_hat_avg = test.copy()\nfit1 = ExponentialSmoothing(np.asarray(train['Count']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit()\ny_hat_avg['Holt_Winter'] = fit1.forecast(len(test))\nplt.figure(figsize=(16,8))\nplt.plot( train['Count'], label='Train')\nplt.plot(test['Count'], label='Test')\nplt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter')\nplt.legend(loc='best')\nplt.show()","8f6a4d9b":"rms = sqrt(mean_squared_error(test.Count, y_hat_avg.Holt_Winter))\nprint(rms)","f1b890d5":"y_hat_avg = test.copy()\nfit1 = sm.tsa.statespace.SARIMAX(train.Count, order=(2, 1, 4),seasonal_order=(0,1,1,7)).fit()\ny_hat_avg['SARIMA'] = fit1.predict(start=\"2013-11-1\", end=\"2013-12-31\", dynamic=True)\nplt.figure(figsize=(16,8))\nplt.plot( train['Count'], label='Train')\nplt.plot(test['Count'], label='Test')\nplt.plot(y_hat_avg['SARIMA'], label='SARIMA')\nplt.legend(loc='best')\nplt.show()","acf532da":"rms = sqrt(mean_squared_error(test.Count, y_hat_avg.SARIMA))\nprint(rms)","8e48d6a1":"We are provided with a Time Series problem involving prediction of number of commuters of JetRail, a new high speed rail service by Unicorn Investors. We are provided with 2 years of data(Aug 2012-Sept 2014) and using this data we have to forecast the number of commuters for next 7 months.","790651c3":"We will now calculate RMSE to check to accuracy of our model.","f1f981ee":"Let\u2019s visualize the data (train and test together) to know how it varies over a time period.","bd256c00":"We can see that Naive method outperforms both Average method and Moving Average method for this dataset. Now we will look at Simple Exponential Smoothing method and see how it performs.","9c1653a1":"We can see that using Seasonal ARIMA generates a similar solution as of Holt\u2019s Winter.","bd7e3828":"Model                | RMSE \n---------------------|------\nNaive Method         |  43.9 \nSimple Average       | 109.5\nMoving Average       | 46.72\nSimple Exponential S.| 43.35\nHolt's Linear trend  | 43.05\nHolt's Winter        | 23.96\nARIMA                | 26.06","b162fd72":"# Method 1: Start with a Naive Approach\nForecasting technique which assumes that the next expected point is equal to the last observed point is called Naive Method.\nNow we will implement the Naive method to forecast the prices for test data.","7f995c11":"We chose the data of last 2 months only. We will now calculate RMSE to check to accuracy of our model.","a5fa33c7":"We can see that this method maps the trend accurately and hence provides a better solution when compared with above models. We can still tune the parameters to get even a better model.","3d7bbfe4":"Thank You","a98e1413":"# Method 3 \u2013 Moving Average","a3240ab5":"# Method 7 \u2013 ARIMA\nAnother common Time series model that is very popular among the Data scientists is ARIMA. It stand for Autoregressive Integrated Moving average. While exponential smoothing models were based on a description of trend and seasonality in the data, ARIMA models aim to describe the correlations in the data with each other. An improvement over ARIMA is Seasonal ARIMA. It takes into account the seasonality of dataset just like Holt\u2019 Winter method. ","0209dba2":"# Method 5 \u2013 Holt\u2019s Linear Trend method\n","f0cc0661":"# Method 6 \u2013 Holt-Winters Method","cc2c45a5":"# Method 2: \u2013 Simple Average\nForecasting technique which forecasts the expected value equal to the average of all previously observed points is called Simple Average technique.","0c2ccb9d":"We will now calculate RMSE to check to accuracy of our model on test data set.","8abc3592":"We will now calculate RMSE to check to accuracy of our model.","8afe2af6":"Holt extended simple exponential smoothing to allow forecasting of data with a trend. It is nothing more than exponential smoothing applied to both level(the average value in the series) and trend. To express this in mathematical notation we now need three equations: one for level, one for the trend and one to combine the level and trend to get the expected forecast y\u0302","70a2ef20":"We need a method that takes into account both trend and seasonality to forecast future prices. One such algorithm that we can use in such a scenario is Holt\u2019s Winter method. The idea behind triple exponential smoothing(Holt\u2019s Winter) is to apply exponential smoothing to the seasonal components in addition to level and trend.","f9f026ab":"We will now calculate RMSE to check to accuracy of our model.\n\n","c90d05ff":"We will now calculate RMSE to check to accuracy of our model.","1b91ad70":"We can see from the graph that mapping correct trend and seasonality provides a far better solution. We chose seasonal_period = 7 as data repeats itself weekly. Other parameters can be tuned as per the dataset. I have used default parameters while building this model. You can tune the parameters to achieve a better model.","16debd8b":"We can see that implementing Simple exponential model with alpha as 0.6 generates a better model till now. We can tune the parameter using the validation set to generate even a better Simple exponential model.\n","ec39ba88":"# Method 4 \u2013 Simple Exponential Smoothing","c6f895ee":"We can see that this model didn\u2019t improve our score. Hence we can infer from the score that this method works best when the average at each time period remains constant. Though the score of Naive method is better than Average method, but this does not mean that the Naive method is better than Average method on all datasets. We should move step by step to each model and confirm whether it improves our model or not.","710fc450":"We can infer from the RMSE value and the graph above, that Naive method isn\u2019t suited for datasets with high variability. It is best suited for stable datasets. We can still improve our score by adopting different techniques. Now we will look at another technique and try to improve our score.","37d78e99":"#### I\u2019m subsetting and aggregating dataset at daily basis to explain the different methods.\n\n- Subsetting the dataset from (August 2012 \u2013 Dec 2013)\n- Creating train and test file for modeling. The first 14 months (August 2012 \u2013 October 2013) are used as training data and next 2 months (Nov 2013 \u2013 Dec 2013) as testing data.\n- Aggregating the dataset at daily basis","5cf89e77":"We will now calculate RMSE to check to accuracy of our model."}}