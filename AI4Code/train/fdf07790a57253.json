{"cell_type":{"2ac86f37":"code","210cb0b3":"code","203a2ba9":"code","31671ae3":"code","60962bb9":"code","6175d8a5":"code","6bcb67f2":"code","cc1f9b7a":"code","7039853a":"code","5ac104c4":"code","c685f6c6":"code","1105061d":"code","ee29b4c0":"code","4f4abb4f":"code","9751e332":"code","79706b48":"code","8fd8154b":"code","52226e9e":"code","690c3779":"code","89a0aea5":"code","d989943d":"code","aedebc5e":"code","dd290d60":"code","18091075":"code","985d52c6":"code","ddca8c5f":"code","d8998a27":"code","d6249e2d":"code","bd9a1a19":"code","83abfb60":"code","7d7eaed9":"code","9299ea3d":"code","e8fc0e2c":"code","d31c41e3":"code","dded2837":"code","66a6178f":"code","f08df25c":"code","109ea22a":"markdown","1d0e534f":"markdown","b6a496fd":"markdown","95de9378":"markdown","4269585d":"markdown","d35dba26":"markdown","4352eea8":"markdown","075449cf":"markdown","63bec783":"markdown","06665682":"markdown","1d054a9e":"markdown","4f3d8256":"markdown","9d962172":"markdown","0d4b7d7d":"markdown","7b0b6ff6":"markdown","d8482769":"markdown","1d2262b1":"markdown","29701e14":"markdown","d110f710":"markdown","8e4af0b4":"markdown","15d8e6d2":"markdown","f7836299":"markdown","5ccda4f5":"markdown","e646e5cc":"markdown"},"source":{"2ac86f37":"# Importing libraries\n\nfrom __future__ import print_function\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report\nfrom sklearn import metrics\nfrom sklearn import tree\nimport warnings\nwarnings.filterwarnings('ignore')","210cb0b3":"PATH = '..\/input\/crop-recommendation-dataset\/Crop_recommendation.csv'\ndf = pd.read_csv(PATH)","203a2ba9":"df.head()","31671ae3":"df.tail()","60962bb9":"df.size","6175d8a5":"df.shape","6bcb67f2":"df.columns","cc1f9b7a":"df['label'].unique()","7039853a":"df.dtypes","5ac104c4":"df['label'].value_counts()","c685f6c6":"sns.heatmap(df.corr(),annot=True)","1105061d":"features = df[['N', 'P','K','temperature', 'humidity', 'ph', 'rainfall']]\ntarget = df['label']\nlabels = df['label']","ee29b4c0":"# Initializing empty lists to append all model's name and corresponding name\nacc = []\nmodel = []","4f4abb4f":"# Splitting into train and test data\n\nfrom sklearn.model_selection import train_test_split\nXtrain, Xtest, Ytrain, Ytest = train_test_split(features,target,test_size = 0.2,random_state =2)","9751e332":"from sklearn.tree import DecisionTreeClassifier\n\nDecisionTree = DecisionTreeClassifier(criterion=\"entropy\",random_state=2,max_depth=5)\n\nDecisionTree.fit(Xtrain,Ytrain)\n\npredicted_values = DecisionTree.predict(Xtest)\nx = metrics.accuracy_score(Ytest, predicted_values)\nacc.append(x)\nmodel.append('Decision Tree')\nprint(\"DecisionTrees's Accuracy is: \", x*100)\n\nprint(classification_report(Ytest,predicted_values))","79706b48":"from sklearn.model_selection import cross_val_score","8fd8154b":"# Cross validation score (Decision Tree)\nscore = cross_val_score(DecisionTree, features, target,cv=5)","52226e9e":"score","690c3779":"import pickle\n# Dump the trained Naive Bayes classifier with Pickle\nDT_pkl_filename = 'DecisionTree.pkl'\n# Open the file to save as pkl file\nDT_Model_pkl = open(DT_pkl_filename, 'wb')\npickle.dump(DecisionTree, DT_Model_pkl)\n# Close the pickle instances\nDT_Model_pkl.close()","89a0aea5":"from sklearn.naive_bayes import GaussianNB\n\nNaiveBayes = GaussianNB()\n\nNaiveBayes.fit(Xtrain,Ytrain)\n\npredicted_values = NaiveBayes.predict(Xtest)\nx = metrics.accuracy_score(Ytest, predicted_values)\nacc.append(x)\nmodel.append('Naive Bayes')\nprint(\"Naive Bayes's Accuracy is: \", x)\n\nprint(classification_report(Ytest,predicted_values))","d989943d":"# Cross validation score (NaiveBayes)\nscore = cross_val_score(NaiveBayes,features,target,cv=5)\nscore","aedebc5e":"import pickle\n# Dump the trained Naive Bayes classifier with Pickle\nNB_pkl_filename = 'NBClassifier.pkl'\n# Open the file to save as pkl file\nNB_Model_pkl = open(NB_pkl_filename, 'wb')\npickle.dump(NaiveBayes, NB_Model_pkl)\n# Close the pickle instances\nNB_Model_pkl.close()","dd290d60":"from sklearn.svm import SVC\n\nSVM = SVC(gamma='auto')\n\nSVM.fit(Xtrain,Ytrain)\n\npredicted_values = SVM.predict(Xtest)\n\nx = metrics.accuracy_score(Ytest, predicted_values)\nacc.append(x)\nmodel.append('SVM')\nprint(\"SVM's Accuracy is: \", x)\n\nprint(classification_report(Ytest,predicted_values))","18091075":"# Cross validation score (SVM)\nscore = cross_val_score(SVM,features,target,cv=5)\nscore","985d52c6":"from sklearn.linear_model import LogisticRegression\n\nLogReg = LogisticRegression(random_state=2)\n\nLogReg.fit(Xtrain,Ytrain)\n\npredicted_values = LogReg.predict(Xtest)\n\nx = metrics.accuracy_score(Ytest, predicted_values)\nacc.append(x)\nmodel.append('Logistic Regression')\nprint(\"Logistic Regression's Accuracy is: \", x)\n\nprint(classification_report(Ytest,predicted_values))","ddca8c5f":"# Cross validation score (Logistic Regression)\nscore = cross_val_score(LogReg,features,target,cv=5)\nscore","d8998a27":"import pickle\n# Dump the trained Naive Bayes classifier with Pickle\nLR_pkl_filename = 'LogisticRegression.pkl'\n# Open the file to save as pkl file\nLR_Model_pkl = open(DT_pkl_filename, 'wb')\npickle.dump(LogReg, LR_Model_pkl)\n# Close the pickle instances\nLR_Model_pkl.close()","d6249e2d":"from sklearn.ensemble import RandomForestClassifier\n\nRF = RandomForestClassifier(n_estimators=20, random_state=0)\nRF.fit(Xtrain,Ytrain)\n\npredicted_values = RF.predict(Xtest)\n\nx = metrics.accuracy_score(Ytest, predicted_values)\nacc.append(x)\nmodel.append('RF')\nprint(\"RF's Accuracy is: \", x)\n\nprint(classification_report(Ytest,predicted_values))","bd9a1a19":"# Cross validation score (Random Forest)\nscore = cross_val_score(RF,features,target,cv=5)\nscore","83abfb60":"import pickle\n# Dump the trained Naive Bayes classifier with Pickle\nRF_pkl_filename = 'RandomForest.pkl'\n# Open the file to save as pkl file\nRF_Model_pkl = open(RF_pkl_filename, 'wb')\npickle.dump(RF, RF_Model_pkl)\n# Close the pickle instances\nRF_Model_pkl.close()","7d7eaed9":"import xgboost as xgb\nXB = xgb.XGBClassifier()\nXB.fit(Xtrain,Ytrain)\n\npredicted_values = XB.predict(Xtest)\n\nx = metrics.accuracy_score(Ytest, predicted_values)\nacc.append(x)\nmodel.append('XGBoost')\nprint(\"XGBoost's Accuracy is: \", x)\n\nprint(classification_report(Ytest,predicted_values))","9299ea3d":"# Cross validation score (XGBoost)\nscore = cross_val_score(XB,features,target,cv=5)\nscore","e8fc0e2c":"import pickle\n# Dump the trained Naive Bayes classifier with Pickle\nXB_pkl_filename = 'XGBoost.pkl'\n# Open the file to save as pkl file\nXB_Model_pkl = open(XB_pkl_filename, 'wb')\npickle.dump(XB, XB_Model_pkl)\n# Close the pickle instances\nXB_Model_pkl.close()","d31c41e3":"plt.figure(figsize=[10,5],dpi = 100)\nplt.title('Accuracy Comparison')\nplt.xlabel('Accuracy')\nplt.ylabel('Algorithm')\nsns.barplot(x = acc,y = model,palette='dark')","dded2837":"accuracy_models = dict(zip(model, acc))\nfor k, v in accuracy_models.items():\n    print (k, '-->', v)","66a6178f":"data = np.array([[104,18, 30, 23.603016, 60.3, 6.7, 140.91]])\nprediction = RF.predict(data)\nprint(prediction)","f08df25c":"data = np.array([[83, 45, 60, 28, 70.3, 7.0, 150.9]])\nprediction = RF.predict(data)\nprint(prediction)","109ea22a":"![](https:\/\/images.unsplash.com\/photo-1560493676-04071c5f467b?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=968&q=80)","1d0e534f":"# Support Vector Machine (SVM)","b6a496fd":"## Accuracy Comparison","95de9378":"## Making a prediction","4269585d":"### Saving trained Decision Tree model","d35dba26":"# About the data","4352eea8":"### Saving trained Random Forest model","075449cf":"# Logistic Regression","63bec783":"# Random Forest","06665682":"### Saving trained Guassian Naive Bayes model","1d054a9e":"### **So, without further ado, Let's dive in and code ...**","4f3d8256":"Thanks for sticking till the end\n\nHope you enjoyed this notebook :)\n\nShow your appreciation by a upvote ...\n\nHappy learning !!\n\nCatch you guys on the next one\n\nPeace \u270c\ufe0f","9d962172":"# XGBoost","0d4b7d7d":"### Saving trained XGBoost model","7b0b6ff6":"### Saving trained Logistic Regression model","d8482769":"# Guassian Naive Bayes","1d2262b1":"## So here comes the fun part \ud83d\ude04\n\nI have also made a web application for this and deployed it in cloud. You can view it [here](https:\/\/harvestify.herokuapp.com\/).\n\nI have also combined a fertilizer recommendation system and a disease detection system in this project.\nYou can check my kernel of disease detection using ResNet [here](https:\/\/www.kaggle.com\/atharvaingle\/plant-disease-classification-resnet-99-2)\n\nAlso, I have made the project open source. So, feel free to suggest more improvements and submit a pull request.\n\n### Github links:\n- [Full project - [Harvestify]](https:\/\/github.com\/Gladiator07\/Harvestify) - has all the data and notebooks used for developing the application\n- [Deployed project](https:\/\/harvestify.herokuapp.com\/) - only has the code of deployed application\n\n*PS: Ignore my frontend skills :)*","29701e14":"## Motivation \ud83d\udcaa\ud83d\udcaa\n\nPrecision agirculture is in trend nowadays. Precision agriculture is a modern farming technique that uses the data of soil charachteristics, soil types, crop yield data, weather conditions and suggests the farmers with the most optimal crop to grow in their farms for maximum yield and profit. This technique can reduce the crop failures and will help the farmers to take informed decision about their farming strategy.\n\nIn order to mitigate the agrarian crisis in the current status quo, there is a need for better recommendation systems to alleviate the crisis by helping the farmers to make an informed decision before starting the cultivation of crops.","d110f710":"# CROP RECOMMENDATION SYSTEM ","8e4af0b4":"## Read this kernel till the last, there's a cool thing at the end :)","15d8e6d2":"### Seperating features and target label","f7836299":"# Decision Tree","5ccda4f5":"The data used in this project is made by augmenting and combining various publicly available datasets of India like weather, soil, etc. You can access the dataset [here](https:\/\/www.kaggle.com\/atharvaingle\/crop-recommendation-dataset). This data is relatively simple with very few but useful features unlike the complicated features affecting the yield of the crop.\n\nThe data have Nitrogen, Phosphorous, Pottasium and pH values of the soil. Also, it also contains the humidity, temperature and rainfall required for a particular crop. ","e646e5cc":"# Goal \ud83c\udfaf\n**To recommend optimum crops to be cultivated by farmers based on several parameters and help them make an informed decision before cultivation**"}}