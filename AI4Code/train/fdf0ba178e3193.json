{"cell_type":{"a60f103a":"code","ae5d00b2":"code","db84d4fa":"code","ccdc533e":"code","8c45ab1c":"code","0ca4896f":"code","75056caa":"code","351365b4":"code","dd5cfbb6":"code","b6d2a3b7":"markdown","ac839450":"markdown","3d21f937":"markdown","96547f4a":"markdown","bab39287":"markdown","83fb33d9":"markdown","8c8196b5":"markdown","886c5c0c":"markdown","0c5dc2ca":"markdown"},"source":{"a60f103a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ae5d00b2":"data = pd.read_csv(\"\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")\ndata","db84d4fa":"A = data[data[\"class\"] == \"Abnormal\"]\nN = data[data[\"class\"] == \"Normal\"]\n# scatter plot\nplt.scatter(A.pelvic_radius,A.sacral_slope,color=\"red\",label=\"kotu\")\nplt.scatter(N.pelvic_radius,N.sacral_slope,color=\"green\",label=\"iyi\")\nplt.xlabel(\"pelvic_radius\")\nplt.ylabel(\"sacral_slope\")\nplt.legend()\nplt.show()","ccdc533e":"data[\"class\"] = [1 if each == \"Abnormal\" else 0 for each  in data[\"class\"]]\ny = data[\"class\"].values\nx_data = data.drop([\"class\"],axis=1)\nprint(y)","8c45ab1c":"#normalization\nx = (x_data - np.min(x_data))\/(np.max(x_data) - np.min(x_data))","0ca4896f":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)","75056caa":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3) \nknn.fit(x_train, y_train)\nprediction = knn.predict(x_test)","351365b4":"print(\" {} nn score: {}\".format(3,knn.score(x_test, y_test)))","dd5cfbb6":"score_list = []\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train, y_train)\n    score_list.append(knn2.score(x_test, y_test))\nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","b6d2a3b7":"Let's read our data and look into it.","ac839450":"We did test-train split.\n\nNow we need look at the K closest labeled data points\nwith using classification method.","3d21f937":"We found KNN score on 3 point but we cannot know this is the best score.\n\nWe need to find best k value. Let's do it.","96547f4a":"Next station is test-train split. Let's do it.","bab39287":"So we have x and y axes which are pelvic_radius and sacral_slope.\n\nNow we need to enumerate our A and N features.","83fb33d9":"Yeap,we can see the picture. the best accuracy is between 12 and 14.","8c8196b5":"We have KNN model.\n\nTime to Print the knn score.","886c5c0c":"we need to do normalization. It is just mathematichal operation. Don't be afraid.","0c5dc2ca":"So we have information about illness.We could not understand the features but it is not problem.\nLet's name our classes."}}