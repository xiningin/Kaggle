{"cell_type":{"36b176da":"code","b5b26f23":"code","69b8d11a":"code","fe23fa7a":"code","7c1a97d1":"code","d7b1407c":"code","8c64986d":"code","deffca55":"code","b7ca80d5":"code","1a22d3a5":"code","00d59250":"code","da2e34ca":"code","430593dc":"code","989fffc8":"code","0d1d8d0e":"code","4548b9f1":"code","4089779b":"code","8c812809":"code","dc05ead4":"code","efdec040":"code","f58dda98":"code","1be62c58":"code","dcf02b3f":"code","0aa2b40b":"code","d00dbb36":"code","24336b13":"code","ee518834":"code","af70f9f2":"code","6e3ac126":"code","9bd4002c":"code","c8d2d6b9":"code","2c994d7c":"code","a72397f8":"code","cfbae097":"code","6abe13c9":"code","f0401daa":"code","69f0fe33":"code","9b8be5f1":"code","d62a7b1d":"code","91aa8563":"code","35b0d73b":"markdown","d0a79f6c":"markdown","e4763d27":"markdown","f500d69a":"markdown","5fad2a63":"markdown","38221d0c":"markdown","49e89c4b":"markdown","c3e282ab":"markdown","187e5cc6":"markdown","ab426a0f":"markdown","e69c47b7":"markdown","52df0cc6":"markdown","be71cdbd":"markdown","072f48d9":"markdown"},"source":{"36b176da":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b5b26f23":"data = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndata.head()","69b8d11a":"data.info()","fe23fa7a":"#unique values\nfor col in data.columns:\n    print(col,\": \",data[col].unique())","7c1a97d1":"#Numbers of entries for each class\nstroke = len(data[data['stroke']==1])\nno_stroke = len(data[data['stroke']==0])\nprint(\"Stroke: \",len(data[data['stroke']==1]))\nprint(\"No Stroke: \",len(data[data['stroke']==0]))","d7b1407c":"#Number of entries for each gender category\nfor gen in data['gender'].unique():\n    print(gen,\": \",len(data[data['gender']==gen]))","8c64986d":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.subplot(1,2,1)\nplt.title('Stroke=1')\nax1 = sns.countplot(x=\"gender\", data=data[data[\"stroke\"]==1])\n\nplt.subplot(1,2,2)\nplt.title('Stroke=0')\nax2 = sns.countplot(x=\"gender\", data=data[data[\"stroke\"]==0])","deffca55":"print(\"No Stroke\")\nfor gen in data[data['stroke']==0]['gender'].unique():\n    print(gen,\": \",len(data.loc[(data['gender']==gen) & (data['stroke']==0)])\/no_stroke)\n    \nprint(\"Stroke\")\nfor gen in data[data['stroke']==1]['gender'].unique():\n    print(gen,\": \",len(data.loc[(data['gender']==gen) & (data['stroke']==1)])\/stroke)","b7ca80d5":"#Hypertension\nplt.subplot(1,2,1)\nplt.title('Stroke=1')\nax1 = sns.countplot(x=\"hypertension\", data=data[data[\"stroke\"]==1])\n\nplt.subplot(1,2,2)\nplt.title('Stroke=0')\nax2 = sns.countplot(x=\"hypertension\", data=data[data[\"stroke\"]==0])","1a22d3a5":"print(\"No Stroke\")\nfor i in data[data['stroke']==0]['hypertension'].unique():\n    print(i,\": \",len(data.loc[(data['hypertension']==i) & (data['stroke']==0)])\/no_stroke)\n    \nprint(\"Stroke\")\nfor i in data[data['stroke']==1]['hypertension'].unique():\n    print(i,\": \",len(data.loc[(data['hypertension']==i) & (data['stroke']==1)])\/stroke)","00d59250":"#Heart Disease\nplt.subplot(1,2,1)\nplt.title('Stroke=1')\nax1 = sns.countplot(x=\"heart_disease\", data=data[data[\"stroke\"]==1])\n\nplt.subplot(1,2,2)\nplt.title('Stroke=0')\nax2 = sns.countplot(x=\"heart_disease\", data=data[data[\"stroke\"]==0])","da2e34ca":"print(\"No Stroke\")\nfor i in data[data['stroke']==0]['heart_disease'].unique():\n    print(i,\": \",len(data.loc[(data['heart_disease']==i) & (data['stroke']==0)])\/no_stroke)\n    \nprint(\"Stroke\")\nfor i in data[data['stroke']==1]['heart_disease'].unique():\n    print(i,\": \",len(data.loc[(data['heart_disease']==i) & (data['stroke']==1)])\/stroke)","430593dc":"#Marital Status\nplt.subplot(1,2,1)\nplt.title('Stroke=1')\nax1 = sns.countplot(x=\"ever_married\", data=data[data[\"stroke\"]==1])\n\nplt.subplot(1,2,2)\nplt.title('Stroke=0')\nax2 = sns.countplot(x=\"ever_married\", data=data[data[\"stroke\"]==0])","989fffc8":"print(\"No Stroke\")\nfor i in data[data['stroke']==0]['ever_married'].unique():\n    print(i,\": \",len(data.loc[(data['ever_married']==i) & (data['stroke']==0)])\/no_stroke)\n    \nprint(\"Stroke\")\nfor i in data[data['stroke']==1]['ever_married'].unique():\n    print(i,\": \",len(data.loc[(data['ever_married']==i) & (data['stroke']==1)])\/stroke)","0d1d8d0e":"#Work-type\nplt.subplot(1,2,1)\nplt.title('Stroke=1')\nax1 = sns.countplot(x=\"work_type\", data=data[data[\"stroke\"]==1])\n\nplt.subplot(1,2,2)\nplt.title('Stroke=0')\nax2 = sns.countplot(x=\"work_type\", data=data[data[\"stroke\"]==0])","4548b9f1":"print(\"No Stroke\")\nfor i in data[data['stroke']==0]['work_type'].unique():\n    print(i,\": \",len(data.loc[(data['work_type']==i) & (data['stroke']==0)])\/no_stroke)\n    \nprint(\"Stroke\")\nfor i in data[data['stroke']==1]['work_type'].unique():\n    print(i,\": \",len(data.loc[(data['work_type']==i) & (data['stroke']==1)])\/stroke)","4089779b":"#Residence type\nplt.subplot(1,2,1)\nplt.title('Stroke=1')\nax1 = sns.countplot(x=\"Residence_type\", data=data[data[\"stroke\"]==1])\n\nplt.subplot(1,2,2)\nplt.title('Stroke=0')\nax2 = sns.countplot(x=\"Residence_type\", data=data[data[\"stroke\"]==0])","8c812809":"print(\"No Stroke\")\nfor i in data[data['stroke']==0]['Residence_type'].unique():\n    print(i,\": \",len(data.loc[(data['Residence_type']==i) & (data['stroke']==0)])\/no_stroke)\n    \nprint(\"Stroke\")\nfor i in data[data['stroke']==1]['Residence_type'].unique():\n    print(i,\": \",len(data.loc[(data['Residence_type']==i) & (data['stroke']==1)])\/stroke)","dc05ead4":"#Smoking status\nplt.subplot(1,2,1)\nplt.title('Stroke=1')\nax1 = sns.countplot(x=\"smoking_status\", data=data[data[\"stroke\"]==1])\n\nplt.subplot(1,2,2)\nplt.title('Stroke=0')\nax2 = sns.countplot(x=\"smoking_status\", data=data[data[\"stroke\"]==0])","efdec040":"print(\"No Stroke\")\nfor i in data[data['stroke']==0]['smoking_status'].unique():\n    print(i,\": \",len(data.loc[(data['smoking_status']==i) & (data['stroke']==0)])\/no_stroke)\n    \nprint(\"Stroke\")\nfor i in data[data['stroke']==1]['smoking_status'].unique():\n    print(i,\": \",len(data.loc[(data['smoking_status']==i) & (data['stroke']==1)])\/stroke)","f58dda98":"#Age\n\nplt.subplot(1,3,1)\nn, bins, patches = plt.hist(x=data[data[\"stroke\"]==1][\"age\"], bins='auto', color='#0504aa',\n                            alpha=0.7, rwidth=0.85)\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.title('Visualizing effect of age')\nplt.text(23, 45,\"Stroke = 1\")\nmaxfreq = n.max()\nplt.ylim(ymax=np.ceil(maxfreq \/ 10) * 10 if maxfreq % 10 else maxfreq + 10)\nplt.show()\n\nplt.subplot(1,3,2)\nn, bins, patches = plt.hist(x=data[data[\"stroke\"]==0][\"age\"], bins='auto', color='#0504aa',\n                            alpha=0.7, rwidth=0.85)\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.title('Visualizing effect of age')\nplt.text(23, 75,\"Stroke = 0\")\nmaxfreq = n.max()\nplt.ylim(ymax=np.ceil(maxfreq \/ 10) * 10 if maxfreq % 10 else maxfreq + 10)\nplt.show()","1be62c58":"#Average glucose level\n\nplt.subplot(1,3,1)\nn, bins, patches = plt.hist(x=data[data[\"stroke\"]==1][\"avg_glucose_level\"], bins='auto', color='#0504aa',\n                            alpha=0.7, rwidth=0.85)\nplt.xlabel('Avg Glucose Level')\nplt.ylabel('Frequency')\nplt.title('Visualizing effect of avg glucose level')\nplt.text(23, 45,\"Stroke = 1\")\nmaxfreq = n.max()\nplt.ylim(ymax=np.ceil(maxfreq \/ 10) * 10 if maxfreq % 10 else maxfreq + 10)\nplt.show()\n\nplt.subplot(1,3,2)\nn, bins, patches = plt.hist(x=data[data[\"stroke\"]==0][\"avg_glucose_level\"], bins='auto', color='#0504aa',\n                            alpha=0.7, rwidth=0.85)\nplt.xlabel('Avg Glucose Level')\nplt.ylabel('Frequency')\nplt.title('Visualizing effect of avg glucose level')\nplt.text(23, 75,\"Stroke = 0\")\nmaxfreq = n.max()\nplt.ylim(ymax=np.ceil(maxfreq \/ 10) * 10 if maxfreq % 10 else maxfreq + 10)\nplt.show()","dcf02b3f":"#BMI\n\nplt.subplot(1,3,1)\nn, bins, patches = plt.hist(x=data[data[\"stroke\"]==1][\"bmi\"], bins='auto', color='#0504aa',\n                            alpha=0.7, rwidth=0.85)\nplt.xlabel('BMI')\nplt.ylabel('Frequency')\nplt.title('Visualizing effect of BMI')\nplt.text(23, 45,\"Stroke = 1\")\nmaxfreq = n.max()\nplt.ylim(ymax=np.ceil(maxfreq \/ 10) * 10 if maxfreq % 10 else maxfreq + 10)\nplt.show()\n\nplt.subplot(1,3,2)\nn, bins, patches = plt.hist(x=data[data[\"stroke\"]==0][\"bmi\"], bins='auto', color='#0504aa',\n                            alpha=0.7, rwidth=0.85)\nplt.xlabel('BMI')\nplt.ylabel('Frequency')\nplt.title('Visualizing effect of BMI')\nplt.text(23, 75,\"Stroke = 0\")\nmaxfreq = n.max()\nplt.ylim(ymax=np.ceil(maxfreq \/ 10) * 10 if maxfreq % 10 else maxfreq + 10)\nplt.show()","0aa2b40b":"sns.scatterplot(data = data, x ='age', y = 'bmi', hue = 'stroke')","d00dbb36":"sns.scatterplot(data = data, x ='age', y = 'avg_glucose_level', hue = 'stroke')","24336b13":"sns.scatterplot(data = data, x ='avg_glucose_level', y = 'bmi', hue = 'stroke')","ee518834":"#Encoding the categoircal features\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ncategorical_cols = ['gender','ever_married','work_type','Residence_type','smoking_status']\ndata[categorical_cols] = data[categorical_cols].apply(lambda col: le.fit_transform(col.astype(str)))\ndata.head()\n","af70f9f2":"#Searching columns with nan values\n\nprint(\"Age: \", np.any(np.isnan(data[\"age\"])))\nprint(\"Avg Glucose level: \", np.any(np.isnan(data[\"avg_glucose_level\"])))\nprint(\"BMI: \", np.any(np.isnan(data[\"bmi\"])))","6e3ac126":"print(\"Number of nan values in bmi: \",len(np.where(np.isnan(data[\"bmi\"]) == True)[0]))","9bd4002c":"data_num = data[['age','avg_glucose_level','bmi']]\nsns.heatmap(data_num.corr(), annot = True)","c8d2d6b9":"Y = data['stroke']\nX = data.drop(['stroke','id'],axis=1)\nX.head()","2c994d7c":"from sklearn.impute import KNNImputer\n\nknn_impute = KNNImputer(n_neighbors=3,weights='distance').fit(X,Y)\nX_imputed = knn_impute.transform(X)","a72397f8":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_imputed, Y, test_size=0.4)","cfbae097":"#Visualizing feature importance using decision trees\n\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\nimportance = model.feature_importances_\nfor i,v in enumerate(importance):\n\tprint('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()","6abe13c9":"from imblearn.over_sampling import SMOTENC\n\noversample = SMOTENC([0,2,3,4,5,6,9])\nX_train, y_train = oversample.fit_resample(X_train, y_train)","f0401daa":"#Verifying\n\nprint(\"Stroke: \",len(np.where(y_train == 1)[0]))\nprint(\"No Stroke: \",len(np.where(y_train == 0)[0]))","69f0fe33":"from numpy import mean\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nss = RobustScaler()\nX_train_scaled = ss.fit_transform(X_train)\n# define model\n#model = GradientBoostingClassifier()\n#parameters={'loss':('deviance', 'exponential'), 'learning_rate':[1, 0.5, 0.25, 0.1, 0.05, 0.01],\n#    'n_estimators':[16, 32, 64, 100, 200,500,1000],'max_depth':np.linspace(1, 32, 32, endpoint=True),\n#    'min_samples_split':np.linspace(0.1, 1.0, 10, endpoint=True),'min_samples_leaf':np.linspace(0.1, 0.5, 5, endpoint=True),\n#    'max_features':list(range(1,X_train.shape[1]))}\n#gridSearch = GridSearchCV(model, params,scoring='roc_auc',n_jobs=-1,cv=3,verbose=3)\n#gridSearch.fit(X_train_scaled, y_train)\n#print(gridSearch.cv_results_['params'])\n#print(gridSearch.cv_results_['mean_test_score'])","9b8be5f1":"model = GradientBoostingClassifier(loss='deviance',learning_rate=0.5,n_estimators=5500,max_depth=3,\n                                  min_samples_split=0.1,min_samples_leaf=0.01,max_features=7)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3)\nscores = cross_validate(model, X_train_scaled, y_train, scoring=('f1','roc_auc'), cv=cv, n_jobs=-1)\nprint('Mean F1 Score: %.3f' % mean(scores['test_f1']))\nprint('Mean ROC-AUC Score: %.3f' % mean(scores['test_roc_auc']))","d62a7b1d":"print(\"Stroke: \",len(np.where(y_test == 1)[0]))\nprint(\"No Stroke: \",len(np.where(y_test == 0)[0]))","91aa8563":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import plot_roc_curve\n\nX_test_scaled = ss.transform(X_test)\nmodel.fit(X_train_scaled,y_train)\ny_predict = model.predict(X_test_scaled)\n\nplot_confusion_matrix(model, X_test_scaled, y_test)  \nplt.show()  \n\nplot_roc_curve(model, X_test_scaled, y_test)  \nplt.show()         ","35b0d73b":"Since we have such scarce data for the stroke class, I have used SMOTE method to oversample the data for better training of the model.","d0a79f6c":"Given that we have much more data for the class \"no stroke\", we can say that the dataset is highly imbalanced. ","e4763d27":"Considering a large number of missing values in BMI, we need to come up with an efficient method for imputing missing values. ","f500d69a":"People living in urban areas have a higher chance of stroke compared to rural. ","5fad2a63":"Stroke is more prevalent in older people, hence age can be a good feature for classification. ","38221d0c":"People formerly smoked and never smoked have a higher chance of stroke than people who smokes or unkown.","49e89c4b":"The data has high percentage of people not having hypertension, although people having hypertension has a slightly higher chance of having stroke.","c3e282ab":"As we can infer, BMI is not having high correlation with any of the two numerical features.\n\nWe will use K-NearestNeighbor to compute the missing values in BMI","187e5cc6":"Splitting the data between training and test sets with test size of 0.4","ab426a0f":"Data Exploration","e69c47b7":"There is a higher percentage of married people, although married people have a higher chance of stroke.","52df0cc6":"Children and people who have never worked have a much lower chance of having a stroke. Again data is dominated by one category, people doing private jobs who have almost equal probability of having and not having a stroke. Self-employed people have a higher chances of stroke. ","be71cdbd":"There is only one entry for other gender with no stroke. Comparing males and females, we can see there is a high percentage of females in the data. We cannot infer anything based on gender since females are having high number both for stroke and no stroke.","072f48d9":"Again most of the people are not having heart_disease, but people having heart disease have slightly higher chance of having a stroke."}}