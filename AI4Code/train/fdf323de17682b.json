{"cell_type":{"246985cc":"code","4fe12c01":"code","186bbf3d":"code","c9c43ece":"code","a03f0f32":"code","2b03a95a":"code","27b449e8":"code","cefec66b":"code","9dd057a5":"code","ac928e94":"code","4cbe4c7d":"code","907dcae8":"code","fbaf51fa":"code","72218aae":"code","8c4b7450":"code","6f96ab54":"code","e7dc0598":"code","50f30245":"code","cdd9eef3":"code","e63a0cf5":"code","04d43963":"code","e7422314":"code","780cfb9f":"markdown","ed43c8d5":"markdown","fba12c0d":"markdown","a1b7a80e":"markdown","975473af":"markdown","73eb8b70":"markdown","38fae199":"markdown","a92ad322":"markdown","df743bde":"markdown","4858d8af":"markdown","1a836eaf":"markdown","1cedc809":"markdown"},"source":{"246985cc":"import numpy as np\n#from sklearn.datasets import make_regression\nimport matplotlib.pyplot as plt","4fe12c01":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","186bbf3d":"import pandas as pd\n\ndf = pd.read_csv(os.path.join(dirname, filename))","c9c43ece":"df.head()","a03f0f32":"# Conversion du dataset du DataFrame en tableaux NumPy\nx = np.array(df['Year'])\ny = np.array(df['Transistors']) # Pour Nearby Galaxy Catalog","2b03a95a":"# Echelle logarithmique sur l'axe y\ny = np.log(y)\n# D\u00e9calage de l'origine en x\nx = x - 1970","27b449e8":"plt.scatter(x,y)","cefec66b":"print(x.shape)\nprint(y.shape)\n# redimensionner x\nx = x.reshape(x.shape[0], 1)\n\n# redimensionner y\ny = y.reshape(y.shape[0], 1)\nprint(x.shape)\nprint(y.shape)","9dd057a5":"X = np.hstack((x, np.ones(x.shape)))\nprint(X.shape)","ac928e94":"np.random.seed(0) # pour produire toujours le meme vecteur theta al\u00e9atoire\ntheta = np.random.randn(2, 1)\ntheta","4cbe4c7d":"def model(X, theta):\n    return X.dot(theta)","907dcae8":"plt.scatter(x, y)\nplt.plot(x, model(X, theta), c='r')","fbaf51fa":"def cost_function(X, y, theta):\n    m = len(y)\n    return 1\/(2*m) * np.sum((model(X, theta) - y)**2)","72218aae":"cost_function(X, y, theta)","8c4b7450":"def grad(X, y, theta):\n    m = len(y)\n    return 1\/m * X.T.dot(model(X, theta) - y)","6f96ab54":"def gradient_descent(X, y, theta, learning_rate, n_iterations):\n    \n    cost_history = np.zeros(n_iterations) # cr\u00e9ation d'un tableau de stockage pour enregistrer l'\u00e9volution du Cout du modele\n    \n    for i in range(0, n_iterations):\n        theta = theta - learning_rate * grad(X, y, theta) # mise a jour du parametre theta (formule du gradient descent)\n        cost_history[i] = cost_function(X, y, theta) # on enregistre la valeur du Cout au tour i dans cost_history[i]\n        \n    return theta, cost_history","e7dc0598":"np.random.seed(0) # pour produire toujours le meme vecteur theta al\u00e9atoire\ntheta = np.random.randn(2, 1)\nn_iterations = 500\nlearning_rate = 0.00001\n\n\ntheta_final, cost_history = gradient_descent(X, y, theta, learning_rate, n_iterations)","50f30245":"theta_final # voici les parametres du modele une fois que la machine a \u00e9t\u00e9 entrain\u00e9e","cdd9eef3":"# cr\u00e9ation d'un vecteur pr\u00e9dictions qui contient les pr\u00e9dictions de notre modele final\npredictions = model(X, theta_final)\n\n# Affiche les r\u00e9sultats de pr\u00e9dictions (en rouge) par rapport a notre Dataset (en bleu)\nplt.scatter(x, y)\nplt.plot(x, predictions, c='r')","e63a0cf5":"plt.plot(range(n_iterations), cost_history)","04d43963":"def coef_determination(y, pred):\n    u = ((y - pred)**2).sum()\n    v = ((y - y.mean())**2).sum()\n    return 1 - u\/v","e7422314":"coef_determination(y, predictions)","780cfb9f":"# 1. Dataset","ed43c8d5":"Finalement, cr\u00e9ation d'un vecteur parametre $\\theta$, initialis\u00e9 avec des coefficients al\u00e9atoires. Ce vecteur est de dimension (2, 1). Si on d\u00e9sire toujours reproduire le meme vecteur $\\theta$, on utilise comme avant np.random.seed(0).","fba12c0d":"Important: v\u00e9rifier les dimensions de x et y. On remarque que y n'a pas les dimensions (100, 1). On corrige le probleme avec np.reshape","a1b7a80e":"# 4. Gradients et Descente de Gradient\nOn impl\u00e9mente la formule du gradient pour la **MSE**\n\n$\\frac{\\partial J(\\theta) }{\\partial \\theta} = \\frac{1}{m} X^T.(X.\\theta - y)$\n\nEnsuite on utilise cette fonction dans la descente de gradient:\n\n$\\theta = \\theta - \\alpha \\frac{\\partial J(\\theta) }{\\partial \\theta}$\n","975473af":"# 7. Evaluation finale\nPour \u00e9valuer la r\u00e9elle performance de notre modele avec une m\u00e9trique populaire (pour votre patron, client, ou vos collegues) on peut utiliser le **coefficient de d\u00e9termination**, aussi connu sous le nom $R^2$. Il nous vient de la m\u00e9thode des moindres carr\u00e9s. Plus le r\u00e9sultat est proche de 1, meilleur est votre modele","73eb8b70":"# 5. Phase d'entrainement\nOn d\u00e9finit un **nombre d'it\u00e9rations**, ainsi qu'un **pas d'apprentissage $\\alpha$**, et c'est partit !\n\nUne fois le modele entrain\u00e9, on observe les resultats par rapport a notre Dataset","38fae199":"# R\u00e9gression Lin\u00e9aire Simple Numpy\n\nGuillaume Saint-Cirgue\nhttps:\/\/machinelearnia.com\/\n\nYoutube: coming soon\n\narticle associ\u00e9: coming soon","a92ad322":"Merci d'avoir suivi ce tutoriel. Abonnez-vous a ma chaine youtube pour ne pas louper d'autres tutos (chaque semaine de nouvelles vid\u00e9os!) https:\/\/www.youtube.com\/channel\/UCmpptkXu8iIFe6kfDK5o7VQ","df743bde":"# 3. Fonction Cout :  Erreur Quadratique moyenne\nOn mesure les erreurs du modele sur le Dataset X, y en impl\u00e9menterl'erreur quadratique moyenne,  **Mean Squared Error (MSE)** en anglais.\n\n$ J(\\theta) = \\frac{1}{2m} \\sum (X.\\theta - y)^2 $\n\nEnsuite, on teste notre fonction, pour voir s'il n'y a pas de bug","4858d8af":"# 6. Courbes d'apprentissage\nPour v\u00e9rifier si notre algorithme de Descente de gradient a bien fonctionn\u00e9, on observe l'\u00e9volution de la fonction cout a travers les it\u00e9rations. On est sens\u00e9 obtenir une courbe qui diminue a chaque it\u00e9ration jusqu'a stagner a un niveau minimal (proche de z\u00e9ro). Si la courbe ne suit pas ce motif, alors le pas **learning_rate** est peut-etre trop \u00e9lev\u00e9, il faut prendre un pas plus faible.","1a836eaf":"Cr\u00e9ation de la matrice X qui contient la colonne de Biais. Pour ca, on colle l'un contre l'autre le vecteur x et un vecteur 1 (avec np.ones) de dimension \u00e9gale a celle de x","1cedc809":"# 2. Modele Lin\u00e9aire\nOn impl\u00e9mente un modele $F = X.\\theta$, puis on teste le modele pour voir s'il n'y a pas de bug (bonne pratique oblige). En plus, cela permet de voir a quoi ressemble le modele initial, d\u00e9fini par la valeur de $\\theta$"}}