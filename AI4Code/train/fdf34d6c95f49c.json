{"cell_type":{"63b1e196":"code","38361561":"code","87572434":"code","80b81f01":"code","4b1b9f10":"code","d4312444":"code","f5c1306e":"code","3f87f3ef":"code","be341d1b":"markdown","ccd98662":"markdown","66b679a2":"markdown","c8e3a7e5":"markdown","4ae65d89":"markdown","5f1be901":"markdown","dfa7d483":"markdown","455a7040":"markdown","73459378":"markdown","a93874e0":"markdown","e1b0b08f":"markdown","e64796b9":"markdown"},"source":{"63b1e196":"import csv\nimport os\nimport re\nimport string\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom mlxtend.frequent_patterns import fpgrowth\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer","38361561":"class LemmaTokenizer(object):\n\n    def __init__(self):\n        self.lemmatizer = WordNetLemmatizer()\n\n    def __call__(self, document):\n        lemmas = []\n        \n        # Pre-proccessing of one document at the time\n        \n        # Removing puntuation\n        translator_1 = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n        document = document.translate(translator_1)\n\n        # Removing numbers\n        document = re.sub(r'\\d+', ' ', document)\n\n        # Removing special characters\n        document = re.sub(r\"[^a-zA-Z0-9]+\", ' ', document)\n\n        # The document is a string up to now, after word_tokenize(document) we'll work on every word one at the time\n        for token in word_tokenize(document):\n            \n            # Removing spaces\n            token = token.strip()\n            \n            # Lemmatizing\n            token = self.lemmatizer.lemmatize(token)\n\n            # Removing stopwords\n            if token not in stopwords and len(token) > 2:\n                lemmas.append(token)\n        return lemmas","87572434":"def generate_wordclouds(X, in_X_tfidf, k, in_word_positions):\n\n    # Clustering\n    in_model = KMeans(n_clusters=k, random_state=42, n_jobs=-1)\n    in_y_pred = in_model.fit_predict(X)\n    in_cluster_ids = set(in_y_pred)\n    silhouette_avg = silhouette_score(X, in_y_pred)\n    print(\"For n_clusters =\", k, \"The average silhouette_score is :\", silhouette_avg)\n\n    # Number of words with highest tfidf score to display\n    top_count = 100\n\n    for in_cluster_id in in_cluster_ids:\n        # compute the total tfidf for each term in the cluster\n        in_tfidf = in_X_tfidf[in_y_pred == in_cluster_id]\n        # numpy.matrix\n        tfidf_sum = np.sum(in_tfidf, axis=0)\n        # numpy.array of shape (1, X.shape[1])\n        tfidf_sum = np.asarray(tfidf_sum).reshape(-1)\n        top_indices = tfidf_sum.argsort()[-top_count:]\n        term_weights = {in_word_positions[in_idx]: tfidf_sum[in_idx] for in_idx in top_indices}\n        wc = WordCloud(width=1200, height=800, background_color=\"white\")\n        wordcloud = wc.generate_from_frequencies(term_weights)\n        fig, ax = plt.subplots(figsize=(10, 6), dpi=100)\n        ax.imshow(wordcloud, interpolation='bilinear')\n        ax.axis(\"off\")\n        fig.suptitle(f\"Cluster {in_cluster_id}\")\n        plt.show()\n\n    return in_cluster_ids\n\n\ndef custom_import_stopwords(filename):\n    in_stopword_list = []\n    in_flag = 0\n    in_word_cnt = 0\n\n    with open(filename, encoding=\"utf8\") as f:\n        for row in csv.reader(f):\n            if in_flag == 0:\n                in_flag = 1\n            else:\n                in_stopword_list.append(row[0])\n                in_word_cnt += 1\n\n    print(f\"{in_word_cnt} stopwords imported\")\n    return in_stopword_list","80b81f01":"in_dir = \"..\/input\/tnewsgroups\/T-newsgroups\"\nfiles = sorted(os.listdir(in_dir), key=int)\nfiles\n\nstopwords = custom_import_stopwords('..\/input\/stopwords\/english_stopwords.csv')\n\n# Custom tokenizer for tfidf representation\nvectorizer = TfidfVectorizer(input='filename', tokenizer=LemmaTokenizer())\n\n# Here we need the correct path in order to give it to the vectorizer\nprint(\"Generating TFIDF sparse matrix...\")\nfilepath = [os.path.join(in_dir, f) for f in files]\nX_tfidf = vectorizer.fit_transform(filepath)","4b1b9f10":"# Dimensionality reduction\nsvd = TruncatedSVD(n_components=30, random_state=42)\nX_svd = svd.fit_transform(X_tfidf)\nprint(f\"Total variance explained: {np.sum(svd.explained_variance_ratio_):.2f}\")\n# The variance explained is quite low for real applications. We will investigate it later.","d4312444":"# Trials, uncomment the line to start clustering but mind the parameter for the number of clusters\nword_positions = {v: k for k, v in vectorizer.vocabulary_.items()}\n# cluster_ids = generate_wordclouds(X_svd, X_tfidf, 2, word_positions)\n# cluster_ids = generate_wordclouds(X_svd, X_tfidf, 3, word_positions)\n# cluster_ids = generate_wordclouds(X_svd, X_tfidf, 4, word_positions)\n# cluster_ids = generate_wordclouds(X_svd, X_tfidf, 5, word_positions)\n# cluster_ids = generate_wordclouds(X_svd, X_tfidf, 6, word_positions)","f5c1306e":"model = KMeans(n_clusters=4, random_state=42, n_jobs=-1)\ny_pred = model.fit_predict(X_svd)\ncluster_ids = set(y_pred)\n\nmin_support = 0.3\ndist_words = sorted(v for k, v in word_positions.items()) # distinct words in the vocabulary\nfor cluster_id in cluster_ids:\n    print(f\"FP-Growth results on Cluster {cluster_id} with min support {min_support}\")\n    tfidf = X_tfidf[y_pred == cluster_id]\n    # encoded as binary \"presence\/absence\" representation as required by mlxtend\n    tfidf[tfidf > 0] = 1\n    # df is a pandas sparse dataframe\n    df = pd.DataFrame.sparse.from_spmatrix(tfidf, columns=dist_words)\n    fset = fpgrowth(df, min_support=min_support, use_colnames=True).sort_values(by='support', ascending=False)\n    print(fset, '\\n')","3f87f3ef":"svd = TruncatedSVD(n_components=2000, random_state=42)\nX_svd = svd.fit_transform(X_tfidf)\n\ncum_variance = np.cumsum(svd.explained_variance_ratio_)\nidx = np.argmax(cum_variance > .8)\nsvd = TruncatedSVD(n_components=idx, random_state=42)\nX_svd = svd.fit_transform(X_tfidf)\n_ = generate_wordclouds(X_svd, X_tfidf, 4, word_positions)","be341d1b":"Here are reported word sets with support at least above 30% sorted by support.\nAs you can see, compared to a wordcloud visualization, cluster topics are harder to identify.\nNonetheless, with our prior knowledge coming from wordclouds, we can spot some key words like:\n- Cluster 0 -> \u201cspace\u201d \n- Cluster 1 -> \u201cteam\u201d, \u201cbaseball\u201d, \u201cgame\u201d\n- Cluster 2 -> \u201cgovernment\u201d, \u201cpeople\u201d","ccd98662":"Setup 1 of 3\n\nFirst we import all the usefull libraries we will use in the notebook","66b679a2":"As you can see, we are dealing with about 40k different features. These are all the distinct words found in our corpus (i.e. the collection of our news). We can transform and reduce our input feature space through the **Principal Component Analysis (PCA)**.\n\nLet\u2019s focus on PCA applied by means of the **Singular Value Decomposition (SVD)**.\nEach component extracted by the decomposition will express a given amount of the variance of our data.\nIn the best case, all the variance is expressed by a low number of new features.\nThe TruncatedSVD class let us decide how many components we want to retain in the new space.\nAdditionally, it provides you the variance those components encode with some inner attributes.\n\nLet\u2019s try with a low number of features, e.g. 30, and use them to cluster our points.\n\n**Please remember that the SVD algebric method transforms the initial space. Hence, you will not end up with the 30 words the best separate your data. Instead, you will have new components (i.e. projections of your points) in a new 30-dimensional space.**\n\nThe fit_transform method performs the decomposition and maps the input data to the new vector space.","c8e3a7e5":"The outcome is pretty much similar to the one achieved with the first SVD transformation. We can still identify three well-defined clusters, plus a noisier one. Even if the explained variance was significantly increased, this result tells us that the final descriptive capability of our model is fairly robust to the retained explained variance.\n\nIndeed, further improvements should rather be searched in different steps of the pipeline, such as the TFIDF generation (e.g. the choice of the stopwords, the use of thresholds for the maximum and minimum document frequency) or the clustering algorithm.","4ae65d89":"What if we wouldn't reduce dimensionality?\n\nEvery previous result was generated with a transformed dataset by means of the Truncated SVD.\nHowever, we skipped over the discussion on the number of components.\nActually, our new representation expresses only a small percentage of the explained variance, which is an extremely low number in almost every real-world case.\n\nWith this in mind, we can enhance the truncated representation and then explore the results with the best configuration so far (K = 4).\n\nThe choice of the number of components which span the new vector space is typically a matter of the amount of explained variance desired and varies from application to application.\nTo sufficiently distantiate from previous results, let\u2019s see how clustering behaves when we retain 80% of the explained variance.","5f1be901":"Here is where it all starts..\n\nWe first import the corpus of articles and the stopwords from the input folder.\n\nNote: os.listdir returns filenames in random order. To tackle that, we can just sort them using their integer cast as ordering criteria. Then we are able to proceed in making the tfidf sparse matrix through TfidfVectorizer.","dfa7d483":"Setup 2 of 3\n\nWe now define a custom tokenizer for **TfidfVectorizer** which is the function we will use to trasform the document in tfidf representation.\nBasically when we will call **TfidfVectorizer(input='filename', tokenizer=LemmaTokenizer())** on every document (code cell 4), one document at the time will go through this LemmaTokenizer class where it will be processed as you like. \n\nNote: pre-processing is very important for performance!","455a7040":"# TEXT DOCUMENTS CLUSTERING TUTORIAL USING TFIDF, PCA, FP-GROWTH AND KMEANS\n\nFocus:\nHi, in this tutorial notebook we'll take a look on how to divide in clusters text documents, specifically we will try to detect topics out of a set of real-world news data, the famous **T-newsgroups** dataset.\nThen, we will describe each cluster through frequent itemset mining.\n\nCaveat:\nWe will **focus on implementation** rather then performance. Better score can be achieved through a better preprocessing and algorithm's choice.\n\nAbout the dataset:\nThe 20 Newsgroups dataset was originally collected in Lang 1995. It includes approximately 20,000 documents, partitioned across 20 different newsgroups, each corresponding to a different topic.\nFor the sake of this laboratory, we chose T \u2264 20 topics and sampled uniformly only documents belonging to them.\nAs a consequence, you have K \u2264 20, 000 documents uniformly distributed across T different topics.\nYou can download the dataset at:\nhttps:\/\/github.com\/dbdmg\/data-science-lab\/blob\/master\/datasets\/T-newsgroups.zip?raw=true\nEach document is located in a different file, which contains the raw text of the news. The name of the file in an integer number and corresponds to its ID.","73459378":"To extend the study, we can now analyze the most frequent sets of words in each cluster separately, just as we did with wordclouds.\nWe will use the mlxtend library.\n\nGiven the dimensions in play, we prefer the **FP-Growth algorithm** over Apriori, since the former does not require to create the candidate sets explicitly. Note that we will use pandas here and, specifically, its sparse data structures to reduce the memory penalty.","a93874e0":"The overall clustering outcome with K = 5 seems worse to K = 4. Cluster 1 and Cluster 3 contain mainly noise.\nThat could advise us that we are moving away from the right number of topics present in the original dataset. However, let\u2019s take it one step further and try with K = 6.\n\nThe outcome looks pretty much like the one with K = 5: we ended up with two well-defined clusters, one a bit blurred but still identifiable and noise for the rest of them. Since no improvement was achieved in the last two K values, we can decide to stop the analysis here and select K = 4 for the next steps.\n","e1b0b08f":"Setup 3 of 3\n\nHere we define two functions:\n- the first clusters the tfidf representation matrix of the corpus, computes silhouette score and generates the wordcloud of the 100 word with tfidf score in descending order for each cluster.\n- The second imports the english stopwords from a file in the input folder.","e64796b9":"Now that we have an initial vector representation, we can use it to cluster our documents. We will focus on the K-means algorithm with K starting from 2 and increasing. Next, we can choose the K value that better helps us to spot topics in the final clusters. Doing so, we are impersonating a domain expert that analyzes and evaluates the final subdivision. Remember that, in our context, one clear topic per cluster is the desired outcome.\n\nOne simple approach to describe the clusters would exploit wordcloud images. These \u201cclouds\u201d include the most important words in collection of texts for a certain metric. The more a word is significant the bigger it appears in the picture. Fortunately, we already have a \u201csignificance\u201d metric: the TFIDF weight of each word. Let\u2019s exploit its very meaning and choose it as the \u201cimportance\u201d metric for the wordcloud."}}