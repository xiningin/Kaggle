{"cell_type":{"b1232ccc":"code","72cb3ee4":"code","b3cac2d0":"code","0b4a22e4":"code","683db9b6":"code","b7ef46c2":"code","19ffc9dc":"code","48b3176e":"code","ddf4c5b7":"code","5316fc4f":"code","a4aa0f3b":"code","c96eceeb":"code","f46a48a7":"code","24005410":"code","40726cd5":"code","f2e26701":"code","72b8df05":"code","a5ce81e4":"code","83c2607b":"code","a64194e7":"code","c1478510":"code","22fd6712":"code","d6b508c8":"code","2ec4f3d8":"code","90472709":"code","18c538b5":"code","2eb15c03":"code","3e5937ba":"code","5c55734d":"code","832d7bf3":"code","7e006840":"code","ceec500d":"code","efa4265a":"code","90fb6fe6":"code","d274e5b7":"code","f228ce0d":"code","f37c0c54":"code","a25af0f5":"code","a6ed4f5d":"code","8bef0c2c":"code","5582bafa":"code","fadb3669":"markdown","afe17774":"markdown","32e06535":"markdown","0f67fd54":"markdown","079273dd":"markdown","317544e6":"markdown","921ccd3b":"markdown","d1154296":"markdown","d73499e2":"markdown","4d82f60b":"markdown","03c4807d":"markdown","3e6ecb9b":"markdown","e1680e47":"markdown","8fe5b366":"markdown","dc2df970":"markdown","22925f92":"markdown","fa0cb81a":"markdown","039bf41e":"markdown","a0c2f48e":"markdown","fa5379aa":"markdown"},"source":{"b1232ccc":"# Importing required Libraries \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","72cb3ee4":"# Walking into dir to find files\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b3cac2d0":"# Getting data\ntrain_data= pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data= pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","0b4a22e4":"print(train_data.shape)\nprint()\nprint(train_data.info())","683db9b6":"sns.heatmap(data= train_data.isnull(),cbar= False,yticklabels= False,cmap = 'viridis')","b7ef46c2":"train_data.isnull().sum()","19ffc9dc":"print(test_data.shape)\nprint()\nprint(test_data.info())","48b3176e":"sns.heatmap(data= test_data.isnull(),cbar= False,yticklabels= False,cmap = 'viridis')","ddf4c5b7":"test_data.isnull().sum()","5316fc4f":"train_data= train_data.drop('Cabin', axis=1)\ntrain_data= train_data.dropna(subset= ['Age', 'Embarked'])","a4aa0f3b":"train_data.head()","c96eceeb":"print(train_data.shape)\nprint()\nprint(train_data.info())","f46a48a7":"test_data= test_data.drop('Cabin', axis=1)","24005410":"train_data.describe(include='all')","40726cd5":"plt.figure(figsize=(6,4))\nsns.heatmap(train_data.corr(),cmap='Blues',annot=False)","f2e26701":"# Survived correlation matrix\nk = 11 #number of variables for heatmap\ncols = train_data.corr().nlargest(k, 'Survived')['Survived'].index\ncm = train_data[cols].corr()\nplt.figure(figsize=(10,6))\nsns.heatmap(cm, annot=True, cmap = 'viridis')","72b8df05":"num_data= pd.DataFrame(train_data[['PassengerId', 'Survived', 'Pclass','Age','Fare','Parch','SibSp']])\nl = num_data.columns.values\nnumber_of_columns=7\nnumber_of_rows = len(l)-1\/number_of_columns\nplt.figure(figsize=(15,20))\nfor i in range(0,len(l)):\n    plt.subplot(4,2,i+1)\n    sns.set_style('whitegrid')\n    sns.boxplot(num_data[l[i]],color= 'green',orient=\"v\")\n    plt.tight_layout()","a5ce81e4":"plt.figure(figsize=(20,25))\nfor i in range(0,len(l)):\n    plt.subplot(4,2,i+1)\n    sns.distplot(train_data[l[i]],kde=True) ","83c2607b":"# Feature Engineering \n\n# Categorizing Age into child and adults\nAge= pd.cut(train_data['Age'], [0,18,80]) \n# Creating new category Title from Name\ntrain_data['Title']= train_data['Name'].apply(lambda x: x.split(', ')[1].split('. ')[0].strip())\nprint(train_data['Title'].unique())\n\n#As we have data of relationship(s) of an individual we can sum-up realationships and Categorize them as Family  \ntrain_data['Family_Size']= train_data['SibSp'] + train_data['Parch'] + 1\n\n#Function to Categorize Family Size\ndef FamSize_cat(size):\n    category= \"\"\n    if(size<=1):\n        category= \"alone\"\n    elif(size<=4):\n        category= \"small\"\n    else:\n        category= \"large\"\n    return category\n\ntrain_data[\"FamSize_Cat\"]= train_data[\"Family_Size\"].map(FamSize_cat)\n","a64194e7":"feature_data= pd.DataFrame(train_data[['Survived', 'Sex', 'Embarked', 'Title', 'Family_Size','FamSize_Cat']])\nfig=plt.figure(figsize=(20,45))\nbackground_color = '#f6f5f7'\nfig.patch.set_facecolor(background_color) \nfor indx,val in enumerate(feature_data.columns):\n    ax=plt.subplot(4,2,indx+1)\n    ax.set_facecolor(background_color)\n    ax.set_title(val,fontweight='bold',fontfamily='serif')\n    for i in ['top','right']:\n        ax.spines[i].set_visible(False)\n    ax.grid(linestyle=':',axis='y')\n    sns.countplot(feature_data[val],palette='turbo')\n    plt.setp(ax.get_xticklabels(), rotation=90)","c1478510":"fig=plt.figure(figsize=(20,45))\nbackground_color = '#f6f5f7'\nfig.patch.set_facecolor(background_color) \nfor indx,val in enumerate(feature_data.columns):\n    ax=plt.subplot(4,2,indx+1)\n    ax.set_facecolor(background_color)\n    ax.set_title(val,fontweight='bold',fontfamily='serif')\n    for i in ['top','right']:\n        ax.spines[i].set_visible(False)\n    ax.grid(linestyle=':',axis='y')\n    sns.countplot(feature_data[val],hue=train_data['Survived'],palette='turbo')\n    plt.setp(ax.get_xticklabels(), rotation=90)","22fd6712":"plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.scatterplot(data=train_data,x=train_data['Age'],y=train_data['Fare'],hue=train_data['Survived'],palette=\"BuPu\")\nplt.title('Scatterplot for age vs fare',fontdict={'fontweight': 'bold', 'fontfamily':'serif' })\nplt.subplot(1,2,2)\nsns.scatterplot(data=train_data,x=train_data['Fare'],y=train_data['Age'],hue=train_data['Survived'],palette=\"BuPu\")\nplt.title('Scatterplot for fare vs age',fontdict={'fontweight': 'bold', 'fontfamily':'serif' })\nplt.show()","d6b508c8":"#pivot without Age\npivot1= train_data.pivot_table('Survived',index=['Sex'],columns=['Embarked','Pclass'])\npivot1","2ec4f3d8":"a= sns.palettes.color_palette('pastel')\nplt1= pivot1.plot(kind='bar',figsize=(20,8),color=a)\nplt.grid() \nplt.tick_params(left = False, labelleft = False)\nplt.title('Survival Rate: Sex with Passenger Class and Embarked',\n          fontdict={'fontweight': 'bold', 'fontfamily':'serif','fontsize':20})\nfor bar in plt1.patches:\n            plt1.annotate(format(bar.get_height(), '.1f'),\n                               (bar.get_x() + bar.get_width() \/ 2,\n                                bar.get_height()), ha='center', va='center',\n                               size=12, xytext=(0, 8),\n                               textcoords='offset points')","90472709":"# Without 'Embarked'\npivot2= train_data.pivot_table('Survived',['Sex',Age],'Pclass')\npivot2","18c538b5":"import matplotlib.spines as sp\nplt2= pivot2.plot(kind='bar',figsize=(20,8),color= a)\nplt.grid()\nplt.tick_params(left = False, right = False , labelleft = False ,labelbottom = True, bottom = False)\nplt.title('Survival Rate: Sex and Age with Passenger Class', \n          fontdict={'fontweight': 'bold', 'fontfamily':'serif', 'fontsize':20})\nfor bar in plt2.patches:\n            plt2.annotate(format(bar.get_height(), '.1f'),\n                               (bar.get_x() + bar.get_width() \/ 2,\n                                bar.get_height()), ha='center', va='center',\n                               size=12, xytext=(0, 8),\n                               textcoords='offset points')","2eb15c03":"# Final Pivot\npivot3= train_data.pivot_table('Survived',index=['Sex',Age],columns=['Embarked','Pclass'])\npivot3","3e5937ba":"ax= pivot3.plot(kind='barh',figsize=(10,30),color= a)\nplt.grid()\nplt.tick_params( right = False , labelleft = True ,labelbottom = False, bottom = False)\nplt.title('Survival Rate: Sex and Age with Passenger Class & Embarked', \n          fontdict={'fontweight': 'bold', 'fontfamily':'serif', 'fontsize':20})\nfor bar in ax.patches:\n    ax.annotate(format(bar.get_width(), '.1f'),\n                   (bar.get_width(), bar.get_y() + bar.get_height() \/ 2), ha='center', va='center',\n                   size=15, xytext=(12,0),\n                   textcoords='offset points')","5c55734d":"from sklearn.preprocessing import LabelEncoder","832d7bf3":"print(train_data.info()) \nprint(\" \")\nprint(test_data.info())","7e006840":"print(train_data['Sex'].unique())\nprint(train_data['Embarked'].unique())","ceec500d":"labelencoder= LabelEncoder()\n# Train data\ntrain_data.iloc[:, 4]= labelencoder.fit_transform(train_data.iloc[:, 4].values) #Sex\ntrain_data.iloc[:, 10] = labelencoder.fit_transform(train_data.iloc[:, 10].values) #Embarked\n# Test data\ntest_data.iloc[:, 3]= labelencoder.fit_transform(test_data.iloc[:, 3].values) #Sex\ntest_data.iloc[:, 9] = labelencoder.fit_transform(test_data.iloc[:, 9].values) #Embarked","efa4265a":"print(train_data['Sex'].unique())\nprint(train_data['Embarked'].unique())","90fb6fe6":"# Pclass is dropped because it has most negetive correlation against passenger survived\ndata_train= train_data.drop(['PassengerId', 'Name', 'Ticket', 'Title', 'Family_Size', 'FamSize_Cat', 'Pclass'], axis=1)\ndata_train.head(5)","d274e5b7":"X_train= data_train.iloc[:,1:6].values\nY_train= data_train.iloc[:,0].values","f228ce0d":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test= train_test_split(X_train,Y_train, test_size= 0.2, random_state= 0)","f37c0c54":"from sklearn.preprocessing import StandardScaler\nsc= StandardScaler()\nX_train= sc.fit_transform(X_train)\nX_test= sc.fit_transform(X_test)","a25af0f5":"def models(X_train, Y_train):\n    \n    #Using Logistic Regression\n    from sklearn.linear_model import LogisticRegression\n    log= LogisticRegression()\n    log.fit(X_train,Y_train)\n    \n    #Useing KNeighbours\n    from sklearn.neighbors import KNeighborsClassifier\n    knn= KNeighborsClassifier(n_neighbors= 5, metric= 'minkowski', p=2)\n    knn.fit(X_train,Y_train)\n    \n    #Using SupportVectorClassifier (SVC)\n    from sklearn.svm import SVC\n    svc_rbf= SVC(kernel= 'rbf', random_state= 0)\n    svc_rbf.fit(X_train,Y_train)\n    \n    from sklearn.svm import SVC\n    svc_lin= SVC(kernel= 'linear', random_state= 0)\n    svc_lin.fit(X_train,Y_train)\n    \n    #Using GaussianNB\n    from sklearn.naive_bayes import GaussianNB\n    gauss= GaussianNB()\n    gauss.fit(X_train,Y_train)\n    \n    #Using Decision Tree\n    from sklearn.tree import DecisionTreeClassifier\n    tree= DecisionTreeClassifier(criterion= 'entropy', random_state= 0)\n    tree.fit(X_train,Y_train)\n    \n    #Using Random Forest Classifier\n    from sklearn.ensemble import RandomForestClassifier\n    forest= RandomForestClassifier(n_estimators=10, criterion='entropy',random_state=0)\n    forest.fit(X_train,Y_train)\n    \n    LR_Train= log.score(X_train,Y_train)\n    KNN_Train= knn.score(X_train,Y_train)\n    SVC_linTrain= svc_lin.score(X_train,Y_train)\n    SVC_rgfTrain= svc_rbf.score(X_train,Y_train)\n    GaussTrain=  gauss.score(X_train,Y_train)\n    TreeTrain= tree.score(X_train,Y_train)\n    ForestTrain= forest.score(X_train,Y_train)\n   \n    #traing accuracy\n    print('[0]Logistic Regression Training Accuracy: ', LR_Train)\n    print('[1]K Neighbors Regression Training Accuracy: ', KNN_Train )\n    print('[2]SVC Linear Training Accuracy: ', SVC_linTrain)\n    print('[3]SVC RBF Training Accuracy: ', SVC_rgfTrain)\n    print('[4]GaussianNB  Training Accuracy: ', GaussTrain)\n    print('[5]Decision Tree Training Accuracy: ', TreeTrain )\n    print('[6]Random Forest Training Accuracy: ', ForestTrain)\n    \n    models = pd.DataFrame({\n    'Model': ['Logistic','KNN','SVC_RGF', 'SVC_L',\n             'Randomfo',  'Gaussian'],\n    'Score': [LR_Train, KNN_Train,SVC_rgfTrain, SVC_linTrain,ForestTrain, GaussTrain ]})\n\n    models.sort_values(by = 'Score', ascending = False)\n    plt.figure(figsize=(10,6))\n    sns.barplot(x='Model',y='Score',data=models)\n    plt.title('Training Accuracy')\n    plt.show()\n    return log, knn, svc_lin, svc_rbf, gauss, tree, forest","a6ed4f5d":"model= models(X_train,Y_train)","8bef0c2c":"from sklearn.metrics import confusion_matrix\nfor i in range(len(model)):\n    cm= confusion_matrix(Y_test, model[i].predict(X_test))\n    \n    #Extract TN, FP, FN, TP\n    TN, FP, FN, TP= confusion_matrix(Y_test, model[i].predict(X_test)).ravel()\n    test_score= (TP+TN)\/(TN+FP+FN+TP)\n    \n    print(cm)\n    print('Model[{}] Testing Accuracy= \"{}\"'.format(i, test_score))\n    print()","5582bafa":"models = pd.DataFrame({'Model': ['Logistic','KNN','SVC_RGF', 'SVC_L','Randomfo',  'Gaussian'],\n                        'Score': [0.76, 0.78, 0.83, 0.79, 0.69,0.79]})\n\nmodels.sort_values(by = 'Score', ascending = False)\nplt.figure(figsize=(10,6))\nsns.barplot(x='Model',y='Score',data=models)\nplt.title('Testing Accuracy')\nplt.show()","fadb3669":"# EDA","afe17774":"### Summary statistics","32e06535":"### Check Correlation","0f67fd54":"### Check Outliers","079273dd":"# Data preparation","317544e6":"|Name|ValueStr|ValueInt|\n|---|---|---|\n|Sex|M|1|\n| |F|0|\n|Embarked|S|2|\n| |C|0|\n| |Q|1|","921ccd3b":"### Testing data ","d1154296":"# Understanding data","d73499e2":"### Check Distribution","4d82f60b":"- **Traing Dataset has null values, in columns Age, Cabin, Embarked there are null values**\n- Cabin is 'Cabin number' and we can drop entire column(Cabin) assuming that it is a \"not so important\" Key \n- We will drop the subset of null values of columns Age and Embarked from the dataset. ","03c4807d":"# Titanic Dataset Analysis\n- **Task:**\n    - To perform EDA on the dataset\n    - To prredict that, the passengers onboard survived or not","3e6ecb9b":"### Testing data","e1680e47":"- **Entries reduced to 712 to clean our dataset** \n- **No null values found**\n- We can proceed further as our data is cleaned now","8fe5b366":"### Training data","dc2df970":"- **Total number of entries (tuples):** 891\n- **We find some non-numeric columns in the data**\n- *Few null values observed*","22925f92":"# Getting Data","fa0cb81a":"- **Testing Dataset has null values, in columns Age, Cabin, Fare there are null values**\n- Cabin is 'Cabin number' and we can drop entire column(Cabin) assuming that it is a \"not so important\" Key \n- We will **NOT** drop any entries(tuple) from this dataset because we need to predict survival chance of every passenger in this dataset. ","039bf41e":"### Pivot Analysis","a0c2f48e":"# Data cleaning","fa5379aa":"### Training data"}}