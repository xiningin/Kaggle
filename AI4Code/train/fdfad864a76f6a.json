{"cell_type":{"e8a3aef1":"code","1fc085b3":"code","8afd7331":"code","2ae54636":"code","d1237c77":"code","6e2ea69c":"code","a6079a85":"code","db88c0c4":"code","889e304f":"code","023ba80a":"code","4abc416f":"code","2637e0a6":"code","2ce51d5d":"code","3bc0743a":"code","90a210d9":"code","7de6cf4c":"code","ef6fadda":"code","f1ebadc1":"code","d479102f":"code","b85e689c":"code","b19de4c6":"code","096b48b7":"code","d2977eb1":"code","abb40e87":"code","dbb333f9":"code","61a6aa27":"code","34770b32":"code","08aebd76":"code","bb6f48fa":"code","875b4cb9":"code","4b850b08":"code","facfaa33":"code","6352f31b":"code","79088722":"code","977b97e8":"code","dfe2fd64":"code","06cc6516":"code","5c8f55b7":"markdown","34984b59":"markdown","eedc5e48":"markdown","f96ee42b":"markdown","392a2390":"markdown","c9a13954":"markdown","eac5b237":"markdown","f49c7714":"markdown","e95308dd":"markdown","21ef502c":"markdown","66eb4abc":"markdown","79140362":"markdown","95a3c2b4":"markdown","9f9f3c9f":"markdown","3380ce36":"markdown","11822ee4":"markdown","7e5ffd2f":"markdown","e35fc60d":"markdown","0db00f58":"markdown","5b59df81":"markdown","4e0a2180":"markdown"},"source":{"e8a3aef1":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)","1fc085b3":"dataset = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","8afd7331":"dataset.head()\n","2ae54636":"dataset = dataset.drop('Unnamed: 32', axis =1)\n","d1237c77":"dataset.describe()","6e2ea69c":"dataset.isnull().values.any()","a6079a85":"dataset.isnull().values.sum()","db88c0c4":"dataset.isnull().sum()\n","889e304f":"dataset.shape","023ba80a":"dataset['diagnosis'].agg(['count', 'size', 'nunique'])\n","4abc416f":"\npd.value_counts(dataset['diagnosis'])\n","2637e0a6":"sns.set_style('whitegrid')\nplt.figure(figsize=(12, 6))\nsns.countplot(x=\"diagnosis\", data=dataset, palette='magma');\n","2ce51d5d":"plt.figure(figsize=(20, 17))\nmatrix = np.triu(dataset.corr())\nsns.heatmap(dataset.corr(), annot=True, linewidth=.8, mask=matrix, cmap=\"rocket\");\n","3bc0743a":"fig, ax = plt.subplots(2, 2, figsize=(15, 15))\nsns.scatterplot(x='fractal_dimension_mean', y='area_mean', hue=\"diagnosis\",\n                data=dataset, ax=ax[0][0], palette='magma')\nsns.scatterplot(x='fractal_dimension_worst', y='area_worst', hue=\"diagnosis\",\n                data=dataset, ax=ax[0][1], palette='magma')\nsns.scatterplot(x='smoothness_se', y='radius_worst', hue=\"diagnosis\",\n                data=dataset, ax=ax[1][0], palette='magma')\nsns.scatterplot(x='symmetry_se', y='radius_worst', hue=\"diagnosis\",\n                data=dataset, ax=ax[1][1], palette='magma');\n","90a210d9":"# Creating a list of columns with only the columns that represent the mean.\nmean_columns = ['diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n             'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n             'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\n\n# Creating a list of columns with only the columns that represent the worst values.\nworst_columns = ['diagnosis', 'radius_worst', 'texture_worst',\n              'perimeter_worst', 'area_worst', 'smoothness_worst',\n              'compactness_worst', 'concavity_worst', 'concave points_worst',\n              'symmetry_worst', 'fractal_dimension_worst']\n","7de6cf4c":"sns.pairplot(dataset[mean_columns], hue=\"diagnosis\", palette='husl');\n","ef6fadda":"sns.pairplot(dataset[worst_columns], hue=\"diagnosis\", palette='viridis');\n","f1ebadc1":"X = dataset.iloc[:,2:].values\n# X = dataset.drop(['diagnosis','id'],axis=1).values\ny = dataset.iloc[:, 1:2].values","d479102f":"X","b85e689c":"y","b19de4c6":"X.shape","096b48b7":"y.shape","d2977eb1":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y.ravel())\n","abb40e87":"y  # 1 - Malignant and 0 - benign","dbb333f9":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=1)\n","61a6aa27":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n","34770b32":"X_train","08aebd76":"accuracy_scores = {}\ndef predictor(predictor, params):\n    global accuracy_scores\n    if predictor == 'lr':\n        print('Training Logistic Regression on Training Set')\n        from sklearn.linear_model import LogisticRegression\n        classifier = LogisticRegression(**params)\n\n    elif predictor == 'svm':\n        print('Training Support Vector Machine on Training Set')\n        from sklearn.svm import SVC\n        classifier = SVC(**params)\n\n    elif predictor == 'knn':\n        print('TrainingK-Nearest Neighbours on Training Set')\n        from sklearn.neighbors import KNeighborsClassifier\n        classifier = KNeighborsClassifier(**params)\n\n    elif predictor == 'dt':\n        print('Training LDecision Tree Classifier on Training Set')\n        from sklearn.tree import DecisionTreeClassifier\n        classifier = DecisionTreeClassifier(**params)\n\n    elif predictor == 'nb':\n        print('Training Naive Bayes Classifier on Training Set')\n        from sklearn.naive_bayes import GaussianNB\n        classifier = GaussianNB(**params)\n        \n    elif predictor == 'rfc':\n        print('Training Random Forest Classifier on Training Set')\n        from sklearn.ensemble import RandomForestClassifier\n        classifier = RandomForestClassifier(**params)\n\n\n    classifier.fit(X_train, y_train)\n\n    print('''Predicting Single Cell Result''')\n    single_predict = classifier.predict(sc.transform([[\n        17.99, 10.38, 122.8, 1001, 0.1184, 0.2776, 0.3001, 0.1471, 0.2419,\n        0.07871, 1.095, 0.9053, 8.589, 153.4, 0.006399, 0.04904, 0.05373, 0.01587,\n        0.03003, 0.006193, 25.38, 17.33, 184.6, 2019, 0.1622, 0.6656, 0.7119, 0.2654, 0.4601, 0.1189\n    ]])) \n    if single_predict > 0.5 :\n        print(\"Cancer is Malignant \\n\")\n    else :\n        print(\"Cancer is Benign \\n\")\n\n    print('''Prediciting Test Set Result''')\n    y_pred = classifier.predict(X_test)\n    result = np.concatenate((y_pred.reshape(len(y_pred), 1),\n                             y_test.reshape(len(y_test), 1)), 1)\n    print(result,'\\n')\n    print('''Making Confusion Matrix''')\n    from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n    y_pred = classifier.predict(X_test)\n    cm = confusion_matrix(y_test, y_pred)\n    print(cm,'\\n')\n    print('True Positives :',cm[0][0])\n    print('False Positives :',cm[0][1])\n    print('False Negatives :',cm[1][0])\n    print('True Negatives :', cm[0][1],'\\n')\n\n    print('''Classification Report''')\n    print(classification_report(y_test, y_pred,\n          target_names=['M', 'B'], zero_division=1))\n\n    print('''Evaluating Model Performance''')\n    accuracy = accuracy_score(y_test, y_pred)\n    print(accuracy,'\\n')\n\n    print('''Applying K-Fold Cross validation''')\n    from sklearn.model_selection import cross_val_score\n    accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=10)\n    print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n    accuracy_scores[classifier] = accuracies.mean()*100\n    print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100),'\\n')   \n","bb6f48fa":"predictor('lr', {'penalty': 'l1', 'solver': 'saga', 'max_iter': 5000})\n","875b4cb9":"predictor('svm', {'C': 1, 'gamma': 0.8,\n          'kernel': 'linear', 'random_state': 0})\n","4b850b08":"predictor('svm', {'C': 1, 'gamma': 0.1, 'kernel': 'rbf', 'random_state': 0})\n","facfaa33":"predictor('knn', {'n_neighbors': 5, 'n_jobs':1})\n","6352f31b":"predictor('dt', {'criterion': 'gini', 'max_features': 'auto', 'splitter': 'random' ,'random_state': 0})\n","79088722":"predictor('nb', {})\n","977b97e8":"predictor('rfc', {'criterion': 'entropy',\n          'max_features': 'auto', 'n_estimators': 250,'random_state': 0})\n","dfe2fd64":"maxKey = max(accuracy_scores, key=lambda x: accuracy_scores[x])\nprint('The model with highest K-Fold Validation Accuracy score is  {0} with an accuracy of  {1:.2f}'.format(\n    maxKey, accuracy_scores[maxKey]))\n","06cc6516":"plt.figure(figsize=(12, 6))\nmodel_accuracies = list(accuracy_scores.values())\nmodel_names = ['LogisticRegression', 'SVC',\n                 'K-SVC','KNN','Decisiontree', 'GaussianNB','RandomForest']\nsns.barplot(x=model_accuracies, y=model_names, palette='mako');\n","5c8f55b7":"## Training Logisitic Regression","34984b59":"# Exploratory Data Analysis","eedc5e48":"# Data Preprocessing","f96ee42b":"## Training Kernel Support Vector Machine","392a2390":"## Training Random Forest","c9a13954":"## **Please do leave your valuable feedbacks in the comments and any improvements or suggestions are welcomed!**","eac5b237":"## Encoding Dependent Variable with Label Encoder","f49c7714":"# Summary\n- SVM performed Best on this dataset with an accuracy of 97.80%\n- Logisitic Regression is just behind with an accuracy of 97.58%\n","e95308dd":"## Training Decision Trees","21ef502c":"## Training Naive Bayes","66eb4abc":"### Diagnosis Column have 2 unique values Malignant(M) and Benign(B) having count of 212 and 357 respectively.","79140362":"# Training Models on Training Set and Prediciting Results","95a3c2b4":"## Loading Dataset\n","9f9f3c9f":"## Training Support Vector Machine ","3380ce36":"As we can observe from the heatmaps that there are many negative correlations in this dataset. Lets observe these by plotting it out.\n\nNegative Correlations\nThe column 'fractal_dimension_mean' had many negative correlations with many other attributes like 'area_mean', 'area_worst' etc. We'll plot some scatter plots for these.\n\nFractal analysis of images of breast tissue specimens provides a numeric description of tumour growth patterns as a continuous number between 1 and 2. This number is known as the Fractal Dimension","11822ee4":"### Standardizing training data\n","7e5ffd2f":"## Training K-Nearest Neighbours","e35fc60d":"As you can see the Model with highest accuracy is Support Vector Machine with an accuracy of 97.80\n","0db00f58":"### Luckily There are no NAN values\n","5b59df81":"### Splitting Dataset into Training Set and Test Set","4e0a2180":"### Dataset have 569 rows and 32 Columns"}}