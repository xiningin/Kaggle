{"cell_type":{"1a0fa18f":"code","426aef7a":"code","5c1acefa":"code","e96a4e37":"code","1b2cdac2":"code","533481ab":"code","19f60c40":"code","294d085d":"code","908e0a60":"code","c156698b":"code","2f854eb2":"markdown"},"source":{"1a0fa18f":"import pandas as pd\nimport numpy as np\nimport multiprocessing\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport gc\nfrom time import time\nimport datetime\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import log_loss\nwarnings.simplefilter('ignore')\nsns.set()\n%matplotlib inline","426aef7a":"files = ['..\/input\/lish-moa\/test_features.csv', \n         '..\/input\/lish-moa\/train_targets_scored.csv',\n         '..\/input\/lish-moa\/train_features.csv',\n         '..\/input\/lish-moa\/train_targets_nonscored.csv',\n         '..\/input\/lish-moa\/sample_submission.csv']\n\nwith multiprocessing.Pool() as pool:\n    test, train_target, train, train_nonscored, sub = pool.map(pd.read_csv, files)","5c1acefa":"# One-Hot encoding\nfor feature in ['cp_time', 'cp_type', 'cp_dose']:\n    concat = pd.concat([train[feature], test[feature]], ignore_index=True)\n    dummies = pd.get_dummies(concat, dummy_na=True, dtype=np.uint8, prefix=feature)\n    train = pd.concat([train, dummies.iloc[:train.shape[0]]], axis=1)\n    test = pd.concat([test, dummies.iloc[:test.shape[0]]], axis=1)","e96a4e37":"targets = [col for col in train_target.columns if col != 'sig_id']\nprint('Number of different labels:', len(targets))","1b2cdac2":"features = [col for col in train.columns if col not in ['sig_id', 'cp_time', 'cp_type', 'cp_dose']]\nprint('Number of features:', len(features))","533481ab":"X = train[features]","19f60c40":"params = {'num_leaves': 491,\n          'min_child_weight': 0.03,\n          'feature_fraction': 0.3,\n          'bagging_fraction': 0.4,\n          'min_data_in_leaf': 106,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.01,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'binary_logloss',\n          \"verbosity\": 0,\n          'reg_alpha': 0.4,\n          'reg_lambda': 0.6,\n          'random_state': 47\n         }","294d085d":"accumulative_loss = 0\nskf = StratifiedKFold(n_splits=3, random_state=47, shuffle=True)\n\nprint('Execution time | Model number | logloss | new logloss | best coeff')\n# 206 different models. One for each label\nfor model, target in enumerate(targets, 1):\n    y = train_target[target]\n    start_time = time()\n    preds = np.zeros(test.shape[0])\n    oof = np.zeros(X.shape[0])\n\n    for trn_idx, test_idx in skf.split(X, y):\n        \n        trn_data = lgb.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n        val_data = lgb.Dataset(X.iloc[test_idx], label=y.iloc[test_idx])\n        clf = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=0, early_stopping_rounds=20)\n        oof[test_idx] = clf.predict(X.iloc[test_idx])\n        preds += clf.predict(test[features]) \/ skf.n_splits\n\n    loss = log_loss(y, oof)\n    \n    # Hacking the metric\n    coeffs = [3, 2, 1.5, 1.4, 1.3, 1.2, 1.1, 1.0, 0.9, 0.8, 0.7]\n    best_coeff = 0\n    best_loss = loss\n    for coeff in coeffs:\n        new_oof = oof.copy()\n        new_oof[new_oof < new_oof.mean() \/ coeff] = 0\n        new_loss = log_loss(y, new_oof)\n        if new_loss < loss:\n            preds[preds < preds.mean() \/ coeff] = 0\n            best_coeff = coeff\n            best_loss = new_loss\n    \n    if best_coeff:\n        preds[preds < preds.mean() \/ best_coeff] = 0\n    # End of metric hacking\n    sub[target] = preds\n\n    accumulative_loss += best_loss\n    print('{}\\t\\t{}\\t{:.5f}\\t\\t{:.5f}\\t\\t{}'.format(str(datetime.timedelta(seconds=time() - start_time))[:7], model, loss, best_loss, best_coeff))\n    del preds, oof, start_time, y, loss, best_loss, new_oof\n    gc.collect();","908e0a60":"print('Overall mean loss: {:.5f}'.format(accumulative_loss \/ 206))","c156698b":"sub.to_csv('submission.csv', index=False)","2f854eb2":"In this kernel I am building 206 models (one for each label). Because there is no easier way to make LightGBM work with multilabel\/multiclass task. And this competitions is a case because we have 206 targets to predict.\n\n## Versions\n* v2: Label encoding of categorical features. CV: 0.01627, LB: 0.02040\n* v4: One-Hot encoding of categorical features. Metric hackind added - all predictions below threshold set to 0. CV: 0.01622, LB: 0.02209"}}