{"cell_type":{"e93a78f7":"code","05a4edc6":"code","e188dc5b":"code","f8b55082":"code","3c245603":"code","cb6a3bc7":"code","b5e09ef4":"code","97144483":"code","ff03c8b4":"code","86140a80":"code","0e29ef88":"code","2370d6b9":"code","0d4f2f49":"code","5866ad4e":"code","51029c43":"code","30693fef":"code","38627169":"code","39f680f1":"code","2def1a65":"code","24131033":"code","f364d63d":"code","43742670":"code","f3447206":"code","448e4deb":"code","c5f146e2":"code","77abd0f0":"code","722f8dc4":"code","c42bb219":"code","8af10c36":"code","3df25490":"code","6199a56a":"code","133fc684":"code","cdcc5aee":"code","6dd26eda":"code","bb25ae4a":"code","176d50ce":"code","dec9e8fc":"code","a8be9469":"code","46d8740f":"code","82673a4c":"code","a02ecf6b":"code","5ce1281e":"code","f5bbd2e6":"code","f90d8e18":"markdown","0955adf2":"markdown","762db5a5":"markdown","4e29a540":"markdown","363ce855":"markdown","61e5d26a":"markdown","354fa113":"markdown","8d692e35":"markdown","eb942b01":"markdown"},"source":{"e93a78f7":"#this is code by kaggle to set up working directory\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","05a4edc6":"#setting random state for reproducible results\nRANDOM_STATE=0","e188dc5b":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nstopwords= set(stopwords.words('english'))\nimport re ","f8b55082":"data = pd.read_csv('\/kaggle\/input\/brooklyn-99-dataset-season-14\/seasons_1234(updated).csv')","3c245603":"#get basic overview of how the data looks like\ndata","cb6a3bc7":"print(data.name.value_counts())\nprint('\\n')\nprint(data.name.value_counts()[:20])","b5e09ef4":"# first we decapitalize all the labels and lines, to eliminate useless variability in the data\ndata['name'] = data.name.str.lower()\ndata['line'] = data.line.str.lower()\n# also we notice that both label boyle and charles is used, so we change all observations of 'boyle' in variable name to 'charles'\ndata['name'] = data.name.replace('boyle','charles')\n# the dataset is quite imbalanced and there are a lot of labels (categories) which are very rare \n# so we decide to categorize only the most common 13\n# also, we coudl check some typos in the labels before this (try to do it on your own)\nrel_names = list(data.name.value_counts()[:13].index)\n#we rename all labels which cannot be found in list rel_names to 'other'\ndata['name'] = data.name.apply(lambda x: x if x in rel_names else 'other')","97144483":"# there are some comments in the lines which are not actually said by the actor, so we delete these from the lines, \n# but first we label such observations\ndata['contains_brackets'] = data.line.apply(lambda x: 1 if ('[' in x and ']' in x) else 0)","ff03c8b4":"data[data['contains_brackets'] == 1]","86140a80":"data.iloc[2,1]","0e29ef88":"#we use regex to delete all the brackets and their content\ndata['no_brackets_line'] = data.line.apply(lambda x: re.sub('\\[.*\\]','',x))\n#we tokenize the words -> list of tokens are created\ndata['tokens'] = data.no_brackets_line.apply(word_tokenize)\n#from these tokens, we get length of the line, it might be helpful, maybe some characters in the series, use shorter\/longer sentences\ndata['len'] = data.tokens.apply(lambda x: len(x))\n#we mark lines which contain ! or ? because bag of words approach as implemented by TFIDF Vectorizer from sklearn, which we are going to use\n#does not take punctuation in consideration\ndata['excl_mar'] = data.line.apply(lambda x: 1 if '!' in x else 0)\ndata['question'] = data.line.apply(lambda x: 1 if '?' in x else 0)","2370d6b9":"# as next step we delete stopwords... however, this might not be a good step (try eliminating this step) \n# because usage of stopwords could also be characteristic for some characters (people in the series)\ndata['clear_tokens'] = data.tokens.apply(lambda x: [k for k in x if k not in stopwords])","0d4f2f49":"#at the end we join the list of clear tokens to form whole texts again - this is what sklearn TfidfVectorizer needs as input\n# btw. see the documentation of the vectorizer at https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html\ndata['final_line'] = data.clear_tokens.apply(lambda x: ' '.join(x))","5866ad4e":"#we have option to use only the lines as input for the prediction or add them to the variables (features) which we created\nFEATURES = ['final_line','question','excl_mar','len','contains_brackets']\n#FEATURES = ['line']","51029c43":"#this automatically creates stratified train\/test split, see documentation\nx_train, x_test, y_train, y_test = train_test_split(data[FEATURES],\n                                                    data[['name']],\n                                                    stratify=data[['name']],\n                                                    test_size=0.2,random_state=RANDOM_STATE)","30693fef":"x_train\n#the final lines still look quite messy (still room for improvement), but we are going to build a model anyway","38627169":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report\n#maximum number of text features (how many top n-grams it will take)\nMAX_F = 300\n#this will take unigrams, bigrams and trigrams and for each observation creates a tf-idf matrix of the 300 most common\ntf_vectorizer = TfidfVectorizer(ngram_range=(1,3), max_features = MAX_F)\n#label encoder is used to factorize the names used as labels\nle=LabelEncoder() \ny_train = le.fit_transform(y_train)\ny_test = le.transform(y_test)","39f680f1":"#retype categorial variables as factors\nx_train = x_train.astype({'question':'category',\n                          'excl_mar': 'category',\n                          'contains_brackets':'category'})\nx_test = x_test.astype({'question':'category',\n                          'excl_mar': 'category',\n                          'contains_brackets':'category'})","2def1a65":"# check data types \nx_train.dtypes","24131033":"#we can see that we have mixed datatypes, especially the final_line variable needs to be processed further via tf-idf, \n#so we will process it separately \n#fit the vectorizer on the train and use it on test (we still assume that we would not have the test data in advance so using it \n#for training would be negative for the generality of the model)\nx_train_text = tf_vectorizer.fit_transform(x_train['final_line'])\nx_test_text = tf_vectorizer.transform(x_test['final_line'])","f364d63d":"#now we want to join the newly created data matrixes back with the variables which we not processed, for this we first convert them back\n#to pandas dataframe\nx_train_text_pd = pd.DataFrame.sparse.from_spmatrix(x_train_text)\n#and concatenate them with the other variables\nx_train = pd.concat([x_train.iloc[:,1:].reset_index(drop=True),x_train_text_pd],axis=1)","43742670":"#the same for test set\nx_test_text_pd = pd.DataFrame.sparse.from_spmatrix(x_test_text)\nx_test = pd.concat([x_test.iloc[:,1:].reset_index(drop=True),x_test_text_pd],axis=1)","f3447206":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier","448e4deb":"#create classifier objects, there are many hyperparameters we just use defaults ... check documentation\n#for really setting and fine-tuning hyperparameters it would be best to create also validation dataset and we could either go iteratively \n#based on bias and variance or we could directly use GridSearch (or its variations)\nDT = DecisionTreeClassifier()\nRF = RandomForestClassifier()\nXGB = XGBClassifier(tree_method=\"gpu_hist\", enable_categorical=True, use_label_encoder=False)","c5f146e2":"# train and predict decision tree and followingly other classifiers\nDT.fit(x_train,y_train)\ny_pred = DT.predict(x_test)\n#inverse_transform relables numbers used as labels back to names, for better readability ... try it without this method\nprint(classification_report(le.inverse_transform(y_test), le.inverse_transform(y_pred)))","77abd0f0":"RF.fit(x_train,y_train)\ny_pred = RF.predict(x_test)\nprint(classification_report(le.inverse_transform(y_test), le.inverse_transform(y_pred)))","722f8dc4":"XGB.fit(x_train,y_train)\ny_pred = XGB.predict(x_test)\nprint(classification_report(le.inverse_transform(y_test), le.inverse_transform(y_pred)))","c42bb219":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Dense, LSTM, Dropout, Bidirectional\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.preprocessing import LabelEncoder","8af10c36":"x_train, x_test, y_train, y_test = train_test_split(data[['line']],data[['name']],stratify=data[['name']],test_size=0.2,random_state=RANDOM_STATE)","3df25490":"num_classes = len(y_train[\"name\"].unique())","6199a56a":"tokenizer=Tokenizer(oov_token=\"'oov'\")\ntokenizer.fit_on_texts(x_train[\"line\"])","133fc684":"maxlen = 200\nx_train = pad_sequences(tokenizer.texts_to_sequences(x_train['line']), maxlen=maxlen)\nx_test = pad_sequences(tokenizer.texts_to_sequences(x_test['line']), maxlen=maxlen)","cdcc5aee":"enc = LabelEncoder()\nenc.fit(y_train[\"name\"])","6dd26eda":"y_train_cat = to_categorical(enc.transform(y_train[\"name\"]), num_classes=num_classes)\ny_test_cat = to_categorical(enc.transform(y_test[\"name\"]), num_classes=num_classes)","bb25ae4a":"y_train_cat.shape","176d50ce":"glove_dir=\"\/kaggle\/input\/glove-embeddings\/\"\n\nembedding_index = {}\nf = open(os.path.join(glove_dir,'glove.6B.100d.txt'),encoding='utf8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:],dtype='float32')\n    embedding_index[word] = coefs\nf.close()\nprint('Found %s word vectors ' % len(embedding_index))","dec9e8fc":"max_words = len(tokenizer.word_index) + 1\nembedding_dim = 100\nembedding_matrix = np.zeros((max_words,embedding_dim))\n\nfor word, idx in tokenizer.word_index.items():\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[idx]=embedding_vector","a8be9469":"embedding_matrix.shape","46d8740f":"model=Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=False))\nmodel.add(Bidirectional(LSTM(32)))\nmodel.add(Dense(32, activation=\"relu\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(num_classes, activation=\"softmax\"))\nmodel.compile(optimizer=\"Adam\", loss='categorical_crossentropy', metrics=['accuracy'])\nprint(model.summary())","82673a4c":"model_checkpoint_callback = ModelCheckpoint(\n    filepath='\/kaggle\/working\/lstm_checkpoint.h5',\n    save_weights_only=True,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True)","a02ecf6b":"model.fit(x_train, y_train_cat, epochs=50, batch_size=128, validation_data=(x_test, y_test_cat), callbacks=[model_checkpoint_callback])","5ce1281e":"model.load_weights('\/kaggle\/working\/lstm_checkpoint.h5')","f5bbd2e6":"y_pred = model.predict(x_test)\nprint(classification_report([np.argmax(p) for p in y_test_cat], [np.argmax(p) for p in y_pred]))","f90d8e18":"### Libraries","0955adf2":"## LSTM","762db5a5":"We can see, the models do not have amazing predicting power, although for so many classes with relatively small dataset (for NLP task), the results are not that bad, either. You could try predicting for example only 3 classes (Jake, Holt, other). The dataset is still pretty messy so you can think about leaving stopwords, editing contractions, deleting observations which are too long, etc.\n\nAlso, this is not the state-of-the-art approach. If you are interested in something more fancy, a language-model based LSTM neural network prediction follows. (spoiler alert - it does not help much)","4e29a540":"The dataset is quite imbalanced, with Jake (main character) having more then 25% of lines in the whole dataset. The labels (variable \"name\") are also quite messy mixing capital letters with non-capital letters and using various labels for the same category (background knowledge is important - charles and boyle is one person - first name and surname). Also, take a look at some lines and think about what might be wrong with them and how we could fix them. At the end, we will want to have a ML model which can predict based on a line, which person said it","363ce855":"### Data Split","61e5d26a":"### Preprocessing","354fa113":"### Short EDA","8d692e35":"### Advanced preprocessing","eb942b01":"### Baseline model"}}