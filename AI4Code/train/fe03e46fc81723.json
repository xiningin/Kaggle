{"cell_type":{"6a471985":"code","86641bb0":"code","57d67490":"code","0c2f3631":"code","01f49eec":"code","2778e372":"code","b4741198":"code","becc2e19":"code","a9396705":"code","2c4860f6":"code","add67ff5":"code","25d1aa3a":"code","4150c804":"code","0c143031":"code","576c4300":"code","c8889727":"code","075bdc40":"code","55091fc2":"code","5bd93e6c":"code","6bab734f":"code","0c290cea":"code","ee7debb0":"code","36dfc04b":"code","59c56aa1":"code","2c3e145f":"code","8806d277":"code","f1f98bf4":"code","0c63522b":"code","e6c1a32f":"code","ad137b19":"markdown","52671095":"markdown","5a93f70d":"markdown","4747f4ee":"markdown","7b448d35":"markdown","47244045":"markdown","f3760ffe":"markdown","90f0e46f":"markdown","5a639cb2":"markdown","cd51a05f":"markdown","88e9a427":"markdown","b654eb44":"markdown"},"source":{"6a471985":"from pathlib import Path\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.preprocessing import OneHotEncoder, PowerTransformer, FunctionTransformer\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.metrics import classification_report\n\n%pip install imbalanced-learn\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline","86641bb0":"file_path = Path('..\/input\/credit-card-customers\/BankChurners.csv')\n\nchurn_data = pd.read_csv(file_path)\nchurn_data = churn_data.iloc[:,:-2]\nchurn_data.head(5)","57d67490":"# Get dataset information \n\nprint(churn_data.info())","0c2f3631":"# statistic summary\nchurn_data.describe()","01f49eec":"# we want to see our target variable proportion (Attrition Flag)\n# between existing customer (no churn) and  attrited customer (churn)\nchurn = churn_data['Attrition_Flag'].value_counts()\nfig = plt.figure(figsize=(6,6))\nax = fig.add_axes([0,0,1,1])\nax.axis('equal')\nlabels = churn.index.tolist()\nax.pie(churn, labels=labels, autopct='%.1f%%', textprops={'size': 16})\nplt.title('Attrition Flag Columns Precentage', fontsize=15)\nplt.show()","2778e372":"# create list of columns that contain numeric value \nnum_columns = ['Customer_Age', 'Months_on_book', 'Credit_Limit', 'Total_Revolving_Bal',\n               'Avg_Open_To_Buy','Total_Amt_Chng_Q4_Q1','Total_Trans_Amt','Total_Trans_Ct',\n               'Total_Ct_Chng_Q4_Q1','Avg_Utilization_Ratio']","b4741198":"# cumulative distribution function\ndef cdf(data):\n    n = len(data)\n    x = np.sort(data)\n    y = np.arange(1, n + 1) \/ n\n    return x, y\n\n# distribution visualization function \ndef distplot_num(df, col, bins=None):\n    fig, ax = plt.subplots(ncols=2, figsize=(12,6))\n    dist1 =sns.distplot(df[df['Attrition_Flag'] == 'Existing Customer'][col], bins=bins,\n                 color='blue', label='Existing Customer', ax=ax[0])\n    _ = sns.distplot(df[df['Attrition_Flag'] == 'Attrited Customer'][col], bins=bins,\n                 color='orange', label='Attrited Customer', ax=ax[0])\n    dist1.set(title=f'Probalbility Mass Function')\n    ax[0].legend()\n    \n\n    x_exist, y_exist = cdf(df[df['Attrition_Flag'] == 'Existing Customer'][col])\n    x_attrited, y_attrited = cdf(df[df['Attrition_Flag'] == 'Attrited Customer'][col])\n    \n    dist2 = sns.lineplot(x_exist, y_exist, markers=True, color='blue', ax=ax[1])\n    _ = sns.lineplot(x_attrited, y_attrited, markers=True, color='orange', ax=ax[1])\n    \n    dist2.set(xlabel=col, title=f'Cumulative Distribution Function')\n    plt.tight_layout()\n    plt.show()\n    \n\n# measures of central tendency\ndef central_tendency_num(df, col):\n    \"\"\"\n    Calculate measurement of central tendency for numeric feature using mean\n    \n    Args:\n    df: pandas.DataFrame\n    col: selected column that want to being calculated\n    \n    \"\"\"\n    mean_exist_cust = df[df['Attrition_Flag'] == 'Existing Customer'][col].mean()\n    mean_attrited_cust = df[df['Attrition_Flag'] == 'Attrited Customer'][col].mean()\n    print(f\"{col} mean for Existing Customer: {mean_exist_cust}\")\n    print(f\"{col} mean for Attrited Customer: {mean_attrited_cust}\")\n    \n# measures of dispersion\ndef dispersion_num(df, col):\n    \"\"\"\n    Calculate measurement of dispersion from distribution (using standar deviation)\n    \n    Args:\n    df: pandas.DataFrame \n    col: selected column that want to being calculated\n    \n    \"\"\"\n    dispersion_exist_cust = df[df['Attrition_Flag'] == 'Existing Customer'][col].var()\n    dispersion_attrited_cust = df[df['Attrition_Flag'] == 'Attrited Customer'][col].var()\n    print(f\"{col} dispersion (var) for Existing Customer: {dispersion_exist_cust}\")\n    print(f\"{col} dispersion (var) for Attrited Customer: {dispersion_attrited_cust}\")","becc2e19":"# visualize Customer_Age distribution\ndistplot_num(churn_data, num_columns[0], bins=[20,40,60,80])\ncentral_tendency_num(churn_data, num_columns[0])\ndispersion_num(churn_data, num_columns[0])\n\nprint(f'kurtosis: {churn_data[num_columns[0]].kurtosis()}')\nprint(f'skewness: {churn_data[num_columns[0]].skew()}')","a9396705":"# visualize Months_on_book distribution  \ndistplot_num(churn_data, num_columns[1], bins=None)\ncentral_tendency_num(churn_data, num_columns[1])\ndispersion_num(churn_data, num_columns[1])\n\nprint(f'kurtosis: {churn_data[num_columns[1]].kurtosis()}')\nprint(f'skewness: {churn_data[num_columns[1]].skew()}')","2c4860f6":"# visualize Credit_Limit distribution  \ndistplot_num(churn_data, num_columns[2], bins=None)\ncentral_tendency_num(churn_data, num_columns[2])\ndispersion_num(churn_data, num_columns[2])\n\nprint(f'kurtosis: {churn_data[num_columns[2]].kurtosis()}')\nprint(f'skewness: {churn_data[num_columns[2]].skew()}')","add67ff5":"# visualize Total_Revolving_Bal distribution  \ndistplot_num(churn_data, num_columns[3])\ncentral_tendency_num(churn_data, num_columns[3])\ndispersion_num(churn_data, num_columns[3])\n\nprint(f'kurtosis: {churn_data[num_columns[3]].kurtosis()}')\nprint(f'skewness: {churn_data[num_columns[3]].skew()}')","25d1aa3a":"# visualize Avg_Open_To_Buyl distribution  \ndistplot_num(churn_data, num_columns[4])\ncentral_tendency_num(churn_data, num_columns[4])\ndispersion_num(churn_data, num_columns[4])\n\nprint(f'kurtosis: {churn_data[num_columns[4]].kurtosis()}')\nprint(f'skewness: {churn_data[num_columns[4]].skew()}')","4150c804":"# visualize Total_Amt_Chng_Q4_Q1 distribution  \ndistplot_num(churn_data, num_columns[5])\ncentral_tendency_num(churn_data, num_columns[5])\ndispersion_num(churn_data, num_columns[5])\n\nprint(f'kurtosis: {churn_data[num_columns[5]].kurtosis()}')\nprint(f'skewness: {churn_data[num_columns[5]].skew()}')","0c143031":"# visualize Total_Trans_Amt distribution  \ndistplot_num(churn_data, num_columns[6])\ncentral_tendency_num(churn_data, num_columns[6])\ndispersion_num(churn_data, num_columns[6])\n\nprint(f'kurtosis: {churn_data[num_columns[6]].kurtosis()}')\nprint(f'skewness: {churn_data[num_columns[6]].skew()}')","576c4300":"# visualize Total_Trans_Ct distribution  \ndistplot_num(churn_data, num_columns[7])\ncentral_tendency_num(churn_data, num_columns[7])\ndispersion_num(churn_data, num_columns[7])\n\nprint(f'kurtosis: {churn_data[num_columns[7]].kurtosis()}')\nprint(f'skewness: {churn_data[num_columns[7]].skew()}')","c8889727":"# visualize Total_Ct_Chng_Q4_Q1 distribution  \ndistplot_num(churn_data, num_columns[8])\ncentral_tendency_num(churn_data, num_columns[8])\ndispersion_num(churn_data, num_columns[8])\n\nprint(f'kurtosis: {churn_data[num_columns[8]].kurtosis()}')\nprint(f'skewness: {churn_data[num_columns[8]].skew()}')","075bdc40":"# visualize Avg_Utilization_Ratio distribution  \ndistplot_num(churn_data, num_columns[9])\ncentral_tendency_num(churn_data, num_columns[9])\ndispersion_num(churn_data, num_columns[9])\n\nprint(f'kurtosis: {churn_data[num_columns[9]].kurtosis()}')\nprint(f'skewness: {churn_data[num_columns[9]].skew()}')","55091fc2":"cat_columns = ['Gender','Dependent_count','Education_Level','Marital_Status','Income_Category', \n               'Card_Category', 'Months_Inactive_12_mon', 'Contacts_Count_12_mon', 'Total_Relationship_Count']","5bd93e6c":"fig, axes = plt.subplots(3,3, figsize=(15,15))\n\nfor col, ax in zip(cat_columns, axes.flatten()):\n    # cross category column with target column (normalized)\n    cross_tab = pd.crosstab(churn_data[col], churn_data['Attrition_Flag'])\n    cross_tab.plot.bar(ax=ax)\n    plt.suptitle('Categorical Feature Proportion Plot', y=1, va=\n                 'baseline', fontsize=20)\n    plt.tight_layout()\nplt.show()","6bab734f":"# to reduce dimensional of data, we have to select what feature to be used as predictor \n# according to EDA that we've done, let's select what feature that give more useful information and possibilities. \n# check dataset columns\nprint(churn_data.columns)","0c290cea":"# remove uneeded columns\nchurn_data_selected = churn_data.drop(['CLIENTNUM', 'Credit_Limit', 'Months_Inactive_12_mon', 'Contacts_Count_12_mon', 'Months_on_book', 'Card_Category'], axis=1)","ee7debb0":"# binning for customer age feature (transform numerical to categorical )\n\nchurn_data_selected['Customer_Age'] = pd.cut(churn_data_selected['Customer_Age'], [20,40,60,80],\n                                            labels=[\"early adult\", \"middle age\", \"elderly\"])","36dfc04b":"print(churn_data_selected['Education_Level'].unique())\nprint(churn_data_selected['Income_Category'].unique())","59c56aa1":"# encode for ordinal feature\n# why using manual encode? because LabelEncoder from sklearn perform encoding sorted Alphabetically\n\n# setting education level dict\neducation_level = {\n    'Unknown': -1,\n    'Uneducated': 0,\n    'High School': 1,\n    'College': 2,\n    'Graduate': 3,\n    'Post-Graduate': 4,\n    'Doctorate': 5\n}\n\n# setting income category dict\nincome_category = {\n    'Unknown': -1,\n    'Less than $40K': 0,\n    '$40K - $60K': 1,\n    '$60K - $80K': 2,\n    '$80K - $120K': 3,\n    '$120K +': 4,\n}\n\nage_level = {\n    \"early adult\": 0, \n    \"middle age\": 1,\n    \"elderly\": 2,\n}\n\n# notes: we set Unknown with value -1 because we cannot assume what it's level\n\n# replace education level to integer\nchurn_data_selected['Education_Level'] = churn_data_selected['Education_Level'].map(education_level)\nchurn_data_selected['Income_Category'] = churn_data_selected['Income_Category'].map(income_category)\nchurn_data_selected['Customer_Age'] = churn_data_selected['Customer_Age'].map(age_level)","2c3e145f":"X = churn_data_selected.drop(columns='Attrition_Flag')\ny = churn_data_selected['Attrition_Flag']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(f'train shape: {X_train.shape, y_train.shape}')\nprint(f'test shape: {X_test.shape, y_test.shape}')","8806d277":"# preprocessing\n\nnumerical_pipeline = Pipeline([\n    ('transform', FunctionTransformer(np.log1p))\n])\n\ncategorical_pipeline = Pipeline([\n    ('one_hot', OneHotEncoder())\n])\n\npreprocessor = ColumnTransformer([\n    ('numeric', numerical_pipeline, ['Total_Revolving_Bal', 'Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt',\n                                     'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']),\n    ('categoric', categorical_pipeline, ['Gender', 'Marital_Status'])\n])\n\n# sampling strategy\nsmote = SMOTE(random_state=42)","f1f98bf4":"# Binomial Logistic Regression\n\npipeline_log = Pipeline([\n    ('preprocess', preprocessor),\n    ('resampling', smote),\n    ('clf', LogisticRegression(max_iter=500))\n])\n\npipeline_log.fit(X_train, y_train)\ny_hat = pipeline_log.predict(X_test)\n\nprint(f'classification report logistic:\\n{classification_report(y_test, y_hat)}')","0c63522b":"# Random Forest\npipeline_rdf = Pipeline([\n    ('preprocess', preprocessor),\n    ('resampling', smote),\n    ('clf', RandomForestClassifier(random_state=42))\n])\n\npipeline_rdf.fit(X_train, y_train)\ny_hat = pipeline_rdf.predict(X_test)\n\nprint(f'classification report rdf:\\n{classification_report(y_test, y_hat)}')","e6c1a32f":"# Support Vector\npipeline_svm = Pipeline([\n    ('preprocess', preprocessor),\n    ('smote', smote),\n    ('clf', SVC())\n])\n\npipeline_svm.fit(X_train, y_train)\ny_hat = pipeline_svm.predict(X_test)\n\nprint(f'classification report svm:\\n{classification_report(y_test, y_hat)}')","ad137b19":"### Preprocessing and ML Pipeline","52671095":"## Numeric Feature vs Target","5a93f70d":"## Feature Selection and Feature Engineering","4747f4ee":"### Training Model","7b448d35":"### Feature Selection","47244045":"### Conclusion\n\nAfter training several classifier model with simple approach, we can see for each classification report above that Random Forest model is the best (for now) for this classification task with the highest performance (94% accuracy). \n\n### Next Step?\nWe can doing hyperparameter tuning.\n\n### Hope this help\nLeave comment if there was an error in my explaination especially about EDA. Thanks a lot!!","f3760ffe":"### Univariate (Target Variable)","90f0e46f":"### Dataset Splitting","5a639cb2":"# Exploratory Data Analysis ","cd51a05f":"### Feature Engineering","88e9a427":"Exploratory Data Analysis (EDA) is the first step (and common) for Data analysis or Data Sceince flow. The main concept behind EDA is to indentify what kind of data, understanding the pattern, and trying to find some valuable information in the data (insight). In general, EDA is carried out in several ways:\n* Univariate Analysis \u2014 descriptive analysis with one variable.\n* Bivariate Analysis \u2014 analysis relationship between two variable.\n* Multivariate Analysis \u2014 analysis with three or more variable.","b654eb44":"## Categoric Feature vs Target"}}