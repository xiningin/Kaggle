{"cell_type":{"b7c4eaa5":"code","3791eea4":"code","da42de61":"code","b9761686":"code","d823675d":"code","3d2d21c3":"code","81d5f270":"code","24480ba2":"code","19423980":"code","774f2774":"code","9c40e390":"code","39d88fbe":"code","645fb96f":"code","4918b24f":"code","777baf77":"code","19d750dd":"code","e51459a0":"code","83d68696":"code","19475661":"code","c82e2033":"code","5973e292":"code","291371f4":"code","c56c5fd1":"markdown","7b7f0b73":"markdown"},"source":{"b7c4eaa5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3791eea4":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\nfrom sklearn.datasets import make_blobs\nfrom sklearn.model_selection import GridSearchCV\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\n\n","da42de61":"def showplot():\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n\n    # plot the decision function\n    ax = plt.gca()\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n\n    # create grid to evaluate model\n    xx = np.linspace(xlim[0], xlim[1], 30)\n    yy = np.linspace(ylim[0], ylim[1], 30)\n    YY, XX = np.meshgrid(yy, xx)\n    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n    Z = clf.decision_function(xy).reshape(XX.shape)\n\n    # plot decision boundary and margins\n    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n               linestyles=['--', '-', '--'])\n    # plot support vectors\n    ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n               linewidth=1, facecolors='none', edgecolors='k')\n    # plt.figure(figsize=(20,20))\n    plt.show()","b9761686":"# we create 40 separable points\nX, y = make_blobs(n_samples=40, centers=2, random_state=4)\n\n# fit the model, don't regularize for illustration purposes\nclf = svm.SVC(kernel='linear', C=10,gamma=1)\nclf.fit(X, y)\nshowplot()\n","d823675d":"parameters = {'gamma': [0.01, 0.1, 10],\n              'C': [0.001, 0.01, 0.1]}\n\nmodel = svm.SVC(kernel = 'rbf')\n\ngrid = GridSearchCV(estimator = model, \n                    param_grid = parameters, \n                    cv = 5, \n                    n_jobs = -1, \n                    scoring = 'accuracy', \n                    verbose = 1, \n                    return_train_score = True)\ngrid.fit(X, y)","3d2d21c3":"print('Score = %3.2f'%grid.score(X, y))\ncv_results = pd.DataFrame(grid.cv_results_)\ncv_results.head()\n\nprint('Best Score', grid.best_score_)\nprint('Best hyperparameters', grid.best_params_)","81d5f270":"model_opt = svm.SVC(C = 0.001, gamma = 0.01, kernel = 'rbf')\nmodel_opt.fit(X, y)","24480ba2":"from sklearn import metrics\ny_pred = model_opt.predict(X)\nprint('Accuracy', metrics.accuracy_score(y, y_pred))\nprint('Classification Report: \\n', metrics.classification_report(y, y_pred))","19423980":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","774f2774":"# Import the dataset using Seaborn library\niris=pd.read_csv('..\/input\/iris\/Iris.csv',index_col='Id')","9c40e390":"iris.head()","39d88fbe":"# Creating a pairplot to visualize the similarities and especially difference between the species\nsns.pairplot(data=iris, hue='Species', palette='Set2')","645fb96f":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix","4918b24f":"# Separating the independent variables from dependent variables\nx=iris.iloc[:,:-1]\ny=iris.iloc[:,4]\nx_train,x_test, y_train, y_test=train_test_split(x,y,test_size=0.30)","777baf77":"from sklearn.svm import SVC\nmodel=SVC()\nmodel.fit(x_train, y_train)","19d750dd":"pred=model.predict(x_test)\nprint(confusion_matrix(y_test,pred))\nprint(classification_report(y_test, pred))","e51459a0":"mnist_train = pd.read_csv('..\/input\/mnist-dataset\/mnist_train.csv', index_col = False)\nmnist_test = pd.read_csv('..\/input\/mnist-dataset\/mnist_test.csv', index_col = False)","83d68696":"mnist_train.head()","19475661":"def get_accuracy(X_train, X_validate, y_train, y_validate, k):\n    '''fitting the SVC model for various kernels like linear, polynomial and rbf\n    and finding the accuracy for each kernel for the train and validate sets. Based on the accuracy\n    an appropriate kernel will be chosen for hyperparameter tuning and final model building with optimum hyperparameters'''\n    \n    #Caling teh scale_df() and storing the scaled df\n    #X_train_scaled = scale_df(X_train)\n    #X_validate_scaled = scale_df(X_validate)\n    \n    #Building a likear model for kernel type k passed in the parameter\n    SVC_model = svm.SVC(kernel = k)\n    \n    #Fitting the model for the training set\n    SVC_model.fit(X_train, y_train)\n    #Predicting the labels for the training set\n    y_train_predict = SVC_model.predict(X_train)\n    #Accuracy for the training set\n    train_accuracy = metrics.accuracy_score(y_train, y_train_predict)\n    #Classification Report\n    #c_report_train = metrics.classification_report(y_train, y_train_predict)\n    \n    #Fitting the model for validation set\n    SVC_model.fit(X_validate, y_validate)\n    #Predicting the labels for the validation set\n    y_validate_predict = SVC_model.predict(X_validate)\n    #Accuracy for the validation set\n    validate_accuracy = metrics.accuracy_score(y_validate, y_validate_predict)\n    #Classification Report\n    #c_report_validate = metrics.classification_report(y_validate, y_validate_predict)\n    \n    #returning the accuracy for the train and validate set\n    return(train_accuracy, validate_accuracy)","c82e2033":"def scale_df(X):\n    '''Scaling the data set using StandardScaler() and returning the scaled data set'''\n    scale = StandardScaler()\n    X_scaled = scale.fit_transform(X)\n    return(X_scaled)\n\nX_ = mnist_train.iloc[:, 1:]\ny_ = mnist_train.iloc[:, 0]\nprint(X_.shape)\nprint(y_.shape)\n\nX_train, X_validate, y_train, y_validate = train_test_split(scale_df(X_), y_, test_size = 0.20, random_state = 30, stratify = y_)\nprint(X_train.shape)\nprint(X_validate.shape)\nprint(y_train.shape)\nprint(y_validate.shape)","5973e292":"train_accuracy_linear, validate_accuracy_linear = get_accuracy(X_train, X_validate, y_train, y_validate, 'linear')\n# train_accuracy_poly, validate_accuracy_poly = get_accuracy(X_train, X_validate, y_train, y_validate, 'poly')\n# train_accuracy_rbf, validate_accuracy_rbf = get_accuracy(X_train, X_validate, y_train, y_validate, 'rbf')","291371f4":"print('Kernel = Linear')\nprint('Train Accuracy = ', train_accuracy_linear)\n#print('Train Classification Report: \\n', c_report_train_linear)\nprint('Validate Accuracy = ', validate_accuracy_linear)\n#print('Validate Classification Report: \\n', c_report_validate_linear)\n\n# print('\\n Kernel = Polynomial')\n# print('Train Accuracy = ', train_accuracy_poly)\n# #print('Train Classification Report: \\n', c_report_train_poly)\n# print('Validate Accuracy = ', validate_accuracy_poly)\\\n# #print('Validate Classification Report: \\n', c_report_validate_poly)\n\n# print('\\n Kernel = RBF')\n# print('Train Accuracy = ', train_accuracy_rbf)\n# #print('Train Classification Report: \\n', c_report_train_rbf)\n# print('Validate Accuracy = ', validate_accuracy_rbf)\n# #print('Validate Classification Report: \\n', c_report_validate_rbf)","c56c5fd1":"## Iris Dataset","7b7f0b73":"## Let use a bigger dataset"}}