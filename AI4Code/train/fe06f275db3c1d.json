{"cell_type":{"5cf9124a":"code","4de97013":"code","68cd794d":"code","3898a1af":"code","e7f1ddd5":"code","e7682901":"code","3cf81185":"code","65cc1c33":"code","87cf0a68":"code","cd18a387":"code","15cae1dc":"code","2e985449":"code","4cdfd8a0":"code","9008620a":"code","64a7901a":"code","41ce20e2":"code","b6d830ee":"code","0cb4d1c4":"code","9199190a":"code","e7d42220":"code","cd5eb286":"markdown","d4f8768b":"markdown","3b60cf6b":"markdown","3676ddbd":"markdown","07fcff09":"markdown"},"source":{"5cf9124a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4de97013":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split","68cd794d":"nltk.download(['stopwords','punkt','wordnet'])","3898a1af":"df=pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv',encoding = \"ISO-8859-1\")\ndf.head()","e7f1ddd5":"df.shape","e7682901":"X=df['v2']\nX.head()","3cf81185":"y=pd.get_dummies(df['v1'])\ny=y['spam']\ny.head()","65cc1c33":"stemmer=PorterStemmer()\nlemmatizer=WordNetLemmatizer()","87cf0a68":"def stemmatize(msg):\n  msg=msg.lower()\n  msg=re.sub('[^a-zA-Z]',' ',msg)\n  msg=[stemmer.stem(word) for word in msg.split() if word not in set(stopwords.words(\"english\"))]\n  msg=' '.join(msg)\n  return msg","cd18a387":"def lemmatize(msg):\n    msg=msg.lower()\n    msg=re.sub('[^a-zA-Z]',' ',msg)\n    msg=[lemmatizer.lemmatize(word) for word in msg.split() if word not in set(stopwords.words(\"english\"))]\n    msg=' '.join(msg)\n    return msg","15cae1dc":"X=df['v2'].apply(lemmatize)\nX.head()","2e985449":"# Number of unique tokens\nws=set()\nfor i in X:\n  ws.update(set(i.split()))\nlen(ws)","4cdfd8a0":"# convert to vectors - Bag of words\nbow=CountVectorizer()\nX=bow.fit_transform(X.tolist())","9008620a":"# convert to vectors - TF-IDF\ntfidf=TfidfVectorizer()\nX=tfidf.fit_transform(X)","64a7901a":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.1)\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","41ce20e2":"nb_clf=MultinomialNB().fit(X_train,y_train)\ny_pred=nb_clf.predict(X_test)","b6d830ee":"acc_nb_stem_bow = accuracy_score(y_test,y_pred)\nprint(acc_nb_stem_bow)\nprint(confusion_matrix(y_test,y_pred))","0cb4d1c4":"acc_nb_lem_bow = accuracy_score(y_test,y_pred)\nprint(acc_nb_lem_bow)\nprint(confusion_matrix(y_test,y_pred))","9199190a":"acc_nb_stem_tfidf = accuracy_score(y_test,y_pred)\nprint(acc_nb_stem_tfidf)\nprint(confusion_matrix(y_test,y_pred))","e7d42220":"acc_nb_lem_tfidf = accuracy_score(y_test,y_pred)\nprint(acc_nb_lem_tfidf)\nprint(confusion_matrix(y_test,y_pred))","cd5eb286":"Naive bayes - Stemmatized - Bag of words","d4f8768b":"Naive bayes - Lemmatized - TF-IDF","3b60cf6b":"Naive bayes - Stemmatized - TF-IDF","3676ddbd":"Naive bayes - Lemmatized - Bag of words\n","07fcff09":"Naive Bayes"}}