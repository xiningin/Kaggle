{"cell_type":{"580c6501":"code","a7c243a5":"code","e6eca565":"code","ac959c6a":"code","b5d5991f":"code","7d182907":"code","bb4d7938":"code","1c2d60eb":"code","4f598179":"code","9bac30fc":"code","fd3455df":"code","e82b2095":"code","5d5f0159":"code","70280793":"code","ba2b87af":"code","1eabb5d7":"code","bbc9d6c2":"code","66897606":"code","c9019320":"code","c18e9023":"code","12683d2f":"code","3da07d53":"code","d978d533":"code","f1011a07":"code","98b3976a":"code","44565de9":"code","1352dfe7":"code","4c5d8648":"code","9d8e37b6":"code","53f094f7":"code","bb2f78ce":"code","d6f5aa0d":"code","b729ceaf":"code","cf85b1bc":"code","0796ce6f":"code","0b86f830":"code","c4170dfa":"code","77251c08":"code","572eee7b":"code","e0c033f5":"code","1c04c368":"code","5ebc21a3":"code","807e485d":"code","9b74c164":"code","9d5a824f":"code","1c19a7cc":"code","dbd48bf2":"code","a1642861":"code","30ba8fcb":"markdown","cb245a25":"markdown","cc5886af":"markdown","b9137670":"markdown","de0482c1":"markdown","18930079":"markdown","bdcfa09c":"markdown","49df9b46":"markdown","dffc37db":"markdown","0c12614c":"markdown","89258f53":"markdown"},"source":{"580c6501":"import pandas as pd\nimport numpy as np\nimport os\nprint(os.listdir(\"..\/input\"))","a7c243a5":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","e6eca565":"from fastai.text import *\nfrom fastai import *\nimport re","ac959c6a":"df = pd.read_csv('..\/input\/wine-reviews\/winemag-data_first150k.csv',index_col=0)\ndf.shape","b5d5991f":"df.head()","7d182907":"data_lm = (TextList.from_df(df=df,path='.',cols='description') \n            .random_split_by_pct(0.1)\n            .label_for_lm()           \n            .databunch(bs=48))","bb4d7938":"data_lm.save('tmp_lm')","1c2d60eb":"data_lm.vocab.itos[:10]","4f598179":"data_lm.train_ds[0][0]","9bac30fc":"data_lm.train_ds[0][0].data[:10]","fd3455df":"data_lm = TextLMDataBunch.load('.', 'tmp_lm', bs=48)","e82b2095":"data_lm.show_batch()","5d5f0159":"fnames=['..\/input\/wt1031\/itos_wt103.pkl',\n       '..\/input\/wt1031\/lstm_wt103.pth']","70280793":"def language_model_learner(data:DataBunch, bptt:int=70, emb_sz:int=400, nh:int=1150, nl:int=3, pad_token:int=1,\n                  drop_mult:float=1., tie_weights:bool=True, bias:bool=True, qrnn:bool=False, pretrained_model=None,\n                  pretrained_fnames:OptStrTuple=None, **kwargs) -> 'LanguageLearner':\n    \"Create a `Learner` with a language model from `data`.\"\n    dps = default_dropout['language'] * drop_mult\n    vocab_size = len(data.vocab.itos)\n    model = get_language_model(vocab_size, emb_sz, nh, nl, pad_token, input_p=dps[0], output_p=dps[1],\n                weight_p=dps[2], embed_p=dps[3], hidden_p=dps[4], tie_weights=tie_weights, bias=bias, qrnn=qrnn)\n    learn = LanguageLearner(data, model, bptt, split_func=lm_split, **kwargs)\n    if pretrained_model is not None:\n        model_path = Path('..\/input\/wt1031\/')\n        fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n        learn.load_pretrained(*fnames)\n        learn.freeze()\n    if pretrained_fnames is not None:\n        fnames = [learn.path\/learn.model_dir\/f'{fn}.{ext}' for fn,ext in zip(pretrained_fnames, ['pth', 'pkl'])]\n        learn.load_pretrained(*fnames)\n        learn.freeze()\n    return learn","ba2b87af":"learn = language_model_learner(data_lm,path='.', pretrained_model=' ', drop_mult=0.3)","1eabb5d7":"learn.lr_find()","bbc9d6c2":"learn.recorder.plot(skip_end=10)","66897606":"learn.fit_one_cycle(1, 5e-2)","c9019320":"learn.save('fit_head')","c18e9023":"learn.load('fit_head');","12683d2f":"learn.unfreeze()","3da07d53":"learn.fit_one_cycle(5, 1e-3)","d978d533":"learn.save('fine_tuned')","f1011a07":"learn.load('fine_tuned');","98b3976a":"TEXT = \"i taste hints of\"\nN_WORDS = 40\nN_SENTENCES = 2","44565de9":"print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))","1352dfe7":"learn.save_encoder('fine_tuned_enc')","4c5d8648":"min_samples=10\nlst=df.variety.value_counts()\nwines=lst[lst>min_samples].keys()\nsubdf=df[df.variety.isin(wines)]","9d8e37b6":"subdf.shape,df.shape","53f094f7":"data_clas = (TextList.from_df(df=subdf,path='.',cols='description', vocab=data_lm.vocab)\n             .random_split_by_pct(0.1)\n             .label_from_df('variety')\n             .databunch(bs=48))","bb2f78ce":"data_clas.save('tmp_clas')","d6f5aa0d":"data_clas = TextClasDataBunch.load('.', 'tmp_clas', bs=48)","b729ceaf":"data_clas.show_batch()","cf85b1bc":"learn = text_classifier_learner(data_clas, drop_mult=0.5)\nlearn.load_encoder('fine_tuned_enc')\nlearn.freeze()","0796ce6f":"learn.lr_find()","0b86f830":"learn.recorder.plot(skip_end=8)","c4170dfa":"learn.fit_one_cycle(1, 2e-2)\nprint ('')","77251c08":"learn.save('first')","572eee7b":"learn.load('first');","e0c033f5":"for i in range(2,5):\n    learn.freeze_to(-i)\n    learn.fit_one_cycle(1,slice((1*10**-i)\/(2.6**4),1*10**-i))\n    learn.save('sub-'+str(i))\n    print ('')","1c04c368":"learn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-5\/(2.6**4),1e-3))\nprint ('')","5ebc21a3":"learn.save('final')","807e485d":"learn.fit_one_cycle(5, slice(1e-5,1e-3))\nprint ('')","9b74c164":"learn.save('final')","9d5a824f":"learn.show_results(rows=10)","1c19a7cc":"learn.predict(\"tannins are well proportioned both grained and supple\")[0]","dbd48bf2":"learn.predict(\"a light wine with hints of bitterness and fruit\")[0]","a1642861":"learn.predict(\"a wine full of flavor and color, mostly white\")[0]","30ba8fcb":"## Test Sentence Completion","cb245a25":"## Numericalization\nAfter the tokens have been developed from the text, these are converted to a list of integers representing all the words i.e. our vocabulary.<br>\nNOTE: Only tokens that appear at list twice are retained, with a maximum vocabulary size of 60,000 (by default). The remaining tokens are replaced by the unknown token `UNK`.\n\nThe correspondance from ids tokens is stored in the `vocab` attribute of our datasets, in a dictionary called `itos` (for int to string).","cc5886af":"Here we will take advantage of transfer learning and the fastai provided model WT103. This model was pretrained on a cleaned subset of wikipeia called [wikitext-103](https:\/\/einstein.ai\/research\/blog\/the-wikitext-long-term-dependency-language-modeling-dataset)). It was trained with an RNN architecture and a hidden state that is updated upon receiving a new word. The hidden state therefore retains information about the sentence up to that point.<br>\nThis understanding of the text is utilized to build the classifier, however, we first need to fine-tine the pretrained model to our wine domain. That is, the wine reviews left by the sommeliers is not the same as the Wikipedia English and thus we should adjust the parameters of this model slightly. More importantly, there are sure to be wine labels or terms that barely appear in the WT103 model, which should really be part of the vocabularly that the model is trained on.","b9137670":"## Predictions On Fake Reviews","de0482c1":"## Predictions On Validation Set","18930079":"## Further Training","bdcfa09c":"# Data Classifier","49df9b46":"NOTE: In contrast to image classification (whereby images being an array of pixel values can be used as inputs for a CNN), the descriptions are composed of words and therefore mathematical functions are useless. Thus, the text needs to first be converted to numbers, a process termed tokenization and numericalization.","dffc37db":"Each line contains one review along with the corresponding country, designation, points awarded, price, province, region, variety and winery.<br>\nFirst, we want to create a language model, which gains an appreciation for wine semantics. We will do this by creating a language model from the wine descriptions (using transfer learning as discussed below).","0c12614c":"## Language Model Using ULMFiT\n","89258f53":"## Tokenization\nThis first step splits the raw sentences into words (or more correctly, tokens). Whilst this can be completed simply by splitting the sentences by spaces, we can achieve a more refined tokenization result by capturing:\n- punctuation\n- contractions of two different words e.g. isn't or don't\n- non-text e.g. HTML code\n\nNOTE: special tokens are also implemented (tokens beginning with xx), to replace unknown tokens or to introduce different text fields e.g. capitilization."}}