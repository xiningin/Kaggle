{"cell_type":{"eca8ecf2":"code","117d20b2":"code","c7f5cfb2":"code","cf30114e":"code","480e7b5e":"code","16edb754":"code","a6b47c4e":"code","3781feb5":"code","6b308a83":"code","b4189b40":"code","81c8223e":"code","f740b4c4":"code","d53e2816":"code","3175f718":"code","0647b6db":"code","d8f16e96":"code","c1f86b55":"code","51fa0344":"code","cfcb890f":"code","0099877a":"code","1bfba39e":"code","756fcd1f":"code","b543e804":"code","800e4588":"code","e6908548":"code","58efabbb":"code","ac53a2c8":"code","eab142e1":"code","249062ee":"code","66d0852b":"code","1e130a0d":"code","9f41822a":"code","016225f0":"code","9f38182a":"code","11bcf783":"code","ded23318":"code","4865212f":"code","e101ecec":"code","92400da9":"code","777cf5f7":"code","2e3f112f":"code","f4bc9330":"code","fcbb2761":"code","988f87bb":"code","444b170e":"code","b5d6f39f":"code","7c64a1db":"code","2bff7ad9":"code","b3979932":"code","08454768":"code","f128e6a6":"markdown","2699a675":"markdown","74d5d99d":"markdown","5cd09fde":"markdown","aaaaa66c":"markdown","c2384863":"markdown","2de6c004":"markdown","ade223f2":"markdown","c289c8e3":"markdown","8f4f173c":"markdown","b3d0a126":"markdown","9f9650c4":"markdown","2d93910a":"markdown","5bec5fec":"markdown","9ffdf3c9":"markdown","23be86ae":"markdown","e0970f8d":"markdown","9668b33a":"markdown","6e4dfc89":"markdown","dc8badca":"markdown","cd6e351f":"markdown","6264fc25":"markdown","ee1a6238":"markdown","e7a47191":"markdown","6153ad63":"markdown"},"source":{"eca8ecf2":"import numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom sklearn.preprocessing import OneHotEncoder, QuantileTransformer, StandardScaler, MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_validate, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.base import clone\nfrom sklearn.model_selection import train_test_split","117d20b2":"train_data = pd.read_csv('\/kaggle\/input\/adult-pmr3508\/train_data.csv')\nprint(train_data.shape)\ntrain_data.head()","c7f5cfb2":"print(\"Workclass: \",pd.unique(train_data['workclass']))\nprint(\"\\nEducation: \",train_data.loc[:,['education','education.num']].sort_values(['education.num','education']).drop_duplicates(keep='first'))\nprint(\"\\nMarital status: \",pd.unique(train_data['marital.status']))\nprint(\"\\nOccupation: \", pd.unique(train_data['occupation']))\nprint(\"\\nRelationship: \", pd.unique(train_data['relationship']))\nprint(\"\\nRace: \", pd.unique(train_data['race']))\nprint(\"\\nSex: \", pd.unique(train_data['sex']))\nprint(\"\\nNative country: \", pd.unique(train_data['native.country']))\nprint(\"\\nIncome: \", pd.unique(train_data['income']))","cf30114e":"fig = px.histogram(train_data, x='age', color='income')\nfig.update_layout(height=500,width=800)\nfig.show()\n\nfig = px.histogram(train_data, x='education.num', color='income')\nfig.update_layout(height=500,width=800)\nfig.show()\n\nfig = px.histogram(train_data, x='marital.status', color='income')\nfig.update_layout(height=500,width=800)\nfig.show()\n\nfig = px.histogram(train_data, x='occupation', color='income')\nfig.update_layout(height=500,width=800)\nfig.show()\n\nfig = px.histogram(train_data, x='workclass', color='income')\nfig.update_layout(height=500,width=800)\nfig.show()\n\nfig = px.histogram(train_data, x='relationship', color='income')\nfig.update_layout(height=500,width=800)\nfig.show()\n\nfig = px.histogram(train_data, x='race', color='income')\nfig.update_layout(height=500,width=800)\nfig.show()\n\nfig = px.histogram(train_data, x='sex', color='income')\nfig.update_layout(height=500,width=800)\nfig.show()\n\nfig = px.histogram(train_data, x='native.country', color='income')\nfig.update_layout(height=500,width=800)\nfig.show()\n\nfig = px.histogram(train_data, x='fnlwgt', color='income')\nfig.update_layout(height=500,width=800)\nfig.show()\n\nfig = px.histogram(train_data, x='capital.gain', color='income')\nfig.update_layout(height=500,width=800)\nfig.show()\n\nfig = px.histogram(train_data, x='capital.loss', color='income')\nfig.update_layout(height=500,width=800)\nfig.show()\n\nfig = px.histogram(train_data, x='hours.per.week', color='income')\nfig.update_layout(height=500,width=800)\nfig.show()","480e7b5e":"fig = make_subplots(cols=2, rows=2)\n\nfig.add_trace(go.Box(x=train_data['income'],y=train_data['capital.gain'],name='capital gain'), row=1, col=1)\nfig.add_trace(go.Box(x=train_data['income'],y=train_data['capital.loss'],name='capital loss'), row=1, col=2)\nfig.add_trace(go.Box(x=train_data['income'],y=train_data['age'],name='age'), row=2, col=1)\nfig.add_trace(go.Box(x=train_data['income'],y=train_data['hours.per.week'],name='hours'), row=2, col=2)\n\nfig.show()","16edb754":"print(train_data['workclass'].value_counts()['?'])\nprint(train_data['occupation'].value_counts()['?'])\nprint(train_data['native.country'].value_counts()['?'])\n\n# Pelos gr\u00e1ficos, \u00e9 poss\u00edvel identificar as modas de workclass (private) e occupation (prof-specialty)\ntrain_data['workclass'] = train_data['workclass'].replace('?', 'Private')\ntrain_data['occupation'] = train_data['occupation'].replace('?', 'Prof-specialty')\ntrain_data = train_data.drop(columns=['native.country'])\ntrain_data.isnull().sum()","a6b47c4e":"train_data = train_data.dropna()\ntrain_data.shape","3781feb5":"train_y = train_data.iloc[:,-1]\ntrain_x = train_data.iloc[:, :-1]\n\ntrain_y = np.where(train_y == \">50K\", 1, 0)","6b308a83":"enc1 = OneHotEncoder(handle_unknown='ignore')\nenc2 = OneHotEncoder(handle_unknown='ignore')\nenc3 = OneHotEncoder(handle_unknown='ignore')\nenc4 = OneHotEncoder(handle_unknown='ignore')\n","b4189b40":"ohe_sex = pd.DataFrame(enc1.fit_transform(train_x.iloc[:,[10]]).toarray())\nprint(enc1.categories_)\n\nohe_race = pd.DataFrame(enc2.fit_transform(train_x.iloc[:,[9]]).toarray())\nprint(enc2.categories_)\n\nohe_rel = pd.DataFrame(enc3.fit_transform(train_x.iloc[:,[8]]).toarray())\nprint(enc3.categories_)\n\nohe_workclass = pd.DataFrame(enc4.fit_transform(train_x.iloc[:,[2]]).toarray())\nprint(enc4.categories_)","81c8223e":"ohe_sex = ohe_sex.rename(columns={0:\"female\", 1:\"male\"})\nohe_race = ohe_race.rename(columns={0:'Amer-Indian-Eskimo',1:'Asian-Pac-Islander',2:'Black',3:'Other',4:'White'})\nohe_rel = ohe_rel.rename(columns={0:'Husband',1:'Not-in-family',2:'Other-relative',3:'Own-child',4:'Unmarried',5:'Wife'})\nohe_workclass = ohe_workclass.rename(columns={0:'Federal-gov',1:'Local-gov',2:'Never-worked',3:'Private',4:'Self-emp-inc',\n                                              5:'Self-emp-not-inc',6:'State-gov',7:'Without-pay'})","f740b4c4":"display(ohe_workclass.shape)","d53e2816":"aux = pd.concat([train_x.reset_index(), ohe_rel, ohe_race, ohe_sex, ohe_workclass], axis=1)\nprint(aux.shape)\naux.head()","3175f718":"train_x = aux.drop(columns=['sex','race','relationship','education','workclass','marital.status','occupation','Id','index'])\nprint(train_x.shape)\ntrain_x.head()","0647b6db":"print(train_x.columns)","d8f16e96":"aux = pd.concat([train_x,pd.DataFrame(np.reshape(train_y,(-1,1)))], axis=1)\naux = aux.rename(columns={0:'label'})\n\ncorr_mtx = aux.corr()\nfig = go.Figure(data=go.Heatmap(z=corr_mtx, x=corr_mtx.columns, y=corr_mtx.columns))\nfig.update_layout(\n    height=1000)\nfig.show()","c1f86b55":"qt = QuantileTransformer(output_distribution='normal', random_state=0)\nss = StandardScaler()\nmm = MinMaxScaler()\n\ntrain_x['age'] = mm.fit_transform(np.reshape(train_x['age'].to_numpy(), (-1,1)))\ntrain_x['education.num'] = mm.fit_transform(np.reshape(train_x['education.num'].to_numpy(), (-1,1)))\ntrain_x['capital.gain'] = mm.fit_transform(np.reshape(train_x['capital.gain'].to_numpy(), (-1,1)))\ntrain_x['capital.loss'] = mm.fit_transform(np.reshape(train_x['capital.loss'].to_numpy(), (-1,1)))\ntrain_x['hours.per.week'] = mm.fit_transform(np.reshape(train_x['hours.per.week'].to_numpy(), (-1,1)))\ntrain_x['fnlwgt'] = mm.fit_transform(np.reshape(train_x['fnlwgt'].to_numpy(), (-1,1)))","51fa0344":"fig = make_subplots(rows=3, cols=2)\ntrace1 = go.Histogram(x=train_x['age'], name=\"Age\")\ntrace2 = go.Histogram(x=train_x['education.num'], name=\"Education\")\ntrace3 = go.Histogram(x=train_x['fnlwgt'], name=\"fnlwgt\")\ntrace4 = go.Histogram(x=train_x['hours.per.week'], name=\"Hours per Week\")\ntrace5 = go.Histogram(x=train_x['capital.gain'], name=\"Capital Gain\")\ntrace6 = go.Histogram(x=train_x['capital.loss'], name=\"Capital Loss\")\n\nfig.add_trace(trace1, row=1, col=1)\nfig.add_trace(trace2, row=1, col=2)\nfig.add_trace(trace3, row=2, col=1)\nfig.add_trace(trace4, row=2, col=2)\nfig.add_trace(trace5, row=3, col=1)\nfig.add_trace(trace6, row=3, col=2)\n\nfig.update_layout(\n    height=1000)\nfig.show()","cfcb890f":"def calculate_accuracy(y_pred, y_true):\n    corrects = 0\n    for i in range(len(y_pred)):\n        if y_pred[i] == y_true[i]:\n            corrects = corrects + 1\n    return corrects\/len(y_pred)","0099877a":"train_x = train_x.loc[:,['age', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week', 'Husband', 'Not-in-family',\n                          'Own-child', 'Unmarried', 'Wife', 'female', 'Private', 'Self-emp-inc']]\nprint(train_x.shape)\nprint(train_x.head())\n","1bfba39e":"# new label\n\nsum_married = train_x['Wife'] + train_x['Husband']\n","756fcd1f":"train_x.loc[:,'Married'] = sum_married\ntrain_x = train_x.drop(columns=['Wife', 'Husband'])\n","b543e804":"### Preparando o validation set","800e4588":"train_split_x, val_split_x, train_split_y, val_split_y = train_test_split(train_x, train_y, test_size=0.2, random_state=0)","e6908548":"parameters_rf = {'n_estimators':[10,50,100,150,200,300,400], 'max_depth':[5,10,15,20,50]}\n\nrf = RandomForestClassifier(random_state=0)\nclf_rf = GridSearchCV(rf,parameters_rf,cv=5) #5-fold CV\nclf_rf.fit(train_split_x, train_split_y)\n","58efabbb":"clf_rf.best_params_","ac53a2c8":"parameters_rf = {'max_depth':[12,13,14,15,16,17]} # \"filtrando\" melhor o hiperpar\u00e2metro max_depth, dado o melhor resultado com 15\nrf = RandomForestClassifier(random_state=0, n_estimators=400)\nclf_rf = GridSearchCV(rf,parameters_rf,cv=5)\nclf_rf.fit(train_split_x, train_split_y)\nclf_rf.best_estimator_","eab142e1":"y_pred = clf_rf.predict(val_split_x)\nrfacc = calculate_accuracy(y_pred, val_split_y)\nprint(rfacc)","249062ee":"clf_lr = LogisticRegression(penalty='l2', random_state=0)\nclf_lr.fit(train_split_x, train_split_y)","66d0852b":"clf_lr.coef_","1e130a0d":"y_pred = clf_lr.predict(val_split_x)\nlracc = calculate_accuracy(y_pred, val_split_y)\nprint(lracc)","9f41822a":"parameters_svm = {'degree':[1,2,3,4]}\nsvm = SVC()\nclf_svm = GridSearchCV(svm, parameters_svm, cv=5)\nclf_svm.fit(train_split_x, train_split_y)","016225f0":"clf_svm.best_params_","9f38182a":"y_pred = clf_svm.predict(val_split_x)\nsvmacc = calculate_accuracy(y_pred, val_split_y)\nprint(svmacc)","11bcf783":"parameters_ab = {'n_estimators':[10,50,100,200], 'learning_rate':[0.05,0.1,0.5,1.0,1.5,2.0]}\nabc = AdaBoostClassifier(random_state=0)\nclf_abc = GridSearchCV(abc, parameters_ab, cv=5)\nclf_abc.fit(train_split_x, train_split_y)","ded23318":"clf_abc.best_params_","4865212f":"parameters_ab = {'learning_rate':[1.3,1.4,1.5,1.6,1.7]}\nabc = AdaBoostClassifier(random_state=0, n_estimators=200)\nclf_abc = GridSearchCV(abc, parameters_ab, cv=5)\nclf_abc.fit(train_split_x, train_split_y)","e101ecec":"clf_abc.best_params_","92400da9":"y_pred = clf_abc.predict(val_split_x)\nabcacc = calculate_accuracy(y_pred, val_split_y)\nprint(abcacc)","777cf5f7":"model = AdaBoostClassifier(random_state=0, n_estimators=200, learning_rate=1.7)\nmodel.fit(train_x, train_y)","2e3f112f":"test_data = pd.read_csv('..\/input\/adult-pmr3508\/test_data.csv')\nprint(test_data.shape)\ntest_data.head()","f4bc9330":"\nenc1 = OneHotEncoder(handle_unknown='ignore')\nenc2 = OneHotEncoder(handle_unknown='ignore')\nenc3 = OneHotEncoder(handle_unknown='ignore')\nenc4 = OneHotEncoder(handle_unknown='ignore')\n\nohe_sex = pd.DataFrame(enc1.fit_transform(test_data.iloc[:,[10]]).toarray())\nprint(enc1.categories_)\n\nohe_race = pd.DataFrame(enc2.fit_transform(test_data.iloc[:,[9]]).toarray())\nprint(enc2.categories_)\n\nohe_rel = pd.DataFrame(enc3.fit_transform(test_data.iloc[:,[8]]).toarray())\nprint(enc3.categories_)\n\nohe_workclass = pd.DataFrame(enc4.fit_transform(test_data.iloc[:,[2]]).toarray())\nprint(enc4.categories_)","fcbb2761":"test_data['workclass'].value_counts()","988f87bb":"ohe_sex = ohe_sex.rename(columns={0:\"female\", 1:\"male\"})\nohe_race = ohe_race.rename(columns={0:'Amer-Indian-Eskimo',1:'Asian-Pac-Islander',2:'Black',3:'Other',4:'White'})\nohe_rel = ohe_rel.rename(columns={0:'Husband',1:'Not-in-family',2:'Other-relative',3:'Own-child',4:'Unmarried',5:'Wife'})\nohe_workclass = ohe_workclass.rename(columns={0:'?',1:'Federal-gov',2:'Local-gov',3:'Never-worked',4:'Private',5:'Self-emp-inc',\n                                              6:'Self-emp-not-inc',7:'State-gov',8:'Without-pay'})","444b170e":"aux2 = pd.concat([test_data.reset_index(), ohe_rel, ohe_race, ohe_sex, ohe_workclass], axis=1)\nprint(aux2.shape)\naux2.head()","b5d6f39f":"aux2['Private'] = aux2['Private']+aux2['?']\ntest_data = aux2.loc[:,['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss',\n       'hours.per.week', 'Husband', 'Not-in-family', 'Other-relative',\n       'Own-child', 'Unmarried', 'Wife', 'Amer-Indian-Eskimo',\n       'Asian-Pac-Islander', 'Black', 'Other', 'White', 'female', 'male',\n       'Federal-gov', 'Local-gov', 'Private', 'Self-emp-inc',\n       'Self-emp-not-inc', 'State-gov', 'Without-pay']]\nprint(test_data.shape)\ntest_data.head()","7c64a1db":"qt = QuantileTransformer(output_distribution='normal', random_state=0)\nss = StandardScaler()\nmm = MinMaxScaler()\n\ntest_data['age'] = mm.fit_transform(np.reshape(test_data['age'].to_numpy(), (-1,1)))\ntest_data['education.num'] = mm.fit_transform(np.reshape(test_data['education.num'].to_numpy(), (-1,1)))\ntest_data['capital.gain'] = mm.fit_transform(np.reshape(test_data['capital.gain'].to_numpy(), (-1,1)))\ntest_data['capital.loss'] = mm.fit_transform(np.reshape(test_data['capital.loss'].to_numpy(), (-1,1)))\ntest_data['hours.per.week'] = mm.fit_transform(np.reshape(test_data['hours.per.week'].to_numpy(), (-1,1)))\ntest_data['fnlwgt'] = mm.fit_transform(np.reshape(test_data['fnlwgt'].to_numpy(), (-1,1)))","2bff7ad9":"test_x = test_data.loc[:,['age', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week', 'Husband', 'Not-in-family',\n                          'Own-child', 'Unmarried', 'Wife', 'female', 'Private', 'Self-emp-inc']]\n\nsum_married = test_x['Wife'] + test_x['Husband']\ntest_x.loc[:,'Married'] = sum_married\ntest_x = test_x.drop(columns=['Wife', 'Husband'])\nprint(test_x.shape)\n\n","b3979932":"test_y = model.predict(test_x)\ntest_y_str = np.where(test_y == 1, \">50K\", \"<=50K\")\ny_df = pd.DataFrame(test_y_str, columns=['income'])\n","08454768":"y_df.to_csv('submission.csv', index=True, index_label='Id')","f128e6a6":"Podemos observar um dataset de 32560 dados, 15 features (sendo uma delas o id de cada dado\/pessoa) e a label (income >50K ou <=50K).\n\nAs features `age`, `fnlwgt` (final weight, n\u00famero esperado de pessoas que essa linha representa), `capital.gain`, `capital.loss`, `hours.per.week` s\u00e3o quantitativas.\n\nJ\u00e1 as features `workclass`, `education` (`education.num` como classifica\u00e7\u00e3o em forma num\u00e9rica do atributo anterior), `marital.status`, `occupation`, `relationship`, `race`, `sex`, `native.country` s\u00e3o qualitativas. Podemos, no entanto, encarar a feature `education.num` como quantitativa, uma vez que quanto maior o grau de escolaridade da pessoa, maior o `education.num`.\n\n\nAbaixo, podemos observar os valores que cada coluna pode assumir:","2699a675":"# PMR3508 - Multiple models for Adult Dataset","74d5d99d":"### Testando AdaBoost\n\nAqui, realizamos um GridSearch, a fim de encontrar os melhores valores para os hiperpar\u00e2metros relativos ao n\u00famero de estimadores (\"\u00e1rvores\") e learning rate.","5cd09fde":"Podemos ver que alguns campos possuem `?` como valor. Como comp\u00f5es uma parte consider\u00e1vel dos dados (cerca de 12%), atribuiremos a moda das categorias a eles. Eliminaremos entretanto a coluna `native.country` da an\u00e1lise.","aaaaa66c":"### Conhecendo a base de dados","c2384863":"Dada a quantidade consider\u00e1vel de '?' em `workclass` (6% dos dados), trataremos tais valores como 'private' (\"mediana\" dos dados).","2de6c004":"Vamos avaliar a relev\u00e2ncia de cada uma das features para o algoritmo, atrav\u00e9s de uma matriz de correla\u00e7\u00e3o:","ade223f2":"### Testando Random Forest\n\nPara o random forest, precisamos escolher os melhores valores para os hiperpar\u00e2metros relacionados ao n\u00famero de estimadores (\"\u00e1rvores\") e profundidade dos estimadores. Para esse classificador e para o AdaBoost, realizamos primeiro um grid search mais \"bruto\", para encontrar uma ordem de grandeza aproximada para os hiperpar\u00e2metros, e depois, um grid search mais fino, em fun\u00e7\u00e3o do resultado do primeiro. Isto, para reduzir o tempo de c\u00e1lculo gasto em cada modelo. Estamos usando cross validation (5-fold CV) para encontrar os melhores hiperpar\u00e2metros.","c289c8e3":"### Avaliando e treinando os modelos","8f4f173c":"Hash: 47\n\nEsta segunda vers\u00e3o do notebook tem como objetivo testar e comparar m\u00faltiplos modelos de machine learning.","b3d0a126":"Segundo a matriz de correla\u00e7\u00e3o, as features `fnlwgt`, `State-gov`, `Without-pay`, `Asian-Pac-Islander` t\u00eam baixa correla\u00e7\u00e3o com a label. `female` e `male` possuem correla\u00e7\u00e3o negativa entre si, podendo uma delas ser eliminada.\n\nT\u00eam maiores correla\u00e7\u00f5es com a label (em valor absoluto): `age`, `education.num`, `capital.gain`, `capital.loss`, `hours.per.week`, `Husband`, `Not-in-family`, `Own-child`, `Unmarried`, `Wife`, `female`, `male`, `Private` e `Self-emp-inc`.","9f9650c4":"### Testando SVM\n\nPara o SVM, devemos escolher o hiperpar\u00e2metro relativo ao grau da fun\u00e7\u00e3o que define o hiperespa\u00e7o que divide os dados nas duas diferentes categorias (income > ou <=50k).","2d93910a":"A fim de entender melhor os dados com que estamos lidando, temos os seguintes histogramas:","5bec5fec":"Escolheremos o modelo que gerou melhor acur\u00e1cia. No caso, o melhor resultado foi obtido com o AdaBoost, com learning rate = 1.7. Portanto, iremos treinar o modelo, com os melhores hiperpar\u00e2metros que encontramos para ele e usando todo o train set original. Em seguida, aplicaremos o modelo treinado no test set.","9ffdf3c9":"Atrav\u00e9s desses gr\u00e1ficos, podemos observar que grande parte dos atributos influenciam na renda da pessoa. Entretanto, o atributo `fnlwgt` n\u00e3o influencia na label. Maiores capital gain e capital loss est\u00e3o relacionados a rendas maiores que 50k, mas ao mesmo tempo, cerca de 99% dos dados possuem capital gain igual a 0 e o mesmo ocorre para o capital loss, segundo os box-plots acima.\nA distribui\u00e7\u00e3o de idade das pessoas que t\u00eam renda maior que 50k possui mediana maior que a das que possuem menos renda, e a quantidade de horas trabalhadas tamb\u00e9m \u00e9 maior entre os de maior renda.\n\nExistem profiss\u00f5es e estados civis mais relacionados a maiores rendas.\nOs dados sobre pa\u00eds de origem s\u00e3o categ\u00f3ricos e bastante desbalanceados, de forma a n\u00e3o valer a pena trabalhar com eles.\n","23be86ae":"### Importando bibliotecas","e0970f8d":"As features `Wife` e `Husband`, se iguais a 1, implicam em `female` e `male` iguais a 1, respectivamente. Podemos juntar `Wife` e `Husband` em uma \u00fanica feature `Married` (como s\u00e3o features bin\u00e1rias, esta pode ser calculada como a soma das duas primeiras), a fim de reduzir o n\u00famero de atributos a serem computados e diminuir um pouco a redund\u00e2ncia de informa\u00e7\u00f5es.\n\n\u00c9 poss\u00edvel observar que com tal redu\u00e7\u00e3o de redund\u00e2ncias, as m\u00e9tricas melhoraram um pouco.","9668b33a":"### Normalizando os dados","6e4dfc89":"Ao lidar com modelos de classifica\u00e7\u00e3o como SVM e regress\u00e3o com penaliza\u00e7\u00e3o, a aplica\u00e7\u00e3o de uma normaliza\u00e7\u00e3o \u00e9 importante, para que os atributos estejam distribu\u00eddos numa escala semelhante. Dadas as distribui\u00e7\u00f5es dos atributos (vistas acima, nos histogramas plotados), aplicaremos o MinMaxScaler para `age`, `education.num`, `capital.gain`, `capital.loss`, `hours.per.week` e `fnlwgt`.","dc8badca":"### Preparando o test set","cd6e351f":"Agora, com o modelo treinado, vamos prever os resultados para a base de teste.","6264fc25":"### Analisando os dados","ee1a6238":"### Testando regress\u00e3o log\u00edstica\n\nEstamos usando aqui o Ridge penalization.","e7a47191":"A fim de trabalhar com as vari\u00e1veis categ\u00f3ricas, realizaremos o one-hot encoding. Aplicaremos isso para `relationship`,`sex`,`marital status` e `race`. Dessa forma, a base de dados ficar\u00e1 assim:","6153ad63":"Neste momento, treinaremos 4 modelos utilizando todas as features obtidas anteriormente. Separaremos o dataset em train set e validation set, e com o conjunto de treino, aplicaremos um 5-fold cross-validation para determinar os melhores hiperpar\u00e2metros para cada modelo. Ser\u00e3o testados:\n* Random Forest\n* Regress\u00e3o Log\u00edstica (com penaliza\u00e7\u00e3o Ridge)\n* SVM\n* AdaBoost\n\nAo final, o modelo com melhor acur\u00e1cia ser\u00e1 selecionado."}}