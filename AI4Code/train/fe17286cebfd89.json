{"cell_type":{"31f52c55":"code","3c1ff548":"code","ae7c579c":"code","d02b7fbc":"code","f7c49637":"code","07af961f":"code","28ea6765":"code","8e9be0f6":"code","7945d17d":"code","061ed04f":"code","2831badd":"code","17614147":"code","0d9753c3":"code","23d4a24f":"code","c80daca0":"code","efe3fe04":"code","e68bcf97":"code","181067aa":"code","231d22be":"code","72c43b67":"code","e058d7d7":"code","f614a572":"code","2c71515f":"code","5b60bd7f":"code","ab6a5b69":"code","1e9d0e5a":"code","54a9476b":"code","0d202a42":"code","784c715f":"code","5e414081":"code","c38d936b":"code","0d0d5a13":"code","c8b32ec8":"markdown","b953ccc4":"markdown","956cf840":"markdown","59bd8c7e":"markdown","bb642924":"markdown","c0098957":"markdown","31adba89":"markdown","d77694a0":"markdown","f0374934":"markdown","11cabd7d":"markdown","b4539b3e":"markdown","95bade59":"markdown","70d191cf":"markdown","efd87b83":"markdown","2b54b57b":"markdown","88a7b46a":"markdown"},"source":{"31f52c55":"import numpy as np\nimport pandas as pd \nimport os\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KDTree\nfrom sklearn.decomposition import PCA\n\nimport re;\nimport logging;\nimport sqlite3;\nimport time;\nimport sys;\nimport multiprocessing;\nimport matplotlib.pyplot as plt;\n\n\n# Tokenisation & TF-IDF\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n","3c1ff548":"df=pd.read_csv(\"..\/input\/covidvaccine-tweets\/covidvaccine.csv\")\n#df = pd.read_csv(\"covidvaccine.csv\")\ndf.head(20)","ae7c579c":"df.shape","d02b7fbc":"df.info()","f7c49637":"df.is_retweet.value_counts()","07af961f":"#We create a pandas dataframe as follows:\ndata = pd.DataFrame(data=df.text)\ndata = data.rename(columns={'text' : 'Tweets'})\ndata.head()","28ea6765":"# We display the first 10 elements of the dataframe:\npd.set_option('max_colwidth',170)\ndisplay(data.head(10))","8e9be0f6":"docs=df.text.head(1000).values\ntype(docs)","7945d17d":"docs_clean = []\nfor doc in docs:\n    doc_2 = re.sub(r':.*$', \":\", doc)\n    docs_clean.append(doc_2)\n\ndocs_clean[:20]\n","061ed04f":"docs2=docs_clean","2831badd":"# remove punctuations\npunctuationChars = '!@#$%^&*(){}{}|;:\",.\/<>?' # you might choose different charcters to drop\nfor i in punctuationChars:\n    docs2 = np.char.replace(docs2, i, ' ')\n# remove apostrophe's (single quotes)\ndocs2 = np.char.replace(docs2,\"'\",' ')\n# remove line feeds\ndocs2 = np.char.replace(docs2,\"\\n\",' ')\n# remove 'http:'\ndocs2 = np.char.replace(docs2,\"https:\",' ')\ndocs2 = np.char.replace(docs2,\"https\",' ')\n\n# make lower case\nfor i,s in enumerate(docs2):\n    docs2[i] = s.lower()\n    \n# Show the cleaned data\n# Show the beginning of each document\n\n#for i in range(len(docs2)):\n#        print(f'\\ndoc{i}: {docs2[i]}') \n\nfor i in range(100):\n       print(f'\\ndoc{i}: {docs2[i]}') ","17614147":"vectorizer = TfidfVectorizer()\nvectors = vectorizer.fit_transform(docs2[0:10])\nfeature_names = vectorizer.get_feature_names()\ndense = vectors.todense()\ndenselist = dense.tolist()\ndf = pd.DataFrame(denselist, columns=feature_names)\ndf","0d9753c3":"def spacy_tokenizer(document):\n    tokens = nlp(document)\n    tokens = [token for token in tokens if (\n        token.is_stop == False and \\\n        token.is_punct == False and \\\n        token.lemma_.strip()!= '')]\n    tokens = [token.lemma_ for token in tokens]\n    return tokens","23d4a24f":"# test data to see what spacy tokenizer can do.\nexample_corpus = [\n    \"Monsters are bad. They likes to eat geese. I saw one goose flying away\", \\\n    \"I saw a monster yesterday. The meaning is so obvious!\", \\\n    \"Why are we talking about bad monsters? They are meanness.\"]","c80daca0":"tfidf_vector = TfidfVectorizer(input = 'content', tokenizer = spacy_tokenizer)\n# test\ncorpus=example_corpus\n# fit: learns vocabulary and idf\n# transform: transforms documents into document-term matrix\nresult_test = tfidf_vector.fit_transform(corpus)\nresult_test","efe3fe04":"dense = result_test.todense()\ndenselist = dense.tolist()\ndf_test = pd.DataFrame(\n    denselist,columns=tfidf_vector.get_feature_names())\ndf_test","e68bcf97":"tfidf_vector = TfidfVectorizer(input = 'content', tokenizer = spacy_tokenizer)\ncorpus = docs2\n\n# fit: learns vocabulary and idf\n# transform: transforms documents into document-term matrix\nresult = tfidf_vector.fit_transform(corpus)\nresult","181067aa":"# We can check which terms are actually considered from the sentences with the get_feature_names method:\ntfidf_vector.get_feature_names()[1:500]","231d22be":"dense = result.todense()\ndenselist = dense.tolist()\ndf = pd.DataFrame(\n    denselist,columns=tfidf_vector.get_feature_names())\ndf\n","72c43b67":"df[[\"australia\", \"manufacture\", \"covid-19\"]]","e058d7d7":"from sklearn.metrics.pairwise import linear_kernel\ncos_df = pd.DataFrame(columns=df.index)\nfor i in range(999):\n    curr_cos_sim = linear_kernel(result[i:i+1], result).flatten()\n    cos_df[i] = curr_cos_sim\n    \ncos_df","f614a572":"from sklearn.cluster import KMeans","2c71515f":"kmeans_models = {}\nfor i in range(2,13+1):\n    current_kmean = KMeans(n_clusters=i).fit(result)\n    kmeans_models[i] = current_kmean","5b60bd7f":"cluster_df = pd.DataFrame()\ncluster_df['Review Texts'] = docs\nfor i in range(2, 13+1):\n    col_name = str(i) +'means_label'\n    cluster_df[col_name] = kmeans_models[i].labels_\ncluster_df","ab6a5b69":"Sum_of_squared_distances = []\nK = range(1,18)\nfor k in K:\n    km = KMeans(n_clusters=k)\n    km = km.fit(result)\n    Sum_of_squared_distances.append(km.inertia_)","1e9d0e5a":"plt.figure(figsize=(16,8))\nplt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","54a9476b":"cluster10 = cluster_df.iloc[:,[0,9]]\ncluster10_0 = cluster10.loc[cluster10[\"10means_label\"] == 0]\ncluster10_0.head(50)","0d202a42":"cluster10_1 = cluster10.loc[cluster10[\"10means_label\"] == 1]\ncluster10_1.head(50)","784c715f":"cluster10_2 = cluster10.loc[cluster10[\"10means_label\"] == 2]\ncluster10_2","5e414081":"cluster10_3 = cluster10.loc[cluster10[\"10means_label\"] == 3]\ncluster10_3.head(50)","c38d936b":"cluster10_4 = cluster10.loc[cluster10[\"10means_label\"] == 4]\ncluster10_4","0d0d5a13":"cluster10_5 = cluster10.loc[cluster10[\"10means_label\"] == 5]\ncluster10_5","c8b32ec8":"## Apply Spacy tokenizer, TF-IDF, K-means for our first 1000 tweets.","b953ccc4":"### Define Spacy Tokenizer","956cf840":"## Cluster_2 focus on topics related to Russia Vaccine.","59bd8c7e":"Read the data","bb642924":"## What's Next?\n### 1. Using KNN or cosine similarity to classify the new tweets\n### 2. Based on the insight we get from the existing clusters, extract the useful Information that related to the topic you are interested.","c0098957":"### Successfully extraxt intended meaning of the words. 14 tokens for the example corpus.","31adba89":"## Create the clustering table","d77694a0":"## Elbow Method to determine the best K","f0374934":"It\u2019s a sparse matrix with 1000 reviews and 3191 terms, out of those 3191000 possible numbers there are 9169 non-zero TF-IDF values. We can check which terms are actually considered from the sentences with the get_feature_names method:","11cabd7d":"### check the cosine similarity","b4539b3e":"### Let's see the weights for words contained in the first Tweet.","95bade59":"## There is no row with is_retweet = True","70d191cf":"## CLuster_3 contains more rumors and negtive reactions.","efd87b83":"### Choose K = 10 to do the experiment","2b54b57b":"The sparse matrix format is an efficient way to store this information, but you might want to convert it to a more readable, dense matrix format using the todense method. \nTo create a pandas DataFrame from the results, you can use the following code:","88a7b46a":"## **Data Preprocessing**"}}