{"cell_type":{"211b5334":"code","f9b4bb26":"code","7dac3bbe":"code","84298db1":"code","a1495be6":"code","ad6ea245":"code","469913c7":"code","346508cf":"code","ac0af645":"code","5782b69a":"code","ef78b510":"code","ef2b97c0":"code","b962ad48":"code","a6d87fc2":"code","806172c4":"code","bb5adcc0":"code","5234c9bc":"code","1aa40305":"code","ce7dd6bc":"code","31a090fe":"code","f3671eef":"code","d87774d3":"code","091efc7f":"code","7a251d3e":"code","753120cd":"code","139eca9c":"code","45879c1f":"code","100642c5":"code","1c799b97":"code","00087a88":"code","37e36489":"code","9866e73d":"code","49ad42e6":"code","65893635":"code","e3361482":"code","51880ecf":"code","71003450":"code","90634c9d":"code","70c7d270":"code","f34f07f4":"code","6e1028ee":"code","dfbd1594":"code","127e779d":"code","77437b33":"code","91cfac5e":"code","b3235067":"code","652c4d54":"code","80b54780":"code","d3e56eca":"code","0bf50ad1":"code","96552349":"code","526e7e6e":"code","81e7e175":"code","63dc20c7":"code","87cb9ded":"markdown","a5e899d9":"markdown","77e1e41e":"markdown","97162ff4":"markdown","e519032f":"markdown","e282a9cd":"markdown","8232e890":"markdown","1ae2949e":"markdown","45d49299":"markdown","bd16bd40":"markdown","de55d351":"markdown","c3125a62":"markdown","94cd2ec6":"markdown","663ce46d":"markdown","fa7853e3":"markdown","2bfbcd38":"markdown","e6b11e0a":"markdown","5826690f":"markdown","fefa5e87":"markdown","e58eff2f":"markdown","4f968eff":"markdown","a22aaf29":"markdown","fa27b551":"markdown","61a80fcc":"markdown","2c601477":"markdown","898acc95":"markdown","0d73695b":"markdown","4f274abb":"markdown","8bfa09b4":"markdown","faaf66fc":"markdown","d00bcafe":"markdown","b029c175":"markdown","3d02a7b6":"markdown","da4e6c01":"markdown","017e50f4":"markdown","a5a1ada8":"markdown","f344115f":"markdown","ee427bc1":"markdown","3063580c":"markdown","e4e5effd":"markdown","dd056af5":"markdown","65621fc7":"markdown","91e8875f":"markdown","15cd7462":"markdown"},"source":{"211b5334":"# Importing the libraries\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\n# libraries for visualization\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nimport plotly\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport matplotlib.gridspec as gridspec\nfrom collections import Counter\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\n\n# libraries for modelling\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.metrics import classification_report,accuracy_score, roc_curve, precision_score\nfrom sklearn.metrics import confusion_matrix, recall_score, roc_auc_score, f1_score, precision_recall_curve\nfrom sklearn.model_selection import cross_val_score\nfrom imblearn.pipeline import make_pipeline\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import IsolationForest","f9b4bb26":"data = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","7dac3bbe":"data.shape","84298db1":"data.head().style.set_properties(**{\"background-color\":\"black\",\n                           \"color\" : \"lawngreen\"})","a1495be6":"data.info()","ad6ea245":"data.columns","469913c7":"data[['Time', 'Amount']].describe().style.set_properties(**{\"background-color\":\"black\",\n                           \"color\" : \"yellow\"})","346508cf":"# Descriptive statistics of  of frauds transactions\n\nsummary = (data[data['Class'] == 1].describe().transpose().reset_index())\n\nsummary = summary.rename(columns = {\"index\" : \"feature\"})\nsummary = np.around(summary,3)\n\nval_lst = [summary['feature'], summary['count'],\n           summary['mean'],summary['std'],\n           summary['min'], summary['25%'],\n           summary['50%'], summary['75%'], summary['max']]\n\ntrace  = go.Table(header = dict(values = summary.columns.tolist(),\n                                line = dict(color = ['#506784']),\n                                fill = dict(color = ['skyblue']),\n                               ),\n                  cells  = dict(values = val_lst,\n                                line = dict(color = ['#506784']),\n                                fill = dict(color = [\"lightgrey\",'#F5F8FF'])\n                               ),\n                  columnwidth = [150,60,100,100,80,80,80,80,80])\nlayout = go.Layout(dict(title = \"Summary for fraudulent Transactions\"))\nfigure = go.Figure(data=[trace],layout=layout)\npy.iplot(figure)\n","ac0af645":"# Descriptive statistics of geniune transactions\n\nsummary = (data[data['Class'] == 0].describe().transpose().reset_index())\n\nsummary = summary.rename(columns = {\"index\" : \"feature\"})\nsummary = np.around(summary,3)\n\nval_lst = [summary['feature'], summary['count'],\n           summary['mean'],summary['std'],\n           summary['min'], summary['25%'],\n           summary['50%'], summary['75%'], summary['max']]\n\ntrace  = go.Table(header = dict(values = summary.columns.tolist(),\n                                line = dict(color = ['#506784']),\n                                fill = dict(color = ['lightgreen']),\n                               ),\n                  cells  = dict(values = val_lst,\n                                line = dict(color = ['#506784']),\n                                fill = dict(color = [\"lightgrey\",'#F5F8FF'])\n                               ),\n                  columnwidth = [130,100,100,100,80,80,80,80,80])\nlayout = go.Layout(dict(title = \"Summary for Geniune Transactions\"))\nfigure = go.Figure(data=[trace],layout=layout)\npy.iplot(figure)\n","5782b69a":"# Percentages of fraudulent and non-fradulent transactions in data\n\nprint(f'Percent of Non-Fraudulent Transactions = {round(data[\"Class\"].value_counts()[0]\/len(data) * 100,3)}%')\nprint(f'Percent of Fraudulent Transactions = {round(data[\"Class\"].value_counts()[1]\/len(data) * 100,3)}%')","ef78b510":"# plotting a pie chart for fraud and non-fraud transactions\n\nfraud_or_not = data[\"Class\"].value_counts().tolist()\n\nlabels = ['Not Fraud','Frauds']\nvalues = [fraud_or_not[0], fraud_or_not[1]]\ncolors = ['skyblue', 'red']\n\ntrace = go.Pie(labels=labels, values=values, textinfo='value', \n               textfont=dict(size=20),\n               marker=dict(colors=colors, line=dict(color='#000000', width=0.1)))\n\nplotly.offline.iplot([trace], filename='styled_pie_chart')\n","ef2b97c0":"print('\\n\\033[1m  Fraudulent Transaction Distribution by amount \\033[0m')\nprint(\"-\"*50)\nprint(data[(data['Class'] == 1)]['Amount'].value_counts().head())","b962ad48":"corr = data.corrwith(data['Class']).reset_index()\n\ncorr.columns = ['Index','Correlations']\ncorr = corr.set_index('Index')\ncorr = corr.sort_values(by=['Correlations'], ascending = False)\n\nplt.figure(figsize=(8, 15))\nfig = sns.heatmap(corr, annot=True, fmt=\"g\", cmap='Set3', linewidths=0.4, linecolor='black')\n\nplt.title(\"Correlation of Variables with Class\", fontsize=20)\nplt.show()","a6d87fc2":"# Heatmap for explainatory variables\n\nplt.subplots(figsize=(12,8))\nplt.title(\"Correlation Matrix\", fontsize=15)\nsns.heatmap(data[['Time', 'Amount','Class']].corr(),linewidths=0.5, cmap=\"Paired\", linecolor='black',annot=True, annot_kws={'size':16},);","806172c4":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(20,10))\n\nfraud_transactions = data.Time[data.Class == 1]\nnormal_transactions = data.Time[data.Class == 0]\n\nax1.hist(fraud_transactions, bins = 50, color='deeppink', edgecolor=\"black\")\nax1.set_xlim([min(fraud_transactions), max(fraud_transactions)])\nax1.set_title('Fraudulent Transactions', fontsize=15)\nax1.set_ylabel(\"Number of Transactions\",  fontsize=13)\n\nax2.hist(normal_transactions, bins = 50, color='deepskyblue', edgecolor=\"black\")\nax2.set_xlim([min(normal_transactions), max(normal_transactions)])\nax2.set_title('Normal Transactions',  fontsize=15)\n\nax2.set_xlabel('Time (in Seconds)',  fontsize=13)\nax2.set_ylabel('Number of Transactions',  fontsize=13)\n\nplt.show()","bb5adcc0":"# converting seconds to time delta to extract hours and mins\n\ntimedelta = pd.to_timedelta(data['Time'], unit='s')\n\ndata['mins'] = (timedelta.dt.components.minutes).astype(int)\ndata['hours'] = (timedelta.dt.components.hours).astype(int)","5234c9bc":"# Countplots for hours vs count of transactions\n\nfig, axs = plt.subplots(3, figsize=(12,20))\n\nfig.subplots_adjust(hspace=.5)\n\nsns.countplot(data['hours'], ax = axs[0], palette=\"Pastel1\")\naxs[0].set_title(\"Distribution of Total Transactions\",fontsize=20)\naxs[0].set_facecolor(\"black\")\n\nsns.countplot(data[(data['Class'] == 1)]['hours'], ax=axs[1], palette='Pastel2')\naxs[1].set_title(\"Distribution of Fraudulent Transactions\", fontsize=20)\naxs[1].set_facecolor('black')\n\nsns.countplot(data[(data['Class'] == 0)]['hours'], ax=axs[2], palette='Set3')\naxs[2].set_title(\"Distribution of Normal Transactions\", fontsize=20)\naxs[2].set_facecolor(\"black\")\n\nplt.show()","1aa40305":"# Exploring the distribuition by Class types throught hours and minutes\n\nplt.figure(figsize=(20,8))\n\nsns.distplot(data[data['Class'] == 0]['hours'], bins=24, color='m')\nsns.distplot(data[data['Class'] == 1][\"hours\"], bins=24, color='r')\n\nplt.title('Fraudulent and Normal Transactions by Hours', fontsize=20)\n\nplt.xlabel(\"Time in Hours\", fontsize=13)\nplt.xlim([-1,25])\nplt.show()","ce7dd6bc":"#Exploring the distribuition by Class types throught hours and minutes\n\nplt.figure(figsize=(20,8))\n\nsns.distplot(data[data['Class'] == 0][\"mins\"], bins =60, color='deeppink')\nsns.distplot(data[data['Class'] == 1][\"mins\"], bins =60, color='coral')\n\nplt.title('Fraud and Normal Transactions by minutes', fontsize=20)\nplt.xlim([-1,61])\nplt.xlabel(\"Time in minutes\", fontsize=12)\nplt.show()","31a090fe":"# Scatter plot of Class vs Amount and Time for Normal Transactions \n\nplt.figure(figsize=(20,8))\n\nfig = plt.scatter(x=data[data['Class'] == 0]['Time'], y=data[data['Class'] == 0]['Amount'], color=\"dodgerblue\", s=50, edgecolor='black')\nplt.title(\"Time vs Transaction Amount in Normal Transactions\", fontsize=20)\nplt.xlabel(\"Time (in seconds)\", fontsize=13)\nplt.ylabel(\"Amount of Transaction\", fontsize=13)\n\nplt.show()","f3671eef":"# Scatter plot of Class vs Amount and Time for Fraudulent Transactions \n\nplt.figure(figsize=(20,8))\n\nfig = plt.scatter(x=data[data['Class'] == 1]['Time'], y=data[data['Class'] == 1]['Amount'], color=\"c\", s=80)\nplt.title(\"Time vs Transaction Amount in Fraud Cases\", fontsize=20)\nplt.xlabel(\"Time (in seconds)\", fontsize=13)\nplt.ylabel(\"Amount of Transaction\", fontsize=13)\n\nplt.show()","d87774d3":"#Looking the V's features\ncolumns = data.iloc[:,1:29].columns\n\nfrauds = data.Class == 1\nnormals = data.Class == 0\n\ngrid = gridspec.GridSpec(14, 2)\nplt.figure(figsize=(20,20*4))\n\nfor n, col in enumerate(data[columns]):\n    ax = plt.subplot(grid[n])\n    sns.distplot(data[col][frauds], color='deeppink', kde_kws={\"color\": \"k\", \"lw\": 1.5},  hist_kws=dict(alpha=1)) \n    sns.distplot(data[col][normals],color='darkturquoise', kde_kws={\"color\": \"k\", \"lw\": 1.5},  hist_kws=dict(alpha=1))\n    ax.set_ylabel('Density', fontsize=13)\n    ax.set_title(str(col), fontsize=20)\n    ax.set_xlabel('')\nplt.show()","091efc7f":"# copy of data for future use\n\ntemp = data.copy()","7a251d3e":"# Finding the 3rd and 1st Quantile for Amount Column\n\nQ3 = np.percentile(data['Amount'], 75)\nQ1 = np.percentile(data['Amount'], 25)\n\n# setting the cutoff\ncutoff = 5.0\n\n# computing the interquartile range\nIQR = (Q3 - Q1)\n\n# computing lower bound and upper bound\nlower_bound = Q1 - (IQR * cutoff)\nupper_bound = Q3 + (IQR * cutoff)\n\n# creating a filter to remove values less than lower bound and greater than\n# upper bound\nfilter_data = (data['Amount'] < lower_bound) | (data['Amount'] > upper_bound)\n\n# filtering data\noutliers = data[filter_data]['Amount']\nfraud_outliers = data[(data['Class'] == 1) & filter_data]['Amount']\nnormal_outliers = data[(data['Class'] == 0) & filter_data]['Amount']\n\nprint(f\"Total Number of Outliers : {outliers.count()}\")\nprint(f\"Number of Outliers in Fraudulent Class : {fraud_outliers.count()}\")\nprint(f\"No of Outliers in Normal Class : {normal_outliers.count()}\")\nprint(f\"Percentage of Fraud amount outliers : {round((fraud_outliers.count()\/outliers.count())*100,2)}%\")","753120cd":"# dropping the outliers\n\ndata = data.drop(outliers.index)\ndata.reset_index(inplace=True, drop=True)","139eca9c":"data.head().style.set_properties(**{\"background-color\":\"black\",\n                           \"color\" : \"skyblue\"})","45879c1f":"data.shape","100642c5":"# applying log transformation of Amount column\n\ndata['Amount'] = np.log(data['Amount'] + 0.001)","1c799b97":"# Box Plot for transformed Amount feature with class\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x =\"Class\", y=\"Amount\", data=data, palette='Set2');\nplt.xlabel(\"Amount\", fontsize=13)\nplt.ylabel(\"Class\", fontsize=13)\nplt.title(\"Box Plot for Transformed Amount Feature\", fontsize=20);","00087a88":"# scaling the time column\n\nrobust_scaler = RobustScaler()\ndata['Time'] = robust_scaler.fit_transform(data['Time'].values.reshape(-1,1))","37e36489":"# Divide into X and Y after removing useless columns\n\nX = data.drop(['Class','hours','mins'], 1)\nY = data.Class","9866e73d":"# Apply SMOTE\n\nprint(f'Original dataset shape : {Counter(Y)}')\n\nsmote = SMOTE(random_state=42)\nX_res, y_res = smote.fit_resample(X, Y)\n\nprint(f'Resampled dataset shape {Counter(y_res)}')","49ad42e6":"# creating a random sample of 5000 points \n\nX_vis = X_res.sample(5000, random_state=42)\ny_vis = y_res.sample(5000, random_state=42)\n\nprint(X_vis.shape)\nprint(y_vis.shape)","65893635":"# training the t-SNE model to reduce dimensionality\n# to 3\n\ntsne3d = TSNE(\n    n_components=3,\n    random_state=42,\n    verbose=2,\n).fit_transform(X_vis)","e3361482":"# plotting a 3D scatter plot \n\ntrace1 = go.Scatter3d(\n    x=tsne3d[:,0],\n    y=tsne3d[:,1],\n    z=tsne3d[:,2],\n    mode='markers',\n    marker=dict(\n        color = y_vis,\n        colorscale = ['deeppink', 'deepskyblue'],\n        colorbar = dict(title = 'Fraud'),\n        line=dict(color='rgb(255, 255, 255)'),\n        opacity=0.5\n    )\n)\n\ndata=[trace1]\n\nlayout = dict(title = 'TSNE (T-Distributed Stochastic Neighbourhood Embedding)',\n              showlegend= False, height=800, width=800,)\n\nfig=dict(data=data, layout=layout)\npy.iplot(fig, filename='3DBubble')","51880ecf":"# creating instance of statrifiedkfold split for 5 splits \nstrat = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\n# splitting the data\nfor train_index, test_index in strat.split(X, Y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = Y.iloc[train_index], Y.iloc[test_index]","71003450":"# Turning the splits into an array\n\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values","90634c9d":"# Creating a utility function to plot correlation matrix and roc_auc_curve\n\ndef show_metrics(model, y_test, y_pred):\n    fig = plt.figure(figsize=(20, 8))\n\n    # Confusion matrix\n    ax = fig.add_subplot(121)\n    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, annot_kws={\"size\": 16}, fmt='g', \n                cmap='Set3', linewidths=1, linecolor='white')\n\n    \n    # labels, title and ticks\n    ax.set_xlabel('Predicted labels', fontsize=15);\n    ax.set_ylabel('True labels', fontsize=15); \n    ax.set_title(\"Confusion Matix\", fontsize=20) \n    ax.xaxis.set_ticklabels(['No Fraud', 'Fraud'], fontsize=12); \n    ax.yaxis.set_ticklabels(['Fraud', 'No Fraud'], fontsize=12);\n\n    # ROC Curve\n    fig.add_subplot(122)\n    \n    \n    auc_roc = roc_auc_score(y_test, model.predict(original_Xtest))\n    fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(original_Xtest)[:,1])\n\n    plt.plot(fpr, tpr, color='darkturquoise', lw=2, marker='o', label='Trained Model (area = {0:0.3f})'.format(auc_roc))\n    plt.plot([0, 1], [0, 1], color='deeppink', lw=2, linestyle='--', label= 'No Skill (area = 0.500)')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate', fontsize=13)\n    plt.ylabel('True Positive Rate', fontsize=13)\n    plt.title('Receiver operating characteristic', fontsize=20)\n    plt.legend(loc=\"lower right\")\n    plt.show()","70c7d270":"# List to append the score and then find the average\naccuracy_lst = []\nprecision_lst = []\nrecall_lst = []\nf1_lst = []\nauc_lst = []\n\n# specifying the parameter grid for logistic regression\nlog_reg_params = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\n# Applying RandomsearchCV to find best model\nrand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n\n\n# iterating over all the splits\nfor train, test in strat.split(original_Xtrain, original_ytrain):\n    \n    # create pipeline with smote and the model \n    # sampling_strategy = minority because we want to only resample the minority class\n    pipeline = make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg)\n    \n    # fit the pipeline\n    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n    best_est = rand_log_reg.best_estimator_\n    prediction = best_est.predict(original_Xtrain[test])\n    \n    # finding mean for all the necessary measures to evaluate performance\n    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    precision_lst.append(precision_score(original_ytrain[test], prediction))\n    recall_lst.append(recall_score(original_ytrain[test], prediction))\n    f1_lst.append(f1_score(original_ytrain[test], prediction))\n    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n\nprint(\"Accuracy: {0:0.2f}%\".format(np.mean(accuracy_lst)*100))\nprint(\"Precision: {0:0.2f}\".format(np.mean(precision_lst)))\nprint(\"Recall: {0:0.2f}\".format(np.mean(recall_lst)))\nprint(\"f1 Score: {0:0.2f}\".format(np.mean(f1_lst)))","f34f07f4":"# predict on test set\n\ny_pred = best_est.predict(original_Xtest)","6e1028ee":"# plot confusion matrix and ROC curve\n\nshow_metrics(best_est, original_ytest, y_pred)","dfbd1594":"# Random forest Classifier\nrf_cfl = RandomForestClassifier(n_estimators = 200, \n                                 max_features = 3, \n                                 min_samples_leaf = 1, \n                                 min_samples_split = 2, \n                                 n_jobs = -1,\n                                random_state = 42)\n\nrf_cfl.fit(original_Xtrain, original_ytrain)\ny_pred = rf_cfl.predict(original_Xtest)","127e779d":"show_metrics(rf_cfl, original_ytest, y_pred)","77437b33":"print('Accuracy :{0:0.5f}'.format(accuracy_score(y_pred , original_ytest))) \nprint('AUC : {0:0.5f}'.format(roc_auc_score(original_ytest , y_pred)))\nprint('Precision : {0:0.5f}'.format(precision_score(original_ytest , y_pred)))\nprint('Recall : {0:0.5f}'.format(recall_score(original_ytest , y_pred)))\nprint('F1 : {0:0.5f}'.format(f1_score(original_ytest , y_pred)))","91cfac5e":"# Uncomment and run to find the best model using ExtraTrees (Takes a lot of time!)\n\n\n\n# accuracy_lst = []\n# precision_lst = []\n# recall_lst = []\n# f1_lst = []\n# auc_lst = []\n\n# # specifying the parameter grid for extra trees\n# extra_trees_params = {\"n_estimators\": [30,50,70,100], 'max_depth': [30,50,70,100], 'min_samples_split':[1,2,3,4,5]}\n\n# # Applying RandomsearchCV to find best model\n# rand_extra_trees = RandomizedSearchCV(ExtraTreesClassifier(random_state=42), extra_trees_params, n_iter=4)\n\n# # iterating over all the splits\n# for train, test in strat.split(original_Xtrain, original_ytrain):\n    \n#     # creating pipeline\n#     pipeline = make_pipeline(SMOTE(sampling_strategy='minority'), rand_extra_trees)\n#     model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n#     best_est = rand_extra_trees.best_estimator_\n#     prediction = best_est.predict(original_Xtrain[test])\n    \n#     # computing performance measures\n#     accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n#     precision_lst.append(precision_score(original_ytrain[test], prediction))\n#     recall_lst.append(recall_score(original_ytrain[test], prediction))\n#     f1_lst.append(f1_score(original_ytrain[test], prediction))\n#     auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n\n# print(\"Accuracy: {0:0.2f}%\".format(np.mean(accuracy_lst)*100))\n# print(\"Precision: {0:0.2f}\".format(np.mean(precision_lst)))\n# print(\"Recall: {0:0.2f}\".format(np.mean(recall_lst)))\n# print(\"f1 Score: {0:2f}\".format(np.mean(f1_lst)))","b3235067":"# training using the best model\n\nbest_model = ExtraTreesClassifier(max_depth=50, n_estimators=70, min_samples_split=5, random_state=42)","652c4d54":"fitted_model = best_model.fit(original_Xtrain, original_ytrain)\n\npredictions = fitted_model.predict(original_Xtest)","80b54780":"show_metrics(fitted_model, original_ytest, predictions)","d3e56eca":"# utility function to compute accuracy of normal transactions\ndef normal_accuracy(values):\n    \n    tp=list(values).count(1)\n    total=values.shape[0]\n    accuracy=np.round(tp\/total,4)\n    \n    return accuracy\n\n# utility function to compute accuracy of fraud transactions\ndef fraud_accuracy(values):\n    \n    tn=list(values).count(-1)\n    total=values.shape[0]\n    accuracy=np.round(tn\/total,4)\n    \n    return accuracy","0bf50ad1":"# create inliers and outliers data\n\ninliers = temp[temp.Class==0]\nins = inliers.drop(['Class'], axis=1)\n\noutliers = temp[temp.Class==1]\nouts = outliers.drop(['Class'], axis=1)\n","96552349":"# training of isolation forest\n\nISF = IsolationForest(random_state=42)\nISF.fit(ins)\n\nnormal_isf = ISF.predict(ins)\nfraud_isf = ISF.predict(outs)\n\nin_accuracy_isf=normal_accuracy(normal_isf)\nout_accuracy_isf=fraud_accuracy(fraud_isf)\nprint(f\"Accuracy in Detecting Normal Cases: {in_accuracy_isf*100}%\")\nprint(f\"Accuracy in Detecting Fraud Cases: {round(out_accuracy_isf*100, 2)}%\")","526e7e6e":"# dropping useless columns\n\nnew_data = temp.drop(['Time', 'Amount', 'V27', 'V28', 'V25', 'V23', 'V7', 'V13', 'V20','V22','mins','hours'], 1)","81e7e175":"# create inliers and outliers data\n\ninliers = new_data[new_data.Class==0]\nins = inliers.drop(['Class'], axis=1)\n\noutliers = new_data[new_data.Class==1]\nouts = outliers.drop(['Class'], axis=1)","63dc20c7":"# training of isolation forest on new data\n\nISF = IsolationForest(random_state=42)\nISF.fit(ins)\n\nnormal_isf = ISF.predict(ins)\nfraud_isf = ISF.predict(outs)\n\nin_accuracy_isf=normal_accuracy(normal_isf)\nout_accuracy_isf=fraud_accuracy(fraud_isf)\nprint(f\"Accuracy in Detecting Normal Cases: {in_accuracy_isf*100}%\")\nprint(f\"Accuracy in Detecting Fraud Cases: {out_accuracy_isf*100}%\")","87cb9ded":"&#9658; **Isolation forest successfully classified ~85% of the fraudulent transactions correction and it gives 96% on normal cases too which is pretty good.**\n\n&#9658; **Let us give a final try by dropping some of the columns which we found useless during EDA. According to graphs, the correlation between amount and time is not very significant. Also, we saw some of the V's which were overlapping like crazy so they definately are of no help to use and can even lower down the accuracy. So, we will drop them and try our Isolation forest again on the new data.**","a5e899d9":"&#9658; **We can clearly see that we don't have any null values in the data which is bound to happen in such datasets as each and every information is very necessary else the transaction isn't processed.**","77e1e41e":"&#9658; **Note that we will take the fresh data for performing anomaly detection. Also, we won't do outlier detection either, as out algorithm would already be doing that!**","97162ff4":"<h2><font color=blue> Isolation Forest<\/font><\/h2>\n\n&#9658; **Isolation Forest is an unsupervised anomaly detection algorithm that uses the two properties \u201cFew\u201d and \u201cDifferent\u201d of anomalies to detect their existence.**<br>\n\n&#9658; **Since anomalies are few and different, they are more susceptible to isolation. This algorithm isolates each point in the data and splits them into outliers or inliers. This split depends on how long it takes to separate the points. If we try to separate a point which is obviously a non-outlier, it\u2019ll have many points in its round, so that it will be really difficult to isolate. On the other hand, if the point is an outlier, it\u2019ll be alone and we\u2019ll find it very easily.**<br>\n","e519032f":"<h2><font color=red>Analysis of Time Column<\/font><\/h2>","e282a9cd":"> &#9658; **Using Extra Tree classifier we got 99.9% accuracy! But wait, let's look at the recall - only 80% which is even lower than our Logistic regression model. So, extra trees is performing bad on fraud data so let's try other models.**","8232e890":"<h2><font color=blue> Handling Class Imbalance <\/font><\/h2>\n\n&#9658; **Imbalanced data is a problem in supervised learning problems which can result is high bias towards majority class. As we have already seen that this data is severly imbalanced so to balance it we can use various techniques such as:** \n\n- **Oversampling**\n- **Undersampling** \n- **SMOTE**\n\n&#9658; **Out of all these three SMOTE is the most effective so we will go with it, In this technique, instead of simply duplicating data from the minority class, we synthesize new data from the minority class. This is a type of data augmentation for tabular data can be very effective. This approach to synthesizing new data is called the Synthetic Minority Oversampling TEchnique, or SMOTE for short.**","1ae2949e":"> &#9658; **Precision  and accuracy are good but the recall is pretty low due the which the model is not performing well on fraudulent data.**","45d49299":"&#9658; **Logistic regression did really well on the fraud class giving almost 95% accuracy and algo does well on the fraud class, predicting 78 fraud cases are correct but 12 cases as wrong so we still have a room for improvement. It we applied model to the imbalanced data instead then it results would be pathetic. You can try that for yourself.**","bd16bd40":"<h3><font color=red>OBSERVATIONS<\/font><\/h3>\n\n&#9658; **There are 113 fraud transactions for just one dollar and 27 fraud transaction for 99.99 dollars. Also, there are 27 fraud transaction for zero amount.<br>**\n\n&#9658; **The reason for zero transaction can be the Zero Authorization which is an account verification method for credit cards that is used to verify a cardholders information without charging the consumer.**","de55d351":"<h3><font color=blue>Class and Amount vs Time<\/font><\/h3>","c3125a62":"<h3><font color=red>OBSERVATIONS<\/font><\/h3>\n\n&#9658; **For some of the features we can observe a good selectivity in terms of distribution for the two values of Class: V4, V11 have clearly separated distributions for Class values 0 and 1, V12, V14, V18 are partially separated, V1, V2, V3, V10 have a quite distinct profile, whilst V20-V28 have similar profiles for the two values of Class and thus not very useful in differentiation of both the classes.**\n\n&#9658; **In general, with just few exceptions (Time and Amount), the features distribution for legitimate transactions (values of Class = 0) is centered around 0, sometime with a long queue at one of the extremities. In the same time, the fraudulent transactions (values of Class = 1) have a skewed (asymmetric) distribution.**","94cd2ec6":">&#9658; **Now let's scale the time column but we will use normal scaling this time.**","663ce46d":"<h3><font color=red>OBSERVATIONS<\/font><\/h3>\n\n&#9658; **The plots of both hours and minutes doesn't have any interesting trend.**","fa7853e3":"<h3><font color=blue>Class vs Time<\/font><\/h3>\n\n&#9658; **Let's plot the distribution of the classes with respect to time for both fraudulent and normal transactions.**","2bfbcd38":"<h3 align=\"center\"><font color=red>OBSERVATIONS<\/font><\/h3>\n\n#### <font color=blue>FRAUDULENT<\/font>\n\n>&#9658; **There are two peaks between 40000 seconds and 100000 seconds which were the maximum number of fraudulent transaction at any time.**\n\n\n#### <font color=blue>NORMAL<\/font>\n\n>&#9658; **Normal transactions have not much to uncover except the fact that there were less transactions somewhere around 20000 seconds and 100000 seconds which is not very useful.**","e6b11e0a":"&#9658; **Best Model : ExtraTreesClassifier(max_depth=50, n_estimators=70, min_samples_split=5, random_state=42)**\n\n&#9658; **Results:** \n\n- **Accuracy: 99.92%**<br>\n- **Precision: 0.79**<br>\n- **Recall: 0.80**<br>\n- **f1 Score: 0.78**<br>","5826690f":"<h1><font color=red>Data Cleaning and Preprocessing<\/font><\/h1>","fefa5e87":"> **Now, Let's try descriptive statistics filtering by fraudulent and non-fraudulent transactions.**","e58eff2f":">&#9658; **Let's look at the distribution of number of transactions for each hour for total, fraudulent and normal transactions.**","4f968eff":"&#9658; **V17, V14, V12 and V10 are negatively correlated. Notice how the lower these values are, the more likely the end result will be a fraud transaction.**<br>\n\n\n&#9658; **V2, V4, V11, and V19 are positively correlated. Notice how the higher these values are, the more likely the end result will be a fraud transaction.**\n\n\n&#9658; **Sadly, features that show correlation with response variable are not explainatory so we don't get much insights from this plot. But, we still have a lot to analyse so let's dive straight in!**","a22aaf29":"<h1><font color=red>Exploratory Data Analysis<\/font><\/h1>","fa27b551":"> &#9658; **As expected, t-SNE is performing well, seperating both the classes. The clusters are not perfectly seperated but it is able to seperate almost 80% of the sample data which is cool.**","61a80fcc":"<h2><font color=red> TASKS FOR YOU <\/font><\/h2>\n\n\n**(I) Provide suggestion, improvements and Criticism! (if any) :)**\n\n**(II) Please do Upvote if you want to see more content like this!**","2c601477":"> &#9658; **We have successfully removed the outliers now let's scale the Amount feature so that our model isn't baised towards amount feature which will also account for the skewness in the data.**","898acc95":"<h3><font color=blue>Descriptive Statistics<\/font><\/h3>\n\n&#9658; **As most of the columns V1, V2,... V28 are transformed using PCA so neither features make much sense and nor will the descriptive statistics so we will leave them and consider only Time and Amount which makes sense.**","0d73695b":"<h3 align=\"center\"><font color=red>OBSERVATIONS<\/font><\/h3>\n\n#### <font color=blue>FRAUDULENT<\/font>\n\n>&#9658; **Mean transaction is around 122 and standard deviation is around 256.**<br><br>\n>&#9658; **Maximum Transaction was 2125 and minimum was 0.**<br>\n\n#### <font color=blue>NORMAL<\/font>\n\n> &#9658; **Mean transaction is around 88 and standard deviation is around 250.**<br><br>\n> &#9658; **Maximum Transaction was 25691 and minimum was 0.**<br>","4f274abb":"<h3><font color=red>OBSERVATIONS<\/font><\/h3>\n\n&#9658; **Mean transaction is somewhere is 88 and standard deviation is around 250.**<br><br>\n&#9658; **The median is 22 which is very less as compared to mean which signifies that there are outliers or our data is highly positive skewed which is effecting the amount and thus the mean. High Skewness can be handled by using log transformation or boxcox transformation.**<br><br>\n&#9658; **The maximum transaction that was done is of 25,691 and minimum is 0.**\n","8bfa09b4":"&#9658; **Let's do some feature engineering on time and transform it to minutes and hours to uncover some of the hidden patterns.**","faaf66fc":"<h1><font color=red>Correlation of Features with Response<\/font><\/h1>\n\n&#9658; **Let's plot the correlation of our response i.e Class with all the other features in the data.**","d00bcafe":"&#9658; **This looks much better! We are doing pretty well on the fraud cases but we are now doing little less better on normal transactions which is fine as we are much more interested in Fraud Cases.**\n\n&#9658; **I think we might be able to do much better using some stacking classifiers as they work on the errors of the previous models which can eventually increase the accuracy but I will leave that for you to try. So, do let me know!**","b029c175":"<h2><font color=blue> Logistic Regression <\/font><\/h2>\n\n&#9658; **Let's start off with a simple model like Linear Regression. Note that I will be doing cross validation using Randomized search as the data is very huge and we will do this cross validation after splitting to avoid Data Leakage as discussed above.**","3d02a7b6":"### <h3><font color=red>OBSERVATIONS<\/font><\/h3>\n\n&#9658; **Due to confidentiality issue, original features V1, V2,... V28 have been transformed using PCA, however, my guess is that these features might be credit card number, expiry date, CVV, cardholder name, transaction location, transaction date-time, etc.**\n\n&#9658; **Only features which have not been transformed with PCA are 'Time',&nbsp; 'Amount' and 'Class'.**<br>\n\n> - **'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset.**<br><br>\n> - **The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning.**<br><br>\n> - **The Feature 'Class' is the response or target variable and it takes value 1 in case of fraud and 0 otherwise.**<br>","da4e6c01":"<h1><font color=red> Modelling <\/font><\/h1>\n\n&#9658; **In this section, we will finally apply models and classify whether a certain transaction done a particular time is fraud or geniune. Thus, this is a binary classification problem.**\n\n&#9658; **Important thing to note here is that we did SMOTE but we won't use that data, Why?**\n\n**If we used that data to predict the classes then it will result in a problem know as 'Data Leakage' which is another term for using test data for prediction or cross validation. So, this sounds like a good point to use Pipelines. Pipelines make our life easier by specifying what order should the operations be done on the data.**\n\n&#9658; **One thing we should keep in mind that we might get very high accuracy but we should focus on optimising out f1_score and recall as we want to perform better on fraud cases as they are the most important.**","017e50f4":"<h1><font color=red> Thanks for Reading! <\/font><\/h1>\n","a5a1ada8":">&#9658; **We can clearly see that now the data is completely balanced so let's use some visualisation technique to visualize this data.**\n\n>&#9658; **Note that, as our data has a lot of columns and humans can only understand 3D so we will use Dimensionality reduction technique to reduce our data to 3D and then plot it. So, let's get started!**","f344115f":"<h2><font color=blue> Random Forest Classifier <\/font><\/h2>\n\n&#9658; **Now, let's try something which can take account of complex realationships. There are many such models but Random forest is bit better as it is a ensemble model and focuses on reducing variance i.e overfitting without much effecting the bias which is all we want. Also, this algorithm works in time complexity, O(d.n.log(n)) where d is the number of features.**\n\n&#9658; **I have shown the best parameters after GridsearchCV and not the whole process itself as it is very time consuming and takes forever so you can try it yourself.**","ee427bc1":"<h2><font color=blue> Extra Trees Classifier <\/font><\/h2>\n\n&#9658; **Extra Trees Classifier is a ensemble model which is a randomised version of Random Forest and lowers variance even more. So, let's apply this model and see how well it can perform.**","3063580c":"<h2><font color=blue>Feature Scaling<\/font><\/h2>\n\n&#9658; **As the amount column is highly skewed so it will be better to apply log transoformation as it can result in nearly normal distribution which is suited for most of the algorithms.**","e4e5effd":"<h3 align=\"center\"><font color=red>OBSERVATIONS<\/font><\/h3>\n\n#### <font color=blue>FRAUDULENT<\/font>\n\n>&#9658; **There are much more outliers as compared to normal transactions.**<br><br>\n>&#9658; **The plot seems to not have any inherent pattern.**<br><br>\n\n#### <font color=blue>NORMAL<\/font>\n\n> &#9658; **There are a less number of outliers as compared to fraudulent transactions.**<br><br>\n> &#9658; **There are a lot of transactions with amount less than 5000.**<br>","dd056af5":"<h2><font color=blue>Outlier Removal<\/font><\/h2>\n\n&#9658; **As we already saw that amount column has a extreme outliers so it necessary to remove them as they can effect the model's performance. We will used Interquartile range to detect outliers which removes anything below the lower limit (25 percentile) and anything above upper limit (75 Percentile).**\n\n&#9658; **Note that, the data we have for fraudulent cases is very low so we wanna keep our cutoff a bit high so as avoid removing much of the fraud cases. Here, as the data is skewed (kind of exponential) so having high cutoff will help us. Let's take the cutoff value as 5.0 instead of 1.5 which is usually used.**","65621fc7":"<h2><font color=blue> Dimensionality Reduction and Clustering <\/font><\/h2>\n\n&#9658; **The dimesionality reduction technique I will use here is called t-SNE. It is state-of-the-art in visualizing high dimensional data and can help us understand how our model will perform or how seperable the data is using clusters.**","91e8875f":"<h2><font color=blue> Splitting the data <\/font><\/h2>\n\n&#9658; **As our data is imbalanced so we will not use train_test_split and instead we will use stratified split which will take the representative of respective populations i.e Fraud Transactions and Normal Transactions.**","15cd7462":"<h3><font color=red>OBSERVATIONS<\/font><\/h3>\n \n&#9658; **This dataset has 492 frauds out of 284,315 transactions. Thus, the dataset is highly unbalanced, the positive class (frauds) account for 0.173% of all transactions.**\n\n&#9658; **Most of the transactions are non-fraud which is obvious. If we use this data for our predictive models and analysis, our algorithms will probably overfit to the non-fraudulent transactions and will answer in non-fraudulent all the time which can result in actual frauds to slip by!**\n\n&#9658; **Note that our task is not to find the obvious, rather we have to find the anomalies and signs of fraud! Thus, we will take care of this imbalance during preprocessing.**"}}