{"cell_type":{"ec185785":"code","074557da":"code","9c66630c":"code","667d8ad6":"code","689b7364":"code","ee6bf086":"code","cebef357":"code","4e50f40f":"code","f2f0a312":"code","7f03b3a8":"code","f3125975":"code","6421d30f":"code","4ae18adb":"code","f1361aff":"code","a47bbe36":"code","a90b83c9":"markdown","850090f4":"markdown","5725c56b":"markdown","04bb256f":"markdown"},"source":{"ec185785":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')","074557da":"TestFilePath = '..\/input\/test.csv'\nTrainFilePath = '..\/input\/train.csv'\n\ntestData = pd.read_csv(TestFilePath)\ntrainData = pd.read_csv(TrainFilePath)\n\ntrainData.head(10)","9c66630c":"trainData[\"Age\"] = trainData[\"Age\"].fillna(-0.5)\ntestData[\"Age\"] = testData[\"Age\"].fillna(-0.5)\nbins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ntrainData['AgeGroup'] = pd.cut(trainData[\"Age\"], bins, labels = labels)\ntestData['AgeGroup'] = pd.cut(testData[\"Age\"], bins, labels = labels)\n\nfig = plt.figure(figsize=(10,5))\n\nsns.barplot(x=\"AgeGroup\", y=\"Survived\", data=trainData)\n","667d8ad6":"# Create plots about Survival for Sibsp & Parch, Fare, Embarked \n\n# Barplot for Passenger Class Survival Rate\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=trainData)\n","689b7364":"# Barplot for Sex Survival Rate\nsns.barplot(x=\"Sex\", y=\"Survived\", data=trainData)","ee6bf086":"# Barplot for Sibling Survival Rate\nsns.barplot(x=\"SibSp\", y=\"Survived\", data=trainData)","cebef357":"# Barplot for Parent\/Children Survival Rate\nsns.barplot(x=\"Parch\", y=\"Survived\", data=trainData)","4e50f40f":"# Barplot for Fare Survival Rate\nfareHist = trainData[trainData.Fare <= 101]['Fare'].plot.hist(\n    figsize=(12, 6),\n    color = ['darkgrey'],\n    bins = 10,\n    fontsize = 16,\n    label = 'Total'\n)\nfareHist.set_title(\"Fare Survival on the Titanic\", fontsize=20)\nfareHist = trainData[(trainData.Survived == 1 ) & (trainData.Fare <= 101)]['Fare'].plot.hist(\n    figsize=(12, 6),\n    color = ['green'],\n    bins = 10,\n    fontsize = 16,\n    label = 'Survivors'\n)\n\nfareHist.legend()","f2f0a312":"# Barplot for Embarked Survival Rate\nsns.barplot(x=\"Embarked\", y=\"Survived\", data=trainData)","7f03b3a8":"trainData.describe()","f3125975":"print(pd.isnull(testData).sum())","6421d30f":"# Fill in Data for NaN values of Age, Cabin (Create new value) and others if needed","4ae18adb":"trainData[\"Cabin\"] = trainData[\"Cabin\"].fillna(\"None\")\ntestData[\"Cabin\"] = testData[\"Cabin\"].fillna(\"None\")","f1361aff":"from sklearn.pipeline import make_pipeline\n\nfrom sklearn.metrics import mean_absolute_error\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.ensemble import VotingClassifier\n\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n\ncols_to_use = ['Pclass', 'Sex', 'AgeGroup', 'SibSp', 'Parch', 'Fare', 'Cabin']\nX = trainData[cols_to_use]\ny = trainData.Survived\npredictX = testData[cols_to_use]\n\n# Create Usable Data with OneHotEncoding\none_hot_encoded_predict_X = pd.get_dummies(predictX)\none_hot_encoded_X = pd.get_dummies(X)\nfinal_X, final_predict_X = one_hot_encoded_X.align(one_hot_encoded_predict_X, join='left', axis=1)\n\n# Create pipelines for different models\nRFtotalscore, GBtotalscore, LRtotalscore, KNtotalscore, DTtotalscore, XGBtotalscore, SVtotalscore, GPtotalscore, VCtotalscore = 0,0,0,0,0,0,0,0,0\nfor rs in range(1,10):\n    RFclassifier = make_pipeline(SimpleImputer(), RandomForestClassifier(random_state = rs))\n    GBclassifier = make_pipeline(SimpleImputer(), GradientBoostingClassifier(random_state = rs))\n    LRclassifier = make_pipeline(SimpleImputer(), LogisticRegression(random_state = rs))\n    KNclassifier = make_pipeline(SimpleImputer(), KNeighborsClassifier())\n    DTclassifier = make_pipeline(SimpleImputer(), DecisionTreeClassifier(random_state = rs))\n    XGBclassifier = make_pipeline(SimpleImputer(), XGBClassifier(random_state = rs))\n    SVclassifier = make_pipeline(SimpleImputer(), SVC(random_state = rs))\n    GPclassifier = make_pipeline(SimpleImputer(), GaussianProcessClassifier(random_state = rs))\n\n    VC = make_pipeline(SimpleImputer(), VotingClassifier(estimators=[('rf', RFclassifier), ('gb', GBclassifier), ('XGB', XGBclassifier)]))\n    # Calculate Cross Validation for pipelines\n    scores = cross_val_score(RFclassifier, final_X, y, scoring='balanced_accuracy')\n    RFscore = scores.mean()\n    RFtotalscore += RFscore\n\n\n    scores = cross_val_score(GBclassifier, final_X, y, scoring='balanced_accuracy')\n    GBscore = scores.mean()\n    GBtotalscore += GBscore\n\n\n    scores = cross_val_score(LRclassifier, final_X, y, scoring='balanced_accuracy')\n    LRscore = scores.mean()\n    LRtotalscore += LRscore\n\n\n    scores = cross_val_score(KNclassifier, final_X, y, scoring='balanced_accuracy')\n    KNscore = scores.mean()\n    KNtotalscore += KNscore\n\n\n    scores = cross_val_score(DTclassifier, final_X, y, scoring='balanced_accuracy')\n    DTscore = scores.mean()\n    DTtotalscore += DTscore\n\n\n    scores = cross_val_score(XGBclassifier, final_X, y, scoring='balanced_accuracy')\n    XGBscore = scores.mean()\n    XGBtotalscore += XGBscore\n\n\n    scores = cross_val_score(SVclassifier, final_X, y, scoring='balanced_accuracy')\n    SVscore = scores.mean()\n    SVtotalscore += SVscore\n\n    scores = cross_val_score(GPclassifier, final_X, y, scoring='balanced_accuracy')\n    GPscore = scores.mean()\n    GPtotalscore += GPscore\n\n    scores = cross_val_score(VC, final_X, y, scoring='balanced_accuracy')\n    VCscore = scores.mean()\n    VCtotalscore += VCscore\n\n# Print Accuracy\nprint('RF Accuracy:', round((RFtotalscore \/ rs * 100),2), '%')\nprint('GB Accuracy:', round((GBtotalscore \/ rs * 100),2), '%')\nprint('LR Accuracy:', round((LRtotalscore \/ rs * 100),2), '%')\nprint('KN Accuracy:', round((KNtotalscore \/ rs * 100),2), '%')\nprint('DT Accuracy:', round((DTtotalscore \/ rs * 100),2), '%')\nprint('XGB Accuracy:', round((XGBtotalscore \/ rs * 100),2), '%')\nprint('SV Accuracy:', round((SVtotalscore \/ rs * 100),2), '%')\nprint('GP Accuracy:', round((GPtotalscore \/ rs * 100),2), '%')\nprint('VC Accuracy:', round((VCtotalscore \/ rs * 100),2), '%')\n","a47bbe36":"VC.fit(final_X, y)\ncompetitionPredictions = VC.predict(final_predict_X)\noutput = pd.DataFrame({'PassengerId': testData.PassengerId,\n                       'Survived': competitionPredictions})\n\noutput.to_csv('submission.csv', index=False)\n","a90b83c9":"We can already see a few things in the 10 columns we just got. \n\nFirst of all there seem to be some missing values in the Age & Cabin columns.\nAlso the names seem to have not only the first & lastname of a person but also their title (Mr.\/Mrs.\/Master.\/etc.).\n\nLet's look at some of the data in detail.","850090f4":"#Predicting Survival on the Titanic\nThis is my code for the Starter Competition about survival on the Titanic. \nLet's start with importing some libraries and removing warnings. ","5725c56b":"Now let's import the data that we're going to be using.","04bb256f":"What can we see in this histogram?:\n* Passengers under the age of 15 have a significantly higher chance of surviving though not between the age of 5 and 10.\n* About 3\/4 of children under 5 years old survived.\n* While one of the biggest age groups the passengers that are between 20 and 30 also have about a 1\/3 chance of surviving."}}