{"cell_type":{"42924442":"code","532b7852":"code","c1e3a665":"code","d4e6f1dd":"code","cb92955a":"code","d7119406":"code","4e5a2f08":"code","112d8884":"code","5f9b6abc":"code","b2b4b181":"code","8880aa6e":"code","57a6fe6b":"code","e4811c60":"code","644b3f9d":"code","b4f95455":"markdown","32398e01":"markdown"},"source":{"42924442":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBRegressor\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","532b7852":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","c1e3a665":"#generating data sets\ntrain_data = train.copy()\ntest_data = test.copy()\n\nvectors = test_data.columns\n\ntrain_data = train_data.loc[:, vectors]\ntest_data = test_data.loc[:, vectors]","d4e6f1dd":"train_data.head()","cb92955a":"test_data.head()","d7119406":"drop_list = []\n\nfor i in test_data.columns:\n    if test_data[i].isna().sum() >= test_data[i].notnull().sum():\n        drop_list.append(i)\n\nfor i in train_data.columns:\n    if train_data[i].isna().sum() >= train_data[i].notnull().sum():\n        drop_list.append(i)\n        \nprint(drop_list)\n\ndrop_list = list(set(drop_list))\n\nprint(drop_list)","4e5a2f08":"# dropping too void columns on both dataframes\ntrain_data.drop(drop_list, axis=1, inplace=True)\ntest_data.drop(drop_list, axis=1, inplace=True)","112d8884":"for i in train_data.columns:\n    nulls_value = train_data[i].isna().sum()\n    message = \"Column {} has {} nulls\".format(i, nulls_value)\n    print(message)","5f9b6abc":"for i in test_data.columns:\n    nulls_value = test_data[i].isna().sum()\n    message = \"Column {} has {} nulls\".format(i, nulls_value)\n    print(message)","b2b4b181":"# checking correlations\n\ndef plot_correlations(data):\n    corr = data.corr()\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)\n    fig.colorbar(cax)\n    ticks = np.arange(0,len(data.columns),1)\n    ax.set_xticks(ticks)\n    plt.xticks(rotation=90)\n    ax.set_yticks(ticks)\n    ax.set_xticklabels(data.columns)\n    ax.set_yticklabels(data.columns)\n    plt.show()\n\nplot_correlations(test_data.copy())\nvector = plot_correlations(train_data.copy())","8880aa6e":"aux = train.copy()\naux2 = train.copy()\n\naux = aux.loc[:, test_data.columns]\naux['NU_NOTA_MT'] = aux2.NU_NOTA_MT\n\nc = aux.corr()\nc.NU_NOTA_MT.sort_values()","57a6fe6b":"new_vector_training = [\n    'NU_NOTA_COMP1',\n    'NU_NOTA_COMP2',\n    'NU_NOTA_COMP4',\n    'NU_NOTA_COMP5',\n    'NU_NOTA_COMP3',\n    'NU_NOTA_REDACAO',\n    'NU_NOTA_LC',\n    'NU_NOTA_CH',\n    'NU_NOTA_CN',\n    'NU_NOTA_MT'\n]\n\nnew_vector_test = [\n    'NU_INSCRICAO',\n    'NU_NOTA_COMP1',\n    'NU_NOTA_COMP2',\n    'NU_NOTA_COMP4',\n    'NU_NOTA_COMP5',\n    'NU_NOTA_COMP3',\n    'NU_NOTA_REDACAO',\n    'NU_NOTA_LC',\n    'NU_NOTA_CH',\n    'NU_NOTA_CN'\n]\n\ntrain_data = train.copy()\ntrain_data = train_data.loc[:, new_vector_training]\ntrain_data.dropna(subset=['NU_NOTA_MT'], inplace=True)\ntrain_data.head()","e4811c60":"y = train_data.NU_NOTA_MT\nX = train_data.drop(['NU_NOTA_MT'], axis=1)\n\nvalidation_data = test.copy()\nvalidation_data_1 = validation_data.loc[:, new_vector_test]\nvalidation_data_2 = validation_data.loc[:, new_vector_test]\n\ntrain_X, validation_X, train_y, validation_y = train_test_split(X, y, random_state = 0)\n\nmodel = XGBRegressor(n_estimators=200, learning_rate=0.1)\nmodel.fit(train_X, train_y, early_stopping_rounds=5, eval_set=[(validation_X, validation_y)], verbose=False)\n\nvalidation_data_1.drop(['NU_INSCRICAO'], axis=1, inplace=True)\npredicted_nota = model.predict(validation_data_1)\nanswer_df = pd.DataFrame({'NU_INSCRICAO': validation_data_2.NU_INSCRICAO, 'NU_NOTA_LC': validation_data_2.NU_NOTA_LC, 'NU_NOTA_MT_PREDICT': predicted_nota})","644b3f9d":"# almost there... now let's replace any note on math with None when note on LC is NaN (this means a missing value, once both are made on the same day\n\ndef replace_notes(row):\n    if row.NU_NOTA_LC == np.NaN:\n        return np.NaN\n    return row.NU_NOTA_MT_PREDICT\n\nanswer_df['NU_NOTA_MT'] = answer_df.apply(replace_notes, axis='columns')\nanswer_df.loc[answer_df.NU_NOTA_LC.isna(), ['NU_NOTA_MT']] = np.NaN\nanswer_df_final = answer_df.loc[:, ['NU_INSCRICAO', 'NU_NOTA_MT']]\n\n# answer_df.head()\n# answer_df_final.head()\nanswer_df_final.to_csv('answer.csv', index=False)","b4f95455":"With the code above, we can see the biggest correlated columns... Perhaps dropping the low correlations can give us better models\n\nI will keep only\n\nNU_NOTA_COMP1        0.299402\nNU_NOTA_COMP2        0.335638\nNU_NOTA_COMP4        0.342282\nNU_NOTA_COMP5        0.343337\nNU_NOTA_COMP3        0.350307\nNU_NOTA_REDACAO      0.379376\nNU_NOTA_LC           0.494695\nNU_NOTA_CH           0.529594\nNU_NOTA_CN           0.584941","32398e01":"With above inspections, we can cut of 3 columns with too much missing data. \nAlso, we can note that NU_NOTA_MT is missing in the same cases as NU_NOTA_LC.\n\nNow, let's explore a bit further into types, and pseudo-numbers that should be categorical or something like that"}}