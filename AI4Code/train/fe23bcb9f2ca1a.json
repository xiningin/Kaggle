{"cell_type":{"9a5d41e1":"code","0e76e19b":"code","561e2cff":"code","899ac445":"code","03cb90d2":"code","53907063":"code","ec2c00fb":"code","b11a9ccf":"code","21b4cf13":"code","9a8b819e":"code","65f4e0ba":"code","d5ecbe91":"code","55ec549a":"code","4b893443":"code","88fb347c":"code","a067a81e":"code","6fb71453":"code","f9a26851":"code","7a3177ba":"code","04caa86e":"code","4bbedfd2":"code","21f1db00":"code","18049e5f":"code","dc470c50":"code","751518ff":"code","6e47d20a":"code","fe97b898":"code","3ae39b0d":"code","d6fb2a66":"markdown","86a8c58c":"markdown","3f67e9b0":"markdown","b988a5bc":"markdown","419ceee2":"markdown","1e9aafa4":"markdown","c7a6b5fc":"markdown","99d3f74c":"markdown","09f2a366":"markdown","6187b8b1":"markdown","843f4bb7":"markdown"},"source":{"9a5d41e1":"!pip install ..\/input\/tensorflowprobability0122\/tensorflow_probability-0.12.2-py2.py3-none-any.whl\n!pip3 install ..\/input\/kerasapplications -q\n!pip3 install ..\/input\/image-classifiers\/image_classifiers-1.0.0-py3-none-any.whl","0e76e19b":"import numpy as np\nimport pandas as pd\nimport os\nimport sys\nimport json\nimport argparse\nimport datetime\nimport random\nfrom pathlib import Path\nfrom tensorflow.keras.utils import to_categorical\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom sklearn.metrics import log_loss\nfrom tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport cv2\nimport math\nimport gc\nimport matplotlib.pyplot as plt\nimport tensorflow_probability as tfp\nfrom classification_models.keras import Classifiers","561e2cff":"df_preds = pd.read_csv('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/sample_submission.csv')\nweightdatapath = Path(\"..\/input\/weight-20210903-full\")\ntestdatapaht = \"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/test\"","899ac445":"height = 256\nwidth = 256\nchannel = 3\nbatch_size = 16\nseed = 46\nviews = ['FLAIR', 'T1w', 'T1wCE', 'T2w']\n\nMRI_TYPE = 0","03cb90d2":"def set_seed(seed=200):\n    tf.random.set_seed(seed)\n\n    np.random.seed(seed)\n\n    random.seed(seed)\n\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\nset_seed(seed)","53907063":"def load_imgs(idx,view, ignore_zeros=True):\n    imgs = {}\n    \n    save_ds = []\n    dir_path = os.walk(os.path.join(\n        testdatapaht, idx, view\n    ))\n    for path, subdirs, files in dir_path:\n        for name in files:\n            image_path = os.path.join(path, name) \n            pyds = pydicom.filereader.dcmread(image_path)\n            slope = float(pyds.RescaleSlope)\n            intercept = float(pyds.RescaleIntercept)\n            img = intercept + pyds.pixel_array * slope\n            img = cv2.resize(img,[height,width])\n            save_ds.append(np.array(img))\n    if len(save_ds) == 0:\n        save_ds = np.zeros((1,256,256))\n    imgs = np.array(save_ds)\n    return imgs","ec2c00fb":"dim = (height,width)\nt_imgs = np.empty((channel, *dim))\ndef data_generation(ID,view,is_Train=True):\n    idx = str(ID).zfill(5)\n    imgs = load_imgs(idx,view, ignore_zeros=False)\n    t_size = imgs.shape\n    t_a = math.ceil(t_size[0] \/ 3)\n    t_img =imgs[:t_a]\n    t_imgs[0] = t_img.mean(axis=0) * 0.3\n    t_img =imgs[t_a:t_size[0] - t_a]\n    t_imgs[1] = t_img.mean(axis=0) * 0.4\n    t_img =imgs[t_size[0] - t_a:]\n    t_imgs[2] = t_img.mean(axis=0) * 0.3\n    img_ = t_imgs.transpose(1,2,0)\n    return img_","b11a9ccf":"def argument_image_val(img):\n    hsv_tf = tf.image.rgb_to_hsv(img)\n    a = tf.transpose(hsv_tf,[2,0,1])\n    a = tf.reshape(a[2],[256,256]) \n    t_v = tf.cond( tfp.stats.percentile(tf.experimental.numpy.ravel(a),q=95)>200,lambda:tf.cast(0,tf.float32),lambda:tf.cast((200 - tfp.stats.percentile(tf.experimental.numpy.ravel(a),q=95))*0.04,tf.float32))\n    t_c = tf.cond( tfp.stats.percentile(tf.experimental.numpy.ravel(a),q=95)>200,lambda:tf.cast(0,tf.float32),lambda:tf.cast((200 - tfp.stats.percentile(tf.experimental.numpy.ravel(a),q=95))*0.005,tf.float32))   \n    t_v_o = tf.cond( tfp.stats.percentile(tf.experimental.numpy.ravel(a),q=95)>300,lambda:tf.cast((tfp.stats.percentile(tf.experimental.numpy.ravel(a),q=95) - 300)*0.5,tf.float32),lambda:tf.cast(0,tf.float32))\n\n    img = tf.image.adjust_brightness(img, delta=(2.8+ t_v - t_v_o))\n    aa = tf.cond( (0.3 + t_c) < 2,lambda:tf.cast((0.3 + t_c),tf.float32),lambda:tf.cast(2,tf.float32))   \n\n    img = tf.image.adjust_contrast(img, tf.cast(aa,tf.float32))\n    img = tf.image.adjust_saturation(img, 1.2)\n    img = tf.cast(img, tf.float32) \/ 255.0\n    return img","21b4cf13":"def _bytes_feature(value):\n  if isinstance(value, type(tf.constant(0))):\n    value = value.numpy() \n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))","9a8b819e":"def serialize_example_test(feature0):\n  feature = {\n      'image': _bytes_feature(feature0.tobytes())\n  }\n  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n  return example_proto.SerializeToString()","65f4e0ba":"for i in range(4):\n    with tf.io.TFRecordWriter(str(\".\/\") + str(\"brain_test_\" + views[i] + \".tfrec\")) as writer:\n        for x in df_preds[\"BraTS21ID\"]:\n            img = data_generation(x,views[i],False)\n            example = serialize_example_test(\n                img)\n            writer.write(example)","d5ecbe91":"def deserialize_example(serialized_string):\n    image_feature_description = {\n        'image': tf.io.FixedLenFeature([], tf.string)\n    }\n    parsed_record = tf.io.parse_single_example(serialized_string, image_feature_description)\n    image = tf.io.decode_raw(parsed_record['image'], tf.float64)\n    image = tf.reshape(image,[height,width,channel])\n    return image","55ec549a":"def _parse_image_function(example_proto): \n  return tf.io.parse_single_example(example_proto, image_feature_description)","4b893443":"MRI_TYPE = 0\nraw_image_dataset = tf.data.TFRecordDataset(str(\".\/\") + str(\"brain_test_\" + views[MRI_TYPE] + \".tfrec\"))\n\nimage_feature_description = {\n    'image': tf.io.FixedLenFeature([], tf.string)\n}\n\nparsed_image_dataset = raw_image_dataset.map(_parse_image_function)\n\nfig = plt.figure(figsize=(35, 20))\npage = 1\ni=0\nfor idx in range(page*15,(page + 1)*15):\n    ax = fig.add_subplot(3, 5, i+1)\n    ax.tick_params(labelbottom=False, bottom=False)\n    ax.tick_params(labelleft=False, left=False)\n    for parsed_record in parsed_image_dataset.take(idx):\n        img = tf.io.decode_raw(parsed_record['image'], tf.float64)\n        img = tf.reshape(img,[256,256,3])\n\n    hsv_tf = tf.image.rgb_to_hsv(img)\n    a = tf.transpose(hsv_tf,[2,0,1])\n    a = tf.reshape(a[2],[256,256]) \n    b  = tfp.stats.percentile(tf.experimental.numpy.ravel(a),q=95)\n\n    img = argument_image_val(img)\n    ax.imshow(img)\n    ax.set_title(str(idx) + \":\" + str(b.numpy()),color=\"Black\")\n    i = i+1\nfig.show()","88fb347c":"MRI_TYPE = 1\nraw_image_dataset = tf.data.TFRecordDataset(str(\".\/\") + str(\"brain_test_\" + views[MRI_TYPE] + \".tfrec\"))\n\nimage_feature_description = {\n    'image': tf.io.FixedLenFeature([], tf.string)\n}\n\nparsed_image_dataset = raw_image_dataset.map(_parse_image_function)\n\nfig = plt.figure(figsize=(35, 20))\npage = 2\ni=0\nfor idx in range(page*15,(page + 1)*15):\n    ax = fig.add_subplot(3, 5, i+1)\n    ax.tick_params(labelbottom=False, bottom=False)\n    ax.tick_params(labelleft=False, left=False)\n    for parsed_record in parsed_image_dataset.take(idx):\n        img = tf.io.decode_raw(parsed_record['image'], tf.float64)\n        img = tf.reshape(img,[256,256,3])\n\n    hsv_tf = tf.image.rgb_to_hsv(img)\n    a = tf.transpose(hsv_tf,[2,0,1])\n    a = tf.reshape(a[2],[256,256]) \n    b  = tfp.stats.percentile(tf.experimental.numpy.ravel(a),q=95)\n\n    img = argument_image_val(img)\n    ax.imshow(img)\n    ax.set_title(str(idx) + \":\" + str(b.numpy()),color=\"Black\")\n    i = i+1\nfig.show()","a067a81e":"MRI_TYPE = 2\nraw_image_dataset = tf.data.TFRecordDataset(str(\".\/\") + str(\"brain_test_\" + views[MRI_TYPE] + \".tfrec\"))\n\nimage_feature_description = {\n    'image': tf.io.FixedLenFeature([], tf.string)\n}\n\nparsed_image_dataset = raw_image_dataset.map(_parse_image_function)\n\nfig = plt.figure(figsize=(35, 20))\npage = 3\ni=0\nfor idx in range(page*15,(page + 1)*15):\n    ax = fig.add_subplot(3, 5, i+1)\n    ax.tick_params(labelbottom=False, bottom=False)\n    ax.tick_params(labelleft=False, left=False)\n    for parsed_record in parsed_image_dataset.take(idx):\n        img = tf.io.decode_raw(parsed_record['image'], tf.float64)\n        img = tf.reshape(img,[256,256,3])\n\n    hsv_tf = tf.image.rgb_to_hsv(img)\n    a = tf.transpose(hsv_tf,[2,0,1])\n    a = tf.reshape(a[2],[256,256]) \n    b  = tfp.stats.percentile(tf.experimental.numpy.ravel(a),q=95)\n\n    img = argument_image_val(img)\n    ax.imshow(img)\n    ax.set_title(str(idx) + \":\" + str(b.numpy()),color=\"Black\")\n    i = i+1\nfig.show()","6fb71453":"MRI_TYPE = 3\nraw_image_dataset = tf.data.TFRecordDataset(str(\".\/\") + str(\"brain_test_\" + views[MRI_TYPE] + \".tfrec\"))\n\nimage_feature_description = {\n    'image': tf.io.FixedLenFeature([], tf.string)\n}\n\nparsed_image_dataset = raw_image_dataset.map(_parse_image_function)\n\nfig = plt.figure(figsize=(35, 20))\npage = 4\ni=0\nfor idx in range(page*15,(page + 1)*15):\n    ax = fig.add_subplot(3, 5, i+1)\n    ax.tick_params(labelbottom=False, bottom=False)\n    ax.tick_params(labelleft=False, left=False)\n    for parsed_record in parsed_image_dataset.take(idx):\n        img = tf.io.decode_raw(parsed_record['image'], tf.float64)\n        img = tf.reshape(img,[256,256,3])\n\n    hsv_tf = tf.image.rgb_to_hsv(img)\n    a = tf.transpose(hsv_tf,[2,0,1])\n    a = tf.reshape(a[2],[256,256]) \n    b  = tfp.stats.percentile(tf.experimental.numpy.ravel(a),q=95)\n\n    img = argument_image_val(img)\n    ax.imshow(img)\n    ax.set_title(str(idx) + \":\" + str(b.numpy()),color=\"Black\")\n    i = i+1\nfig.show()","f9a26851":"class ClassificationModel_senet154(tf.keras.Model):\n    def __init__(self):\n        super(ClassificationModel_senet154, self).__init__()\n        ResNet18, preprocess_input = Classifiers.get('senet154')\n        base_model = ResNet18((height,width, channel), include_top=False, weights='..\/input\/image-classifiers\/senet154_imagenet_1000_no_top.h5')\n        base_model.trainable = False\n        self.efficient_net = base_model\n        self.avgpooling = tf.keras.layers.GlobalAveragePooling2D()\n        self.dence64 = tf.keras.layers.Dense(64, kernel_initializer='lecun_normal',activation='relu')\n        self.dence32 = tf.keras.layers.Dense(32, kernel_initializer='lecun_normal',activation='relu')\n        self.efficient_net = base_model\n        self.dropoup = tf.keras.layers.Dropout(0.2)\n        self.dence1 = tf.keras.layers.Dense(1, activation='sigmoid')\n\n                \n    def call(self, input_tensor, training=True):\n        x = self.efficient_net(input_tensor)\n        #x = self.dence64(x)\n        x = self.avgpooling(x)\n        x = self.dence64(x)\n        x = self.dropoup(x)\n        x = self.dence32(x)\n        return self.dence1(x) ","7a3177ba":"class ClassificationModel_ResNet152V2(tf.keras.Model):\n    def __init__(self):\n        super(ClassificationModel_ResNet152V2, self).__init__()\n        base_model = tf.keras.applications.resnet_v2.ResNet152V2(weights=\"..\/input\/image-classifiers\/resnet152v2_weights_tf_dim_ordering_tf_kernels_notop.h5\",include_top=False,pooling='avg',input_shape=(height,width, 3))\n        base_model.trainable = False\n        self.efficient_net = base_model\n        self.dence64 = tf.keras.layers.Dense(64, kernel_initializer='lecun_normal',activation='relu')\n        self.dence32 = tf.keras.layers.Dense(32, kernel_initializer='lecun_normal',activation='relu')\n        self.efficient_net = base_model\n        self.dropoup = tf.keras.layers.Dropout(0.2)\n        self.dence1 = tf.keras.layers.Dense(1, activation='sigmoid')\n\n                \n    def call(self, input_tensor, training=True):\n        x = self.efficient_net(input_tensor)\n        x = self.dence64(x)\n        x = self.dropoup(x)\n        x = self.dence32(x)\n        return self.dence1(x) ","04caa86e":"class ClassificationModel_seresnext101(tf.keras.Model):\n    def __init__(self):\n        super(ClassificationModel_seresnext101, self).__init__()\n        ResNet18, preprocess_input = Classifiers.get('seresnext101')\n        base_model = ResNet18((height,width, channel), include_top=False, weights='..\/input\/image-classifiers\/seresnext101_imagenet_1000_no_top.h5')\n        base_model.trainable = False\n        self.efficient_net = base_model\n        self.avgpooling = tf.keras.layers.GlobalAveragePooling2D()\n        self.dence64 = tf.keras.layers.Dense(64, kernel_initializer='lecun_normal',activation='relu')\n        self.dence32 = tf.keras.layers.Dense(32, kernel_initializer='lecun_normal',activation='relu')\n        self.efficient_net = base_model\n        self.dropoup = tf.keras.layers.Dropout(0.2)\n        self.dence1 = tf.keras.layers.Dense(1, activation='sigmoid')\n\n                \n    def call(self, input_tensor, training=True):\n        x = self.efficient_net(input_tensor)\n        x = self.avgpooling(x)\n        x = self.dence64(x)\n        x = self.dropoup(x)\n        x = self.dence32(x)\n        return self.dence1(x) ","4bbedfd2":"class ClassificationModel_seresnet152(tf.keras.Model):\n    def __init__(self):\n        super(ClassificationModel_seresnet152, self).__init__()\n        ResNet18, preprocess_input = Classifiers.get('seresnet152')\n        base_model = ResNet18((height,width, channel), include_top=False, weights='..\/input\/image-classifiers\/seresnet152_imagenet_1000_no_top.h5')\n        base_model.trainable = False\n        self.efficient_net = base_model\n        self.avgpooling = tf.keras.layers.GlobalAveragePooling2D()\n        self.dence64 = tf.keras.layers.Dense(64, kernel_initializer='lecun_normal',activation='relu')\n        self.dence32 = tf.keras.layers.Dense(32, kernel_initializer='lecun_normal',activation='relu')\n        self.efficient_net = base_model\n        self.dropoup = tf.keras.layers.Dropout(0.2)\n        self.dence1 = tf.keras.layers.Dense(1, activation='sigmoid')\n\n                \n    def call(self, input_tensor, training=True):\n        x = self.efficient_net(input_tensor)\n        x = self.avgpooling(x)\n        x = self.dence64(x)\n        x = self.dropoup(x)\n        x = self.dence32(x)\n        return self.dence1(x)","21f1db00":"loss_func = tf.keras.losses.BinaryCrossentropy(from_logits=True)\nopt = tf.keras.optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.999)","18049e5f":"weight_Model=[\"ResNet152V2\",\"senet154\",\"seresnet152\",\"seresnext101\"]\nt_pre=[] #For each MRI type data\nm_pre=[] #For each pre-train mode data\nf_pre=[] #For each Fold data","dc470c50":"#FLAIR\nMRI_TYPE = 0\nfor m in [1,3]: #Select MODELs\n    if m == 0:\n        model = ClassificationModel_ResNet152V2()\n    elif m == 1:\n        model = ClassificationModel_senet154()\n    elif m == 2:\n        model = ClassificationModel_seresnet152()\n    elif m == 3:\n        model = ClassificationModel_seresnext101()\n\n    model.compile(\n        optimizer=opt, \n        loss=loss_func,\n        metrics=['accuracy']\n        )\n    \n    for f in range(5): #Fold \n        test_dataset = tf.data.TFRecordDataset(str(\".\/\") + str(\"brain_test_\" + views[MRI_TYPE] + \".tfrec\")).map(deserialize_example).map(argument_image_val).batch(batch_size)\n        t_weight = \"weight-\" + weight_Model[m] + \"-\" + views[MRI_TYPE] + \"_fold_0\" +str(f) + \"-0903.ckpt\"\n        model.load_weights(Path(weightdatapath,t_weight))\n        f_pre.append(model.predict(test_dataset, batch_size=batch_size))\n        tf.keras.backend.clear_session()\n\n    pre = np.array(f_pre)\n    m_pre.append(pre.mean(axis=0))\n    f_pre.clear()\n\npre = np.array(m_pre)\nt_pre.append(pre.mean(axis=0))\nm_pre.clear()\ndel model\ngc.collect()","751518ff":"#T1w\nMRI_TYPE = 1\nf_pre.clear()\nfor m in [0,1]: #Select MODELs\n    if m == 0:\n        model = ClassificationModel_ResNet152V2()\n    elif m == 1:\n        model = ClassificationModel_senet154()\n    elif m == 2:\n        model = ClassificationModel_seresnet152()\n    elif m == 3:\n        model = ClassificationModel_seresnext101()\n\n    model.compile(\n        optimizer=opt, \n        loss=loss_func,\n        metrics=['accuracy']\n        )\n    \n    for f in range(5): #Fold \n        test_dataset = tf.data.TFRecordDataset(str(\".\/\") + str(\"brain_test_\" + views[MRI_TYPE] + \".tfrec\")).map(deserialize_example).map(argument_image_val).batch(batch_size)\n        t_weight = \"weight-\" + weight_Model[m] + \"-\" + views[MRI_TYPE] + \"_fold_0\" +str(f) + \"-0903.ckpt\"\n        model.load_weights(Path(weightdatapath,t_weight))\n        f_pre.append(model.predict(test_dataset, batch_size=batch_size))\n        tf.keras.backend.clear_session()\n\n    pre = np.array(f_pre)\n    m_pre.append(pre.mean(axis=0))\n    f_pre.clear()\n\npre = np.array(m_pre)\nt_pre.append(pre.mean(axis=0))\nm_pre.clear()\ndel model\ngc.collect()","6e47d20a":"#T1wCE\nMRI_TYPE = 2\nf_pre.clear()\nfor m in [1,2]: #Select MODELs\n    if m == 0:\n        model = ClassificationModel_ResNet152V2()\n    elif m == 1:\n        model = ClassificationModel_senet154()\n    elif m == 2:\n        model = ClassificationModel_seresnet152()\n    elif m == 3:\n        model = ClassificationModel_seresnext101()\n\n    model.compile(\n        optimizer=opt, \n        loss=loss_func,\n        metrics=['accuracy']\n        )\n    \n    for f in range(5): #Fold \n        test_dataset = tf.data.TFRecordDataset(str(\".\/\") + str(\"brain_test_\" + views[MRI_TYPE] + \".tfrec\")).map(deserialize_example).map(argument_image_val).batch(batch_size)\n        t_weight = \"weight-\" + weight_Model[m] + \"-\" + views[MRI_TYPE] + \"_fold_0\" +str(f) + \"-0903.ckpt\"\n        model.load_weights(Path(weightdatapath,t_weight))\n        f_pre.append(model.predict(test_dataset, batch_size=batch_size))\n        tf.keras.backend.clear_session()\n\n    pre = np.array(f_pre)\n    m_pre.append(pre.mean(axis=0))\n    f_pre.clear()\n\npre = np.array(m_pre)\nt_pre.append(pre.mean(axis=0))\nm_pre.clear()\ndel model\ngc.collect()","fe97b898":"#T2w\nMRI_TYPE = 3\nf_pre.clear()\nfor m in [0,2]: #Select MODELs\n    if m == 0:\n        model = ClassificationModel_ResNet152V2()\n    elif m == 1:\n        model = ClassificationModel_senet154()\n    elif m == 2:\n        model = ClassificationModel_seresnet152()\n    elif m == 3:\n        model = ClassificationModel_seresnext101()\n\n    model.compile(\n        optimizer=opt, \n        loss=loss_func,\n        metrics=['accuracy']\n        )\n    \n    for f in range(5): #Fold \n        test_dataset = tf.data.TFRecordDataset(str(\".\/\") + str(\"brain_test_\" + views[MRI_TYPE] + \".tfrec\")).map(deserialize_example).map(argument_image_val).batch(batch_size)\n        t_weight = \"weight-\" + weight_Model[m] + \"-\" + views[MRI_TYPE] + \"_fold_0\" +str(f) + \"-0903.ckpt\"\n        model.load_weights(Path(weightdatapath,t_weight))\n        f_pre.append(model.predict(test_dataset, batch_size=batch_size))\n        tf.keras.backend.clear_session()\n\n    pre = np.array(f_pre)\n    m_pre.append(pre.mean(axis=0))\n    f_pre.clear()\n\npre = np.array(m_pre)\nt_pre.append(pre.mean(axis=0))\nm_pre.clear()\ndel model\ngc.collect()","3ae39b0d":"pre = np.array(t_pre)\nfinpre = pre.mean(axis=0)\n\nsubfilename = \"submission.csv\"\nsub = pd.DataFrame(finpre, columns=['MGMT_value'])\n\ndf_preds[\"MGMT_value\"] = sub\n\ndf_preds.to_csv(\n    subfilename,\n    index=False\n    )","d6fb2a66":"# Choice Models\nThe model to be applied to each MRI is selected from the top two models with good CV scores.","86a8c58c":"# Intro\nIn my this code,I try to use 4 pre-trainmode,5 Fold weight for each MRI-TYPE.And,I converte Test Data to TFRecord.  \nBecause,without TFRecord,Notebook is timeout after submittion.  \n**Version 2.0**\nDisplay Images each MRI-TYPES","3f67e9b0":"# Disp Images[T1w]\nDisp T1w Image with brightness.","b988a5bc":"# Load Data\nweight-20210903-full\" is weigth. 4 MRI-TYPE * 5 Fold * 4 model *2 =  160 files","419ceee2":"# Request\n**If you find this Code useful, please upvote !!**","1e9aafa4":"# Disp Images[T1wCE]\nDisp T1wCE Image with brightness.","c7a6b5fc":"# Disp Images[T2w]\nDisp T2w Image with brightness.","99d3f74c":"# Disp Images[FLAIR]\nDisp FLAIR Image with brightness.","09f2a366":"# Argumentation\nThis argumentation is to that Images are too dark are bright, and  too bright are dark.","6187b8b1":"# Convert Gray-Scale images to 3 Channel images\nDICOM is Gray-Scale.But Pre-Train models' input is 3channel.  \nSo,I divided the whole into one-third and get mean each part.  \nAnd I multiplied 0.3 or 0.4 so that total is 1.  \nI call this my method **\"Hazigin Method\"** :-)","843f4bb7":"# TFRecord\nConvert Test Data to TFRecord for each MRI type"}}