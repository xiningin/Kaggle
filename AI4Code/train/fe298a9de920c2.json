{"cell_type":{"66c9b9a9":"code","f650136c":"code","0a0593e5":"code","3ed8c7ca":"code","8fa9d6ec":"code","7923e665":"code","0113bed8":"code","6c1f379f":"code","7d5f0650":"code","e500f40e":"code","258a5039":"code","913742df":"code","af6140e4":"code","58e5fb56":"code","2873a47c":"code","6c3809e3":"code","c4c8aa9a":"code","c87393d8":"code","52a15434":"code","ba4508ed":"code","ae29ac1c":"code","b9d5f90c":"code","d09e924c":"code","38a15c96":"code","712f29f3":"code","6727b177":"code","d48de51d":"code","b8fde9c1":"code","87b41746":"code","f637690f":"code","6e590545":"code","56693738":"code","9837cf14":"code","be621d1f":"code","94cf4205":"code","ce48891c":"code","c2eb4d55":"code","1adadf58":"code","e3778747":"code","12e6e438":"code","f8964e10":"code","bee63f0d":"code","084be4c7":"code","e164d5ae":"code","9c01c7eb":"code","6885ced3":"code","6d0b26b6":"code","22e55145":"code","d242b08c":"code","2c223f72":"code","ab59b442":"code","c60d0d5e":"code","ff3b7c18":"code","acb980da":"code","8a3dd85f":"code","8ea7d566":"code","8b694e33":"code","55409819":"code","17b94071":"code","bfb3741c":"code","d3a3c883":"code","77b7b0fa":"code","49c556c4":"code","4d8490c5":"code","3a1ae036":"code","fb3f2823":"code","70a0b798":"code","ed18db95":"code","1d614ed7":"code","2f183e4c":"markdown","2fdec124":"markdown","46502c8e":"markdown","6bbe9e97":"markdown","a1de39d3":"markdown","296682c2":"markdown","a8df870a":"markdown","c2da3387":"markdown","ae0cd203":"markdown","1cbafce5":"markdown","62f43b25":"markdown","6d5c7789":"markdown","ea87e50c":"markdown","01dca048":"markdown","56aca164":"markdown","014d1e4c":"markdown","de55c63e":"markdown","079b02e3":"markdown","1f06261a":"markdown","eb08da55":"markdown","a754216a":"markdown","6a97dfba":"markdown","1a165e30":"markdown","74c36ed8":"markdown","d23e723b":"markdown","0dd4f8ed":"markdown","9eb015fe":"markdown","76268fb6":"markdown","e68bbdd3":"markdown","372b1f78":"markdown","5017cf00":"markdown","04887f56":"markdown","c5f9a0cd":"markdown","2e937a66":"markdown","5edf8ab1":"markdown","0962c7b8":"markdown","f66a8083":"markdown","6a602ce9":"markdown","21914e82":"markdown","280d0c01":"markdown","c0ee225f":"markdown","4f5d1978":"markdown"},"source":{"66c9b9a9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nimport sklearn.preprocessing\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n","f650136c":"df = pd.read_csv('..\/input\/hourly-energy-consumption\/PJM_Load_hourly.csv', index_col=[0], parse_dates=[0])","0a0593e5":"df.isnull().sum()","3ed8c7ca":"df.head(5)","8fa9d6ec":"# Given there's no missing data, we can resample the data to daily level\ndaily_data = df.resample(rule='D').sum()\n\n# Set frequency explicitly to D\ndaily_data = daily_data.asfreq('D')\n\ndaily_data.head(10)","7923e665":"# We can confirm it is at the right frequency\ndaily_data.index","0113bed8":"daily_data.plot()\n\nplt.show()","6c1f379f":"daily_data = daily_data.drop([daily_data.index.min(), daily_data.index.max()])","7d5f0650":"daily_data.plot()\n\nplt.show()","e500f40e":"from statsmodels.tsa.seasonal import seasonal_decompose\nfrom matplotlib.dates import DateFormatter\ndecomposition = seasonal_decompose(daily_data, model='additive')\n\nfig = decomposition.plot()\n\nprint(fig)\n\n# Define the date format\n#date_form = DateFormatter(\"%m-%d\")\n#fig.xaxis.set_major_formatter(date_form)\n\n\nplt.show()","258a5039":"\nweekly_data = df.resample(rule='W').sum()\ndecomposition = seasonal_decompose(weekly_data, model='additive') # Aggregate to weekly level\n\nfig = decomposition.plot()\nplt.show()","913742df":"print(daily_data)","af6140e4":"# Create new dataset for heatmap\nheatmap_data = daily_data.copy()\n\n# First we need to add weekdays as a column\nheatmap_data['Weekday_Name'] = daily_data.index.day_name()\n\n# Next we add the year as column and group the data up to annual day of week level\nheatmap_data['Year'] =  heatmap_data.index.year\nheatmap_data = heatmap_data.groupby(['Year', 'Weekday_Name']).sum()\n\n# Reset index \nheatmap_data = heatmap_data.reset_index()\n\n# We drop off 2018 because it's not a full year\nheatmap_data = heatmap_data[heatmap_data['Year'] != 1998]\nheatmap_data = heatmap_data[heatmap_data['Year'] != 2002]\n\n# Pivot it to a uniform data format for heatmaps\nheatmap_data = heatmap_data.pivot(index='Year', columns='Weekday_Name', values='PJM_Load_MW')\n\n# Reorder columns\nheatmap_data = heatmap_data[['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']]\n\nheatmap_data.head()","58e5fb56":"# Visualise electricity load via Heatmap\n\nsns.heatmap(heatmap_data, linewidths=.5, cmap='YlOrRd', cbar=True, cbar_kws={\"format\": '%1.0f MWh'}).set_title('Heatmap - by Day of Week')","2873a47c":"# Create new dataset for heatmap\nheatmap_data = df.copy()\n\n# First we need to add weekdays as a column\nheatmap_data['Hour'] = df.index.hour\n\n# Next we add the year as column and group the data up to annual day of week level\nheatmap_data['Year'] =  heatmap_data.index.year\nheatmap_data = heatmap_data.groupby(['Year', 'Hour']).sum()\n\n# Reset index \nheatmap_data = heatmap_data.reset_index()\n\n# We drop off 2018 because it's not a full year\nheatmap_data = heatmap_data[heatmap_data['Year'] != 1998]\nheatmap_data = heatmap_data[heatmap_data['Year'] != 2002]\n\n# Pivot it to a uniform data format for heatmaps\nheatmap_data = heatmap_data.pivot(index='Year', columns='Hour', values='PJM_Load_MW')\n\nheatmap_data.head(100)","6c3809e3":"# Visualise electricity load via Heatmap\n\nsns.heatmap(heatmap_data, linewidths=.5, cmap='YlOrRd', cbar=True, cbar_kws={\"format\": '%1.0f MWh'}).set_title('Heatmap - by Hour of Day')","c4c8aa9a":"# Create new dataset for heatmap\nheatmap_data = daily_data.copy()\n\n# First we need to add weekdays as a column\nheatmap_data['Month'] = daily_data.index.month_name()\n\n# Next we add the year as column and group the data up to annual day of week level\nheatmap_data['Year'] =  heatmap_data.index.year\nheatmap_data = heatmap_data.groupby(['Year', 'Month']).sum()\n\n# Reset index\nheatmap_data = heatmap_data.reset_index()\n\n# We drop off 2018 because it's not a full year\n#heatmap_data = heatmap_data[heatmap_data['Year'] != 1998]\nheatmap_data = heatmap_data[heatmap_data['Year'] != 2002]\n\n# Pivot it to a uniform data format for heatmaps\nheatmap_data = heatmap_data.pivot(index='Year', columns='Month', values='PJM_Load_MW')\n\n# Reorder columns\nheatmap_data = heatmap_data[['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']]\n\nheatmap_data.head(10)","c87393d8":"# Visualise electricity load via Heatmap\nsns.heatmap(heatmap_data, linewidths=.5, cmap='YlOrRd', cbar=True, cbar_kws={\"format\": '%1.0f MWh'}).set_title('Heatmap - by Day of Month')","52a15434":"split_date = '31-Dec-2000'\ndf_train = df.loc[df.index <= split_date].copy() #changed from df_norm\ndf_test = df.loc[df.index > split_date].copy() #changed from df_norm","ba4508ed":"print(df.shape)\nprint(df_train.shape)\nprint(df_test.shape)\n","ae29ac1c":"_ = df_test \\\n    .rename(columns={'PJM_Load_MW': 'TEST SET'}) \\\n    .join(df_train.rename(columns={'PJM_Load_MW': 'TRAINING SET'}), how='outer') \\\n    .plot(figsize=(15,5), title='PJM_Load_MW', style='.')","b9d5f90c":"def create_features(df, label=None):\n    \"\"\"\n    Creates time series features from datetime index\n    \"\"\"\n    df['date'] = df.index\n    df['hour'] = df['date'].dt.hour\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['quarter'] = df['date'].dt.quarter\n    df['month'] = df['date'].dt.month\n    df['year'] = df['date'].dt.year\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['dayofmonth'] = df['date'].dt.day\n    df['weekofyear'] = df['date'].dt.weekofyear\n    \n    X = df[['hour','dayofweek','quarter','month','year',\n           'dayofyear','dayofmonth','weekofyear']]\n    if label:\n        y = df[label]\n        return X, y\n    return X","d09e924c":"X_train, y_train = create_features(df_train, label='PJM_Load_MW') #PJM_Load_MW output in PJM_Load_hourly.csv\nX_test, y_test = create_features(df_test, label='PJM_Load_MW')","38a15c96":"X_train.head(5)","712f29f3":"y_train.head(5)","6727b177":"reg = xgb.XGBRegressor(n_estimators=1000)\nreg.fit(X_train, y_train,\n        eval_set=[(X_train, y_train), (X_test, y_test)],\n        early_stopping_rounds=50,\n       verbose=False) # Change verbose to True if you want to see it train\n\noutput = reg.predict(X_test)","d48de51d":"#print(output)","b8fde9c1":"features = [\"date\", \"hour\", \"dayofweek\", \"quarter\", \"month\", \"year\", \"dayofyear\", \"dayofmonth\", \"weekofyear\"]\n\n# Get feature importances from our random forest model\nimportances = reg.feature_importances_\n\n# Get the index of importances from greatest importance to least\nsorted_index = np.argsort(importances)[::-1]\nsorted_index_top = sorted_index[:10]\nx = range(len(sorted_index_top))\n\n# Create tick labels \nlabels = np.array(features)[sorted_index_top]\nplt.bar(x, importances[sorted_index_top], tick_label=labels)\nplt.title(\"Feature importance analyisis\")\n# Rotate tick labels to vertical\nplt.xticks(rotation=45)\nplt.show()","87b41746":"#Predicted output will be in new colomn called MW Prediction\ndf_test['MW_Prediction'] =reg.predict(X_test)\n#combining the \nPJM_all = pd.concat([df_test, df_train], sort=False)","f637690f":"_ = PJM_all[['PJM_Load_MW','MW_Prediction']].plot(figsize=(15, 5))","6e590545":"from sklearn.metrics import r2_score","56693738":"#lstm_predictions = lstm_model.predict(X_test)\n\nlstm_score = r2_score(y_test, output)\nprint(\"R^2 Score of XGBoost model = \",lstm_score)","9837cf14":"mean_squared_error = mean_squared_error(y_true=df_test['PJM_Load_MW'],\n                   y_pred=df_test['MW_Prediction'])\nprint(\"Mean_squared_error =\", mean_squared_error)","be621d1f":"mean_absolute_error = mean_absolute_error(y_true=df_test['PJM_Load_MW'],\n                   y_pred=df_test['MW_Prediction'])\nprint(\"Mean_absolute_error =\", mean_absolute_error)","94cf4205":"def mean_absolute_percentage_error(y_true, y_pred): \n    \"\"\"Calculates MAPE given y_true and y_pred\"\"\"\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n\nmean_absolute_percentage_error = mean_absolute_percentage_error(y_true=df_test['PJM_Load_MW'],\n                   y_pred=df_test['MW_Prediction'])\n\nprint(\"Mean_absolute_error = \" + str(mean_absolute_percentage_error) + \" %\")\n","ce48891c":"# Feature Engineering first\n\ndef preprocess_xgb_data(df, lag_start=1, lag_end=365):\n    '''\n    Takes data and preprocesses for XGBoost.\n    \n    :param lag_start default 1 : int\n        Lag window start - 1 indicates one-day behind\n    :param lag_end default 365 : int\n        Lag window start - 365 indicates one-year behind\n        \n    Returns tuple : (data, target)\n    '''\n    # Default is add in lag of 365 days of data - ie make the model consider 365 days of prior data\n    for i in range(lag_start,lag_end):\n        df[f'PJM_Load_MW {i}'] = df.shift(periods=i, freq='D')['PJM_Load_MW']\n\n    df.reset_index(inplace=True)\n\n    # Split out attributes of timestamp - hopefully this lets the algorithm consider seasonality\n    df['date_epoch'] = pd.to_numeric(df['Datetime']) # Easier for algorithm to consider consecutive integers, rather than timestamps\n    df['hour'] = df['Datetime'].dt.hour\n    df['dayofweek'] = df['Datetime'].dt.dayofweek\n    df['dayofmonth'] = df['Datetime'].dt.day\n    df['dayofyear'] = df['Datetime'].dt.dayofyear\n    df['weekofyear'] = df['Datetime'].dt.weekofyear\n    df['quarter'] = df['Datetime'].dt.quarter\n    df['month'] = df['Datetime'].dt.month\n    df['year'] = df['Datetime'].dt.year\n    \n    x = df.drop(columns=['Datetime', 'PJM_Load_MW']) #Don't need timestamp and target\n    y = df['PJM_Load_MW'] # Target prediction is the load\n    \n    return x, y","c2eb4d55":"# First we split it up between train and test\n# We will aim for a 12 month forecast horizon (ie predict the last 12 months in the dataset)\ncutoff = '2000-12-31'\n\ndaily_data.sort_index()\n\ntrain = daily_data[:cutoff]\ntest = daily_data[cutoff:]","1adadf58":"example_data = train.copy() #Otherwise it becomes a pointer\n\nexample_x, example_y = preprocess_xgb_data(example_data)\n\nexample_x.head(10)","e3778747":"xtrain = train.copy() #Otherwise it becomes a pointer\nxtest = test.copy() # Otherwise it becomes a pointer\n\ntrain_feature, train_label = preprocess_xgb_data(xtrain)\ntest_feature, test_label = preprocess_xgb_data(xtest)\n\ntest_feature.head(10)","12e6e438":"\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import KFold, train_test_split\n\n# We will try with 1000 trees and a maximum depth of each tree to be 5\n# Early stop if the model hasn't improved in 100 rounds\nmodel = XGBRegressor(\n    max_depth=6 # Default - 6\n    ,n_estimators=1000\n    ,booster='gbtree'\n    ,colsample_bytree=1 # Subsample ratio of columns when constructing each tree - default 1\n    ,eta=0.3 # Learning Rate - default 0.3\n    ,importance_type='weight' # Default is gain\n)\nmodel.fit(\n    train_feature\n    ,train_label\n    ,eval_set=[(train_feature, train_label)]\n    ,eval_metric='mae'\n    ,verbose=True\n    ,early_stopping_rounds=100 # Stop after 100 rounds if it doesn't after 100 times\n)\n\nxtest['PJM_Load_MW Prediction'] = model.predict(test_feature)\nXGB_prediction = xtest[['Datetime', 'PJM_Load_MW Prediction']]\n\n","f8964e10":"XGB_prediction.head()","bee63f0d":"print(XGB_prediction['PJM_Load_MW Prediction'])","084be4c7":"print(test_label)","e164d5ae":"lstm_score_revised_xgboost = r2_score(test_label,XGB_prediction['PJM_Load_MW Prediction'])\nprint(\"R^2 Score of XGBoost model = \",lstm_score_revised_xgboost)","9c01c7eb":"#mean absolute error\n#mean_absolute_error = mean_absolute_error(y_true=test_label,y_pred=XGB_prediction['PJM_Load_MW Prediction'])\n#print(\"Mean_absolute_error =\", mean_absolute_error)\n\n#print(\"MAE XGB: {:.20f}\".format(mean_absolute_error(test_label, XGB_prediction['PJM_Load_MW Prediction'])))\n\n","6885ced3":"df.plot(figsize=(16,4),legend=True)\n\nplt.title('PJM hourly power consumption data - BEFORE NORMALIZATION')\n\nplt.show()","6d0b26b6":"def normalize_data(df):\n    scaler = sklearn.preprocessing.MinMaxScaler()\n    df['PJM_Load_MW']=scaler.fit_transform(df['PJM_Load_MW'].values.reshape(-1,1))\n    return df\n\ndf_norm = normalize_data(df)\ndf_norm.shape","22e55145":"df_norm.plot(figsize=(16,4),legend=True)\n\nplt.title('PJM hourly power consumption data - AFTER NORMALIZATION')\n\nplt.show()","d242b08c":"def load_data(stock, seq_len):\n    X_train = []\n    y_train = []\n    for i in range(seq_len, len(stock)-5):\n        X_train.append(stock.iloc[i-seq_len : i, 0])\n        y_train.append(stock.iloc[i+5, 0])\n\n    #1 last 6189 days are going to be used in test\n    X_test = X_train[24114:]             \n    y_test = y_train[24114:]\n    \n    #2 first 110000 days are going to be used in training\n    X_train = X_train[:24114]           \n    y_train = y_train[:24114]\n    \n    #3 convert to numpy array\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    \n    X_test = np.array(X_test)\n    y_test = np.array(y_test)\n    \n    #4 reshape data to input into RNN models\n    X_train = np.reshape(X_train, (24114, seq_len, 1))\n    \n    X_test = np.reshape(X_test, (X_test.shape[0], seq_len, 1))\n    \n    return [X_train, y_train, X_test, y_test]","2c223f72":"print(df_norm.shape)","ab59b442":"#create train, test data\nseq_len = 70 #choose sequence length\n\nX_train, y_train, X_test, y_test = load_data(df_norm, seq_len)\n\nprint('X_train.shape = ',X_train.shape)\nprint('y_train.shape = ', y_train.shape)\nprint('X_test.shape = ', X_test.shape)\nprint('y_test.shape = ',y_test.shape)","c60d0d5e":"from keras.models import Sequential\nfrom keras.layers import Dense,Dropout,SimpleRNN,LSTM\nfrom keras.preprocessing.sequence import TimeseriesGenerator\n","ff3b7c18":"rnn_model = Sequential()\n\nrnn_model.add(SimpleRNN(40,activation=\"tanh\",return_sequences=True, input_shape=(X_train.shape[1],1)))\nrnn_model.add(Dropout(0.15))\n\nrnn_model.add(SimpleRNN(40,activation=\"tanh\",return_sequences=True))\nrnn_model.add(Dropout(0.15))\n\nrnn_model.add(SimpleRNN(40,activation=\"tanh\",return_sequences=False))\nrnn_model.add(Dropout(0.15))\n\nrnn_model.add(Dense(1))\n\nrnn_model.summary()","acb980da":"rnn_model.compile(optimizer=\"adam\",loss=\"MSE\")\nhistory= rnn_model.fit(X_train, y_train, epochs=17, validation_data=(X_test, y_test), batch_size=1000)","8a3dd85f":"#my_loss= rnn_model.history.history['loss']\n#plt.plot(range(len(my_loss)),my_loss)","8ea7d566":"#rnn_model_1 = Sequential()\n\n#rnn_model_1.add(SimpleRNN(40,activation=\"tanh\",return_sequences=True, input_shape=(X_train.shape[1],1)))\n#rnn_model_1.add(Dropout(0.15))\n\n#rnn_model_1.add(SimpleRNN(40,activation=\"tanh\",return_sequences=True))\n#rnn_model_1.add(Dropout(0.15))\n\n#rnn_model_1.add(SimpleRNN(40,activation=\"tanh\",return_sequences=False))\n#rnn_model_1.add(Dropout(0.15))\n\n#rnn_model_1.add(Dense(1))\n\n#rnn_model_1.summary()\n\n#rnn_model_1.compile(optimizer=\"adam\",loss=\"MSE\")\n#history = rnn_model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), shuffle=False)","8b694e33":"# plot train and validation loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model train vs validation loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()","55409819":"rnn_predictions = rnn_model.predict(X_test)\n\nrnn_score = r2_score(y_test,rnn_predictions)\nprint(\"R^2 Score of RNN model = \",rnn_score)","17b94071":"def plot_predictions(test, predicted, title):\n    plt.figure(figsize=(16,4))\n    plt.plot(test, color='blue',label='Actual power consumption data')\n    plt.plot(predicted, alpha=0.7, color='orange',label='Predicted power consumption data')\n    plt.title(title)\n    plt.xlabel('Time')\n    plt.ylabel('Normalized power consumption scale')\n    plt.legend()\n    plt.show()\n    \nplot_predictions(y_test, rnn_predictions, \"Predictions made by simple RNN model\")","bfb3741c":"lstm_model = Sequential()\n\nlstm_model.add(LSTM(40,activation=\"tanh\",return_sequences=True, input_shape=(X_train.shape[1],1)))\nlstm_model.add(Dropout(0.15))\n\nlstm_model.add(LSTM(40,activation=\"tanh\",return_sequences=True))\nlstm_model.add(Dropout(0.15))\n\nlstm_model.add(LSTM(40,activation=\"tanh\",return_sequences=False))\nlstm_model.add(Dropout(0.15))\n\nlstm_model.add(Dense(1))\n\nlstm_model.summary()","d3a3c883":"lstm_model.compile(optimizer=\"adam\",loss=\"MSE\")\nhistory_lstm=lstm_model.fit(X_train, y_train, epochs=28, validation_data=(X_test, y_test),batch_size=1000)\n","77b7b0fa":"# plot train and validation loss\nplt.plot(history_lstm.history['loss'])\nplt.plot(history_lstm.history['val_loss'])\nplt.title('model train vs validation loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()","49c556c4":"lstm_predictions = lstm_model.predict(X_test)\n\nlstm_score = r2_score(y_test, lstm_predictions)\nprint(\"R^2 Score of LSTM model = \",lstm_score)","4d8490c5":"plot_predictions(y_test, lstm_predictions, \"Predictions made by LSTM model\")","3a1ae036":"plt.figure(figsize=(15,8))\n\nplt.plot(y_test, c=\"orange\", linewidth=3, label=\"Original values\")\nplt.plot(lstm_predictions, c=\"red\", linewidth=3, label=\"LSTM predictions\")\nplt.plot(rnn_predictions, alpha=0.5, c=\"green\", linewidth=3, label=\"RNN predictions\")\nplt.legend()\nplt.title(\"Predictions vs actual data\", fontsize=20)\nplt.show()","fb3f2823":"from scipy.stats import norm\nfrom scipy import stats","70a0b798":"#histogram and normal probability plot\nsns.distplot(X_train['hour'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(X_train['hour'], plot=plt)","ed18db95":"\n# design network\nmodel = Sequential()\nmodel.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\nmodel.add(Dense(1))\nmodel.compile(loss='mae', optimizer='adam')\n# fit network\nhistory = model.fit(train_X, y_train, epochs=50, batch_size=72, validation_data=(test_X, y_test), verbose=2, shuffle=False)\n# plot history\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","1d614ed7":"# plot history\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show()","2f183e4c":"Based on the graph, both validation loss and loss have been decreasing as # of epochs increases","2fdec124":"for Both RNN and LSTM, we need to reshape input to be 3D [samples, timesteps, features]","46502c8e":"Now let's predict our test dataset using our XGBoost Model.\n","6bbe9e97":"Aggregate up to weekly level to reduce the noise. Graph looks alot clearer now.","a1de39d3":"Extra info: \nThe size of a batch must be more than or equal to one and less than or equal to the number of samples in the training dataset.","296682c2":"Train & Test Split","a8df870a":"Interestingly, it means 11am to 9pm is the busiest peak time of the grid.","c2da3387":"To normalise the data, we will be using sklearn MinMaxScalar\n\nNormalisation is required for neural networks(i think? *double check*)","ae0cd203":"Doing the same thing but for hours in the day.","1cbafce5":"Now let us start with building our models and testing which models is the best.\n\nModels that we are testing:\n1. XGBoost\n2. RNN\n3. LSTM","62f43b25":"# Another method for XGBoost (inclusive of lag features)\n\n## Feature Engineering\n\n1. Lag features\n\nConsider this \u2013 you are predicting the stock price for a company. So, the previous day\u2019s stock price is important to make a prediction, right? In other words, the value at time t is greatly affected by the value at time t-1. The past values are known as lags, so t-1 is lag 1, t-2 is lag 2, and so on.\n\nIn this case, I created 365 extra columns each prior day. Today minus 1 day, Today minus 2 days ... until Today minus 365 days.\n\n2. Time Series features\n\ndate, hour, dayofweek, quarter, month, year, dayofyear, dayofmonth, weekofyear","6d5c7789":"Easier reference: R^2 score of RNN model = 0.946558009942673. Therefore, lstm model is better in predicting PJM_Load_MW in this case. (need to note that its not exactly RNN that is better and changing the parameters when building the 2 models may result in a different outcome)","ea87e50c":"Since the frequency currently is in the hourly level, this will make it difficult to visualise. So we will resample it and aggregate it to a daily level.","01dca048":"### Feature importance\nAs seen from the results, level of importance:\n1. date\n2. quarter\n3. year\n4. hour\n5. dayofmonth\n6. month\n7. dayofyear\n8. dayofweek\n9. weekofyear\n","56aca164":"Visualize data after normalization\n\n* After normalization the range of power consumption values changes which we can observe on the y-axis of the graph. In the earlier graph that was displayed it was in the range 0 - 54000\n* Now after normalization we can observe that the data range on y-axis is 0.0 - 1.0","014d1e4c":"You can see start to see a pattern - electricity usage peak and troughs seem to be very seasonal and repetitive. This makes sense, considering office hours, weather patterns, shopping holidays etc.\n\nFurthermore, you can see the trend of the data seems to be trailing upwards in the last few years.","de55c63e":"# Exploratory Analysis\nTo better understand the data, exploratory analysis is done.\n\nThings (or predictions) that I hope can be addressed here:\n1. Electricity consumption has a high correlated with seasons (winter probably has the highest consumption since heating is required?)\n2. Particular days have higher electricity consumption (working days?)\n3. Working hours will have the highest electricity consumption out of the day","079b02e3":"Normalisation is not needed for XGBOOST.\n\nReason: \n\nDecision Trees do not require normalization of their inputs; and since XGBoost is essentially en ensemble algorithm comprised of Decision Trees, it does not require normalization for the inputs either.","1f06261a":"New thing learnt: using seasonal decomposition\n\n**Seasonal Decomposition**\n\nAt a high-level, time series data can be thought of as components put together. That is:\n\n**Data = Level + Trend + Seasonality + Noise**\n\n* Level: the average value in the series.\n* Trend: the increasing or decreasing value in the series.\n* Seasonality: the repeating short-term cycle in the series.\n* Noise\/Residual: the random variation in the series.\n\nUsing the Python statsmodel library, the above components can be 'decomposed' (ie seasonal decomposition):","eb08da55":"# Univariate RNN Forecast Model\nLet's first focus on  only PJM_Load_MW","a754216a":"RNN model","6a97dfba":"Only very little improvement (previously R^2 score:0.7400406595803339)","1a165e30":"Data Visualisation\n\nBefore normalisation of data: As you can see from the graph below, the graph has yet to be normalised. ","74c36ed8":"Build LSTM model ","d23e723b":"It seems like the RNN model is doing a significantly better job than the XGBoost model (R2 Score = 0.7400406595803339) (Maybe need to double check cause the difference is quite large) ","0dd4f8ed":"Now we will create some Time Series Features \n\n* date, hour, dayofweek, quarter, month, year, dayofyear, dayofmonth, weekofyear","9eb015fe":"Another way to visualise seasonality is to use a heatmap - we can base it on a week to see which days have higher electricity usage.\n\nNote we will drop off 1998 and 2002 because it's not a full year and will skewer the heatmap.\n\nFirst let's construct the dataframe table.","76268fb6":"Check for missing data. From the result below, there are no missing data in this dataframe. Thus, we will not be dropping the missing value records or will not be imputing the data. We will proceed with the further data analysis.","e68bbdd3":"Note to self:\nIn the search for normality\n\ndo log transformation?\n\nfigure out what is normal probability plot","372b1f78":"As you can see the graph is now neat.","5017cf00":"Interestingly, the month that records the highest electricity consumption is july in 1999 and august in 2002. This makes sense since it is the hottest period of the year (and hence electricity consumption was probably due to increase aircon use). Contrary to prediction, summer records higher electricity consumption than winter.","04887f56":"We can also do the same with a 'season plot' - that is, compare each year over 12 months:\n\n","c5f9a0cd":"Note: it is important to diagnose overfitting and underfitting of lstm models.","2e937a66":"\n\nseq_length =50 & epoch=17 (R^2 Score = 0.86)\n\nsequence_length = 70 (R^Score = 0.8727933043483013)\n\n0.8732184473692324\n\n","5edf8ab1":"Possible Improvement:using fit_generator instead of fit. \n \nDifference between keras fit and keras fit_generator: https:\/\/www.pyimagesearch.com\/2018\/12\/24\/how-to-use-keras-fit-and-fit_generator-a-hands-on-tutorial\/","0962c7b8":"Now there's a tail end where it's not a full day, so it's dropping off.\n\nFor our purposes, we will just delete that part day.","f66a8083":"# LSTM model\n","6a602ce9":"Just checking the output","21914e82":"Error Matrix\nDifferent types of error matrix that is used:\n* R^2 (correlation score)\n* Mean Squared error\n* Mean Absolute Error\n\nFor XGBoost,\nthe results are:\n* R^2 Score of XGBoost model =  0.7400406595803339\n* Mean_squared_error = 8943038.068857703\n* Mean_absolute_error = 2169.8273519343543\n\nMean_absolute_error = 6.948508186270928 % For better reference.\n","280d0c01":"Creating XGBoost Model","c0ee225f":"The main goal of this project is to perform a 5-hour forcast of energy consumption. The dataset that will be used is PJM_Load_hourly.csv\n\nI think what I will do is to compare the different methods that can be used to improve the model and look at their accuracy to determine which model is the best","4f5d1978":"# XGBoost"}}