{"cell_type":{"2d66bc3f":"code","b1096dc0":"code","de61d94b":"code","3be8cccf":"code","bdb8a0c3":"code","5402b742":"code","8d61d886":"code","79adedc3":"code","0e35af1b":"code","87967b7e":"code","bb6726b1":"code","198157f0":"code","a1c3709d":"code","1b8ec656":"code","5534ba28":"code","11774c97":"markdown","ef384870":"markdown","7af64cbb":"markdown","4c3ac692":"markdown","0d80ed36":"markdown","a0e478a8":"markdown","43680622":"markdown","96770e86":"markdown","19f24b92":"markdown","c0655bdd":"markdown","32b9bff7":"markdown","0b214f49":"markdown","5615edeb":"markdown","eb01cd0d":"markdown","d1e8737b":"markdown","e2e04bd7":"markdown","897d56d9":"markdown","b5634e91":"markdown","e11a959a":"markdown","eacc6265":"markdown","23b07717":"markdown","6726563f":"markdown","2d81fea3":"markdown","292d283b":"markdown","a35dc45d":"markdown","28ea78ca":"markdown","2a3e4306":"markdown","52ef5b73":"markdown","9defd7e7":"markdown","449ff6e5":"markdown","83ec8203":"markdown","83075aa2":"markdown","61601b05":"markdown","49b2a7b0":"markdown"},"source":{"2d66bc3f":"# Import required libraries\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport sklearn\nimport warnings\nwarnings.filterwarnings('ignore')","b1096dc0":"# Import necessary modules\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.model_selection import LeavePOut\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import StratifiedKFold","de61d94b":"from yellowbrick.model_selection import cv_scores\nfrom yellowbrick.model_selection import CVScores\n","3be8cccf":"data = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')","bdb8a0c3":"data.head()","5402b742":"x1 = data.drop('Outcome', axis=1).values \ny1 = data['Outcome'].values","8d61d886":"X_train, X_test, Y_train, Y_test = model_selection.train_test_split(x1, y1, test_size=0.30, random_state=100)\nmodel = LogisticRegression()\nmodel.fit(X_train, Y_train)\nresult = model.score(X_test, Y_test)\nprint(\"Accuracy: %.2f%%\" % (result*100.0))","79adedc3":"X_train, X_test, Y_train, Y_test = model_selection.train_test_split(x1, y1, test_size=0.30, random_state=22)\nmodel = LogisticRegression()\nmodel.fit(X_train, Y_train)\nresult = model.score(X_test, Y_test)\nprint(\"Accuracy: %.2f%%\" % (result*100.0))","0e35af1b":"loocv = LeaveOneOut()\nmodel_loocv = LogisticRegression()\nresults_loocv = cross_val_score(model_loocv, x1, y1, cv=loocv)\nprint(\"Accuracy: %.2f%%\" % (results_loocv.mean()*100.0))","87967b7e":"for k in range(2,11):\n\n    kfold = KFold(n_splits=k, random_state=100)\n    model_kfold = LogisticRegression()\n    results_kfold = model_selection.cross_val_score(model_kfold, x1, y1, cv=kfold)\n    print(\"Accuracy: %.2f%%\" % (results_kfold.mean()*100.0)) ","bb6726b1":"# Create a cross-validation strategy\ncv = KFold(n_splits=10, random_state=100)\n\n# Instantiate the classification model and visualizer\nmodel = LogisticRegression()\n\n# Fit the data to the visualizer\nvisualizer = cv_scores(model, x1, y1, cv=cv, scoring='accuracy')","198157f0":"for k in range(2,11):\n\n    skfold = StratifiedKFold(n_splits=k, random_state=100)\n    model_skfold = LogisticRegression()\n    results_skfold = cross_val_score(model_skfold, x1, y1, cv=skfold)\n\n    print(\"Accuracy: %.2f%%\" % (results_skfold.mean()*100.0))","a1c3709d":"# Create a cross-validation strategy\ncv = StratifiedKFold(n_splits=10, random_state=100)\n\n# Instantiate the classification model and visualizer\nmodel = LogisticRegression()\nvisualizer = CVScores(model, cv=cv, scoring='accuracy')\n\n# Fit the data to the visualizer\nvisualizer.fit(x1, y1)\n\n# Finalize and render the figure\nvisualizer.show()           ","1b8ec656":"for k in range(2,11):\n\n    kfold2 = ShuffleSplit(n_splits=k, test_size=0.30, random_state=100)\n    model_shufflecv = LogisticRegression()\n    results_4 = model_selection.cross_val_score(model_shufflecv, x1, y1, cv=kfold2)\n    print(\"Accuracy: %.2f%% (%.2f%%)\" % (results_4.mean()*100.0, results_4.std()*100.0))","5534ba28":"# Create a cross-validation strategy\ncv = ShuffleSplit(n_splits=10, random_state=100)\n\n# Instantiate the classification model and visualizer\nmodel = LogisticRegression()\nvisualizer = CVScores(model, cv=cv, scoring='accuracy')\n\n# Fit the data to the visualizer\nvisualizer.fit(x1, y1)\n\n# Finalize and render the figure\nvisualizer.show()  ","11774c97":"<center><img src=\"https:\/\/www.researchgate.net\/profile\/Mariia_Fedotenkova\/publication\/311668395\/figure\/fig5\/AS:613923871019041@1523382265447\/A-schematic-illustration-of-K-fold-cross-validation-for-K-5-Original-dataset-shown.png\"><\/center>","ef384870":"<a id=\"3\"><\/a>\n<h1 style='font-family:georgia;font-size:30px;background:PURPLE; border:0; color:white'><center>ADVANTAGES OF CROSS-VALIDATION<\/center><h1>","7af64cbb":"<center><img src = 'https:\/\/i.stack.imgur.com\/B9CCp.png'><\/center>","4c3ac692":"We can see that the accuracy for the model on the test data is approximately 77% with randomstate=22.","0d80ed36":"<a id=\"20\"><\/a>\n<h1 style='font-family:georgia;font-size:30px;background:PURPLE; border:0; color:white'><center>TABLE OF CONTENTS<\/center><h1>","a0e478a8":"* [WHAT IS CROSS-VALIDATION?](#1)\n\n* [METHODS OF CROSS-VALIDATION](#2)\n    * [1) VALIDATION](#3)\n    * [2) LOOCV (LEAVE ONE OUT CROSS-VALIDATION)](#4)\n    * [3) K-FOLD CROSS-VALIDATION](#5)\n    * [4) STRATIFIED CROSS-VALIDATION](#6)\n    * [5) ADVERSARIAL VALIDATION](#7)\n\n\n* [ADVANTAGES OF CROSS-VALIDATION](#8)\n* [IMPORTING LIBRARIES](#9)\n* [DATA INSPECTION](#10)\n* [HOLD-OUT VALIDATION APPROACH](#11)\n* [LEAVE ONE OUT VALIDATION](#12)\n* [K-FOLD CROSS VALIDATION APPROACH](#13)\n* [STRATIFIED K-FOLD CROSS VALIDATION APPROACH](#14)\n* [SHUFFLE SPLIT APPROACH](#15)\n* [CLOSING COMMENTS](#16)","43680622":"<a id=\"7\"><\/a>\n<h1 style='font-family:georgia;font-size:30px;background:PURPLE; border:0; color:white'><center>LEAVE ONE OUT VALIDATION<\/center><h1>","96770e86":"While the LOOCV gives 77% which isn't much impressive and for this result we can't afford the computation costs.","19f24b92":"<a id=\"6\"><\/a>\n<h1 style='font-family:georgia;font-size:30px;background:PURPLE; border:0; color:white'><center>HOLD-OUT VALIDATION APPROACH<\/center><h1>","c0655bdd":"<a id=\"10\"><\/a>\n<h1 style='font-family:georgia;font-size:30px;background:PURPLE; border:0; color:white'><center>SHUFFLE SPLIT APPROACH<\/center><h1>","32b9bff7":"References for Adversarial Validation:\n    \nhttp:\/\/fastml.com\/adversarial-validation-part-one\/\n\nhttp:\/\/fastml.com\/adversarial-validation-part-two\/","0b214f49":"You see that accuracy is changing with every random_state(random samples chosen for test and train). This method can't be reliable when you would need exact figures.","5615edeb":"<center><img src=\"https:\/\/sguru.org\/wp-content\/uploads\/2018\/03\/Thank-You-Gift-Box-Image.jpg\"><\/center>","eb01cd0d":"<a id=\"9\"><\/a>\n<h1 style='font-family:georgia;font-size:30px;background:PURPLE; border:0; color:white'><center>STRATIFIED K-FOLD CROSS VALIDATION APPROACH<\/center><h1>","d1e8737b":"<h1 style='font-family:georgia;font-size:40px;background:PURPLE; border:0; color:white'><center>VALIDATING MACHINE LEARNING MODELS <\/center><h1>","e2e04bd7":"* More accurate estimate of out-of-sample accuracy.\n* More \u201cefficient\u201d use of data as every observation is used for both training and testing.","897d56d9":"<a id=\"2\"><\/a>\n<h1 style='font-family:georgia;font-size:30px;background:PURPLE; border:0; color:white'><center>METHODS OF CROSS-VALIDATION<\/center><h1>","b5634e91":"<a id=\"5\"><\/a>\n<h1 style='font-family:georgia;font-size:30px;background:PURPLE; border:0; color:white'><center>DATA INSPECTION<\/center><h1>","e11a959a":"## **4) STRATIFIED CROSS-VALIDATION**\n\nStratification is the process of rearranging the data so as to ensure that each fold is a good representative of the whole. For example, in a binary classification if our target labels are imbalanced,it is advisory to arrange the data such that in every fold, each class comprises of about half the instances. If this is not covered, the model learns only to predict the majority label and when unseen data is fed it will predict only majority label which might deteriorate model's predictions.","eacc6265":"<a id=\"1\"><\/a>\n<h1 style='font-family:georgia;font-size:30px;background:PURPLE; border:0; color:white'><center>WHAT IS CROSS-VALIDATION?<\/center><h1>","23b07717":"From the above two validation methods, we\u2019ve learnt:\n\nWe should train the model on a large portion of the dataset. Otherwise we\u2019ll fail to read and recognise the underlying trend in the data. This will eventually result in a higher bias.\n\nWe also need a good ratio of testing data points. As we have seen above, less amount of data points can lead to a variance error while testing the effectiveness of the model.\n\nWe should iterate on the training and testing process multiple times. We should change the train and test dataset distribution. This helps in validating the model effectiveness properly.\n\nDo we have a method which takes care of all these 3 requirements?","6726563f":"## **5) ADVERSARIAL VALIDATION**\n\nMany data science competitions suffer from a test set being markedly different from a training set (a violation of the \u201cidentically distributed\u201d assumption). It is then difficult to make a representative validation set. As a result, the internal cross-validation techniques might give scores that are not even in the ballpark of the test score. In such cases, adversarial validation offers an ideal solution.\n\nThe general idea is to check the degree of similarity between training and tests in terms of feature distribution. If It does not seem to be the case, we can suspect they are quite different. This intuition can be quantified by combining train and test sets, assigning 0\/1 labels (0 \u2013 train, 1-test) and evaluating a binary classification task.\nSpecifically, we\u2019ll run the distinguishing classifier in cross-validation mode, to get predictions for all training examples. Then we\u2019ll see which training examples are misclassified as test and use them for validation.\n\nTo be more precise, we\u2019ll choose a number of misclassified examples that the model was most certain about. It means that they look like test examples but in reality are training examples. Cross-validation provides predictions for all the training points. Now we\u2019d like to sort the training points by their estimated probability of being test examples.\nWe did the ascending sort, so for validation we take a desired number of examples from the end. We will note that if differences between models in validation are pretty slim.\n\nSteps to do Adversarial Validation:\n\nRemove the target variable from the train set\n\n![Capture11.PNG](attachment:Capture11.PNG)\n\nCreate a new target variable which is 1 for each row in the train set, and 0 for each row in the test set\n\n![13.PNG](attachment:13.PNG)\n\nCombine the train and test datasets\n\n![Capture12.PNG](attachment:Capture12.PNG)\n\nUsing the above newly created target variable, fit a classification model and predict probabilities for each row to be in the test set.","2d81fea3":"<h1 style='font-family:georgia;font-size:30px;background:PURPLE; border:0; color:white'><center>REFERENCES<\/center><h1>","292d283b":"We can see that the accuracy for the model on the test data is approximately 75% with randomstate=100.","a35dc45d":"<a id=\"8\"><\/a>\n<h1 style='font-family:georgia;font-size:30px;background:PURPLE; border:0; color:white'><center>K-FOLD CROSS VALIDATION APPROACH<\/center><h1>","28ea78ca":"<a id=\"4\"><\/a>\n<h1 style='font-family:georgia;font-size:30px;background:PURPLE; border:0; color:white'><center>IMPORTING LIBRARIES<\/center><h1>","2a3e4306":"<a id=\"11\"><\/a>\n<h1 style='font-family:georgia;font-size:30px;background:PURPLE; border:0; color:white'><center>CLOSING COMMENTS<\/center><h1>","52ef5b73":"In this kernel, we have learnt about the various model validation techniques using scikit-learn.\n\nThe mean accuracy result for the various techniques is summarised below:\n\n* Leave One Out Cross-Validation: Mean Accuracy of 77.86%\n* Holdout Validation Approach: Accuracy of 75.32%\n* K-fold Cross-Validation: Mean Accuracy of 77%\n* Stratified K-fold Cross-Validation: Mean Accuracy of 77.3%\n* Repeated Random Test-Train Splits: Mean Accuracy of 77.3%\n\nWe can conclude that the cross-validation technique improves the performance of the model and is a better model validation strategy. The model can be further improved by doing exploratory data analysis, data pre-processing, feature engineering, or trying out other machine learning algorithms instead of the logistic regression algorithm we built in this kernel.","9defd7e7":"## 1) **VALIDATION**\nIn this method, we perform training on the 50% of the given data-set and rest 50% is used for the testing purpose. The major drawback of this method is that we perform training on the 50% of the dataset, it may possible that the remaining 50% of the data contains some important information which we are leaving while training our model i.e higher bias.","449ff6e5":"**Cross-validation is a technique in which we train our model using the subset of the data-set and then evaluate using the complementary subset of the data-set.**\n\nDoes this definition sound familiar to what you do \"train_test_split\"?\nYou must be thinking if we have a parameter already for evaluation why do we need Cross-validation?\n    \nWell here is the catch! In train-test-split you keep aside (70%,80%) of data for training and remaining for testing the model, but what if the data points that can bring some pattern is in test dataset and you end up losing it? Also when you split the data with some random_state and calculate the accuracy or any other metric it changes with change in random_state, in this case how would you decide what's the maximum and minimum accuracy,error your model produce? \n    \nThere comes Cross-validation as our Saviour. Wonder how?\n    \nCross-validation starts by shuffling the data (to prevent any unintentional ordering errors) and splitting it into k folds. Then k models are fit on k\u22121 of the data (called the training split) and evaluated on 1\/k of the data (called the test split). The results from each evaluation are averaged together for a final score, then the final model is fit on the entire dataset for operationalization.\n\nThis way there's no leaving out any part of dataset covering all trends!","83ec8203":"\n## 3) **K-FOLD CROSS-VALIDATION**\n\nYes! That method is known as \u201ck-fold cross validation\u201d. \n\nIt\u2019s easy to follow and implement. Below are the steps for it:\n\n* Randomly split your entire dataset into \"kfolds\".\n* For each k-fold in your dataset, build your model on k \u2013 1 folds of the dataset. Then, test the model to check the effectiveness for kth fold.\n* Record the error you see on each of the predictions.\n* Repeat this until each of the k-folds has served as the test set.\n* The average of your k recorded errors is called the cross-validation error and will serve as your performance metric for the model.\n\nNow you might wonder what could be the way to decide the value of k?\n\nWell let me tell you there's no thumb rule for choosing the value of k and it completel depends on size of your dataset. However k=10 is considered to be ideal value as more than 10 would really take longer to validate.\n\nHigher the value of K leads to LOOCV approach where you will incure high computation costs.\n\nLower value of K leads to validation approach where you will lose the important data.","83075aa2":"<center><img src=\"https:\/\/miro.medium.com\/max\/842\/1*SEBYJsfhpcYC43CutV1V5g.png\"><\/center>","61601b05":"https:\/\/www.youtube.com\/watch?v=Bcw8S449QW4\n\nhttps:\/\/www.youtube.com\/watch?v=fKz-SgScM3Q\n\nhttps:\/\/www.scikit-yb.org\/en\/latest\/api\/model_selection\/cross_validation.html","49b2a7b0":"## 2) **LOOCV (LEAVE ONE OUT CROSS-VALIDATION)**\n\nIn this method, we perform training on the whole data-set but leaves only one data-point of the available data-set and then iterates for each data-point. Suppose you have 1000 records in your dataset and in this method you leave out one data point and use the rest 999 records for validation. In this way the validation is run for 1000 times and what if you have millions of records?\nWould you run million times? That's practically impossible!\n\nAn advantage of using this method is that we make use of all data points and hence it is low bias.\nThe major drawback of this method is that it leads to higher variation in the testing model as we are testing against one data point. If the data point is an outlier it can lead to higher variation. Another drawback is it takes a lot of execution time as it iterates over \u2018the number of data points\u2019 times."}}