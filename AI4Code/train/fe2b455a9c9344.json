{"cell_type":{"e666e1b4":"code","9163bc03":"code","32778747":"code","8328e71e":"code","4a936400":"code","5e158a45":"code","aa5846f5":"code","c7e0951f":"code","051f47c5":"code","aef7bad3":"code","16bfac6a":"code","f3d50aca":"code","13022b80":"code","fc7d1771":"code","7806cb09":"code","7ca7e406":"code","185aee34":"code","364ccdac":"code","399887b0":"code","95408c8a":"code","179bcf36":"code","613268fa":"code","36e929ca":"code","7236e895":"code","ea3e77b1":"code","c5dc8b45":"code","4d86499d":"code","6bf48b5e":"code","f2908476":"code","c2ff4223":"code","a153f7d5":"code","bad2b5a9":"code","f8ce672f":"code","f9dedca4":"code","1c11eae0":"code","225d429d":"code","ac56345b":"code","21af4c80":"code","51cdd85a":"code","dae1826f":"code","49eaaddd":"code","9d0e069d":"code","73deface":"code","f9f51fa5":"code","4d6b2f71":"code","f39962e4":"code","65c2e9ef":"markdown","94418fa1":"markdown","41c220f9":"markdown","df4689ec":"markdown","e42566e1":"markdown","22f0d3cd":"markdown","e6289af4":"markdown"},"source":{"e666e1b4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom warnings import filterwarnings as filt \nfrom scipy.stats import skew, norm \nimport plotly.express as px \n\nfilt('ignore')\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = (12,8)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9163bc03":"df = pd.read_csv('\/kaggle\/input\/ascvd-heart-risk\/heartRisk.csv')\ndf.shape","32778747":"df.head()","8328e71e":"df.describe().T","4a936400":"df.isnull().values.sum()","5e158a45":"for ind, col in enumerate(df.loc[:, 'Age':].columns):\n    plt.figure(ind)\n    sns.distplot(df[col])","aa5846f5":"corr = df.corr()\nlower_corr = corr.where(np.tril(np.ones(df.corr().shape), k = -1).astype(bool))\nsns.heatmap(lower_corr, fmt = '.2f', annot = True, cmap = 'icefire')","c7e0951f":"sns.lmplot(data = df, x = 'Age', y = 'Risk', hue = 'isMale', col = 'isMale')","051f47c5":"sns.scatterplot(data = df, x = 'Systolic', y = 'Risk', hue = 'isDiabetic')","aef7bad3":"sns.countplot(df.isDiabetic , hue = df.isSmoker)","16bfac6a":"# systolic is a blood pressure and if its below 120 mm Hg its considered as normal and but if its in range 120 - 139 then there's a risk \ndef sustolic(x):\n    if x < 120: return 'normal'\n    elif x >= 120 and x <= 139 : return 'elevated'\n    else: return 'high_pressure'","f3d50aca":"df['Systolic_levels'] = df.Systolic.apply(lambda x : sustolic(x))\ndf.head()","13022b80":"dummy_sus = pd.get_dummies(df.Systolic_levels, prefix = 'Blood_pressure')\ndf = df.drop(['Systolic_levels'], axis = 1)\ndf = pd.concat([df, dummy_sus], axis = 1)","fc7d1771":"df.head()","7806cb09":"sns.heatmap(df.loc[:, 'Risk':].corr().abs(), fmt = '.2f', annot = True)","7ca7e406":"sns.heatmap(df.loc[:, 'Risk':].drop(['Blood_pressure_high_pressure'], axis = 1).corr().abs(), fmt = '.2f', annot = True)","185aee34":"df = df.drop(['Blood_pressure_high_pressure'], axis = 1)","364ccdac":"df.head()","399887b0":"from eli5 import show_weights\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.ensemble import RandomForestRegressor\nfrom pdpbox.pdp import * \nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import train_test_split","95408c8a":"def permImp(x, y):\n    model = RandomForestRegressor().fit(x, y)\n    perm = PermutationImportance(model).fit(x, y)\n    return show_weights(perm, feature_names = x.columns.tolist())\n\ndef plot_mi(score):\n    score = score.sort_values('mi_score', ascending = True)\n    plt.barh(score.index, score.mi_score)\n    return \n\ndef mi_score(x, y):\n    score = pd.DataFrame(mutual_info_regression(x, y, discrete_features = False, random_state = 123), index = x.columns, columns = ['mi_score']).sort_values('mi_score', ascending = False)\n    plot_mi(score)\n    return score\n\ndef isolate(x, y, col):\n    model = RandomForestRegressor().fit(x, y)\n    dist = pdp_isolate(model, model_features = x.columns, dataset = x, feature = col)\n    return pdp_plot(dist, feature_name = col)","179bcf36":"corr = df.corr().abs()\nlower_corr = corr.where(np.tril(np.ones(df.corr().shape), k = -1).astype(bool))\nsns.heatmap(lower_corr, fmt = '.2f', annot = True, cmap = 'icefire')","613268fa":"x = df.drop(['Risk'], axis = 1)\ny = df.Risk\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3)\nx_train.shape, x_test.shape","36e929ca":"permImp(x, y)","7236e895":"mscore = mi_score(x, y)","ea3e77b1":"isolate(x, y, 'Age')","c5dc8b45":"from sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error","4d86499d":"def best_model(x, y, fold = 10):\n    models = [LinearRegression(), Lasso(), Ridge(), SVR(), DecisionTreeRegressor(), RandomForestRegressor(), LGBMRegressor()]\n    mnames = ['linar regg', 'lasso', 'ridge', 'svm', 'decision tree', 'random forest', 'lgbm']\n    scalers = [None, StandardScaler(), RobustScaler(), MinMaxScaler()]\n    snames = ['none', 'std', 'robust', 'minmax']\n    scores = [[] for _ in range(4)]\n    \n    print(f'Total number of iterations : {len(mnames) * len(snames)}')\n    for model in models:\n        for ind, scaler in enumerate(scalers):\n            if scaler:\n                model = Pipeline(steps = [('scaler', scaler), ('model', model)])\n            score = cross_val_score(model, x, y, cv = fold, scoring = 'neg_mean_squared_error').mean()\n            rmse = np.sqrt(-1 * score)\n            scores[ind].append(rmse)\n            \n    return pd.DataFrame(scores, index = snames, columns = mnames).T\n\ndef report(xt, yt, xtest, ytest, pred, model):\n    print(' REPORT '.center(60, '='))\n    print()\n    print(f'Training RMSE :====> {np.sqrt(mean_squared_error(yt, model.predict(xt)))}')\n    print(f'Testing  RMSE :====> {np.sqrt(mean_squared_error(ytest, pred))}')\n    print()\n    print(f\"Training  MAE :====> {mean_absolute_error(yt, model.predict(xt))}\")\n    print(f'Testing   MAE :====> {mean_absolute_error(ytest, pred)}')\n    print()\n    return pd.DataFrame({\n        'actual value' : ytest.values,\n        'predicted value' : pred\n    }).head()\n\ndef get_score(xt, yt, xtest, ytest, model, scaler = None, predict = True):\n    if scaler:\n        model = Pipeline(steps = [('scaler', scaler), ('model', model)])\n    \n    model.fit(xt, yt)\n    pred = model.predict(xtest)\n    return report(xt, yt, xtest, ytest, pred, model)\n    \ndef gridcv(x, y, model, params, scaler = None, fold = 10):\n    if scaler:\n        model = Pipeline(steps = [('scaler', scaler), ('model', model)])\n    clf = GridSearchCV(model, param_grid = params, cv = fold, scoring = 'neg_mean_squared_error', return_train_score = True, verbose = 2, n_jobs = -1)\n    clf.fit(x, y)\n    res = pd.DataFrame(clf.cv_results_)\n    res[['mean_train_score', 'mean_test_score']] = np.sqrt(-1 * res[['mean_train_score', 'mean_test_score']]) \n    res = res[['mean_train_score', 'mean_test_score', 'params']]\n    return clf, res.sort_values('mean_test_score', ascending = True)\n\ndef plot_cv(res):\n    sns.lineplot(x = res.reset_index().index, y = res.mean_train_score)\n    sns.lineplot(x = res.reset_index().index, y = res.mean_test_score)\n    plt.title('RMSE comparision')\n    plt.legend(['train', 'test'])\n    ","6bf48b5e":"best_model(x_train, y_train)","f2908476":"get_score(x_train, y_train, x_test, y_test, LGBMRegressor(), StandardScaler())","c2ff4223":"sns.distplot(df.Risk, fit = norm)","a153f7d5":"og_risk = df.Risk\ndf['Risk'] = np.round(np.log1p(df.Risk), 2)","bad2b5a9":"df.Risk.head()","f8ce672f":"sns.distplot(df.Risk, fit = norm)","f9dedca4":"new_x = df.drop(['Risk'], axis = 1)\nnew_y = df.Risk\nnew_x_train, new_x_test, new_y_train, new_y_test = train_test_split(new_x, new_y, test_size = 0.3)\nnew_x_train.shape, x_test.shape","1c11eae0":"best_model(new_x_train, new_y_train)","225d429d":"get_score(new_x_train, new_y_train, new_x_test, new_y_test, SVR(), StandardScaler())","ac56345b":"params = {\n    'C' : [1, 50, 100, 500],\n    'kernel' : ['rbf', 'sigmoid'],\n    'gamma' : ['scale', 'auto'],\n    'epsilon' : [0.1, 0.01, 1, 0.5]\n}\n\npip_params = {f\"model__{key}\" : values for key, values in params.items()}\npip_params","21af4c80":"clf, results = gridcv(new_x_train, new_y_train, SVR(), pip_params, StandardScaler(), 5)","51cdd85a":"plot_cv(results)","dae1826f":"results.head()","49eaaddd":"clf.best_estimator_","9d0e069d":"get_score(new_x_train, new_y_train, new_x_test, new_y_test, clf.best_estimator_)","73deface":"from sklearn.manifold import TSNE","f9f51fa5":"tsne = TSNE(n_components = 2, n_iter=5000)\ntsne_x = tsne.fit_transform(new_x)\ntsne_x","4d6b2f71":"tsne_x = pd.DataFrame(tsne_x, columns = ['x', 'y'])\ntsne_x = pd.concat([tsne_x, new_x[['isMale', 'isSmoker', 'Blood_pressure_normal']]], axis = 1)\ntsne_x.head()","f39962e4":"sns.scatterplot(data = tsne_x, x = 'x', y = 'y', hue = 'Blood_pressure_normal')","65c2e9ef":"its already good enough, but lets try to reduce the error even more ","94418fa1":"to not fall into dummy trap lets drop high_pressure ","41c220f9":"form this plot we can interprete that greater the age greater the chance of heart risk rate, which is why age had high positive corr to risk","df4689ec":"### feature importance ","e42566e1":"finally we were able to reduce the RMSE to 0.082","22f0d3cd":"### model building ","e6289af4":"after normalizing the target variable, all models RMSE reduced tremendously "}}