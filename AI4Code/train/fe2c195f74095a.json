{"cell_type":{"fb8e1686":"code","0d807383":"code","767c7d63":"code","7acc1cda":"code","cf55a655":"code","54200f44":"code","41d6e8d6":"code","d8ef8c9c":"code","4d1bf8ae":"code","dd839359":"code","e8ad934d":"code","dd29ba1a":"code","b94105bf":"code","4f1e0492":"code","ec759974":"code","ded33e92":"markdown","786be102":"markdown","7e906873":"markdown","ce8c6948":"markdown","98497f5c":"markdown","426a9ec5":"markdown"},"source":{"fb8e1686":"# Import relevant packages. \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","0d807383":"def relu(Z):\n    A = (0 < Z) * Z\n    return A\n\ndef sigmoid(Z):\n    A = 1\/(1+np.exp(-Z)+1e-8)\n    return A\n\ndef softmax(Z):\n    A = np.exp(Z)\/np.sum(np.exp(Z),axis=0,keepdims=True)\n    return A\n    \ndef one_hot_encoding(Y,n_classes=-1):    \n    classes = np.reshape(np.unique(Y),(-1,1))\n    Y_enc = np.equal(classes,Y)*1\n    return Y_enc\n\ndef one_hot_decoding(Y):    \n    return np.argmax(Y,axis=0).reshape((1,-1))\n    ","767c7d63":"def nn_layer_forward(X, W, b, activation = 'relu'):\n    \n    if activation == 'relu':\n        Z = np.dot(W,X) + b\n        A = relu(Z)\n    elif activation == 'sigmoid':\n        Z = np.dot(W,X) + b\n        A = sigmoid(Z) \n    elif activation == 'softmax':\n        Z = np.dot(W,X) + b\n        A = softmax(Z) \n    else:\n        Z = np.dot(W,X) + b\n        A = sigmoid(Z)   \n        \n    return Z,A  \n\n\ndef nn_layer_backward(W, b, Z, A_prev, A, dA, activation = 'relu'):\n    \n    m = Z.shape[0]\n    \n    if activation == 'relu':\n        gp = (0 < Z)*1\n        dZ = dA*gp\n    elif activation == 'sigmoid':   \n        gp = A * (1 - A)\n        dZ = dA*gp\n    elif activation == 'softmax':\n        dZ = A*(1-dA)\n            \n    dW = np.dot(dZ,A_prev.T) \/ m\n    db = np.sum(dZ,axis=1,keepdims=True)\/m                # Dont forget axis=1, keepdims=True\n    dA_prev = np.dot(W.T,dZ)            # The multiplication here are a dot product.\n    return dA_prev, dW, db","7acc1cda":"def random_index_mini_batches(m,mini_batch_size = 64, seed = 0):\n\n    random_indx = np.random.permutation(m)\n    \n    n_mini_batches = np.floor(m\/mini_batch_size)\n    indx_list = []\n    ini = 0\n    for i in range(int(n_mini_batches)):\n        indx_list.append(random_indx[ini:ini+mini_batch_size])\n        ini = ini + mini_batch_size\n    \n    if np.floor(m\/mini_batch_size) < m\/mini_batch_size:\n        indx_list.append(random_indx[ini:])\n    \n    return indx_list","cf55a655":"class DNN:\n\n\n    def __init__(self, layers_dim,activations):       \n        self.layers_dim = list(layers_dim)\n        self.activations = list(activations)        \n        self.n_layers = len(self.layers_dim)-1        \n        self.Ws = [None]*(self.n_layers+1) # include layer 0\n        self.bs = [None]*(self.n_layers+1) # include layer 0\n        self.Zs = [None]*(self.n_layers+1) # include layer 0\n        self.As = [None]*(self.n_layers+1) # include layer 0\n        self.dAs = [None]*(self.n_layers+1) # include layer 0\n        self.dWs = [None]*(self.n_layers+1) # include layer 0\n        self.dbs = [None]*(self.n_layers+1) # include layer 0\n        \n    def initialize_parameters(self):\n        \n        for l in range(1,self.n_layers+1):            \n            self.Ws[l] = np.random.rand(self.layers_dim[l],self.layers_dim[l-1])*0.01\n            self.bs[l] = np.zeros((self.layers_dim[l],1))\n\n    def forward_prop(self, X):        \n        self.As[0] = X\n        for l in range(1,self.n_layers+1):\n            self.Zs[l],self.As[l] = nn_layer_forward(self.As[l-1], self.Ws[l], self.bs[l],self.activations[l])\n        \n        AL = self.As[self.n_layers]        \n        return AL\n    \n    def calculate_cost(self,Y):\n        m = Y.shape[0]\n        AL = self.As[self.n_layers]\n        \n        if self.activations[self.n_layers] == 'softmax':\n            L = -np.sum(np.sum(Y*np.log(AL+1e-8),axis=0,keepdims=True))\/m\n            dAL = Y\/(AL+1e-8)\n        else:\n            L = - np.sum(Y*np.log(AL) + (1-Y)*np.log(1-AL))\/m\n            dAL = (-(Y\/AL) + ((1-Y)\/(1-AL)))  \n            \n        self.dAs[self.n_layers] = dAL\n        \n        return L\n    \n    def back_prop(self):\n        for l in reversed(range(1,self.n_layers+1)):\n            self.dAs[l-1], self.dWs[l], self.dbs[l] = nn_layer_backward(self.Ws[l], self.bs[l], self.Zs[l], self.As[l-1], self.As[l], self.dAs[l],  self.activations[l])\n            \n    \n    def update_parameters(self, learning_rate = 0.001):\n        for l in range(1,self.n_layers+1):\n            self.Ws[l] = self.Ws[l] - learning_rate*self.dWs[l]\n            self.bs[l] = self.bs[l] - learning_rate*self.dbs[l]\n            \n    def train(self,X,Y,n_epochs=10000,learning_rate=0.0001,mini_batch_size = -1,verbose=1):\n        \n        if mini_batch_size == -1:\n            mini_batch_size = X.shape[1]\n        m = X.shape[1]  \n        \n        for epoch in range(n_epochs):   \n            cost_total = 0\n            indx_list = random_index_mini_batches(m,mini_batch_size=mini_batch_size)\n            for i_mini_batch in range(0,len(indx_list)):                \n                X_minibatch = X[:,indx_list[i_mini_batch]]\n                Y_minibatch = Y[:,indx_list[i_mini_batch]]\n                dnn.forward_prop(X_minibatch)\n                L = dnn.calculate_cost(Y_minibatch)\n                dnn.back_prop()\n                dnn.update_parameters(learning_rate)\n                cost_total += L\n            if epoch%np.round(n_epochs*0.1) == 0 and verbose:\n                print('Epoch:'+ str(epoch),'-','Cost: ' + str(cost_total))\n\n        \n        ","54200f44":"m = 1000\nX = np.array(np.random.rand(2,m)-0.5)*100\nr = np.sqrt(np.square(X[0,:])+np.square(X[1,:]))\nY = (r < 30)*1.0\nY = np.reshape(20*np.sin(X[0,:]\/10) > X[1,:],(1,-1))\n\nplt.scatter(X[0,:],X[1,:],c=Y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Train Data')","41d6e8d6":"layers_dim = (X.shape[0],30,10,Y.shape[0])\nactivations = (None,'relu','relu','sigmoid')\ndnn = DNN(layers_dim,activations)\ndnn.initialize_parameters()\ndnn.train(X,Y,n_epochs=100000)","d8ef8c9c":"X_test = np.array(np.random.rand(2,m)-0.5)*100  \nYhat = np.round(dnn.forward_prop(X_test))\nplt.scatter(X_test[0,:],X_test[1,:],c=Yhat)\n\nplt.xlabel('X')\nplt.ylabel('Estimated Y')\nplt.title('Result on Test Data')","4d1bf8ae":"m = 1000\nX = np.array(np.random.rand(2,m)-0.5)*100\nr = np.sqrt(np.square(X[0,:])+np.square(X[1,:]))\nY = np.reshape(20*np.sin(X[0,:]\/10) > X[1,:],(1,-1))\nY = Y*1\nY[0,r < 20] = 2\n\nplt.scatter(X[0,:],X[1,:],c=Y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Train Data')\n\nY = one_hot_encoding(Y)\n","dd839359":"layers_dim = (X.shape[0],20,5,Y.shape[0])\nactivations = (None,'relu','relu','softmax')\ndnn = DNN(layers_dim,activations)\ndnn.initialize_parameters()\ndnn.train(X,Y,n_epochs=100000)\n\n","e8ad934d":"X_test = np.array(np.random.rand(2,m)-0.5)*100  \nYhat = np.round(dnn.forward_prop(X_test))\nYhat = one_hot_decoding(Yhat)\nplt.scatter(X_test[0,:],X_test[1,:],c=Yhat)\nplt.xlabel('X')\nplt.ylabel('Estimated Y')\nplt.title('Result on Test Data')\n\nprint(dnn.calculate_cost(Yhat))\n\n","dd29ba1a":"train_data = pd.read_csv('..\/input\/mnist-in-csv\/mnist_train.csv')\nnp_data = np.array(train_data)\nX_train = np_data[:,1:].T\nX_train = X_train\/255\nY_train = np.reshape(np_data[:,0],(1,-1))\nY_train = one_hot_encoding(Y_train)\n\nprint(X_train.shape,Y_train.shape)\nprint(np.max(X_train))","b94105bf":"layers_dim = (X_train.shape[0],32,Y_train.shape[0])\nactivations = (None,'relu','softmax')\ndnn = DNN(layers_dim,activations)\ndnn.initialize_parameters()\ndnn.train(X_train,Y_train,n_epochs=100,learning_rate=0.01,mini_batch_size=32)","4f1e0492":"test_data = pd.read_csv('..\/input\/mnist-in-csv\/mnist_test.csv')\nnp_data = np.array(test_data)\nX_test = np_data[:,1:].T\nX_test = X_test\/255\nY_test = np.reshape(np_data[:,0],(1,-1))\n\nYhat = dnn.forward_prop(X_test)\nYhat = one_hot_decoding(Yhat)\n\nprecision = np.sum(Yhat == Y_test)\/Y_test.shape[1]\nprint(np.sum(Yhat == Y_test),Y_test.shape[1])\nprint(Yhat.shape,Y_test.shape)\nprint('Precision: ',precision)","ec759974":"indx = int(np.round(np.random.rand()*X_test.shape[1]))\n\nvect_img = np.reshape(X_test[:,indx],(-1,1))\ndim = (int(np.sqrt(vect_img.shape[0])),int(np.sqrt(vect_img.shape[0])))\nimg = np.reshape(vect_img,dim)\n\nplt.imshow(img,cmap='Greys')\nYhat = dnn.forward_prop(vect_img)\nYhat = one_hot_decoding(Yhat)\nprint('Predicted: ',Yhat[0])\nprint('True Label: ', Y_test[:,indx])","ded33e92":"### Test Case 1: Binary Classification","786be102":"# ****Deep Neural Network from Scratch****\n\nThis notebook shows an implementation of a Deep Neural Network from scratch using only Numpy. It is largely based on the Deep Learning Specialization given in Coursera, especially regarding the notation and matrix definition","7e906873":"## Test on Sythetic data\n\nIn this section 2 tests on synthetic data are presented.","ce8c6948":"## DNN Construction\n\nIn this section the auxiliar functions as well as the main DNN class are constructed.","98497f5c":"### Test Case 2: Three Classes Classification","426a9ec5":"### Test Case 3: MNIST"}}