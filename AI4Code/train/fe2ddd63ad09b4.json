{"cell_type":{"fc199dc3":"code","d9f78cb6":"code","df3394c3":"code","2b994202":"code","d3cd4b27":"code","e31ecf1f":"code","f313add7":"code","96292099":"code","afbe7fa8":"code","b933d188":"code","da53f63c":"code","83a65e60":"code","36512791":"code","9ba4884c":"code","70e10e03":"code","ddfe03d3":"code","3347f569":"code","27affd25":"code","c0886ac1":"code","6889b7ca":"code","9db2d86d":"code","99688480":"code","b1ce01fc":"code","da477cfe":"code","fc7f4335":"code","488f2f68":"code","6ee66b60":"code","8573216c":"code","e19cc4b7":"code","7a05c65a":"code","076daf68":"code","a798daad":"code","3bf96df4":"code","4da59dfd":"code","d547fa49":"code","74365709":"code","1a075368":"code","cd11ad3c":"code","f4e57df4":"code","cb226e80":"code","7292c510":"code","b65e8e49":"code","f4e80c31":"code","ef8267f8":"code","01146fa1":"code","1f09d85a":"code","944e1551":"code","12afaffe":"code","8399e76c":"code","1a336d1c":"code","1373c27e":"code","08c1145f":"markdown","c03001f0":"markdown","4692e9b6":"markdown","b0583dc5":"markdown","c554f8d6":"markdown","209cea29":"markdown","aaf8eaa0":"markdown","ec2ab35a":"markdown","f6d4fed6":"markdown","d62fdb9c":"markdown","0fe6c646":"markdown","555c6a1e":"markdown","cf83292c":"markdown","14175ccd":"markdown","08a2468b":"markdown","b610ca48":"markdown","95221232":"markdown","c02ee4c5":"markdown","4126c139":"markdown","7a90bec5":"markdown","dee1d520":"markdown","aafe2542":"markdown"},"source":{"fc199dc3":"# Loading packages\nimport pandas as pd #Analysis \nimport matplotlib.pyplot as plt #Visulization\nimport seaborn as sns #Visulization\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nimport numpy as np #Analysis \nfrom scipy.stats import norm #Analysis \nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer #Analysis \nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import roc_auc_score, roc_curve, mean_squared_error\nfrom sklearn.cluster import KMeans\nfrom scipy.special import boxcox1p\nimport xgboost as xgb\nimport lightgbm as lgb\n\nimport warnings \nwarnings.filterwarnings('ignore')\nimport gc\n\nimport os\nprint(os.listdir(\"..\/input\/2019-2nd-ml-month-with-kakr\/\"))\n\n# Any results you write to the current directory are saved as output.","d9f78cb6":"def pre_df(train_df, test_df):\n    train_df = train_df.drop(['id'], axis=1)\n    test_df = test_df.drop(['id'], axis=1)\n    \n    #target\n    train_df['log_price'] = np.log(train_df.price)\n    \n    #date \n    train_df['date'] = train_df.date.map(lambda x : x[:6])\n    test_df['date'] = test_df.date.map(lambda x : x[:6])\n    train_df.date = train_df.date.astype(int)\n    test_df.date = test_df.date.astype(int)\n    \n    #delete sqft_lot15\n    train_df = train_df.drop(['sqft_lot15'], axis=1)\n    test_df = test_df.drop(['sqft_lot15'], axis=1)\n\n    #zip_level\n    a = train_df[['zipcode', 'price']].groupby('zipcode').mean() \n    label = [j+1 for j in range(27)]\n    a['zip_level'] = pd.cut(a.price, bins=27, labels=label)\n    a = a.drop(['price'], axis=1)\n    train_df = train_df.merge(a, on='zipcode', how='left')\n    test_df = test_df.merge(a, on='zipcode', how='left')\n    train_df.zip_level = train_df.zip_level.astype(int)\n    test_df.zip_level = test_df.zip_level.astype(int)\n\n    #zip_mean_price with Kmeans\n    train_df['coord_cluster'] = None\n    test_df['coord_cluster'] = None\n    for i in train_df.zipcode.unique():\n        df = train_df.loc[train_df.zipcode == i]\n        coord = df[['lat','long']]\n        num = (np.ceil(len(df) \/ 15)).astype(int)\n        kmeans = KMeans(n_clusters=num, random_state=125).fit(coord)\n        coord_cluster = kmeans.predict(coord)\n        df['coord_cluster'] = coord_cluster\n        df['coord_cluster'] = df['coord_cluster'].map(lambda x: 'c_' + str(x).rjust(2, '0'))\n        train_df.loc[df.index, 'coord_cluster'] = df['coord_cluster']\n\n        t_df = test_df.loc[test_df.zipcode == i]\n        t_coord = t_df[['lat','long']]\n        coord_cluster = kmeans.predict(t_coord)\n        t_df['coord_cluster'] = coord_cluster\n        t_df['coord_cluster'] = t_df['coord_cluster'].map(lambda x: 'c_' + str(x).rjust(2, '0'))\n        test_df.loc[t_df.index, 'coord_cluster'] = t_df['coord_cluster']\n\n    train_df['test'] = train_df['zipcode'].astype(str) + train_df['coord_cluster']\n    test_df['test'] = test_df['zipcode'].astype(str) + test_df['coord_cluster']\n    k = train_df[['price','test']].groupby('test').mean()\n    k = k.rename(columns={'price' : 'mean_price'})\n    train_df = pd.merge(train_df, k, how='left', on='test')\n    test_df = pd.merge(test_df, k, how='left', on='test')\n    train_df = train_df.rename(columns={'price_x' : 'price', 'price_y' : 'mean_price'})\n    train_df = train_df.drop(['coord_cluster', 'test'], axis=1)\n    test_df = test_df.drop(['coord_cluster', 'test'], axis=1)\n    \n    #is_re\n    train_df['is_re'] = 0\n    train_df.loc[train_df.loc[train_df.yr_renovated != 0].index, 'is_re'] = 1\n    train_df.loc[train_df.loc[train_df.yr_renovated == 0].index, 'is_re'] = 0\n    test_df['is_re'] = 0\n    test_df.loc[test_df.loc[test_df.yr_renovated != 0].index, 'is_re'] = 1\n    test_df.loc[test_df.loc[test_df.yr_renovated == 0].index, 'is_re'] = 0\n\n    #yr_built\n    train_df.loc[train_df.loc[train_df.yr_built < train_df.yr_renovated].index, 'yr_built'] = train_df.loc[\n        train_df.yr_built < train_df.yr_renovated, 'yr_renovated']\n    test_df.loc[test_df.loc[test_df.yr_built < test_df.yr_renovated].index, 'yr_built'] = test_df.loc[\n        test_df.yr_built < test_df.yr_renovated, 'yr_renovated']\n\n    #is_ba\n    train_df['is_ba'] = 0\n    train_df.loc[train_df.loc[train_df.sqft_basement != 0].index, 'is_ba'] = 1\n    train_df.loc[train_df.loc[train_df.sqft_basement == 0].index, 'is_ba'] = 0\n    test_df['is_ba'] = 0\n    test_df.loc[test_df.loc[test_df.sqft_basement != 0].index, 'is_ba'] = 1\n    test_df.loc[test_df.loc[test_df.sqft_basement == 0].index, 'is_ba'] = 0\n\n    train_df = train_df.drop(['sqft_basement','yr_renovated'], axis=1)\n    test_df = test_df.drop(['sqft_basement','yr_renovated'], axis=1)\n    \n    #living_rate\n    train_df['living_rate'] = train_df['sqft_living'] \/ train_df['sqft_lot']\n    test_df['living_rate'] = test_df['sqft_living'] \/ test_df['sqft_lot']\n    \n    #new_grade\n    train_df['new_grade'] = train_df['grade'] + train_df['condition'] + train_df['view']\n    test_df['new_grade'] = test_df['grade'] + test_df['condition'] + test_df['view']\n    \n    #per_living\n    train_df['per_living'] = 0\n    test_df['per_living'] = 0\n    for i in train_df.zipcode.unique():\n        tr_df = train_df.loc[train_df.zipcode == i]\n        num = len(tr_df)\n        te_df = test_df.loc[test_df.zipcode == i]\n        df = pd.concat([tr_df, te_df], axis=0)\n\n        min = df.sqft_living.min()\n        max = df.sqft_living.max()\n        ran = max-min\n\n        df['per_living'] = (df['sqft_living'] - min) \/ran\n        df['k'] = pd.cut(df.per_living, bins=10, labels=[0,1,2,3,4,5,6,7,8,9])\n        df['k'] = df['k'].astype(int)\n\n        train_df.loc[tr_df.index, 'per_living'] = df['k'][:num]\n        test_df.loc[te_df.index, 'per_living'] = df['k'][num:]\n    print('finish')\n    return train_df, test_df\nprint('data preparation function')","df3394c3":"def zip_onehot(train_data, test_data):\n    dummy = pd.get_dummies(train_data.zipcode, prefix='zipcode')\n    train_data = pd.concat([train_data, dummy], axis=1)\n\n    dummy = pd.get_dummies(test_data.zipcode, prefix='zipcode')\n    test_data = pd.concat([test_data, dummy], axis=1)\n\n    train_data = train_data.drop(['zipcode'], axis=1)\n    test_data = test_data.drop(['zipcode'], axis=1)\n    return train_data, test_data\n#\uc0c1\uc704\uad8c zipcode : 98118, 98033, 98006, 98122, 98112, 98106, 98108\nprint('one-hot encoder function')","2b994202":"def result(train_data):\n    train_df = pd.read_csv('..\/input\/2019-2nd-ml-month-with-kakr\/train.csv')\n    target = np.log(train_df.price)\n    high_train = train_data.loc[train_df.price >= train_df.price.quantile(0.995)]\n    h_target = target.loc[high_train.index]\n    \n    dtest = xgb.DMatrix(high_train)\n    y_pred = model.predict(dtest)\n    print('high_data error : ', np.sqrt(np.mean(np.square(np.exp(h_target) - np.exp(y_pred)))))\n\n    dtest = xgb.DMatrix(train_data)\n    y_pred = model.predict(dtest)\n    print('all_data error : ', np.sqrt(np.mean(np.square(np.exp(target) - np.exp(y_pred)))))\nprint('check result function')","d3cd4b27":"#xgb_params\ndef rmse_exp(predictions, dmat):\n    labels = dmat.get_label()\n    error = np.expm1(predictions) - np.expm1(labels)\n    squared_error = np.square(error)\n    mean = np.mean(squared_error)\n    return ('rmse_exp', np.sqrt(mean))\n\nxgb_params = {\n    'eta': 0.02,\n    'max_depth': 6,\n    'subsample': 0.8,\n    'colsample_bytree': 0.4,\n#     'tree_method': 'gpu_hist',    # \ucd5c\uc801\ud654\ub41c \ubd84\ud560 \uc9c0\uc810\uc744 \ucc3e\uae30 \uc704\ud55c algorithm \uc124\uc815 + \uce90\uae00\uc758 GPU \uc0ac\uc6a9\n#     'predictor': 'gpu_predictor', # \uc608\uce21 \uc2dc\uc5d0\ub3c4 GPU\uc0ac\uc6a9\n    'objective': 'reg:linear',    # \ud68c\uadc0\n    'eval_metric': 'rmse',        # kaggle\uc5d0\uc11c \uc694\uad6c\ud558\ub294 \uac80\uc99d\ubaa8\ub378\n    'silent': True,               # \ud559\uc2b5 \ub3d9\uc548 \uba54\uc138\uc9c0 \ucd9c\ub825\ud560\uc9c0 \ub9d0\uc9c0\n    'seed': 4777,\n}\nprint('xgb params')","e31ecf1f":"#data load\ntrain_df = pd.read_csv('..\/input\/2019-2nd-ml-month-with-kakr\/train.csv')\ntest_df = pd.read_csv('..\/input\/2019-2nd-ml-month-with-kakr\/test.csv')\n#create result dataframe\ntest_result = pd.DataFrame(data={'id':test_df.id})\ntrain_result = pd.DataFrame(data={'id':train_df.id})","f313add7":"a = train_df[['zipcode', 'price']].groupby('zipcode').mean()\na.head()","96292099":"#zip_level\na = train_df[['zipcode', 'price']].groupby('zipcode').mean()\nlabel = [j+1 for j in range(27)]\na['zip_level'] = pd.cut(a.price, bins=27, labels=label)\na = a.drop(['price'], axis=1)\n\ntrain_df = train_df.merge(a, on='zipcode', how='left')\ntest_df = test_df.merge(a, on='zipcode', how='left')\ntrain_df.zip_level = train_df.zip_level.astype(int)\ntest_df.zip_level = test_df.zip_level.astype(int)","afbe7fa8":"print(train_df.price.corr(train_df.zip_level))\nfig = plt.figure(figsize=(10, 6))\nsns.boxplot(train_df.zip_level, train_df.price)","b933d188":"#zip_mean_price with Kmeans\ntrain_df['coord_cluster'] = None\ntest_df['coord_cluster'] = None\n\nfor i in train_df.zipcode.unique():\n    df = train_df.loc[train_df.zipcode == i]\n    coord = df[['lat','long']]\n    num = (np.ceil(len(df) \/ 15)).astype(int)\n    kmeans = KMeans(n_clusters=num, random_state=125).fit(coord)\n    coord_cluster = kmeans.predict(coord)\n    df['coord_cluster'] = coord_cluster\n    df['coord_cluster'] = df['coord_cluster'].map(lambda x: 'c_' + str(x).rjust(2, '0'))\n    train_df.loc[df.index, 'coord_cluster'] = df['coord_cluster']\n\n    t_df = test_df.loc[test_df.zipcode == i]\n    t_coord = t_df[['lat','long']]\n    coord_cluster = kmeans.predict(t_coord)\n    t_df['coord_cluster'] = coord_cluster\n    t_df['coord_cluster'] = t_df['coord_cluster'].map(lambda x: 'c_' + str(x).rjust(2, '0'))\n    test_df.loc[t_df.index, 'coord_cluster'] = t_df['coord_cluster']\n\ntrain_df['test'] = train_df['zipcode'].astype(str) + train_df['coord_cluster']\ntest_df['test'] = test_df['zipcode'].astype(str) + test_df['coord_cluster']\nk = train_df[['price','test']].groupby('test').mean()\nk = k.rename(columns={'price' : 'mean_price'})\n\ntrain_df = pd.merge(train_df, k, how='left', on='test')\ntest_df = pd.merge(test_df, k, how='left', on='test')\ntrain_df = train_df.rename(columns={'price_x' : 'price', 'price_y' : 'mean_price'})\n\ntrain_df = train_df.drop(['coord_cluster', 'test'], axis=1)\ntest_df = test_df.drop(['coord_cluster', 'test'], axis=1)","da53f63c":"fig = plt.figure(figsize=(12,8))\nsns.scatterplot(x='long',y='lat',hue='mean_price',size='price',sizes=(5,100), data=train_df)\nprint(train_df.price.corr(train_df.mean_price))","83a65e60":"#is_re\ntrain_df['is_re'] = 0\ntrain_df.loc[train_df.loc[train_df.yr_renovated != 0].index, 'is_re'] = 1\ntrain_df.loc[train_df.loc[train_df.yr_renovated == 0].index, 'is_re'] = 0\n\ntest_df['is_re'] = 0\ntest_df.loc[test_df.loc[test_df.yr_renovated != 0].index, 'is_re'] = 1\ntest_df.loc[test_df.loc[test_df.yr_renovated == 0].index, 'is_re'] = 0\n\n#yr_built\ntrain_df.loc[train_df.loc[train_df.yr_built < train_df.yr_renovated].index, 'yr_built'] = train_df.loc[\n    train_df.yr_built < train_df.yr_renovated, 'yr_renovated']\n\ntest_df.loc[test_df.loc[test_df.yr_built < test_df.yr_renovated].index, 'yr_built'] = test_df.loc[\n    test_df.yr_built < test_df.yr_renovated, 'yr_renovated']\n\n#is_ba\ntrain_df['is_ba'] = 0\ntrain_df.loc[train_df.loc[train_df.sqft_basement != 0].index, 'is_ba'] = 1\ntrain_df.loc[train_df.loc[train_df.sqft_basement == 0].index, 'is_ba'] = 0\n\ntest_df['is_ba'] = 0\ntest_df.loc[test_df.loc[test_df.sqft_basement != 0].index, 'is_ba'] = 1\ntest_df.loc[test_df.loc[test_df.sqft_basement == 0].index, 'is_ba'] = 0\n\ntrain_df = train_df.drop(['sqft_basement','yr_renovated'], axis=1)\ntest_df = test_df.drop(['sqft_basement','yr_renovated'], axis=1)","36512791":"train_df['living_rate'] = train_df['sqft_living'] \/ train_df['sqft_lot']\ntest_df['living_rate'] = test_df['sqft_living'] \/ test_df['sqft_lot']","9ba4884c":"train_df['new_grade'] = train_df['grade'] + train_df['condition'] + train_df['view']\ntest_df['new_grade'] = test_df['grade'] + test_df['condition'] + test_df['view']","70e10e03":"train_df['per_living'] = 0\ntest_df['per_living'] = 0\nfor i in train_df.zipcode.unique():\n    tr_df = train_df.loc[train_df.zipcode == i]\n    num = len(tr_df)\n    te_df = test_df.loc[test_df.zipcode == i]\n    df = pd.concat([tr_df, te_df], axis=0)\n\n    min = df.sqft_living.min()\n    max = df.sqft_living.max()\n    ran = max-min\n\n    df['per_living'] = (df['sqft_living'] - min) \/ran\n    df['k'] = pd.cut(df.per_living, bins=10, labels=[0,1,2,3,4,5,6,7,8,9])\n    df['k'] = df['k'].astype(int)\n\n    train_df.loc[tr_df.index, 'per_living'] = df['k'][:num]\n    test_df.loc[te_df.index, 'per_living'] = df['k'][num:]","ddfe03d3":"def skew(train_data, test_data, ca_var):\n    skewness = train_data[ca_var].skew()\n    skew_var = skewness.index[skewness.values >= 0.5]\n\n    lam = 0.15\n    for var in skew_var:\n        #c = 'box_' + var\n        train_data[var] = boxcox1p(train_data[var], lam)\n        test_data[var] = boxcox1p(test_data[var], lam)\n        #train_data[var] = np.log1p(train_data[var])\n        #test_data[var] = np.log1p(test_data[var])\n    print(skew_var)    \n    return train_data, test_data","3347f569":"ca_var = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_living15','mean_price']\ntrain_df, test_df = skew(train_df, test_df, ca_var)","27affd25":"train_df = pd.read_csv('..\/input\/2019-2nd-ml-month-with-kakr\/train.csv')\ntest_df = pd.read_csv('..\/input\/2019-2nd-ml-month-with-kakr\/test.csv')","c0886ac1":"train_data, test_data = pre_df(train_df, test_df)\nca_var = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_living15', 'mean_price']\ntrain_data, test_data = skew(train_data, test_data, ca_var)\ntrain_data, test_data = zip_onehot(train_data, test_data)\n\ntarget = train_data.log_price\ntrain_data = train_data.drop(['price', 'log_price'], axis=1)\nprint(len(train_data.columns), len(test_data.columns))","6889b7ca":"%%time\n# transforming\ndtrain = xgb.DMatrix(train_data, target)\ndtest = xgb.DMatrix(test_data)\n\n# cross validation\ncv_output = xgb.cv(xgb_params,\n                   dtrain,                        \n                   num_boost_round=5000,         # the number of boosting trees\n                   early_stopping_rounds=100,    # val loss\uac00 \uacc4\uc18d \uc0c1\uc2b9\ud558\uba74 \uc911\uc9c0\n                   nfold=5,                      # set folds of the closs validation\n                   verbose_eval=250,             # \uba87 \ubc88\uc9f8\ub9c8\ub2e4 \uba54\uc138\uc9c0\ub97c \ucd9c\ub825\ud560 \uac83\uc778\uc9c0\n                   feval=rmse_exp,               # price \uc18d\uc131\uc744 log scaling \ud588\uae30 \ub54c\ubb38\uc5d0, \ub2e4\uc2dc exponential\n                   maximize=False,\n                   show_stdv=False,              # \ud559\uc2b5 \ub3d9\uc548 std(\ud45c\uc900\ud3b8\ucc28) \ucd9c\ub825\ud560\uc9c0 \ub9d0\uc9c0\n                   )\n\n# scoring\nbest_rounds = cv_output.index.size\nscore = round(cv_output.iloc[-1]['test-rmse_exp-mean'], 2)\n\nprint(f'\\nBest Rounds: {best_rounds}')\nprint(f'Best Score: {score}')","9db2d86d":"#1\ubc88\nmodel = xgb.train(xgb_params, dtrain, num_boost_round=best_rounds)\n\ndtest = xgb.DMatrix(test_data)\ny_pred = model.predict(dtest)\ntest_result['pred_1'] = np.exp(y_pred)\n\nresult(train_data)\ndtest = xgb.DMatrix(train_data)\ntrain_result['pred_1'] = np.exp(model.predict(dtest))","99688480":"train_df = pd.read_csv('..\/input\/sample-code-with-2019-ml-month-2nd\/train_df_2.csv')\ntest_df = pd.read_csv('..\/input\/sample-code-with-2019-ml-month-2nd\/test_df_2.csv')","b1ce01fc":"train_df = train_df.drop(['alone','dist_arr', 'p_living','p_above','p_lot', 'p_mean','l_mean'], axis=1)\ntest_df = test_df.drop(['c_index','over','p_mean', 'l_mean'], axis=1)","da477cfe":"train_data, test_data = pre_df(train_df, test_df)\nca_var = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_living15', 'mean_price', 'a_mean']\ntrain_data, test_data = skew(train_data, test_data, ca_var)\ntrain_data, test_data = zip_onehot(train_data, test_data)\n\ntarget = train_data.log_price\ntrain_data = train_data.drop(['price','log_price'], axis=1)\nprint(len(train_data.columns), len(test_data.columns))","fc7f4335":"%%time\n# transforming\ndtrain = xgb.DMatrix(train_data, target)\ndtest = xgb.DMatrix(test_data)\n\n# cross validation\ncv_output = xgb.cv(xgb_params,\n                   dtrain,                        \n                   num_boost_round=5000,         # the number of boosting trees\n                   early_stopping_rounds=100,    # val loss\uac00 \uacc4\uc18d \uc0c1\uc2b9\ud558\uba74 \uc911\uc9c0\n                   nfold=5,                      # set folds of the closs validation\n                   verbose_eval=100,             # \uba87 \ubc88\uc9f8\ub9c8\ub2e4 \uba54\uc138\uc9c0\ub97c \ucd9c\ub825\ud560 \uac83\uc778\uc9c0\n                   feval=rmse_exp,               # price \uc18d\uc131\uc744 log scaling \ud588\uae30 \ub54c\ubb38\uc5d0, \ub2e4\uc2dc exponential\n                   maximize=False,\n                   show_stdv=False,              # \ud559\uc2b5 \ub3d9\uc548 std(\ud45c\uc900\ud3b8\ucc28) \ucd9c\ub825\ud560\uc9c0 \ub9d0\uc9c0\n                   )\n\n# scoring\nbest_rounds = cv_output.index.size\nscore = round(cv_output.iloc[-1]['test-rmse_exp-mean'], 2)\n\nprint(f'\\nBest Rounds: {best_rounds}')\nprint(f'Best Score: {score}')","488f2f68":"#2\ubc88\nmodel = xgb.train(xgb_params, dtrain, num_boost_round=best_rounds)\ndtest = xgb.DMatrix(test_data)\ny_pred = model.predict(dtest)\ntest_result['pred_2'] = np.exp(y_pred)\n\nresult(train_data)\ndtest = xgb.DMatrix(train_data)\ntrain_result['pred_2'] = np.exp(model.predict(dtest))","6ee66b60":"train_df = pd.read_csv('..\/input\/sample-code-with-2019-ml-month-2nd\/train_df_2.csv')\ntest_df = pd.read_csv('..\/input\/sample-code-with-2019-ml-month-2nd\/test_df_2.csv')","8573216c":"train_df = train_df.drop(['alone','dist_arr', 'p_living','p_above', 'p_lot'], axis=1)\ntest_df = test_df.drop(['c_index','over'], axis=1)","e19cc4b7":"train_data, test_data = pre_df(train_df, test_df)\nca_var = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_living15', 'mean_price', 'p_mean', 'l_mean', 'a_mean']\ntrain_data, test_data = skew(train_data, test_data, ca_var)\ntrain_data, test_data = zip_onehot(train_data, test_data)\n\ntarget = train_data.log_price\ntrain_data = train_data.drop(['price','log_price'], axis=1)\nprint(len(train_data.columns), len(test_data.columns))","7a05c65a":"%%time\n# transforming\ndtrain = xgb.DMatrix(train_data, target)\ndtest = xgb.DMatrix(test_data)\n\n# cross validation\ncv_output = xgb.cv(xgb_params,\n                   dtrain,                        \n                   num_boost_round=5000,         # the number of boosting trees\n                   early_stopping_rounds=100,    # val loss\uac00 \uacc4\uc18d \uc0c1\uc2b9\ud558\uba74 \uc911\uc9c0\n                   nfold=5,                      # set folds of the closs validation\n                   verbose_eval=100,             # \uba87 \ubc88\uc9f8\ub9c8\ub2e4 \uba54\uc138\uc9c0\ub97c \ucd9c\ub825\ud560 \uac83\uc778\uc9c0\n                   feval=rmse_exp,               # price \uc18d\uc131\uc744 log scaling \ud588\uae30 \ub54c\ubb38\uc5d0, \ub2e4\uc2dc exponential\n                   maximize=False,\n                   show_stdv=False,              # \ud559\uc2b5 \ub3d9\uc548 std(\ud45c\uc900\ud3b8\ucc28) \ucd9c\ub825\ud560\uc9c0 \ub9d0\uc9c0\n                   )\n\n# scoring\nbest_rounds = cv_output.index.size\nscore = round(cv_output.iloc[-1]['test-rmse_exp-mean'], 2)\n\nprint(f'\\nBest Rounds: {best_rounds}')\nprint(f'Best Score: {score}')","076daf68":"#2\ubc88\nmodel = xgb.train(xgb_params, dtrain, num_boost_round=best_rounds)\ndtest = xgb.DMatrix(test_data)\ny_pred = model.predict(dtest)\ntest_result['pred_6'] = np.exp(y_pred)\n\nresult(train_data)\ndtest = xgb.DMatrix(train_data)\ntrain_result['pred_6'] = np.exp(model.predict(dtest))","a798daad":"train_df = pd.read_csv('..\/input\/dataset\/train_data_2.csv')\ntest_df = pd.read_csv('..\/input\/dataset\/test_data_2.csv')","3bf96df4":"train_data = train_df.copy()\ntest_data = test_df.copy()\ntrain_data, test_data = zip_onehot(train_data, test_data)","4da59dfd":"%%time\n# transforming\ndtrain = xgb.DMatrix(train_data, target)\ndtest = xgb.DMatrix(test_data)\n\n# cross validation\ncv_output = xgb.cv(xgb_params,\n                   dtrain,                        \n                   num_boost_round=5000,         # the number of boosting trees\n                   early_stopping_rounds=100,    # val loss\uac00 \uacc4\uc18d \uc0c1\uc2b9\ud558\uba74 \uc911\uc9c0\n                   nfold=5,                      # set folds of the closs validation\n                   verbose_eval=100,             # \uba87 \ubc88\uc9f8\ub9c8\ub2e4 \uba54\uc138\uc9c0\ub97c \ucd9c\ub825\ud560 \uac83\uc778\uc9c0\n                   feval=rmse_exp,               # price \uc18d\uc131\uc744 log scaling \ud588\uae30 \ub54c\ubb38\uc5d0, \ub2e4\uc2dc exponential\n                   maximize=False,\n                   show_stdv=False,              # \ud559\uc2b5 \ub3d9\uc548 std(\ud45c\uc900\ud3b8\ucc28) \ucd9c\ub825\ud560\uc9c0 \ub9d0\uc9c0\n                   )\n\n# scoring\nbest_rounds = cv_output.index.size\nscore = round(cv_output.iloc[-1]['test-rmse_exp-mean'], 2)\n\nprint(f'\\nBest Rounds: {best_rounds}')\nprint(f'Best Score: {score}')","d547fa49":"#3\ubc88\nmodel = xgb.train(xgb_params, dtrain, num_boost_round=best_rounds)\ndtest = xgb.DMatrix(test_data)\ny_pred = model.predict(dtest)\ntest_result['pred_3'] = np.exp(y_pred)\n\nresult(train_data)\ndtest = xgb.DMatrix(train_data)\ntrain_result['pred_3'] = np.exp(model.predict(dtest))","74365709":"def feature(x):\n    x['whether_to_renovated']=(x.yr_renovated!=0).astype(int)\n    x.loc[x.yr_renovated==0,'yr_renovated']=x[x.yr_renovated==0].yr_built\n    x=pd.DataFrame.drop(x,columns='yr_built')\n    x['garret']=(x.floors%1==0.5).astype(int)\n    x.loc[x.floors%1==0.5,'floors']=np.floor(x[x.floors%1==0.5].floors)\n    \n#     x['rooms_mul']=x['bedrooms']*x['bathrooms']\n    x['living_per_floors']=x['sqft_living']\/x['floors']\n    x['total_score']=x['condition']+x['grade']+x['view']\n    x['living_per_lot']=x['sqft_living']\/x['sqft_lot']\n    x['diff_of_rooms']=np.abs(x['bedrooms']-x['bathrooms'])\n    x['diff_lots']=np.abs(x['sqft_lot15']-x['sqft_lot'])\n    x['diff_living']=np.abs(x['sqft_living15']-x['sqft_living'])\n    x['diff_living_per_floor']=(x.sqft_living15-x.sqft_living)\/x.floors\n    x['exist_special']=x.garret+x.waterfront+x.whether_to_renovated\n#     x['where_water']=np.abs(x.long*x.lat*x.waterfront)\n#     x['lat*lot']=x.lat*x.sqft_lot 9.9e4\n#     x['lat*long']=np.abs(x.lat*x.long)\n#     x['base*above']=x.sqft_basement*x.sqft_above\n#     x['total*score']=x.condition*x.grade*x.view\n    return x\n\ndef datererange(x):\n    x.loc[:,'date']=x.loc[:,'date'].str[:6].astype(int)\n    index=np.sort(x.date.unique())\n    for i in range(len(index)):\n        x.loc[x.date==index[i],'date']=i+1\n    return x\n    \ndef log1pscale(x,cols):\n    for i in cols:\n        x.loc[:,i]=np.log1p(x.loc[:,i])\n    return x\n\ndef logscale(x,cols):\n    for i in cols:\n        x.loc[:,i]=np.log(x.loc[:,i])\n    return x\n\ndef seed():\n    return np.random.randint(10000)\n\ndata=pd.read_csv('..\/input\/2019-2nd-ml-month-with-kakr\/train.csv').drop(columns='id')\ntest=pd.read_csv('..\/input\/2019-2nd-ml-month-with-kakr\/test.csv').drop(columns='id')\n\ntarget=data.price\nlog_target=np.log(target)\ndata=data.drop(columns='price')\n\nraw=data.copy()\n\ndata=feature(data)\ntest=feature(test)\n\nlog1p_cols=['sqft_living','sqft_living15','sqft_lot','sqft_lot15','sqft_basement','living_per_floors',\n           'diff_lots','diff_living',]\ndata=log1pscale(data,log1p_cols)\ntest=log1pscale(test,log1p_cols)\ndata=datererange(data)\ntest=datererange(test)\n\ndata.describe()\n\neliminate_list=['price','sqft_lot15']\nfor i in eliminate_list:\n    if i in data.columns:\n        data=data.drop(columns=i)\n    if i in test.columns:\n        if i != 'price':\n            test=test.drop(columns=i)\n            \nx_sample,x_unseen,y_sample,y_unseen=train_test_split(data,log_target,test_size=1\/5)\nwatchlist=[(x_sample,y_sample),(x_unseen,y_unseen)]\n\n\nmodelx=xgb.XGBRegressor(tree_method='gpu_hist',\n                        n_estimators=100000,\n                        num_round_boost=500,\n                        show_stdv=False,\n                        feature_selector='greedy',\n                        verbosity=0,\n                        reg_lambda=10,\n                        reg_alpha=0.01,\n                        learning_rate=0.001,\n                        seed=seed(),\n                        colsample_bytree=0.8,\n                        colsample_bylevel=0.8,\n                        subsample=0.8,\n                        n_jobs=-1,\n                        gamma=0.005,\n                        base_score=np.mean(log_target)\n                       )\n\nmodelx.fit(x_sample,y_sample,verbose=False,eval_set=watchlist,\n             eval_metric='rmse',\n          early_stopping_rounds=1000)","1a075368":"from sklearn.metrics import mean_squared_error as mse\n#xgb_score=mse(np.exp(modelx.predict(x_unseen)),np.exp(y_unseen))**0.5\n\nxgb_train_pred=np.exp(modelx.predict(data))\nxgb_pred=np.exp(modelx.predict(test))\n\n\nprint(\"RMSE unseen : {}\".format(\\\n        mse(np.exp(modelx.predict(x_unseen)),np.exp(y_unseen))**0.5))\n\n\nfig, ax = plt.subplots(figsize=(10,10))\nxgb.plot_importance(modelx, ax=ax)\nplt.show()","cd11ad3c":"#4\ubc88 : https:\/\/www.kaggle.com\/marchen911\/xgboost-lightgbm\/notebook \ucf54\ub4dc\ntest_result['pred_4'] = pd.DataFrame(xgb_pred)\ntrain_result['pred_4'] = pd.DataFrame(xgb_train_pred)","f4e57df4":"train_df = pd.read_csv('..\/input\/2019-2nd-ml-month-with-kakr\/train.csv')\ntest_df = pd.read_csv('..\/input\/2019-2nd-ml-month-with-kakr\/test.csv')","cb226e80":"train_data, test_data = pre_df(train_df, test_df)\ntrain_data = pd.concat([train_data, train_result], axis=1)\n\ntest_data = pd.concat([test_data, test_result], axis=1)\ntest_data = test_data.drop(['id'], axis=1)","7292c510":"ca_var = ['sqft_living', 'sqft_lot', 'sqft_above','sqft_living15','mean_price',  'pred_1', 'pred_2', 'pred_3', 'pred_4', 'pred_6']\ntrain_data, test_data = skew(train_data, test_data, ca_var)\ntrain_data, test_data = zip_onehot(train_data, test_data)\n\ntarget = train_data.log_price\ntrain_data = train_data.drop(['id', 'price','log_price'], axis=1)\nprint(len(train_data.columns), len(test_data.columns))","b65e8e49":"%%time\n# transforming\ndtrain = xgb.DMatrix(train_data, target)\ndtest = xgb.DMatrix(test_data)\n\n# cross validation\ncv_output = xgb.cv(xgb_params,\n                   dtrain,                        \n                   num_boost_round=5000,         # the number of boosting trees\n                   early_stopping_rounds=100,    # val loss\uac00 \uacc4\uc18d \uc0c1\uc2b9\ud558\uba74 \uc911\uc9c0\n                   nfold=5,                      # set folds of the closs validation\n                   verbose_eval=100,             # \uba87 \ubc88\uc9f8\ub9c8\ub2e4 \uba54\uc138\uc9c0\ub97c \ucd9c\ub825\ud560 \uac83\uc778\uc9c0\n                   feval=rmse_exp,               # price \uc18d\uc131\uc744 log scaling \ud588\uae30 \ub54c\ubb38\uc5d0, \ub2e4\uc2dc exponential\n                   maximize=False,\n                   show_stdv=False,              # \ud559\uc2b5 \ub3d9\uc548 std(\ud45c\uc900\ud3b8\ucc28) \ucd9c\ub825\ud560\uc9c0 \ub9d0\uc9c0\n                   )\n\n# scoring\nbest_rounds = cv_output.index.size\nscore = round(cv_output.iloc[-1]['test-rmse_exp-mean'], 2)\n\nprint(f'\\nBest Rounds: {best_rounds}')\nprint(f'Best Score: {score}')","f4e80c31":"#5\ubc88\nmodel = xgb.train(xgb_params, dtrain, num_boost_round=best_rounds)\ndtest = xgb.DMatrix(test_data)\ny_pred = model.predict(dtest)\ntest_result['pred_5'] = np.exp(y_pred)\n\nresult(train_data)\ndtest = xgb.DMatrix(train_data)\ntrain_result['pred_5'] = np.exp(model.predict(dtest))","ef8267f8":"from scipy.cluster.hierarchy import dendrogram, linkage  \nfrom scipy.spatial.distance import  pdist\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# system\nfrom datetime import datetime\nimport os\nsns.set()","01146fa1":"solutions_set = test_result[['pred_1','pred_2','pred_3','pred_4','pred_5','pred_6']]","1f09d85a":"solutions_set.head()","944e1551":"# Scaling\nscaler = MinMaxScaler()  \nsolutions_set_scaled = scaler.fit_transform(solutions_set)\nsolutions_set_scaled = pd.DataFrame(solutions_set_scaled, columns = solutions_set.columns)","12afaffe":"# transpose and convert solutions set to numpy\nnp_solutions_set = solutions_set_scaled.T.values\n# calculate the distances\nsolutions_set_dist = pdist(np_solutions_set)\n# hierarchical clusterization\nlinked = linkage(solutions_set_dist, 'ward')\n\n# dendrogram\nfig = plt.figure(figsize=(8, 5))\ndendrogram(linked, labels = solutions_set_scaled.columns)\nplt.title('clusters')\nplt.show()","8399e76c":"c1 = (solutions_set['pred_5'] + solutions_set['pred_1'] + solutions_set['pred_3'] + solutions_set['pred_4']) \/ 4\nc2 = (solutions_set['pred_2'] + solutions_set['pred_6'])\/2\npred = c1 * 0.55+c2 * 0.45","1a336d1c":"#pred\ntt = pd.read_csv('..\/input\/2019-2nd-ml-month-with-kakr\/test.csv')\nsub_id = tt.id\nsub = pd.DataFrame(data={'id':sub_id,'price':pred})","1373c27e":"sub.to_csv('submission.csv', index=False)","08c1145f":"## Second dataset","c03001f0":"## Fourth dataset","4692e9b6":"## Fifth dataset","b0583dc5":"### 4. living_rate \n\nsqft_living \uac12\uc744 sqft_lot\ub85c \ub098\ub208 \uac12\uc785\ub2c8\ub2e4. *\uc5ec\ub2f4\uc73c\ub85c \uc774 \uc218\uce58\ub85c \uc9d1\uc758 \uc885\ub958(\uc800\ud0dd\uc778\uc9c0, \ub8f8\uc778\uc9c0, \ud1a0\uc9c0\uc778\uc9c0)\ub97c \uad6c\ubd84\ud574\ubcf4\ub824\uace0 \ud588\uc9c0\ub9cc \uc2e4\ud328\ud588\uc2b5\ub2c8\ub2e4..*","c554f8d6":"### 6. per_living\n\nzipcode \ub0b4\uc5d0\uc11c sqft_living\uc758 \ud06c\uae30\ub97c 10\uac1c\uc758 \ub4f1\uae09\uc744 \ubd80\uc5ec\ud55c \uac12\uc785\ub2c8\ub2e4. sqft_living\uc774 \ud074\uc218\ub85d \ub192\uc740 \ub4f1\uae09\uc744 \ubd80\uc5ec\ubc1b\uc2b5\ub2c8\ub2e4.","209cea29":"### 3. yr_built, is_re, is_ba\n\n* yr_built \ubcc0\uc218\uc758 \uacbd\uc6b0 yr_renovated\uc640 \ud569\ucce4\uc744 \ub54c \uc131\ub2a5\uc774 \uc99d\uac00\ud558\uc600\uc2b5\ub2c8\ub2e4. \n\n* is_re\uc640 is_ba\ub294 \ub9ac\ubaa8\ub378\ub9c1\uc744 \ud588\ub294\uc9c0, \uc9c0\ud558\uc2e4\uc774 \uc788\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 \ubcc0\uc218\uc785\ub2c8\ub2e4. \ud574\ub2f9 \ubcc0\uc218\uc640 yr_renovated, sqft_basement\ub97c \uac19\uc774 \uc0ac\uc6a9\ud588\uc744 \uacbd\uc6b0 \uc131\ub2a5\uc758 \ubcc0\ud654\uac00 \uac70\uc758 \uc5c6\uc5c8\uc9c0\ub9cc(\uc624\ud788\ub824 \uac10\uc18c..), yr_renovated\uc640 sqft_basement\ub97c \uc0ad\uc81c\ud55c \ud6c4 \uc0ac\uc6a9\ud588\uc744 \ub54c\ub294 \uc131\ub2a5\uc774 \uc88b\uc544\uc84c\uc2b5\ub2c8\ub2e4.","aaf8eaa0":"# \ubcc0\uc218 \uc0dd\uc131\n\n\uc0c8\ub85c \ub9cc\ub4e0 \ubcc0\uc218\ub294 \ucd1d 9\uac1c\uc785\ub2c8\ub2e4. \ub610\ud55c \uae30\uc874\uc5d0 \uc788\ub294 date \ubcc0\uc218\ub294 \ub144\ub3c4\uc640 \uc6d4\ub9cc \uc0ac\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4(6\uc790\ub9ac).","ec2ab35a":"### 2. mean_price\n\nKmeans\ub85c \uc9c0\uc5ed\uc744 \uad6c\ubd84\ud55c \uc9c0\uc5ed\uc758 \ud3c9\uade0 price \uac12\uc744 \ub098\ud0c0\ub0b4\ub294 \ubcc0\uc218\uc785\ub2c8\ub2e4. zipcode \ub0b4\uc5d0\uc11c (data \uc218\/15)\ub9cc\ud07c cluster\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. \uc0dd\uc131\ud55c cluster\uc5d0\uc11c \ud3c9\uade0 price \uac12\uc744 \uad6c\ud569\ub2c8\ub2e4.\n\n\uc544\ub798 scatterplot\uc744 \uc0b4\ud3b4\ubcf4\uba74, \uc9d1\uac12\uc774 \ub192\uc740 \ubd80\ubd84\uacfc \uc0c1\ub2f9\ubd80\ubd84 \uc77c\uce58\ud558\uace0 \uc788\ub294 \uac83\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.","f6d4fed6":"# Overview\n\n \ub370\uc774\ud130\uc758 Feature\ub85c \uae30\ubcf8\uc801\uc778 EDA\ub3c4 \uc9c4\ud589\ud558\uc600\uc73c\ub098, \ub9ce\uc740 \ucee4\ub110\uc5d0\uc11c \uc88b\uc740 EDA\uacfc\uc815\uc744 \ub2e4\ub8e8\uace0 \uc788\uc5b4 \uc774\ubc88 \ucee4\ub110\uc740 \ub9cc\ub4e4\uc5b4\ubcf4\uc558\ub358 Feature\uc5d0 \ub300\ud55c \uc124\uba85\uc744 \uc704\uc8fc\ub85c \uc791\uc131\ud558\uc600\uc2b5\ub2c8\ub2e4. \n \n \uc0ac\uc6a9\ud55c \ubaa8\ub378\uc740 Xgboost \uc774\uba70 \uc11c\ub85c \ub2e4\ub978 5\uac1c\uc758 \ub370\uc774\ud130 \uc14b\uc73c\ub85c \uacb0\uacfc\uac12\uc744 \uc5bb\uace0, \uacb0\uacfc\uac12\uc744 \ucd94\uac00\ud55c \ub370\uc774\ud130\ub85c \ub2e4\uc2dc \ubaa8\ub378\uc744 \ud559\uc2b5\uc2dc\ucf30\uc2b5\ub2c8\ub2e4. \ucd5c\uc885\uc801\uc73c\ub85c 6\uac1c\uc758 \uacb0\uacfc\uac12\uc744 \uc559\uc0c1\ube14\ud558\uc5ec \uc81c\ucd9c\ud558\uc600\uc2b5\ub2c8\ub2e4. \uc0ac\uc6a9\ud55c \ub370\uc774\ud130\uc14b\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4.\n1. \uae30\ubcf8 \ub370\uc774\ud130(\uae30\uc874 \ubcc0\uc218 + 6\uac1c new \ubcc0\uc218)\n2. \uae30\ubcf8 \ub370\uc774\ud130 + 1\uac00\uc9c0 \ubcc0\uc218(a_mean)\n3. \uae30\ubcf8 \ub370\uc774\ud130 + 3\uac00\uc9c0 \ubcc0\uc218(p_mean, a_mean, l_mean)\n4. \uae30\ubcf8 \ub370\uc774\ud130 + 4\uac00\uc9c0 \ubcc0\uc218(p_living, p_lot, is_basement, sqft_floor)\n5. [XGBoost, Lightgbm ( catboost \uc608\uc815 )]([https:\/\/www.kaggle.com\/marchen911\/xgboost-lightgbm])\uc758 Xgboost \uacb0\uacfc \ub370\uc774\ud130\n6. \uae30\ubcf8 \ub370\uc774\ud130 + 1~5\uc758 \uacb0\uacfc\uac12\n\n\ubcc0\uc218\uc5d0 \ub300\ud55c \uc124\uba85\uc740 \ucf54\ub4dc\uc640 \ud568\uaed8 \uc801\uc5b4\ub193\uc558\uc2b5\ub2c8\ub2e4. \ub610\ud55c \uc5f0\uc18d\ud615 \ubcc0\uc218\uc5d0\ub294 boxcox scale, target \uac12\uc778 price\uc5d0\ub294 log scale\uc744 \uc0ac\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4.","d62fdb9c":"## \ubaa9\ucc28\n\n1. Overview\n2. \ubcc0\uc218 \uc124\uba85\n3. \ubaa8\ub378 \ud559\uc2b5\n4. Model prediction with ensemble","0fe6c646":"## Set up","555c6a1e":"### 5. new_grade\n\ngrade\uc640 condition, view\ub97c \ub354\ud55c \uac12\uc785\ub2c8\ub2e4.","cf83292c":"# 2019 2nd ML month with KaKR House price \uc608\uce21","14175ccd":"# \ubaa8\ub378 \ud559\uc2b5\n\n\uac01 \ub370\uc774\ud130\uc14b\uc73c\ub85c \ubaa8\ub378\uc744 \ud559\uc2b5\uc2dc\ud0a8 \ud6c4 \uacb0\uacfc\ub97c \uc800\uc7a5\ud569\ub2c8\ub2e4.","08a2468b":"### 1. zip_level\n\nzipcode\ub97c \uae30\uc900\uc73c\ub85c \ud574\ub2f9 \uc9c0\uc5ed\uc758 \ud3c9\uade0 \uac00\uaca9\uc744 \uad6c\ud569\ub2c8\ub2e4. \ud3c9\uade0\uac00\uaca9\uc5d0 \ub530\ub77c \uc9c0\uc5ed\uc5d0 \ub4f1\uae09\uc744 \ubd80\uc5ec\ud569\ub2c8\ub2e4. `groupby`\ub97c \uc0ac\uc6a9\ud558\uc5ec \uac04\ub2e8\ud558\uac8c price\uc758 \ud3c9\uade0\uac12\uc744 \uad6c\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\nbin\uac12\uc740 10~40 \uc911, price\uc640 \uac00\uc7a5 \ub192\uc740 \uc0c1\uad00\uacc4\uc218\ub97c \ubcf4\uc600\ub358 27\ub85c \uc815\ud558\uc600\uc2b5\ub2c8\ub2e4.","b610ca48":"### 7. p_mean, a_mean, l_mean\n\n\uac01 \ub370\uc774\ud130\uc758 lat, long\uc744 \uae30\uc900\uc73c\ub85c 0.7km\uc774\ub0b4\uc5d0 \uc788\ub294 \uc9d1\ub07c\ub9ac \ubb36\uc5b4 sqft_living \ub2f9 price, sqf_above \ub2f9 price, sqft_lot \ub2f9 price\uc744 \uacc4\uc0b0\ud55c \ubcc0\uc218\uc785\ub2c8\ub2e4. \uac70\ub9ac \uacc4\uc0b0 \uacf5\uc2dd\uc740 [Geo Data EDA And Feature Engineering](https:\/\/www.kaggle.com\/tmheo74\/geo-data-eda-and-feature-engineering)\uc5d0 \uc788\ub294 \ub0b4\uc6a9\uc744 \ucc38\uace0\ud558\uc600\uc73c\uba70, \ub0b4\uc6a9\uc740 \uc0c8\ub85c \ucee4\ub110\uc744 \ub9cc\ub4e4\uc5b4 \uc62c\ub9ac\ub3c4\ub85d\ud558\uaca0\uc2b5\ub2c8\ub2e4.","95221232":"### 8. \ub370\uc774\ud130 \uc804\ucc98\ub9ac\n\n\ub370\uc774\ud130\ub97c \ubd84\ud3ec \ubaa8\uc591\uc744 \ub2e4\ub4ec\uae30 \uc704\ud574 scale\ub97c \ucd94\uac00\ud558\uc600\uc2b5\ub2c8\ub2e4. \n\nprice\ub294 log scale\uc744 \ud588\uc73c\uba70, \ub098\uba38\uc9c0 \ub370\uc774\ud130\ub294 log scale \ub300\uc2e0 boxcox\ub97c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \uc774\uc720\ub294 boxcox\ub97c \uc0ac\uc6a9\ud588\uc744 \ub584, log_price\uc640 \uc0c1\uad00\uacc4\uc218\uac00 \ub354 \ub192\uac8c\ub098\uc640 boxcox\ub97c \uc0ac\uc6a9\ud558\uae30\ub85c \uacb0\uc815\ud558\uc600\uc2b5\ub2c8\ub2e4.\n\n**\uc774\ub807\uac8c \uc120\ud0dd\ud558\ub294\uac8c \ub9de\ub294\uc9c0 \ucf54\uba58\ud2b8 \uc8fc\uc2dc\uba74 \uac10\uc0ac\ud558\uaca0\uc2b5\ub2c8\ub2e4! :)*","c02ee4c5":"## Third dataset","4126c139":"* \uae30\ubcf8\uc801\uc778 \uacfc\uc815\uc740 https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python \uc758 \ucee4\ub110\ub97c \ub530\ub77c \uc9c4\ud589\ud558\uc600\uc2b5\ub2c8\ub2e4.\n* [House Price Prediction EDA (updated 2019.03.12)](https:\/\/www.kaggle.com\/chocozzz\/house-price-prediction-eda-updated-2019-03-12)\uc5d0\uc11c \ub370\uc774\ud130 \ud0d0\uc0c9\uacfc\uc815\uc744 \uc798 \uc124\uba85\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n* [Geo Data EDA And Feature Engineering](https:\/\/www.kaggle.com\/tmheo74\/geo-data-eda-and-feature-engineering) \ucee4\ub110\uc5d0\uc11c \ub9ce\uc740 \ub3c4\uc6c0\uc744 \ubc1b\uc558\uc2b5\ub2c8\ub2e4.\n* \ubaa8\ub378\uc740 Xgboost\ub97c \uc0ac\uc6a9\ud558\uc600\uc73c\uba70, \ucf54\ub4dc \ubc0f \ud30c\ub77c\ubbf8\ud130\ub294 [A Note on Using a Single Model: XGBoost](https:\/\/www.kaggle.com\/ivoryrabbit\/a-note-on-using-a-single-model-xgboost) \ucee4\ub110\uc744 \ucc38\uace0\ud558\uc600\uc2b5\ub2c8\ub2e4. \ub610\ud55c \ud574\ub2f9 \ucee4\ub110\uc758 `folium`\ub97c \ud65c\uc6a9\ud55c lat, long \uc2dc\uac01\ud654 \uacfc\uc815\uc774 \uc88b\uc740 \ucc38\uace0 \uc790\ub8cc\uac00 \ub418\uc5c8\uc2b5\ub2c8\ub2e4.\n* \ub9c8\uc9c0\ub9c9 \uc559\uc0c1\ube14 \uacfc\uc815\uc740 [Clustering Blender for House Price Problem](https:\/\/www.kaggle.com\/seriousran\/clustering-blender-for-house-price-problem)\ub97c \ucc38\uace0\ud558\uc600\uc2b5\ub2c8\ub2e4.\n* [XGBoost, Lightgbm ( catboost \uc608\uc815 )]([https:\/\/www.kaggle.com\/marchen911\/xgboost-lightgbm])\uc758 \uacb0\uacfc \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4.","7a90bec5":"# Model prediction with ensemble","dee1d520":"## Sixth dataset","aafe2542":"## First dataset"}}