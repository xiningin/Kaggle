{"cell_type":{"72754d32":"code","aa0e15ba":"code","0f6dfc9f":"code","1b8ed1c8":"code","986a9084":"code","34634b5a":"code","4ead8f3b":"code","64cb0103":"code","1e972f83":"code","6c5c88fd":"code","7d2864cd":"code","9a999e06":"code","66b53ad6":"code","3de490a6":"code","bcbae03b":"code","5d4f2ab5":"code","1c87521c":"code","22f12d3f":"code","7716fb53":"code","d3fdb995":"code","36cc7891":"code","4b1e7b49":"code","3651682d":"code","45f0a961":"code","8865513f":"code","65297202":"code","06a7a59c":"code","995fb845":"code","af2e7cc1":"code","48e62213":"code","9130baa3":"code","778d3740":"code","a02d9ba6":"code","cfe9fa14":"code","c680b48d":"code","95370891":"code","881f1fef":"code","38b3e5e8":"code","ca96df18":"markdown","58e8391f":"markdown","c5a82f5a":"markdown","88bacbf0":"markdown","fa11bd22":"markdown","e28888a9":"markdown","d78d9c06":"markdown"},"source":{"72754d32":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom collections import Counter\nfrom operator import itemgetter\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams\n\nimport string\nfrom wordcloud import WordCloud\nimport re\nfrom bs4 import BeautifulSoup\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('ggplot')","aa0e15ba":"train_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","0f6dfc9f":"print(train_df.info())","1b8ed1c8":"print(test_df.info())","986a9084":"print(train_df.head(5))","34634b5a":"fig = plt.figure(figsize=(10,6))\ng = sns.countplot(train_df['target'])","4ead8f3b":"train_df['text_len'] = train_df['text'].apply(lambda x:len(x))\nfig = plt.figure(figsize=(10,6))\ng1 = sns.kdeplot(train_df[train_df['target']==0]['text_len'],label='target=0')\ng2 = sns.kdeplot(train_df[train_df['target']==1]['text_len'],label='target=1')","64cb0103":"train_df['tokenized'] = train_df['text'].apply(lambda x:x.lower().split())\ntrain_df['token_num'] = train_df['tokenized'].apply(lambda x:len(x))\nfig = plt.figure(figsize=(10,6))\ng1 = sns.kdeplot(train_df[train_df['target']==0]['token_num'],label='target=0')\ng2 = sns.kdeplot(train_df[train_df['target']==1]['token_num'],label='target=1')","1e972f83":"train_df['average_word_len'] = train_df['tokenized'].apply(lambda x:np.mean([len(token) for token in x]))\nfig = plt.figure(figsize=(10,6))\ng1 = sns.kdeplot(train_df[train_df['target']==0]['average_word_len'],label='target=0')\ng2 = sns.kdeplot(train_df[train_df['target']==1]['average_word_len'],label='target=1')","6c5c88fd":"stop_words = stopwords.words('english')\npunctuations = string.punctuation","7d2864cd":"def get_corpus(df,target):\n    token_list = []\n    tokens = df[df['target']==target]['tokenized'].tolist()\n    for item in tokens:\n        for token in item:\n            if token not in stop_words and token not in punctuations:\n                token_list.append(token)\n    return Counter(token_list)","9a999e06":"corpus_1 = get_corpus(train_df,0)\ncorpus_2 = get_corpus(train_df,1)\n\ncorpus_1 = sorted(corpus_1.items(),key=itemgetter(1),reverse=True)[:20]\ncorpus_2 = sorted(corpus_2.items(),key=itemgetter(1),reverse=True)[:20]","66b53ad6":"keys1 = [t[0] for t in corpus_1]\nvalues1 = [t[1] for t in corpus_1]\nplt.figure(figsize=(10,6))\ng = sns.barplot(x=values1,y=keys1)","3de490a6":"words_frequency1 = {word:frequency for word,frequency in zip(keys1,values1)}\nwc1 = WordCloud()\nimg1 = wc1.fit_words(words_frequency1)\nfig = plt.figure(figsize=(10,6))\nplt.imshow(img1)\nplt.axis('off')\nplt.show()","bcbae03b":"keys2 = [t[0] for t in corpus_2]\nvalues2 = [t[1] for t in corpus_2]\nplt.figure(figsize=(10,6))\ng = sns.barplot(x=values2,y=keys2)","5d4f2ab5":"words_frequency2 = {word:frequency for word,frequency in zip(keys2,values2)}\nwc2 = WordCloud()\nimg2 = wc2.fit_words(words_frequency2)\nfig = plt.figure(figsize=(10,6))\nplt.imshow(img2)\nplt.axis('off')\nplt.show()","1c87521c":"def ngrams_analysis(df,target):\n    vec = CountVectorizer(stop_words=stop_words,ngram_range=(2,2))\n    fitted = vec.fit_transform(df[df['target']==target]['text']).toarray()\n    fitted = np.sum(fitted,axis=0)\n    gram_fre = {gram:fitted[idx] for gram,idx in vec.vocabulary_.items()}\n    gram_fre = sorted(gram_fre.items(),key=itemgetter(1),reverse=True)[0:20]\n    keys = [t[0] for t in gram_fre]\n    values = [t[1] for t in gram_fre]\n    plt.figure(figsize=(10,6))\n    g = sns.barplot(x=values,y=keys)","22f12d3f":"ngrams_analysis(train_df,0)","7716fb53":"ngrams_analysis(train_df,1)","d3fdb995":"sia = SentimentIntensityAnalyzer()\ntrain_df['compound'] = train_df['text'].apply(lambda x:sia.polarity_scores(x)['compound'])\nplt.figure(figsize=(10,6))\ng1 = sns.kdeplot(train_df[train_df['target']==0]['compound'])\ng2 = sns.kdeplot(train_df[train_df['target']==1]['compound'])","36cc7891":"train_df['hash_tag'] = train_df['text'].apply(lambda x:' '.join(re.findall(r'#(\\w+)',x)))\ntrain_df['keyword'] = train_df['keyword'].astype(str).apply(lambda x:re.sub(r'%20',' ',x))\ntrain_df['tag_kw_loc'] = train_df[['hash_tag','keyword','location']].astype(str).apply(lambda x:re.sub(r'nan','',' '.join(x).lower()),axis=1)\ntrain_df['tag_kw_loc'] = train_df['tag_kw_loc'].fillna('nan')","4b1e7b49":"print(train_df.head(10)['tag_kw_loc'])","3651682d":"def get_kw_list(df,target):\n    kw_list = {}\n    phrase_list = df[df['target']==target]['tag_kw_loc'].astype(str).tolist()\n    for phrese in phrase_list:\n        for token in phrese.split():\n            if token != 'nan':\n                kw_list[token] = kw_list.get(token,0) + 1\n    return kw_list","45f0a961":"kw_list_1 = get_kw_list(train_df,0)\nkw_list_1 = sorted(kw_list_1.items(),key=itemgetter(1),reverse=True)[0:20]\nkw_list_1 = {word:fre for word,fre in kw_list_1}\nwc_kw_1 = WordCloud()\nimg1 = wc_kw_1.fit_words(kw_list_1)\nfig = plt.figure(figsize=(10,6))\nplt.imshow(img1)\nplt.axis('off')\nplt.show()","8865513f":"kw_list_2 = get_kw_list(train_df,1)\nkw_list_2 = sorted(kw_list_2.items(),key=itemgetter(1),reverse=True)[0:20]\nkw_list_2 = {word:fre for word,fre in kw_list_2}\nwc_kw_2 = WordCloud()\nimg2 = wc_kw_2.fit_words(kw_list_2)\nfig = plt.figure(figsize=(10,6))\nplt.imshow(img2)\nplt.axis('off')\nplt.show()","65297202":"ids_with_target_error = [328, 443, 513, 2619, 3640, 3900, 4342, 5781, 6552, 6554, 6570, 6701, 6702, 6729, 6861, 7226]\ntrain_df.loc[train_df['id'].isin(ids_with_target_error), 'target'] = 0","06a7a59c":"abbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}","995fb845":"def replace_abbr(text):\n    text = text.lower()\n    new_tokens = []\n    for token in text.split():\n        if token in abbreviations.keys():\n            new_tokens.append(abbreviations[token])\n        else:\n            new_tokens.append(token)\n    return ' '.join(new_tokens)\n\n\ndef remove_urls(text):\n    pat = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return re.sub(pat,r' ',text)\n\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r' ', text)\n\n\ndef remove_punctuation(text):\n    pat = '[' + string.punctuation.replace('\\'','').replace('#','') + ']+'\n    return re.sub(pat,r' ',text)\n\n\ndef clean_text(line):\n    line = line.strip()\n\n    soup = BeautifulSoup(line,'lxml')\n    line = soup.text\n\n    line = remove_urls(line)\n    line = remove_emoji(line)\n    line = replace_abbr(line)\n    line = remove_punctuation(line)\n\n    line = ' '.join(line.split())\n    return line","af2e7cc1":"def process_data(df):\n    df['text'] = df['text'].apply(clean_text)\n    df['text_len'] = df['text'].apply(lambda x:len(x))\n    df['token_num'] = df['text'].apply(lambda x:len(x.split()))\n    df['average_token_len'] = df['text'].apply(lambda x:np.mean([len(word) for word in x.split()]))\n    \n    sia = SentimentIntensityAnalyzer()\n    df['text_compound'] = df['text'].apply(lambda x:sia.polarity_scores(x)['compound'])\n    \n    df['hash_tag'] = df['text'].apply(lambda x:' '.join(re.findall(r'#(\\w+)',x)))\n    df['keyword'] = df['keyword'].astype(str).apply(lambda x:re.sub(r'%20',' ',x))\n    df['tag_kw_loc'] = df[['hash_tag','keyword','location']].astype(str).apply(lambda x:re.sub(r'nan','',' '.join(x).lower()),axis=1)\n    df['tag_kw_loc'] = df['tag_kw_loc'].fillna('nan')\n    \n    df = df.drop(['hash_tag','keyword','location'],axis=1)\n    return df","48e62213":"train_df = process_data(train_df)\ntest_df = process_data(test_df)","9130baa3":"from transformers import BertConfig,BertForSequenceClassification,BertTokenizer\nimport torch as t\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.optim import Adam\n\nimport numpy as np\nimport pandas as pd\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nfrom sklearn.model_selection import train_test_split,StratifiedKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import f1_score,accuracy_score","778d3740":"class BertModel(nn.Module):\n    def __init__(self, max_features=200):\n        super(BertModel,self).__init__()\n\n        self.max_features = max_features + 4\n        self.num_labels = 2\n        self.config = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states=True)\n        self.model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=self.config)\n\n        self.pool_dropout = nn.Dropout(0.1)\n        self.pool = nn.Linear(self.config.hidden_size,self.config.hidden_size)\n        self.pool_activation = nn.ReLU()\n\n        self.classifier_dropout = nn.Dropout(0.1)\n        self.classifier = nn.Linear(self.config.hidden_size + self.max_features,self.num_labels)\n\n        self.f1_score = 0.0\n\n    def forward(self, codes, labels=None, extra_features=None):\n        outputs = self.model(input_ids=codes, labels=labels)\n        if labels is None:\n            hidden_states = outputs[1][-1]\n        else:\n            hidden_states = outputs[2][-1]\n\n        hidden_states_ = self.pool_dropout(hidden_states[:,0])\n        pooled_hidden_states = self.pool_activation(self.pool(hidden_states_))\n\n        new_hidden_states = t.cat((pooled_hidden_states,extra_features),dim=1)\n        new_hidden_states = self.classifier_dropout(new_hidden_states)\n        logits = self.classifier(new_hidden_states)\n\n        outputs = (logits,)\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs","a02d9ba6":"def generate_data(test_size=0.2):\n    # Generate training data and validation data\n    train_data = train_df\n    test_data = test_df\n    \n    train_index = range(0,len(train_data))\n    train_target = train_data['target']\n    \n    tr_index,val_index,tr_target,val_target = train_test_split(train_index,train_target,test_size=test_size,random_state=10)\n    \n    tr_data = train_data.iloc[tr_index]\n    val_data = train_data.iloc[val_index]\n\n    vec = TfidfVectorizer(stop_words=stop_words,tokenizer=word_tokenize,max_features=MAX_FEATURES)\n    fitted = vec.fit(tr_data['tag_kw_loc'])\n    tr_kw = vec.transform(tr_data['tag_kw_loc']).toarray()\n    val_kw = vec.transform(val_data['tag_kw_loc']).toarray()\n    test_kw = vec.transform(test_data['tag_kw_loc']).toarray()\n        \n    tr_extra_features = np.concatenate([tr_kw,tr_data[['text_len','token_num','average_token_len','text_compound']].values],axis=1)\n    val_extra_features = np.concatenate([val_kw,val_data[['text_len','token_num','average_token_len','text_compound']].values],axis=1)\n    test_extra_features = np.concatenate([test_kw,test_data[['text_len','token_num','average_token_len','text_compound']].values],axis=1)\n    return tr_data,tr_extra_features,val_data,val_extra_features,test_data,test_extra_features,tr_index,val_index","cfe9fa14":"class MyDataSet(Dataset):\n    def __init__(self,df,extra_features):\n        self.df = df\n        self.extra_features = extra_features\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, item):\n        text_str = self.df.iloc[item]['text']\n        codes = self.tokenizer.encode(text_str)\n        target = self.df.iloc[item]['target']\n        extra_features = self.extra_features[item]\n        return codes,target,extra_features\n    \n    \ndef collate_fn(data_set):\n    train_x = [data[0] for data in data_set]\n    train_y = [data[1] for data in data_set]\n    extra_features = [data[2] for data in data_set]\n    max_len = SEN_LEN\n    train_x = [data + [0]*(max_len - len(data)) if len(data) <= max_len else data[0:max_len] for data in train_x]\n    return t.tensor(train_x),t.tensor(train_y),t.tensor(extra_features,dtype=t.float)","c680b48d":"def validation(model,val_df,val_extra_features):\n    val_data_set = MyDataSet(val_df,val_extra_features)\n    val_data_loader = DataLoader(val_data_set,batch_size=BATCH_SIZE,collate_fn=collate_fn)\n\n    labels = []\n    logits = []\n    for codes,targets,extra_features in val_data_loader:\n        if t.cuda.is_available():\n            codes = codes.cuda()\n            extra_features = extra_features.cuda()\n        else:\n            codes = t.autograd.Variable(codes)\n            extra_features = t.autograd.Variable(extra_features)\n\n        labels.extend(targets.numpy().tolist())\n        logits_ = model(codes,extra_features=extra_features)[0]\n        logits_ = t.squeeze(t.argmax(logits_.softmax(dim=-1),dim=-1)).detach().cpu().numpy().tolist()\n        logits.extend(logits_)\n\n    return f1_score(y_true=labels,y_pred=logits),accuracy_score(y_true=labels,y_pred=logits)","95370891":"def predict_test(model,data,extra_features):\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    text = data['text'].values\n    codes = [tokenizer.encode(t) for t in text]\n    codes = [data + [0]*(SEN_LEN- len(data)) if len(data) <= SEN_LEN else data[0:SEN_LEN] for data in codes]\n    pred = []\n    for ids,extra_features in zip(codes,extra_features):\n        ids = t.tensor([ids]).cuda()\n        extra_features = t.tensor([extra_features],dtype=t.float).cuda()\n        logits = model(ids,labels=None,extra_features=extra_features)[0]\n        pred.extend(t.argmax(logits.softmax(dim=-1),dim=-1).detach().cpu().numpy().tolist())\n    return pred","881f1fef":"stop_words = stopwords.words('english') + [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\",\"'m\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would']\nMAX_FEATURES = 200\nBATCH_SIZE = 8\nLEARNING_RATE = 1e-5\nEPOCH = 3\nSEN_LEN = 30","38b3e5e8":"tr_data,tr_extra_features,val_data,val_extra_features,test_data,test_extra_features,tr_index,val_index = generate_data()\n    \ntrain_data_set = MyDataSet(tr_data,tr_extra_features)\ntrain_data_loader = DataLoader(train_data_set,batch_size=BATCH_SIZE,collate_fn=collate_fn)\n\nmodel = BertModel(max_features=MAX_FEATURES)\nopt = Adam(lr=LEARNING_RATE,params=model.parameters())\nif t.cuda.is_available():\n    model.cuda()\n\nstep = 0\n\nfor _ in range(EPOCH):\n    for codes,targets,extra_features in train_data_loader:\n        step += 1\n        if t.cuda.is_available():\n            codes = codes.cuda()\n            targets = targets.cuda()\n            extra_features = extra_features.cuda()\n        else:\n            codes = t.autograd.Variable(codes)\n            targets = t.autograd.Variable(targets)\n            extra_features = t.autograd.Variable(extra_features)\n\n        opt.zero_grad()\n        loss = model(codes,labels=targets,extra_features=extra_features)[0]\n        loss.backward()\n\n        opt.step()\n            \n        if step % 200 == 0:\n            f1_score_,acc= validation(model,val_data,val_extra_features)\n            print('After %d iterations, current training loss is %.5f...'%(step,loss))\n            if f1_score_ > model.f1_score:\n                model.f1_score = f1_score_\n                t.save(model, 'model.pkl')\n                print('Current best f1 score is %.5f, accuracy is %.5f...'%(f1_score_,acc))\n    \nbest_model = t.load('model.pkl')\nbest_model.cuda()\npred_ = predict_test(best_model,test_data,test_extra_features)\nsub_df = pd.DataFrame(data={'id':test_data['id'],'target':pred_})\nsub_df.to_csv('submission.csv',index=False)","ca96df18":"> I didn't do any analysis on stopwords and punctuation, I jsut removed them.","58e8391f":"> 3.**Model.** ","c5a82f5a":"> According to kernel [https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert](http:\/\/), correct some targets.","88bacbf0":"****This's the first NLP competition I've ever participated in. Thanks to several excellent kernels, I've got a result not so bad. In this kernel I'm going to record what I've done with EDA,Data Cleaning and how to make use of the pre-trained bert model with pytorch.****","fa11bd22":"> 2. **DataCleaning**","e28888a9":"> 1. **EDA**\nThis part was inspired by kernel [https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert](http:\/\/)","d78d9c06":">  ****As I'm familiar with pytorch, I found an open source project on Github which provides lots of NLP pretrained models both in Pytorch and Tensorflow, such as GPT-2,Bert,XLNet.**** Here is the project Docs :[https:\/\/huggingface.co\/transformers\/v2.4.0\/](http:\/\/)"}}