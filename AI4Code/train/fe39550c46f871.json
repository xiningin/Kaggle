{"cell_type":{"eef83b95":"code","e39b7dc9":"code","7cee4153":"code","8da76c47":"code","e35c790a":"code","c5c93ddb":"code","75580bc4":"code","3f154171":"code","b56a5c0b":"code","e82a103a":"code","31d0aa21":"code","f41ef2c7":"code","03199a7d":"code","4bfb768c":"code","e5d43172":"code","bc8fad42":"code","6bd5bbd1":"code","e212ebb1":"markdown","d2092e7e":"markdown","987dc3c1":"markdown","dc3742e7":"markdown","5a6f66c2":"markdown","8b5aa311":"markdown","f7fdb0ee":"markdown","aeaf3fe5":"markdown","32408af8":"markdown","18350a92":"markdown","58352e56":"markdown","a9536f11":"markdown","e972c358":"markdown","1313f0f7":"markdown","cfdf9a80":"markdown","2a38fc57":"markdown","a2772f7b":"markdown"},"source":{"eef83b95":"!pip install pytorch_forecasting","e39b7dc9":"import warnings\nimport numpy as np\nimport pandas as pd\nimport copy\nfrom pathlib import Path\nimport torch\nimport torch.nn as nn\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\nfrom pytorch_lightning.loggers import TensorBoardLogger\n\nimport pytorch_forecasting\nfrom pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\nfrom pytorch_forecasting.data import GroupNormalizer\nfrom pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\nfrom pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","7cee4153":"df_train = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv' ,parse_dates=['date'])\ndf_shops = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\ndf_items = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\ndf_item_categories = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\n","8da76c47":"df_train['date'] = pd.to_datetime(df_train['date'], errors='coerce')","e35c790a":"df_train.columns","c5c93ddb":"# change the item count per day to item count per month by using group\ndf_train = df_train.groupby([\"item_id\",\"shop_id\",\"date_block_num\"]).sum().reset_index()\ndf_train = df_train.rename(index=str, columns = {\"item_cnt_day\":\"item_cnt_month\"})\ndf_train = df_train[[\"item_id\",\"shop_id\",\"date_block_num\",\"item_cnt_month\"]]\ndf_train","75580bc4":"df_train['date_block_num'].describe()","3f154171":"max_prediction_length = 1\nmax_encoder_length = 27\ntraining_cutoff = df_train['date_block_num'].max() - max_prediction_length\n\ntraining = TimeSeriesDataSet(\n    df_train[lambda x: x['date_block_num'] <= training_cutoff],\n    time_idx='date_block_num',\n    target=\"item_cnt_month\",\n    group_ids=[\"shop_id\", \"item_id\"],\n    min_encoder_length=0,  \n    max_encoder_length=max_encoder_length,\n    min_prediction_length=1,\n    max_prediction_length=max_prediction_length,\n    static_categoricals=[],\n    static_reals=[\"shop_id\", \"item_id\"],\n    time_varying_known_categoricals=[],  \n    time_varying_known_reals=['date_block_num'],\n    time_varying_unknown_categoricals=[],\n    time_varying_unknown_reals=['date_block_num'],\n    categorical_encoders={'shop_id': pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),'item_id':pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True)},\n    add_relative_time_idx=True,\n    add_target_scales=True,\n    add_encoder_length=True,\n    allow_missing_timesteps=True\n    \n)\n\n","b56a5c0b":"validation = TimeSeriesDataSet.from_dataset(training, df_train, predict=True, stop_randomization=True)\n\nbatch_size = 128\ntrain_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\nval_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)","e82a103a":"# configure network and trainer\npl.seed_everything(42)\ntrainer = pl.Trainer(\n    gpus=1,\n    # clipping gradients is a hyperparameter and important to prevent divergance\n    # of the gradient for recurrent neural networks\n    gradient_clip_val=0.1,\n)\n\n\ntft = TemporalFusionTransformer.from_dataset(\n    training,\n    # not meaningful for finding the learning rate but otherwise very important\n    learning_rate=0.03,\n    hidden_size=16,  # most important hyperparameter apart from learning rate\n    # number of attention heads. Set to up to 4 for large datasets\n    attention_head_size=1,\n    dropout=0.1,  # between 0.1 and 0.3 are good values\n    hidden_continuous_size=8,  # set to <= hidden_size\n    output_size=1,  # 7 quantiles by default\n    loss=pytorch_forecasting.metrics.RMSE(),\n    # reduce learning rate if no improvement in validation loss after x epochs\n    reduce_on_plateau_patience=4,\n)\nprint(f\"Number of parameters in network: {tft.size()\/1e3:.1f}k\")","31d0aa21":"# find optimal learning rate\nres = trainer.tuner.lr_find(\n    tft,\n    train_dataloader=train_dataloader,\n    val_dataloaders=val_dataloader,\n    max_lr=0.1,\n    min_lr=1e-7,\n)\n\nprint(f\"suggested learning rate: {res.suggestion()}\")\nfig = res.plot(show=True, suggest=True)\nfig.show()","f41ef2c7":"early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-7, patience=10, verbose=False, mode=\"min\")\nlr_logger = LearningRateMonitor()  \nlogger = TensorBoardLogger(\"lightning_logs\") \n\ntrainer = pl.Trainer(\n    max_epochs=30,\n    gpus=1,\n    weights_summary=\"top\",\n    gradient_clip_val=0.1,\n    limit_train_batches=30,  \n    callbacks=[lr_logger, early_stop_callback],\n    logger=logger,\n)\n\n\ntft = TemporalFusionTransformer.from_dataset(\n    training,\n    learning_rate=5e-7,\n    hidden_size=16,\n    attention_head_size=1,\n    dropout=0.1,\n    hidden_continuous_size=8,\n    output_size=1, \n    loss=pytorch_forecasting.metrics.RMSE(),\n    log_interval=10,  \n    reduce_on_plateau_patience=4,\n)\nprint(f\"Number of parameters in network: {tft.size()\/1e3:.1f}k\")","03199a7d":"# fit network\ntrainer.fit(\n    tft,\n    train_dataloader=train_dataloader,\n    val_dataloaders=val_dataloader,\n)","4bfb768c":"# load the best model according to the validation loss\n# (given that we use early stopping, this is not necessarily the last epoch)\nbest_model_path = trainer.checkpoint_callback.best_model_path\nbest_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)","e5d43172":"# calcualte root mean squared error on validation set\nactuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\nval_predictions = best_tft.predict(val_dataloader)","bc8fad42":"criterion = nn.MSELoss()\ntorch.sqrt(criterion(actuals,val_predictions))","6bd5bbd1":"df_test = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')\ndf_test=df_test.drop(['ID'],axis=1)\ndf_test['date_block_num']=34\ndf_test['item_cnt_month']=0\n# decoder_data=df_test\n# encoder_data = df_train[lambda x: x.date_block_num > x.date_block_num.max() - max_encoder_length]\n# new_prediction_data = pd.concat([encoder_data, decoder_data], ignore_index=True)\npred,x=best_tft.predict(df_test,return_x=True)","e212ebb1":"<center><h1 style = \"font-size:20px;font-family: Comic Sans MS\">Train and Validation Dataloaders<\/h1><\/center>","d2092e7e":"This is a very important step and some prerequisites for defining a TimeSeriesDaataSet object is that our pandas dataframe must have each row such that they can be identified with a time step and a time series. Fortunately, most datasets are already in this format.In our Datafarme, the time step is defined by the date_block_num column,i.e. for every shop-item combination we have item count values for the months,and our test data wants us to predict the values for one month for all such shop-item combinations.\n\nSome of the important parameters while defining the training object are:\n* **min\/max encoder length**:the mininmum and maximum lengths for encoding or the history length,for example if there are 20 time steps in total and we want a min encoder length=3 and max encoder length=8,then several sequences will be generated which are at least of length min_encoder_length and at most of length max_encoder_length and generally as large history as possible will be used.The advantage of this flexibility in the length is :If your dataset has also very short time series to predict, this flexibility ensures you can make predictions for these as well while using more history for longer time series.\n* **min\/max prediction length**:minimum\/maximum prediction\/decoder length \n* **group ids**:list of column names identifying a time series. This means that the group_ids identify a sample together with the time_idx. If you have only one timeseries, set this to the name of column that is constant.\n\nFor better understanding,refer to the [documentation](https:\/\/pytorch-forecasting.readthedocs.io\/en\/latest\/api\/pytorch_forecasting.data.timeseries.TimeSeriesDataSet.html#pytorch_forecasting.data.timeseries.TimeSeriesDataSet)","987dc3c1":"<center><h1 style = \"font-size:20px;font-family: Comic Sans MS\">Callbacks ,Trainer and final Model<\/h1><\/center>","dc3742e7":"<center><h1 style = \"font-size:20px;font-family: Comic Sans MS\">DATE BLOCK NUM COLUMN<\/h1><\/center>","5a6f66c2":"# Let's Start","8b5aa311":"![1_rljR4jjmzOitL41tpqjg-w.png](attachment:a9e343ac-bdc0-49ae-a724-1f48be8b02cb.png)\n\nDeep Learning may have taken the front seat in computer vision and language processing tasks but ,even though having outperformed the traditional methods in time series forecasting tasks,deep learning architectures have not become mainstream yet for time series forecasting tasks.Other than the hardware requirements,the lack of a high-level API that would work with popular frameworks like PyTorch or Tensorflow has been a crucial impediment making it relatively difficult to use neural networks over the traditional methods(easy to use in the scikit learn ecosystem)\n\nPyTorch Forecasting solves the problem by providing a high level API for PyTorch that can readily make use of the pandas dataframe.The package is built on PyTorch Lightning and PyTorch APIs,making it easier to learn.\n\nPytorch Forecasting aims to ease state-of-the-art timeseries forecasting with neural networks for both real-world cases and research alike.Some interesting provisions of the package include:\n\n* A timeseries dataset class which abstracts handling variable transformations, missing values, randomized subsampling, multiple history lengths, etc.So,in short no specific knowledge on how to create a dataset for training your model in PyTorch is required.\n\n* A base model class which provides basic training of timeseries models along with logging in tensorboard and generic visualizations such actual vs predictions and dependency plots\n\n* Multiple neural network architectures for timeseries forecasting that have been enhanced for real-world deployment and come with in-built interpretation capabilities\n\n* Multi-horizon timeseries metrics\n\n* For scalability, the networks are designed to work with PyTorch Lightning which allows training on CPUs and single and multiple (distributed) GPUs out-of-the-box. \n\n* Hyperparameter tuning with optuna","f7fdb0ee":"<center><h1 style = \"font-size:20px;font-family: Comic Sans MS\">Load the best model from Lightning checkpoint<\/h1><\/center>","aeaf3fe5":"<center><h1 style = \"font-size:20px;font-family: Comic Sans MS\">Use Lightning for suggesting a suitable learning rate <\/h1><\/center>","32408af8":"<center><h1 style = \"font-size:20px;font-family: Comic Sans MS\">Get the validation RMSE<\/h1><\/center>","18350a92":"<center><h1 style = \"font-size:20px;font-family: Comic Sans MS\"> Create The Time Series Dataset<\/h1><\/center>","58352e56":"<center><h1 style = \"font-size:20px;font-family: Comic Sans MS\">SEE THE DATA<\/h1><\/center>","a9536f11":"You can always tune the model hyperparameters using OPTUNA along with pytorch forecasting,that will give a better score,feel free to fork the notebook and play around with the hyperparameters.","e972c358":"<center><h1 style = \"font-size:20px;font-family: Comic Sans MS\">ITEM COUNT\/DAY--->ITEM COUNT\/MONTH<\/h1><\/center>","1313f0f7":"<center><h1 style = \"font-size:20px;font-family: Comic Sans MS\">Predict on the test data<\/h1><\/center>","cfdf9a80":"<center><h1 style = \"font-size:20px;font-family: Comic Sans MS\">Fit the model<\/h1><\/center>","2a38fc57":"<center><h1 style = \"font-size:20px;font-family: Comic Sans MS\">Define the model :Temporal transfusion transformer<\/h1><\/center>\nModel: Temporal Fusion Transformer\nThis is an architecture developed by Oxford University and Google that has beaten Amazon\u2019s DeepAR by 36\u201369% in benchmarks,","a2772f7b":"<center><h1 style = \"font-size:20px;font-family: Comic Sans MS\">IMPORT THE LIBRARIES<\/h1><\/center>"}}