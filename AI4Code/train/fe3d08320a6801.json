{"cell_type":{"6e541300":"code","e55633aa":"code","ca354684":"code","3e695efb":"code","17ac2bf0":"code","9d885d30":"code","cb99de98":"code","c4092e75":"code","3511d065":"code","1a5c275b":"code","ed53411d":"code","e717b2ab":"code","cfee35e6":"code","f851b757":"code","41ec6b28":"code","60a790e8":"code","2fd539ee":"code","d7d11bbe":"code","0bb96d56":"code","8b9a3b26":"code","dcced05c":"code","b2bbfa28":"code","57544376":"code","3b6381d3":"code","815e8fdd":"code","c0ca4e5b":"code","2b962a23":"code","84c057fd":"code","8854cb97":"code","e1140def":"code","247eb370":"code","e73e89d2":"code","5c6c0ce4":"code","4b73752f":"code","4714fbf8":"code","cfd9596b":"code","e6c7ec51":"code","866ee7b6":"code","40fe2578":"code","d3c1f99e":"code","ec71a812":"code","e2e6fb73":"code","1dbadbe0":"markdown","0a2f34d6":"markdown","2b0cb13b":"markdown","b890535f":"markdown","3cff5220":"markdown","08b20654":"markdown","843a1ff4":"markdown","44660c9c":"markdown","52665958":"markdown","e6d3981b":"markdown","24be3748":"markdown","16110d70":"markdown","995d20a8":"markdown","9be6f3ed":"markdown","eed6b33a":"markdown","d5fb7337":"markdown","5deb5aaf":"markdown"},"source":{"6e541300":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly.express as px\nfrom random import randint\nfrom tpot import TPOTRegressor\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e55633aa":"df_stores = pd.read_csv(\"\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/stores.csv\")\ndf_features = pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/features.csv.zip')\ndf_train = pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/train.csv.zip')\ndf_test = pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/test.csv.zip')","ca354684":"df_train.Date =  pd.to_datetime(df_train.Date) \ndf_test.Date =  pd.to_datetime(df_test.Date) \ndf_features.Date = pd.to_datetime(df_features.Date)","3e695efb":"df_train.describe()","17ac2bf0":"df_features.describe()","9d885d30":"df_train_clipped = df_train\ndf_train_clipped['Weekly_Sales'] = df_train.Weekly_Sales.clip(0,df_train.Weekly_Sales.max())\ndf_train_clipped.describe()","cb99de98":"df_features_clipped = df_features\ndf_features_clipped['Temperature'] = df_features.Temperature.clip(df_features.Temperature.quantile(0.01),df_features.Temperature.quantile(0.99))\ndf_features_clipped['Fuel_Price'] = df_features.Fuel_Price.clip(df_features.Fuel_Price.quantile(0.01),df_features.Fuel_Price.quantile(0.99))\ndf_features_clipped['MarkDown1'] = df_features.MarkDown1.clip(df_features.MarkDown1.quantile(0.01),df_features.MarkDown1.quantile(0.99))\ndf_features_clipped['MarkDown2'] = df_features.MarkDown2.clip(df_features.MarkDown2.quantile(0.01),df_features.MarkDown2.quantile(0.99))\ndf_features_clipped['MarkDown3'] = df_features.MarkDown3.clip(df_features.MarkDown3.quantile(0.01),df_features.MarkDown3.quantile(0.99))\ndf_features_clipped['MarkDown4'] = df_features.MarkDown4.clip(df_features.MarkDown4.quantile(0.01),df_features.MarkDown4.quantile(0.99))\ndf_features_clipped['MarkDown5'] = df_features.MarkDown5.clip(df_features.MarkDown5.quantile(0.01),df_features.MarkDown5.quantile(0.99))\ndf_features_clipped['CPI'] = df_features.CPI.clip(df_features.CPI.quantile(0.01),df_features.CPI.quantile(0.99))\ndf_features_clipped['Unemployment'] = df_features.Unemployment.clip(df_features.Unemployment.quantile(0.01),df_features.Unemployment.quantile(0.99))\ndf_features_clipped.describe()","c4092e75":"df_features_clipped.describe()","3511d065":"gTrain = df_train.groupby(['Store','Dept']).count()['Date']\ngTrain.value_counts()","1a5c275b":"#dept = randint(1,45)\ndept = 77\nsel_dept = df_train_clipped['Dept']== dept\ndf = df_train_clipped[sel_dept]\nfig = px.line(df, x='Date', y='Weekly_Sales', color='Store', \n              width=1200, height=800, title='Time Series with Rangeslider')\n\nfig.update_xaxes(rangeslider_visible=True)\nfig.show()","ed53411d":"#store = randint(1,45)\nstore = 45\nsel_store = df_train_clipped['Store']== store\ndf = df_train_clipped[sel_store]\nfig = px.line(df, x='Date', y='Weekly_Sales', color='Dept', \n              width=1200, height=800, title='Time Series with Rangeslider')\n\nfig.update_xaxes(rangeslider_visible=True)\nfig.show()","e717b2ab":"store = randint(1,45)\nsel_store = df_test['Store']== store\ndf1 = df_test[sel_store]\nfig = px.line(df1, x='Date', y='IsHoliday', color='Dept', \n              width=1200, height=800, title='Time Series with Rangeslider')\n\nfig.update_xaxes(rangeslider_visible=True)\nfig.show()","cfee35e6":"# store = randint(1,45)\n# sel_store = feature['Store']== store\n# df_test = test[sel_store]\nfig = px.line(df_features, x='Date', y='Temperature', color='Store', \n              width=1200, height=800, title='Time Series with Rangeslider')\n\nfig.update_xaxes(rangeslider_visible=True)\nfig.show()","f851b757":"def getHoliday(date):\n    if date in ['2010-02-12','2011-02-11','2012-02-10','2013-02-08']:\n        return str('super_bowl')\n    if date in ['2010-09-10','2011-09-09','2012-09-07','2013-09-06']:\n        return str('labor_day')\n    if date in ['2010-11-26','2011-11-25','2012-11-23','2013-11-29']:\n        return str('thx_giving')\n    if date in ['2010-12-31','2011-12-30','2012-12-28','2013-12-13']:\n        return str('xmas')\n    else:\n        return 'not_holiday'","41ec6b28":"df_features_clipped['Holiday'] = df_features_clipped.apply(lambda x: getHoliday(x['Date']),axis=1)","60a790e8":"df_features_clipped.drop(columns='IsHoliday', inplace=True)","2fd539ee":"df_features_clipped.fillna(df_features_clipped.median(), inplace=True)","d7d11bbe":"df_features_clipped['Week_of_Year'] = df_features_clipped.Date.dt.week","0bb96d56":"df_features_clipped.tail()","8b9a3b26":"df_train_feat = pd.merge(df_train_clipped,df_features_clipped, how='inner', on=['Store','Date'])","dcced05c":"df_train_feat.drop(columns='IsHoliday', inplace=True)","b2bbfa28":"df_train_feat = pd.merge(df_train_feat,df_stores, how='inner', on=['Store'])","57544376":"df_train_feat.info()","3b6381d3":"df_test_feat = pd.merge(df_test,df_features_clipped, how='inner', on=['Store','Date'])","815e8fdd":"df_test_feat = pd.merge(df_test_feat,df_stores, how='inner', on=['Store'])","c0ca4e5b":"df_test_feat.info()","2b962a23":"def getInputData(df):\n    numeric_cols = ['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI','Unemployment', 'Week_of_Year','Size']\n    \n    stores_cols = pd.get_dummies(df['Store'])\n    dept_cols = pd.get_dummies(df['Dept'])\n    holiday_cols = pd.get_dummies(df['Holiday'])\n    # weekOfYear_cols = pd.get_dummies(df['Week_of_Year'])\n    type_cols = pd.get_dummies(df['Type'])\n    \n    input_data = pd.concat([df[numeric_cols],\n              stores_cols, dept_cols, holiday_cols, type_cols], axis=1)\n    return input_data","84c057fd":"input_data = getInputData(df_train_feat)","8854cb97":"target = df_train_feat['Weekly_Sales']","e1140def":"# splitting train and test data\nX_train, X_test, y_train, y_test = train_test_split(input_data, target,\n                                                    train_size=0.80, test_size=0.20)","247eb370":"model = TPOTRegressor(generations=5, population_size=20, \n                                   verbosity=2, n_jobs=3, scoring = 'neg_mean_absolute_error')\nmodel.fit(X_train, y_train)\n","e73e89d2":"print(model.score(X_test,y_test))","5c6c0ce4":"model.export('\/kaggle\/working\/sales_forecast.py')","4b73752f":"X_validation = getInputData(df_test_feat)","4714fbf8":"Weekly_Sales = model.predict(X_validation)","cfd9596b":"pd.Series(Weekly_Sales)","e6c7ec51":"keys = ['Store','Dept','Date']\ndf_submission = pd.concat([df_test_feat[keys],pd.Series(Weekly_Sales, name='Weekly_Sales')], axis=1)","866ee7b6":"df_submission","40fe2578":"df_submission['Id'] = df_submission.apply(lambda x: '_'.join([str(x['Store']),str(x['Dept']),str(x['Date'])]),axis=1)\ndf_submission['Id'] = df_submission.Id.apply(lambda x: x.split()[0])","d3c1f99e":"challenge_file = df_submission[['Id','Weekly_Sales']]","ec71a812":"challenge_file","e2e6fb73":"challenge_file.to_csv('\/kaggle\/working\/df_submission.csv',index=False)","1dbadbe0":"Putting together features, test and stores data sets.","0a2f34d6":"Here, another decision was to make use of week of year representation, e.g. week 26 other than 2013-06-28 and it will be used as a feature","2b0cb13b":"## Predicting for test file provided by challenge host","b890535f":"This part is responsible for tunning and selecting a model.","3cff5220":"* Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13\n* Labor Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13\n* Thanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13\n* Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13","08b20654":"## Handling Outliers\nIn order to avoid outliers I just clipped the data based on assumption like: negative sales does not make sense at least from the context I had so far on the data provided and considering that probably top and bottom 1% could be treated as outliers","843a1ff4":"## ML Modeling\nTo generate ML model, I took advantage TPOT automl framework ","44660c9c":"Just wondering on the size of sales time series","52665958":"## Ploting series\nAgain exploring features time series visualy to have a superficial understanding curves characteristics and on the effect of holidays and markdown action.","e6d3981b":"## Data Exploration\nDuring data exploration, I breafly observed data variables distribution","24be3748":"Putting together features, train and stores data sets.","16110d70":"## Converting Date\nIt convert to date format in order to handle datetime.","995d20a8":"## Reading Data\nIt reads the data provided by the host","9be6f3ed":"Preparing data to feed framework\n> I decided to select as features to feed the model: Temperature, Fuel Price, Markdown1-5, CPI, Unemployment, Week_of_Year, Size of Store, Store Type, Holiday information, Dept.","eed6b33a":"   I also applied NA treatment replacing na by feature median","d5fb7337":"Model selection and training","5deb5aaf":"## Adding a column to point out a specific holiday\nI have decided to build up a variable that no only tell about if it is holiday, but to say specifically which holiday it is."}}