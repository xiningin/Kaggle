{"cell_type":{"1194313b":"code","4477618c":"code","7b5a95bd":"code","f5a220a1":"code","84eedf8f":"code","e345920d":"code","995afe8b":"code","3dcc4bab":"code","c7b2590b":"code","d902d9cc":"code","b2c99375":"markdown","3d96ce9e":"markdown","c14d00c1":"markdown","ea9c9060":"markdown","14f5760c":"markdown","4324350d":"markdown","37aa3878":"markdown","5f3cd8b1":"markdown"},"source":{"1194313b":"# IMPORT LIBRARIES\nimport pandas as pd, numpy as np, os, gc\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom datetime import timedelta","4477618c":"dtypes = {}\ndtypes['MachineIdentifier'] = 'str'\ndtypes['AvSigVersion'] = 'category'\ndtypes['HasDetections'] = 'int8'\n\n# LOAD TRAIN DATA\ndf_train = pd.read_csv('..\/input\/microsoft-malware-prediction\/train.csv', usecols=list(dtypes.keys()), dtype=dtypes)\nprint ('Loaded',len(df_train),'rows of train.CSV!')\n\n# LOAD TEST DATA\ndf_test = pd.read_csv('..\/input\/microsoft-malware-prediction\/test.csv', usecols=list(dtypes.keys())[0:-1], dtype=dtypes)\nprint ('Loaded',len(df_test),'rows of test.CSV!')\n\n# LOAD PREDICTIONS FROM MY BEST SUBMISSION ON THE PRIVATE LB\ndf_test2 = pd.read_csv(\"..\/input\/msft-malware-my-best-submission-on-the-private-lb\/xdeepfm_submission_CV0.737726_YTS_KnlV32_publicLB0pt688_private_LB0pt662.csv\")\nprint ('Loaded',len(df_test),'rows of super_blend.csv!')\n\n# ADD TIMESTAMPS\ndatedictAS = np.load('..\/input\/malware-timestamps\/AvSigVersionTimestamps.npy')[()]\ndf_test['Date'] = df_test['AvSigVersion'].map(datedictAS)\ndf_train['Date'] = df_train['AvSigVersion'].map(datedictAS)\ndf_test2 = pd.merge(df_test2, df_test, on='MachineIdentifier', how='left')\ndf_test2['AvSigVersion2'] = df_test2['AvSigVersion'].map(lambda x: np.int(x.split('.')[1]) )","7b5a95bd":"import calendar, math\n\ndef staticPlot(data, col, target='HasDetections', bars=10, show=1.0, sortby='frequency'\n               , verbose=1, top=5, title='',asc=False, dropna=False, minn=0.0):\n    # calcuate density and detection rate\n    cv = data[col].value_counts(dropna=dropna)\n    cvd = cv.to_dict()\n    nm = cv.index.values; lnn = len(nm); lnn2 = lnn\n    th = show * len(data)\n    th2 = minn * len(data)\n    sum = 0; lnn2 = 0\n    for x in nm[0:bars]:\n        lnn2 += 1\n        try: sum += cvd[x]\n        except: sum += cv[x]\n        if sum>th:\n            break\n        try:\n            if cvd[x]<th2: break\n        except:\n            if cv[x]<th2: break\n    if lnn2<bars: bars = lnn2\n    pct = round(100.0*sum\/len(data),2)\n    lnn = min(lnn,lnn2)\n    ratio = [0.0]*lnn; lnn3 = lnn\n    if sortby =='frequency': lnn3 = min(lnn3,bars)\n    elif sortby=='category': lnn3 = 0\n    for i in range(lnn3):\n        if target not in data:\n            ratio[i] = np.nan\n        elif nan_check(nm[i]):\n            ratio[i] = data[target][data[col].isna()].mean()\n        else:\n            ratio[i] = data[target][data[col]==nm[i]].mean()\n    try: all = pd.DataFrame( {'category':nm[0:lnn],'frequency':[cvd[x] for x in nm[0:lnn]],'rate':ratio} )\n    except: all = pd.DataFrame( {'category':nm[0:lnn],'frequency':[cv[x] for x in nm[0:lnn]],'rate':ratio} )\n    if sortby=='rate': \n        all = all.sort_values(sortby, ascending=asc)\n    elif sortby=='category':\n        try: \n            all['temp'] = all['category'].astype('float')\n            all = all.sort_values('temp', ascending=asc)\n        except:\n            all = all.sort_values('category', ascending=asc)\n    if bars<lnn: all = all[0:bars]\n    if verbose==1 and target in data:\n        print('TRAIN.CSV variable',col,'has',len(nm),'categories')\n        print('The',min(bars,lnn),'bars displayed here contain',pct,'% of data.')\n        mlnn = data[col].isna().sum()\n        print(\"The data has %.1f %% NA. The plot is sorted by \" % (100.0*mlnn\/len(data)) + sortby )\n    \n    # plot density and detection rate\n    fig = plt.figure(1,figsize=(15,3))\n    ax1 = fig.add_subplot(1,1,1)\n    clrs = ['red', 'green', 'blue', 'yellow', 'magenta']\n    barss = ax1.bar([str(x) for x in all['category']],[x\/float(len(data)) for x in all['frequency']],color=clrs)\n    for i in range(len(all)-top):\n        barss[top+i].set_color('cyan')\n    if target in data:\n        ax2 = ax1.twinx()\n        if sortby!='category': infected = all['rate'][0:lnn]\n        else:\n            infected=[]\n            for x in all['category']:\n                if nan_check(x): infected.append( data[ data[col].isna() ][target].mean() )\n                elif cvd[x]!=0: infected.append( data[ data[col]==x ][target].mean() )\n                else: infected.append(-1)\n        ax2.plot([str(x) for x in all['category']],infected[0:lnn],'k:o')\n        #ax2.set_ylim(a,b)\n        ax2.spines['left'].set_color('red')\n        ax2.set_ylabel('Detection Rate', color='k')\n    ax1.spines['left'].set_color('red')\n    ax1.yaxis.label.set_color('red')\n    ax1.tick_params(axis='y', colors='red')\n    ax1.set_ylabel('Category Proportion', color='r')\n    if title!='': plt.title(title)\n    plt.show()\n    if verbose==1 and target not in data:\n        print('TEST.CSV variable',col,'has',len(nm),'categories')\n        print('The',min(bars,lnn),'bars displayed here contain',pct,'% of the data.')\n        mlnn = data[col].isna().sum()\n        print(\"The data has %.1f %% NA. The plot is sorted by \" % (100.0*mlnn\/len(data)) + sortby )\n\ndef dynamicPlot(data,col, target='HasDetections', start=datetime(2018,4,1), end=datetime(2018,12,1)\n                ,inc_hr=0,inc_dy=7,inc_mn=0,show=0.99,top=5,top2=4,title='',legend=1,z=0,dots=False):\n    # check for timestamps\n    if 'Date' not in data:\n        print('Error dynamicPlot: DataFrame needs column Date of datetimes')\n        return\n    \n    # remove detection line if category density is too small\n    cv = data[(data['Date']>start) & (data['Date']<end)][col].value_counts(dropna=False)\n    cvd = cv.to_dict()\n    nm = cv.index.values\n    th = show * len(data)\n    sum = 0; lnn2 = 0\n    for x in nm:\n        lnn2 += 1\n        sum += cvd[x]\n        if sum>th:\n            break\n    top = min(top,len(nm))\n    top2 = min(top2,len(nm),lnn2,top)\n\n    # calculate rate within each time interval\n    diff = (end-start).days*24*3600 + (end-start).seconds\n    size = diff\/\/(3600*((inc_mn * 28 + inc_dy) * 24 + inc_hr)) + 5\n    data_counts = np.zeros([size,2*top+1],dtype=float)\n    idx=0; idx2 = {}\n    for i in range(top):\n        idx2[nm[i]] = i+1\n    low = start\n    high = add_time(start,inc_mn,inc_dy,inc_hr)\n    data_times = [low+(high-low)\/2]\n    while low<end:\n        slice = data[ (data['Date']<high) & (data['Date']>=low) ]\n        #data_counts[idx,0] = len(slice)\n        data_counts[idx,0] = 5000*len(slice['AvSigVersion'].unique())\n        for key in idx2:\n            if nan_check(key): slice2 = slice[slice[col].isna()]\n            else: slice2 = slice[slice[col]==key]\n            data_counts[idx,idx2[key]] = len(slice2)\n            if target in data:\n                data_counts[idx,top+idx2[key]] = slice2['HasDetections'].mean()\n        low = high\n        high = add_time(high,inc_mn,inc_dy,inc_hr)\n        data_times.append(low+(high-low)\/2)\n        idx += 1\n\n    # plot lines\n    fig = plt.figure(1,figsize=(15,3))\n    cl = ['r','g','b','y','m']\n    ax3 = fig.add_subplot(1,1,1)\n    lines = []; labels = []\n    if z==1: ax3.plot(data_times,data_counts[0:idx+1,0],'k')\n    for i in range(top):\n        tmp, = ax3.plot(data_times,data_counts[0:idx+1,i+1],cl[i%5])\n        if dots: ax3.plot(data_times,data_counts[0:idx+1,i+1],cl[i%5]+'o')\n        lines.append(tmp)\n        labels.append(str(nm[i]))\n    ax3.spines['left'].set_color('red')\n    ax3.yaxis.label.set_color('red')\n    ax3.tick_params(axis='y', colors='red')\n    if col!='ones': ax3.set_ylabel('Category Density', color='r')\n    else: ax3.set_ylabel('Data Density', color='r')\n    #ax3.set_yticklabels([])\n    if target in data:\n        ax4 = ax3.twinx()\n        for i in range(top2):\n            ax4.plot(data_times,data_counts[0:idx+1,i+1+top],cl[i%5]+\":\")\n            if dots: ax4.plot(data_times,data_counts[0:idx+1,i+1+top],cl[i%5]+\"o\")\n        ax4.spines['left'].set_color('red')\n        ax4.set_ylabel('Detection Rate', color='k')\n    if title!='': plt.title(title)\n    if legend==1: plt.legend(lines,labels,loc=2)\n    plt.show()\n        \n# INCREMENT A DATETIME\ndef add_time(sdate,months=0,days=0,hours=0):\n    month = sdate.month -1 + months\n    year = sdate.year + month \/\/ 12\n    month = month % 12 + 1\n    day = sdate.day + days\n    if day>calendar.monthrange(year,month)[1]:\n        day -= calendar.monthrange(year,month)[1]\n        month += 1\n        if month>12:\n            month = 1\n            year += 1\n    hour = sdate.hour + hours\n    if hour>23:\n        hour = 0\n        day += 1\n        if day>calendar.monthrange(year,month)[1]:\n            day -= calendar.monthrange(year,month)[1]\n            month += 1\n            if month>12:\n                month = 1\n                year += 1\n    return datetime(year,month,day,hour,sdate.minute)\n\n# CHECK FOR NAN\ndef nan_check(x):\n    if isinstance(x,float):\n        if math.isnan(x):\n            return True\n    return False","f5a220a1":"df_train['ones'] = 1\ndynamicPlot(df_train,'ones',title='Training data. (Dotted line uses right y-axis. Solid uses left.)')","84eedf8f":"df_test2['ones'] = 1\ndynamicPlot(df_test2,'ones',title='Original submission')\ndynamicPlot(df_test2,'AvSigVersion2',start=datetime(2018,9,1),end=datetime(2018,11,29),inc_dy=1,top2=4, dots=True)","e345920d":"df_test2.loc[ (df_test2['AvSigVersion2']==275)|(df_test2['AvSigVersion2']==273),'HasDetections'] *= 0.6","995afe8b":"dynamicPlot(df_test2,'ones',title='Adjustment 1')\ndynamicPlot(df_test2,'AvSigVersion2',start=datetime(2018,9,1),end=datetime(2018,11,29),inc_dy=1,top2=4, dots=True)","3dcc4bab":"df_test2.loc[ df_test2['Date']>datetime(2018,11,21,0,0) ,'HasDetections'] *= 0.6","c7b2590b":"dynamicPlot(df_test2,'ones',title='Adjustment 2')\ndynamicPlot(df_test2,'AvSigVersion2',start=datetime(2018,9,1),end=datetime(2018,11,29),inc_dy=1,top2=4,\n            dots=True, title='adjustment 2')","d902d9cc":"df_test2[['MachineIdentifier','HasDetections']].to_csv('PrivateLeaderboard.csv', index=False)","b2c99375":"# Second, view original submission's malware probabilities\nWe notice that the probabilities before and after the sampling window should be lower. So we will correct them.","3d96ce9e":"# Load files and time stamps\nWe will load the output from  [Hung The Nguyen's][1] kernel [here][2]. And attach time stamps.\n\n[1]: https:\/\/www.kaggle.com\/hung96ad\n[2]: https:\/\/www.kaggle.com\/hung96ad\/new-blend","c14d00c1":"![image](http:\/\/playagricola.com\/Kaggle\/private231419.png)","ea9c9060":"# Manual Model Manipulation Trick\nIn this kernel, we load a model's submission file, view it's malware infection rate over time, and modify it manually to match what train.csv's malware rate looks like over time. The original submission file scores Public LB 0.689 and Private LB 0.635. After correction, the updated file scores Public LB 0.693 and Private LB 0.703.  \n  \nI used this trick during the competition to increase Public LB score. But it wasn't until after the competition's end that I learned how to increase Private LB score.","14f5760c":"# Submit updated submission file\nThe original submission file had Public\/Private LB 0.698\/0.635. The new updated file has Public\/Private LB 0.693\/0.703. Using this trick, one can correct the top Kaggle Microsoft Malware public kernel submission files, ensemble them, and score over 0.700 Public LB (and over 0.700 Private LB).","4324350d":"# First, view train's malware probabilities\nWe notice that computers with AvSigVersion dates outside the window of sampling have lower malware probabilities. In the plot below, the dotted line uses the right y-axis and solid line uses left y-axis.","37aa3878":"# All post processing credit to @cdeotte for his kernel below:-\n\nhttps:\/\/www.kaggle.com\/cdeotte\/private-leaderboard-0-703\/comments","5f3cd8b1":"# Third, adjust probabilities before and after sampling window\nWe will lower probabilities before September 26, 2018 and after November 21, 2018."}}