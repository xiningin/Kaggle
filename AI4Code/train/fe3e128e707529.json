{"cell_type":{"601bb181":"code","3244e6b2":"code","e8f525a0":"code","3e48339d":"code","0bdab4ab":"code","9d691e04":"code","ba0ceabd":"code","cfb6b231":"code","baa6779c":"code","43396565":"code","e62ac892":"code","3582fc4e":"code","cce42038":"code","03b8b7b9":"code","43eb12b8":"code","a1f3fe23":"code","5deb5f48":"markdown","4d55aa79":"markdown","2f107978":"markdown"},"source":{"601bb181":"import warnings; warnings.filterwarnings('ignore')\nimport numpy as np,pandas as pd,pylab as pl\nimport tensorflow_hub as th\nimport tensorflow as tf\nimport tensorflow.keras.backend as tfkb\nfrom IPython.display import display,HTML\nimport cv2,PIL.Image\nfrom IPython.core.magic import register_line_magic\nfrom tqdm import tqdm\nfpath='..\/input\/image-examples-for-mixed-styles\/'","3244e6b2":"def load_img(path_to_img):\n    max_dim=512\n    img=tf.io.read_file(path_to_img)\n    img=tf.image.decode_image(img,channels=3)\n    img=tf.image.convert_image_dtype(img,tf.float32)\n    shape=tf.cast(tf.shape(img)[:-1],tf.float32)\n    long_dim=max(shape)\n    scale=max_dim\/long_dim\n    new_shape=tf.cast(shape*scale,tf.int32)\n    img=tf.image.resize(img,new_shape)\n    img=img[tf.newaxis,:]\n    return img\ndef tensor_to_image(tensor):\n    tensor=tensor*255\n    tensor=np.array(tensor,dtype=np.uint8)\n    if np.ndim(tensor)>3:\n        assert tensor.shape[0]==1\n        tensor=tensor[0]\n    return PIL.Image.fromarray(tensor)","e8f525a0":"def preprocess(img):\n    img=img.copy()\n    img=np.expand_dims(img,axis=0) \n    return tf.keras.applications.vgg16\\\n           .preprocess_input(img)\ndef deprocess(img):\n    img=img.copy()[0]                        \n    img[:,:,0]+=103.939\n    img[:,:,1]+=116.779\n    img[:,:,2]+=123.68             \n    img=img[:,:,::-1]              \n    img=np.clip(img,0,255)         \n    return img.astype('uint8') ","3e48339d":"def inputs(original_img,style_img):\n    original_img=original_img.astype('float32')\n    style_img=style_img.astype('float32')\n    original_input=tf.constant(preprocess(original_img))\n    style_input=tf.constant(preprocess(style_img))\n    generated_input=tf.compat.v1\\\n    .placeholder(tf.float32,original_input.shape)\n    return original_input,style_input,generated_input\ndef calculate_original_loss(layer_dict,original_layer_names):\n    loss=0\n    for name in original_layer_names:\n        layer=layer_dict[name]\n        original_features=layer.output[0,:,:,:]  \n        generated_features=layer.output[2,:,:,:] \n        loss+=tfkb.sum(tfkb.square(generated_features-original_features))\n    return loss\/len(original_layer_names)\ndef gram_matrix(x):    \n    features=tfkb.batch_flatten(tfkb.permute_dimensions(x,(2,0,1))) \n    gram=tfkb.dot(features,tfkb.transpose(features))\n    return gram\ndef get_style_loss(style_features,generated_features,size):\n    S=gram_matrix(style_features)\n    G=gram_matrix(generated_features)\n    channels=3\n    return tfkb.sum(tfkb.square(S-G))\/(4.*(channels**2)*(size**2))\ndef calculate_style_loss(layer_dict,style_layer_names,size):\n    loss=0\n    for name in style_layer_names:\n        layer=layer_dict[name]\n        style_features=layer.output[1,:,:,:] \n        generated_features=layer.output[2,:,:,:] \n        loss+=get_style_loss(style_features,generated_features,size) \n    return loss\/len(style_layer_names)\ndef calculate_variation_loss(x):\n    row_diff=tfkb.square(x[:,:-1,:-1,:]-x[:,1:,:-1,:])\n    col_diff=tfkb.square(x[:,:-1,:-1,:]-x[:,:-1,1:,:])\n    return tfkb.sum(tfkb.pow(row_diff+col_diff,1.25))","0bdab4ab":"@register_line_magic\ndef display_img(s):\n    imgs=[picture01,pattern01]\n    pl.figure(1,figsize=(10,4))\n    pl.subplot(121)\n    pl.imshow(cv2.cvtColor(\n        imgs[0],cv2.COLOR_BGR2RGB))\n    pl.subplot(122)\n    pl.imshow(cv2.cvtColor(\n        imgs[1],cv2.COLOR_BGR2RGB));","9d691e04":"hpath='https:\/\/tfhub.dev\/google\/magenta\/'+\\\n      'arbitrary-image-stylization-v1-256\/1'\nhub_module=th.load(hpath)","ba0ceabd":"@register_line_magic\ndef hm_run(pars):\n    pars=pars.split()\n    content_image=load_img(fpath+'pattern'+pars[0]+'.png')\n    style_image=load_img(fpath+'pattern'+pars[1]+'.png')\n    imgs=[content_image,style_image]\n    pl.figure(1,figsize=(10,4))\n    pl.subplot(121)\n    pl.imshow(np.squeeze(imgs[0].numpy()))\n    pl.subplot(122)\n    pl.imshow(np.squeeze(imgs[1].numpy()))\n    stylized_image=hub_module(tf.constant(content_image),\n                              tf.constant(style_image))[0]\n    display(tensor_to_image(stylized_image))","cfb6b231":"%hm_run 10 05","baa6779c":"%hm_run 10 06","43396565":"picture01=cv2.imread(fpath+'pattern10.png')\npattern01=cv2.imread(fpath+'pattern03.png')\npicture01=cv2.resize(picture01,(500,500))\npattern01=cv2.resize(pattern01,(500,500))\npicture01.shape,pattern01.shape,\\\npicture01.dtype,pattern01.dtype","e62ac892":"style_layers=['block1_conv1','block2_conv1','block3_conv1',\n              'block4_conv1','block5_conv1']\n@register_line_magic\ndef train_run(pars):\n    pars=pars.split()\n    epochs=int(pars[0]); ol=float(pars[1])\n    sl=float(pars[2]); vl=float(pars[3])\n    sh=pattern01.shape[0]*pattern01.shape[1]\n    with tf.Graph().as_default():\n        original_input,style_input,generated_input=\\\n        inputs(picture01,pattern01)\n        input_tensor=tf.concat([original_input,style_input,\n                                generated_input],axis=0)\n        print(input_tensor.shape)\n        vgg16_model=tf.keras.applications.vgg16.\\\n        VGG16(input_tensor=input_tensor,include_top=False)\n        vgg16_layer_dict={layer.name:layer \n                          for layer in vgg16_model.layers}\n        original_loss=\\\n        calculate_original_loss(vgg16_layer_dict,\n                                ['block5_conv2'])\n        style_loss=\\\n        calculate_style_loss(vgg16_layer_dict,style_layers,sh)\n        variation_loss=\\\n        calculate_variation_loss(generated_input)\n        loss=ol*original_loss+sl*style_loss+vl*variation_loss       \n        gradients=tfkb.gradients(loss,generated_input)[0]\n        calculate=tfkb.function([generated_input],\n                                [loss,gradients])\n        generated_data=preprocess(picture01)\n        for i in tqdm(range(epochs)):\n            _,gradients_value=calculate([generated_data])\n            generated_data-=gradients_value*.001\n    generated_image=deprocess(generated_data)\n    pl.figure(1,figsize=(8,8))\n    pl.title(\"loss: %.1f*original_loss+%.1f*style_loss+\"%(ol,sl)+\\\n             \"%.1f*variation_loss => %d steps\"%(vl,epochs))\n    pl.imshow(cv2.cvtColor(generated_image,cv2.COLOR_BGR2RGB));","3582fc4e":"%display_img y","cce42038":"%train_run 100 .7 .7 .7","03b8b7b9":"picture01=cv2.imread(fpath+'pattern10.png')\npattern01=cv2.imread(fpath+'picture03.png')\npicture01=cv2.resize(picture01,(500,500))\npattern01=cv2.resize(pattern01,(500,500))","43eb12b8":"%display_img y","a1f3fe23":"%train_run 50 .5 .5 .1","5deb5f48":"## VGG16 Transfer","4d55aa79":"## Hub Modules","2f107978":"## Code Modules & Helpful Functions"}}