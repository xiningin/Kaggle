{"cell_type":{"4ffba1af":"code","2b430631":"code","bd9026af":"code","2cd4de2a":"code","effc48ce":"code","6b091ea6":"code","44dd9fd4":"code","9c278d47":"code","6ba0d5fe":"code","538d69fd":"code","1d18b2e3":"code","73904f8c":"markdown","4a89c45c":"markdown","a0c608d0":"markdown","33670703":"markdown","6ba1d0de":"markdown","0a03f88f":"markdown","9c5d93f2":"markdown"},"source":{"4ffba1af":"import gc\nfrom datetime import datetime, timedelta, date\nimport numpy as np\nimport pandas as pd\nfrom numpy import nanmean\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom multiprocessing import Pool, cpu_count\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom kaggle.competitions import twosigmanews\nimport lightgbm as lgb\nfrom joblib import Parallel, delayed\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import log_loss, make_scorer\n\npd.set_option('max_columns', 50)\npd.options.mode.chained_assignment = None","2b430631":"env = twosigmanews.make_env()\nmarket_train, news_train = env.get_training_data()","bd9026af":"del news_train\ngc.collect()","2cd4de2a":"class InitialPreprocessing(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        return\n\n    def fit(self, X, y = None):\n        print(\"Initial Preprocessing\")\n        return self\n    \n    def transform(self, X, y = None):\n        \n        # We will work on data after 2010\n        if len(X) == 1:\n            X[0]['date'] = X[0]['time'].dt.date\n            X[0] = X[0].loc[X[0]['date'] >= date(2010, 1, 1)]\n        else:\n            X[0]['date'] = X[0]['time'].dt.date\n            X[1]['date'] = X[1]['time'].dt.date\n\n        return X","effc48ce":"class AddingQuantFeatures(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        self.feature_cols = ['close', 'open', 'volume']\n        self.moving_average_number_of_days = [10, 30, 90]\n        self.exponential_moving_average_span = [10, 30, 90]\n        self.bollinger_band_number_of_days = [7]\n        self.ewma = pd.Series.ewm\n        self.no_of_std = 2\n        self.returns_features = ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n                            'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\n        self.drop_features = ['time', 'assetName', 'volume', 'close', 'open',\n                               'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n                               'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n                               'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n                               'returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\n        self.final_drop_features = []\n        \n    def _compute_moving_average(self, X):\n        for column in self.feature_cols:\n            for window_period in self.moving_average_number_of_days:\n                feature_column_name = 'moving_average_{0}_{1}_days'.format(column, window_period)\n                X[feature_column_name] = X.groupby('assetCode')[column].apply(lambda x: x.rolling(window_period).mean())\n        return X\n    \n    def _compute_exponential_moving_average(self, X):\n        for column in self.feature_cols:\n            for span_period in self.exponential_moving_average_span:\n                feature_column_name = 'exponential_moving_average_{0}_{1}_days'.format(column, span_period)\n                X[feature_column_name] = X.groupby('assetCode')[column].apply(lambda x: self.ewma(x, span = span_period).mean())\n        return X\n    \n    def _compute_macd(self, X):\n        for column in self.feature_cols[1:3]:\n            feature_column_name = 'macd_{0}'.format(column)\n            X[feature_column_name] = (X.groupby('assetCode')[column].apply(lambda x: self.ewma(x, span = 12).mean()) - \n                                     X.groupby('assetCode')[column].apply(lambda x: self.ewma(x, span = 26).mean()))\n        return X\n    \n    def _compute_bollinger_band(self, X):\n        \n        # We will not take volume into account here\n        for column in self.feature_cols[1:3]:\n            for window_period in self.bollinger_band_number_of_days:\n                bb_high_feature_column_name = 'bollinger_band_{0}_{1}_days_high'.format(column, window_period)\n                bb_low_feature_column_name = 'bollinger_band_{0}_{1}_days_low'.format(column, window_period)\n                std_feature_column_name = 'moving_average_std_{0}_{1}_days'.format(column, window_period)\n                moving_average_feature_column_name = 'moving_average_{0}_{1}_days'.format(column, window_period)\n                \n                X[std_feature_column_name] = X.groupby('assetCode')[column].apply(lambda x: x.rolling(window_period).std())\n                X[bb_high_feature_column_name] = X[moving_average_feature_column_name] + self.no_of_std * X[std_feature_column_name]\n                X[bb_low_feature_column_name] = X[moving_average_feature_column_name] - self.no_of_std * X[std_feature_column_name]\n        return X\n    \n    def _compute_rsi(self, X):\n        return\n    \n    def _compute_miscellaneous_features(self, X):\n        for column in self.returns_features:\n            feature_column_name = '{0}_average'.format(column)\n            mean_by_date = X.groupby('date')[column].agg({feature_column_name: nanmean}).reset_index()\n            X = pd.merge(X, mean_by_date, how = 'left', on = ['date'])\n            \n            \n        X['close_open_ratio'] = X['close']\/X['open']\n        mean_by_date = X.groupby('date')['close_open_ratio'].agg({'close_open_ratio_average': nanmean}).reset_index()\n        X = pd.merge(X, mean_by_date, how = 'left', on = ['date'])\n        return X\n    \n    def fit(self, X, y = None):\n        print(\"Adding Quant Features\")\n        self.quant_features_functions = [#self._compute_moving_average, \n                                         #self._compute_exponential_moving_average, \n                                         self._compute_miscellaneous_features\n                                        ]\n        return self\n    \n    def transform(self, X, y = None):\n\n        # Apply each quant function to X\n        for quant_function in self.quant_features_functions:\n            X[0] = quant_function(X[0])\n        \n        if len(X) == 1:\n            X = X[0]\n        else:\n            X[0] = X[0].drop(self.drop_features, 1)\n            X = pd.merge(X[1], X[0], how = 'left', on = ['date', 'assetCode'])\n        \n        gc.collect()\n        return X","6b091ea6":"class AddingLagFeatures(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        self.shift_size = 1\n        self.n_lag = [3, 7, 14]\n        self.return_features = ['returnsClosePrevMktres10', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'open', 'close']\n        self.drop_features = [ 'time', 'assetName', 'volume', 'close', 'open',\n                               'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n                               'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n                               'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n                               'returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\n        \n    def fit(self, X, y = None):\n        print(\"Adding Lag Features\")\n        return self\n    \n    def _create_lag_features(self, X):\n        for col in self.return_features:\n            for window in self.n_lag:\n                rolled = X[col].shift(self.shift_size).rolling(window = window)\n                lag_mean = rolled.mean()\n                lag_max = rolled.max()\n                lag_min = rolled.min()\n                X['%s_lag_%s_mean'%(col, window)] = lag_mean\n                X['%s_lag_%s_max'%(col, window)] = lag_max\n                X['%s_lag_%s_min'%(col, window)] = lag_min\n        gc.collect()\n        \n        return X.fillna(-1)\n    \n    def transform(self, X, y = None):\n        assetCodes = X[0]['assetCode'].unique()\n        \n        all_df = []\n        df_codes = X[0].groupby('assetCode')\n        df_codes = [df_code[1][['date', 'assetCode'] + self.return_features] for df_code in df_codes]\n\n        pool = Pool(4)\n        all_df = pool.map(self._create_lag_features, df_codes)\n        new_df = pd.concat(all_df)\n        new_df.drop(self.return_features, axis = 1, inplace = True)\n        X[0] = pd.merge(X[0], new_df, how = 'left', on = ['date', 'assetCode'])\n        \n        if len(X) == 1:\n            X = X[0]\n        else:\n            X[0] = X[0].drop(self.drop_features, 1)\n            X = pd.merge(X[1], X[0], how = 'left', on = ['date', 'assetCode'])\n        pool.close()\n        \n        del all_df, new_df, df_codes, assetCodes\n        gc.collect()\n        \n        return X","44dd9fd4":"class MissingValuesImputer(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        return\n        \n    def fit(self, X, y = None):\n        print(\"Missing Values Imputer\")\n        return self\n    \n    def transform(self, X, y = None):\n        \n        for i in X.columns:\n            if X[i].dtype == \"object\":\n                X[i] = X[i].fillna(\"other\")\n            elif (X[i].dtype == \"int64\" or X[i].dtype == \"float64\"):\n                X[i] = X[i].fillna(X[i].mean())\n            else:\n                pass\n            \n        return X","9c278d47":"class AssetcodeEncoder(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        return\n        \n    def fit(self, X, y = None):\n        print(\"Encoding AssetCode\")\n        return self\n    \n    def transform(self, X, y = None):\n        lbl = {k: v for v, k in enumerate(X['assetCode'].unique())}\n        X['assetCodeT'] = X['assetCode'].map(lbl)\n        return X","6ba0d5fe":"class LGBClassifierCV(BaseEstimator, RegressorMixin):\n    \n    def __init__(self, \n                 axis = 0, \n                 lgb_params = None, \n                 fit_params = None, \n                 cv = 5, \n                 perform_bayes_search = False, \n                 perform_random_search = False, \n                 use_train_test_split = False,\n                 use_full_data = True,\n                 use_kfold_split = True):\n        self.axis = axis\n        self.lgb_params = lgb_params\n        self.fit_params = fit_params\n        self.fit_params = fit_params\n        self.cv = cv\n        self.perform_random_search = perform_random_search\n        self.perform_bayes_search = perform_bayes_search\n        self.use_train_test_split = use_train_test_split\n        self.use_kfold_split = use_kfold_split\n        self.use_full_data = use_full_data\n    \n    def _get_random_search_params(self, X, y):\n        # Define a search space for the parameters\n        lgb_search_params = {\n            'learning_rate': [0.15, 0.1, 0.05, 0.02, 0.01],\n            'num_leaves': [i for i in range(12, 90, 6)],\n            'n_estimators': [100, 200, 300, 400, 500, 600, 800],\n            'min_child_samples': [i for i in range(10, 100, 10)],\n            'colsample_bytree': [0.8, 0.9, 0.95, 1],\n            'subsample': [0.8, 0.9, 0.95, 1],\n            'reg_alpha': [0.1, 0.2, 0.4, 0.6, 0.8],\n            'reg_lambda': [0.1, 0.2, 0.4, 0.6, 0.8],\n        }\n\n        x_train, x_val, y_train, y_val = train_test_split(X, y, test_size = 0.1, shuffle = False)\n        best_eval_score = 0\n        for i in range(100):\n            print(\"random search iteration - \" + str(i+1))\n            params = {k: np.random.choice(v) for k, v in lgb_search_params.items()}\n            \n            # Use 2 cores\/threads\n            params['n_jobs'] = cpu_count() - 1\n            \n            score = self._evaluate_model(x_train, x_val, y_train, y_val, params)\n            if score < best_eval_score or best_eval_score == 0:\n                best_eval_score = score\n                best_params = params\n        print(\"Best evaluation logloss\", best_eval_score)\n        print(best_params)\n        \n        return best_params\n            \n    def _custom_metric(self, dates, pred_proba, num_target, universe):\n        y = 2*pred_proba[:, 1] - 1\n\n        # get rid of outliers\n        r = num_target.clip(-1,1)\n        x = y * r * universe\n        result = pd.DataFrame({'day' : dates, 'x' : x})\n        x_t = result.groupby('day').sum().values\n        return np.mean(x_t) \/ np.std(x_t)\n    \n    def _evaluate_model(self, x_train, x_val, y_train, y_val, params):\n        model = LGBMClassifier(**params)\n        model.fit(x_train, y_train)\n        return log_loss(y_val, model.predict_proba(x_val))\n    \n    def find_best_params_(self, X, y):\n        if self.perform_random_search:\n            self.lgb_optimal_params = self._get_random_search_params(X, y)\n    \n    def fit(self, X, y = None, **fit_params):\n        print(\"LGBClassifierCV\")\n        \n        # Segregate the target columns\n        dates = X['date']\n        num_target = X['returnsOpenNextMktres10'].astype('float32')\n        y = (X['returnsOpenNextMktres10'] >= 0).astype('int8')\n        universe = X['universe'].astype('int8')\n\n        # Drop columns that are not features\n        X.drop(['returnsOpenNextMktres10', 'universe', 'assetCode', 'assetName', 'time', 'date'], axis = 1, inplace = True)\n        print(X.columns)\n        \n        # Scaling the X values\n        self.mins = np.min(X, axis = 0)\n        self.maxs = np.max(X, axis = 0)\n        self.rng = self.maxs - self.mins\n        X = 1 - ((self.maxs - X) \/self.rng)\n        gc.collect()\n        \n         # Find the best params using random search or random search\n        if self.perform_random_search:\n            self.find_best_params_(X, y)\n        else:\n            self.lgb_optimal_params = {'learning_rate': 0.01, \n                                       'num_leaves': 66, \n                                       'n_estimators': 200, \n                                       'min_child_samples': 40, \n                                       'colsample_bytree': 0.9, \n                                       'subsample': 0.9, \n                                       'reg_alpha': 0.4, \n                                       'reg_lambda': 0.2, \n                                       'n_jobs': 3}\n            \n            x_1 = [0.19000424246380565, 2452, 212, 328, 202]\n            x_2 = [0.19016805202090095, 2583, 213, 312, 220]\n            \n            self.lgb_optimal_params_1 = {\n                            'task': 'train',\n                            'boosting_type': 'gbdt',\n                            'objective': 'binary',\n                            'learning_rate': x_1[0],\n                            'num_leaves': x_1[1],\n                            'min_data_in_leaf': x_1[2],\n                            'num_iteration': 239,\n                            'max_bin': x_1[4],\n                            'verbose': 1\n                        }\n\n            self.lgb_optimal_params_2 = {\n                            'task': 'train',\n                            'boosting_type': 'gbdt',\n                            'objective': 'binary',\n                            'learning_rate': x_2[0],\n                            'num_leaves': x_2[1],\n                            'min_data_in_leaf': x_2[2],\n                            'num_iteration': 172,\n                            'max_bin': x_2[4],\n                            'verbose': 1\n                        }\n            \n        self.lgb_optimal_params[\"objective\"] = \"binary\"\n        self.lgb_optimal_params[\"boosting_type\"] = \"gbdt\"\n        \n        # Train on full dataset\n        if self.use_full_data:\n            train_data = lgb.Dataset(X, label = y)\n            self.lgb_model_1 = lgb.train(self.lgb_optimal_params_1, train_set = train_data, num_boost_round = 100)\n            self.lgb_model_2 = lgb.train(self.lgb_optimal_params_2, train_set = train_data, num_boost_round = 100)\n        \n        # Use a simple train-test split\n        if self.use_train_test_split:\n            x_train, x_val, y_train, y_val = train_test_split(X, y, test_size = 0.1, shuffle = False)\n            \n            train_data = lgb.Dataset(x_train, label = y_train)\n            val_data = lgb.Dataset(x_val, label = y_val)\n            \n            self.lgb_model = lgb.train(self.lgb_optimal_params,\n                                       train_set = train_data, \n                                       num_boost_round = 100,\n                                       valid_sets = [val_data], \n                                       early_stopping_rounds = 5)\n            \n        # When not using random search to tune parameters, proceed with a simple Stratified Kfold CV\n        if self.use_kfold_split:\n            kf = StratifiedKFold(n_splits = self.cv, shuffle = False)\n            for fold_index, (train_data, val_data) in enumerate(kf.split(X, y)):\n                print(\"Train Fold Index - \" + str(fold_index))\n\n                lgb_model = lgb.LGBMClassifier(**self.lgb_optimal_params)\n                lgb_model = lgb.train(self.lgb_optimal_params,\n                                       train_set = train_data, \n                                       num_boost_round = 100,\n                                       valid_sets = [val_data], \n                                       early_stopping_rounds = 5)\n                self.estimators_.append(lgb_model)\n        return self\n    \n    def predict(self, X):\n        # Drop columns that are not features\n        X.drop(['assetCode', 'assetName', 'time', 'date'], axis = 1, inplace = True)\n        \n        # Scale the values\n        X = 1 - ((self.maxs - X) \/self.rng)\n\n        # Get the predictions    \n        predictions = (self.lgb_model_1.predict(X) + self.lgb_model_2.predict(X))\/2\n        predictions = (predictions - predictions.min())\/(predictions.max() - predictions.min())\n        predictions = predictions * 2 - 1\n    \n        return predictions","538d69fd":"data_pipeline = Pipeline([\n    ('initial_preprocessor', InitialPreprocessing()),\n    ('quant_features_generator', AddingQuantFeatures()),\n#     ('lag_features_generator', AddingLagFeatures()),\n    ('data_imputer', MissingValuesImputer()),\n    ('asset_encoder', AssetcodeEncoder()),\n    ('lgb', LGBClassifierCV(cv = 5,\n                            perform_random_search = False,\n                            use_train_test_split = False,\n                            use_full_data = True,\n                            use_kfold_split = False)\n    )\n])\n\ndata_pipeline.fit([market_train])","1d18b2e3":"def write_submission(pipeline, env):\n    days = env.get_prediction_days()\n    n_days = 0\n    total_market_obs = []\n    for (market_obs_df, news_obs_df, predictions_template_df) in days:\n        n_days += 1\n        print(n_days,end = ' ')\n        \n#         total_market_obs.append(market_obs_df.copy())\n#         if len(total_market_obs) == 1:\n#             history_df = total_market_obs[0]\n#         else:\n#             history_df = pd.concat(total_market_obs[-(14+1):], ignore_index = True)\n        \n        preds = data_pipeline.predict([market_obs_df])\n        sub = pd.DataFrame({'assetCode': market_obs_df['assetCode'], 'confidence': preds})\n        predictions_template_df = predictions_template_df.merge(sub, how = 'left').drop(\n            'confidenceValue', axis = 1).fillna(0).rename(columns = {'confidence': 'confidenceValue'})\n        \n        env.predict(predictions_template_df)\n        del predictions_template_df, preds, sub\n        gc.collect()\n    env.write_submission_file()\n    \nwrite_submission(data_pipeline, env)","73904f8c":"## Impute Missing Values","4a89c45c":"## Encoding AssetCode","a0c608d0":"## Adding Quant Features","33670703":"## Initial Preprocessing","6ba1d0de":"<h2>3. Model Building <\/h2>","0a03f88f":"## Adding Lag Features","9c5d93f2":"<h2>4. Create a Pipeline<\/h2>"}}