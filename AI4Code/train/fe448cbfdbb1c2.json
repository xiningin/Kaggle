{"cell_type":{"fe2cc424":"code","1361882a":"code","1a7daeec":"code","25c3f6b5":"code","ce0e8480":"code","64897397":"code","c692bda9":"code","aa50dab8":"code","444a4501":"code","ace4f0a0":"code","835f05b7":"code","0c9d1835":"code","e3efe204":"code","8d1b680e":"code","75eb58ba":"code","ad03ba7b":"code","693a3152":"code","e1fead7e":"code","9fcc8bdf":"code","87df12f7":"code","eb63d13d":"code","19169431":"code","1c06c1e2":"code","bfd94071":"code","bfe89986":"code","b7c24f35":"code","30092797":"code","14d0a0b9":"markdown","bfd385ae":"markdown","28730e22":"markdown","d37f28ad":"markdown","7bc3dfe8":"markdown","9a813e40":"markdown","861f2610":"markdown","bc7f5f2e":"markdown","9525252c":"markdown","3f566af1":"markdown"},"source":{"fe2cc424":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport csv\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ntrain_df = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1361882a":"#Filter for Numerical and categorical features\nquant = [f for f in train_df.columns if train_df.dtypes[f] != (\"object\")]\ncategorical = [f for f in train_df.columns if train_df.dtypes[f] == (\"object\")]\nassert len(train_df.columns) == len(categorical)+len(quant),\"Some Columns in both Categorical and numerical features tab\"\n\n#Imputing just the training data here; helps with outlier detection\nisnull_numericalfeatures = dict(train_df[quant].isnull().sum())\nfor key in isnull_numericalfeatures.keys():\n    if isnull_numericalfeatures[key] !=0:\n        train_df[key].fillna(train_df[key].median(), inplace=True)    ","1a7daeec":"# plt.figure(figsize = (30, 30))\n# sns.heatmap(train_df.corr(), cmap = 'Blues', square = True, annot = True)\n# plt.title(\"Visualizing Correlations\", size = 30)\n# plt.show()","25c3f6b5":"# #Visualizing outliers\n# train_df[quant].hist(figsize = (12,10))","ce0e8480":"# quantOutlier = set(quant) - {\"Id\",\"SalePrice\"} \n# for num_feature in quantOutlier:\n#     Q1 = np.percentile(train_df[num_feature], 5, interpolation = 'midpoint')\n#     Q3 = np.percentile(train_df[num_feature], 95, interpolation = 'midpoint')\n#     IQR = Q3 - Q1\n#     upper = Q3 + 1.5*IQR\n#     lower = Q1 - 1.5*IQR\n#     train_df = train_df[(train_df[num_feature]>=lower) & (train_df[num_feature]<=upper)]","64897397":"len(train_df)","c692bda9":"#combine train and test data together fot preprocessing\noutput_label = set(train_df.columns)-set(test_df.columns)\ny_train = train_df[output_label]\ntrain_df_dupl = train_df.drop(output_label,1)\n\nframes = [train_df_dupl, test_df]\ncombined_df = pd.concat(frames)\nlen_train_df = len(train_df_dupl)\n\n\ncombined_id = combined_df[\"Id\"]\ncombined_df = combined_df.drop(\"Id\",1)","aa50dab8":"quant = [f for f in combined_df.columns if combined_df.dtypes[f] != (\"object\")]\ncategorical = [f for f in combined_df.columns if combined_df.dtypes[f] == (\"object\")]\n\n#Imputing Numerical Features - Replacing with Median\nisnull_numericalfeatures = dict(combined_df[quant].isnull().sum())\nfor key in isnull_numericalfeatures.keys():\n    if isnull_numericalfeatures[key] !=0:\n        combined_df[key].fillna(combined_df[key].median(), inplace=True)    \n        \n# #Imputing Categorical Features - Treating Missing ones a new feature\n# isnull_categoricalfeatures = dict(combined_df[categorical].isnull().sum())\n# for key in isnull_categoricalfeatures.keys():\n#     if isnull_categoricalfeatures[key] !=0:\n#         combined_df[key].fillna(\"unknown\", inplace=True)            \n# #         print(key, len(combined_df[key].unique()))\n\nisnull_categoricalfeatures = dict(combined_df[categorical].isnull().sum())\nfor key in isnull_categoricalfeatures.keys():\n    if isnull_categoricalfeatures[key] !=0:\n        d = dict(combined_df[key].value_counts())\n        for k in d.keys():\n            if len(combined_df[combined_df[key]==k])\/len(combined_df)>0.30:\n                combined_df[key].fillna(k, inplace=True)\n                break\n            else:\n                combined_df[key].fillna(\"unknown\", inplace=True)  \n\nprint(\"Showing that hyperparameter value of threshold 0.30 is working good - Tune this hyperparameter for final submission\")","444a4501":"# One Hot Encoding\n# pd.get_dummies(combined_df, columns=['SaleCondition'])\n\n# Ordinal Encoding\nfrom sklearn.preprocessing import OrdinalEncoder\nenc = OrdinalEncoder()\nenc.fit(combined_df[categorical])\ncombined_df[categorical] = enc.transform(combined_df[categorical])\n# After Encoding of Categorical features\nprint([(f,combined_df.dtypes[f]) for f in combined_df.columns if combined_df.dtypes[f] == (\"object\")])","ace4f0a0":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(combined_df[list(combined_df.columns)])\ncombined_df[list(combined_df.columns)] = scaler.transform(combined_df[list(combined_df.columns)])\nx_combined = combined_df.values\n\nx_train_val = x_combined[:len_train_df]\ny_train_val = y_train.values\nx_test = x_combined[len_train_df:]\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.2)\ny_train = y_train.ravel()\ny_val = y_val.ravel()","835f05b7":"# from sklearn.linear_model import LinearRegression\n# from sklearn.metrics import mean_absolute_error\n\n# # Linear Regression\n# print(\"Linear Regression Model ------------\")\n# reg = LinearRegression()\n# reg.fit(x_train, y_train)\n# print(\"Linear Regression Training Score\", reg.score(x_train, y_train))\n# y_pred_train = reg.predict(x_train)\n# y_pred_val = reg.predict(x_val)\n# y_pred_test_lr = reg.predict(x_test)\n# print('Training MAE Error:', mean_absolute_error(y_train, y_pred_train)) \n# print('Validation MAE Error:', mean_absolute_error(y_val, y_pred_val)) \n\n# # Random Forest\n# print(\"\\nRandom Forest Model (10 Trees)------------\")\n# from sklearn.ensemble import RandomForestRegressor\n# rf = RandomForestRegressor(n_estimators=10, random_state=0)\n# rf.fit(x_train, y_train)\n# print(\"Random Forest Training Score\", rf.score(x_train, y_train))\n# y_pred_train = rf.predict(x_train)\n# y_pred_val = rf.predict(x_val)\n# y_pred_test_rf10 = rf.predict(x_test)\n# print('Training MAE Error:', mean_absolute_error(y_train, y_pred_train)) \n# print('Validation MAE Error:', mean_absolute_error(y_val, y_pred_val)) \n\n# # Random Forest\n# print(\"\\nRandom Forest Model (20 Trees)------------\")\n# from sklearn.ensemble import RandomForestRegressor\n# rf = RandomForestRegressor(n_estimators=20, random_state=0)\n# rf.fit(x_train, y_train)\n# print(\"Random Forest Training Score\", rf.score(x_train, y_train))\n# y_pred_train = rf.predict(x_train)\n# y_pred_val = rf.predict(x_val)\n# y_pred_test_rf20 = rf.predict(x_test)\n# print('Training MAE Error:', mean_absolute_error(y_train, y_pred_train)) \n# print('Validation MAE Error:', mean_absolute_error(y_val, y_pred_val)) \n\n# # Random Forest\n# print(\"\\nRandom Forest Model (30 Trees)------------\")\n# from sklearn.ensemble import RandomForestRegressor\n# rf = RandomForestRegressor(n_estimators=30, random_state=0)\n# rf.fit(x_train, y_train)\n# print(\"Random Forest Training Score\", rf.score(x_train, y_train))\n# y_pred_train = rf.predict(x_train)\n# y_pred_val = rf.predict(x_val)\n# y_pred_test_rf30 = rf.predict(x_test)\n# print('Training MAE Error:', mean_absolute_error(y_train, y_pred_train)) \n# print('Validation MAE Error:', mean_absolute_error(y_val, y_pred_val)) \n\n# # Random Forest\n# print(\"\\nRandom Forest Model (100 Trees)------------\")\n# from sklearn.ensemble import RandomForestRegressor\n# rf = RandomForestRegressor(n_estimators=100, random_state=0)\n# rf.fit(x_train, y_train)\n# print(\"Random Forest Training Score\", rf.score(x_train, y_train))\n# y_pred_train = rf.predict(x_train)\n# y_pred_val = rf.predict(x_val)\n# y_pred_test_rf100 = rf.predict(x_test)\n# print('Training MAE Error:', mean_absolute_error(y_train, y_pred_train)) \n# print('Validation MAE Error:', mean_absolute_error(y_val, y_pred_val)) \n\n# # Random Forest\n# print(\"\\nRandom Forest Model (120 Trees)------------\")\n# from sklearn.ensemble import RandomForestRegressor\n# rf = RandomForestRegressor(n_estimators=120, random_state=0)\n# rf.fit(x_train, y_train)\n# print(\"Random Forest Training Score\", rf.score(x_train, y_train))\n# y_pred_train = rf.predict(x_train)\n# y_pred_val = rf.predict(x_val)\n# y_pred_test_rf120 = rf.predict(x_test)\n# print('Training MAE Error:', mean_absolute_error(y_train, y_pred_train)) \n# print('Validation MAE Error:', mean_absolute_error(y_val, y_pred_val)) \n\n# # Regularizarions\n# print(\"\\nRidge Regularization------------\")\n# from sklearn.linear_model import Ridge\n# ridge = Ridge(alpha=1.0)\n# ridge.fit(x_train, y_train)\n# print(\"Ridge Regularization Training Score\", ridge.score(x_train, y_train))\n# y_pred_train = ridge.predict(x_train)\n# y_pred_val = ridge.predict(x_val)\n# y_pred_test_ridge = ridge.predict(x_test)\n# print('Training MAE Error:', mean_absolute_error(y_train, y_pred_train)) \n# print('Validation MAE Error:', mean_absolute_error(y_val, y_pred_val)) \n\n# print(\"\\nLasso Regularization------------\")\n# from sklearn.linear_model import Lasso\n# lasso = Lasso(alpha=0.1)\n# lasso.fit(x_train, y_train)\n# print(\"Lasso Regularization Training Score\", lasso.score(x_train, y_train))\n# y_pred_train = lasso.predict(x_train)\n# y_pred_val = lasso.predict(x_val)\n# y_pred_test_lasso = lasso.predict(x_test)\n# print('Training MAE Error:', mean_absolute_error(y_train, y_pred_train)) \n# print('Validation MAE Error:', mean_absolute_error(y_val, y_pred_val)) \n\n# print(\"\\nElasticNet Regularization------------\")\n# from sklearn.linear_model import ElasticNet\n# enet = ElasticNet(random_state=0)\n# enet.fit(x_train, y_train)\n# print(\"ElasticNet Regularization Training Score\", enet.score(x_train, y_train))\n# y_pred_train = enet.predict(x_train)\n# y_pred_val = enet.predict(x_val)\n# y_pred_test_enet = enet.predict(x_test)\n# print('Training MAE Error:', mean_absolute_error(y_train, y_pred_train)) \n# print('Validation MAE Error:', mean_absolute_error(y_val, y_pred_val)) ","0c9d1835":"# # Random Forest\n# print(\"\\nRandom Forest Model (1000 Trees)------------\")\n# from sklearn.ensemble import RandomForestRegressor\n# rf = RandomForestRegressor(n_estimators=1000, random_state=0)\n# rf.fit(x_train, y_train)\n# print(\"Random Forest Training Score\", rf.score(x_train, y_train))\n# y_pred_train = rf.predict(x_train)\n# y_pred_val = rf.predict(x_val)\n# y_pred_test_rf1000 = rf.predict(x_test)\n# print('Training MAE Error:', mean_absolute_error(y_train, y_pred_train)) \n# print('Validation MAE Error:', mean_absolute_error(y_val, y_pred_val)) ","e3efe204":"# from sklearn.decomposition import PCA\n# pca = PCA(n_components=60)\n# x_train_pca = pca.fit_transform(x_train)\n# x_val_pca = pca.fit_transform(x_val)\n# x_test_pca = pca.fit_transform(x_test)\n# sum(pca.explained_variance_ratio_[:50])\n\n# # Random Forest with PCA, feature selection\n# print(\"\\nRandom Forest Model (100 Trees)------------\")\n# from sklearn.ensemble import RandomForestRegressor\n# rf = RandomForestRegressor(n_estimators=100, random_state=0)\n# rf.fit(x_train_pca, y_train)\n# print(\"Random Forest Training Score\", rf.score(x_train_pca, y_train))\n# y_pred_train = rf.predict(x_train_pca)\n# y_pred_val = rf.predict(x_val_pca)\n# y_pred_test_rf100pca = rf.predict(x_test_pca)\n# print('Training MAE Error:', mean_absolute_error(y_train, y_pred_train)) \n# print('Validation MAE Error:', mean_absolute_error(y_val, y_pred_val)) ","8d1b680e":"rf = RandomForestRegressor(n_estimators=100, random_state=0)\nrf.fit(x_train, y_train)\n\nsorted_idx = rf.feature_importances_.argsort()\ninp_threshold = 0.001\ninp_features_number = len([f for f in rf.feature_importances_ if f>=inp_threshold])\ninp_features = sorted_idx[len(sorted_idx)-inp_features_number:]\nx_train_new = []\nfor x in range(len(x_train)):\n    x = list(x_train[x][inp_features])\n    x_train_new.append(x)\nx_train_new = np.array(x_train_new)\n\nx_val_new = []\nfor x in range(len(x_val)):\n    x = list(x_val[x][inp_features])\n    x_val_new.append(x)\nx_val_new = np.array(x_val_new)\n\nx_test_new = []\nfor x in range(len(x_test)):\n    x = list(x_test[x][inp_features])\n    x_test_new.append(x)\nx_test_new = np.array(x_test_new)","75eb58ba":"# # Random Forest\n# print(\"\\nRandom Forest Model (1000 Trees) Prior Feature Selection------------\")\n# from sklearn.ensemble import RandomForestRegressor\n# rf1 = RandomForestRegressor(n_estimators=1000, random_state=0)\n# rf1.fit(x_train_new, y_train)\n# print(\"Random Forest Training Score\", rf1.score(x_train_new, y_train))\n# y_pred_train = rf1.predict(x_train_new)\n# y_pred_val = rf1.predict(x_val_new)\n# y_pred_test_rf1000fs = rf1.predict(x_test_new)\n# print('Training MAE Error:', mean_absolute_error(y_train, y_pred_train)) \n# print('Validation MAE Error:', mean_absolute_error(y_val, y_pred_val)) ","ad03ba7b":"from xgboost import XGBRegressor\nxg = XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)\nxg.fit(x_train, y_train)\nprint(\"XG Training Score\", xg.score(x_train, y_train))\ny_pred_train = xg.predict(x_train)\ny_pred_val = xg.predict(x_val)\ny_pred_test_xg1000 = xg.predict(x_test)\nprint('Training MAE Error:', mean_absolute_error(y_train, y_pred_train)) \nprint('Validation MAE Error:', mean_absolute_error(y_val, y_pred_val)) ","693a3152":"from xgboost import XGBRegressor\nxg = XGBRegressor(n_estimators=2000, max_depth=9, eta=0.1, subsample=0.7, colsample_bytree=0.8)\nxg.fit(x_train_new, y_train)\nprint(\"Random Forest Training Score\", xg.score(x_train_new, y_train))\ny_pred_train = xg.predict(x_train_new)\ny_pred_val = xg.predict(x_val_new)\ny_pred_test_xg1000fs = xg.predict(x_test_new)\nprint('Training MAE Error:', mean_absolute_error(y_train, y_pred_train)) \nprint('Validation MAE Error:', mean_absolute_error(y_val, y_pred_val)) ","e1fead7e":"from sklearn.feature_selection import SelectFromModel\n\nrf = RandomForestRegressor(n_estimators=100, random_state=0)\nrf.fit(x_train, y_train)\nprint(\"Random Forest Training Score\", rf.score(x_train, y_train))\ny_pred_train = rf.predict(x_train)\ny_pred_val = rf.predict(x_val)\ny_pred_test_rf100 = rf.predict(x_test)\nprint('Training MAE Error:', mean_absolute_error(y_train, y_pred_train)) \nprint('Validation MAE Error:', mean_absolute_error(y_val, y_pred_val)) ","9fcc8bdf":"sel = SelectFromModel(RandomForestRegressor(n_estimators = 100))\nsel.fit(x_train, y_train)\nsel.get_support()\nimp_features = [3,16,29,33,37,42,45,60,61]\n    \nx_train_new = []\nfor x in range(len(x_train)):\n    x = list(x_train[x][imp_features])\n    x_train_new.append(x)\nx_train_new = np.array(x_train_new)\n\nx_val_new = []\nfor x in range(len(x_val)):\n    x = list(x_val[x][imp_features])\n    x_val_new.append(x)\nx_val_new = np.array(x_val_new)\n\nx_test_new = []\nfor x in range(len(x_test)):\n    x = list(x_test[x][imp_features])\n    x_test_new.append(x)\nx_test_new = np.array(x_test_new)\n\nrf = RandomForestRegressor(n_estimators=100, random_state=0)\nrf.fit(x_train_new, y_train)\nprint(\"Random Forest Training Score\", rf.score(x_train_new, y_train))\ny_pred_train = rf.predict(x_train_new)\ny_pred_val = rf.predict(x_val_new)\ny_pred_test_rf100 = rf.predict(x_test_new)\nprint('Training MAE Error:', mean_absolute_error(y_train, y_pred_train)) \nprint('Validation MAE Error:', mean_absolute_error(y_val, y_pred_val)) ","87df12f7":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, dataset, TensorDataset\n\nembedding_size = len(x_train[0])\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nbatch_size=32","eb63d13d":"class NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(embedding_size, int(embedding_size*1.5)),\n            nn.Dropout(0.2),  \n            nn.ReLU(),\n            nn.Linear(int(embedding_size*1.5), int(embedding_size*0.6)),\n            nn.Dropout(0.2),                          \n            nn.ReLU(),        \n            nn.Linear(int(embedding_size*0.6), int(embedding_size*0.1)),\n            nn.ReLU(),        \n            nn.Dropout(0.2),              \n            nn.Linear(int(embedding_size*0.1), 1),   \n            nn.ReLU(),                      \n        )\n    \n    def forward(self, x):\n        logits = self.linear_relu_stack(x)\n        return logits\nmodel = NeuralNetwork().to(device)\nprint(model)\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)    ","19169431":"def train(dataloader, model, loss_fn, optimizer):    \n    size = len(dataloader.dataset)\n    model.train() \n    total_loss = 0\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n        pred = model(X)\n        loss = torch.sqrt(loss_fn(pred, y))    #Sends logits and GT to the loss function\n        total_loss += loss\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n#         print(f\"Training loss: \", loss)\n\n    total_loss = total_loss.item()\n    return total_loss\/size\n\ndef val(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():     \n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            val_loss += torch.sqrt(loss_fn(pred, y)).item()\n#             print(f\"Validation loss: \", loss_fn(pred, y).item())\n    return val_loss\/size\n\ntrain_data=TensorDataset(torch.FloatTensor(x_train), torch.from_numpy(y_train).to(torch.float))\nvalid_data=TensorDataset(torch.FloatTensor(x_val), torch.from_numpy(y_val).to(torch.float))\ntrain_loader=DataLoader(train_data, batch_size=batch_size, shuffle=True)\nvalid_loader=DataLoader(valid_data, batch_size=batch_size, shuffle=True)\n\ntest_data=TensorDataset(torch.FloatTensor(x_test))\ntest_loader=DataLoader(test_data, batch_size=1, shuffle=True)\n","1c06c1e2":"epochs = 1000\nval_loss=1000\npatience=5\nthe_last_loss = 1000\ntrigger_times = 0\ntrain_loss_epoch = []\nval_loss_epoch = []\nimport tqdm\nfor t in tqdm.tqdm(range(epochs)):\n#     print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_loss=train(train_loader, model, loss_fn, optimizer)\n    val_loss=val(valid_loader, model, loss_fn)    \n    train_loss_epoch.append(train_loss)\n    val_loss_epoch.append(val_loss)\n    # Early stopping\n    the_current_loss = val_loss\n    if the_current_loss > the_last_loss:\n        trigger_times += 1\n#         print('trigger times:', trigger_times)\n        if trigger_times >= patience:\n            print('Early stopping!\\nStart to test process.')\n            break\n    else:\n#         print('trigger times: 0')\n        trigger_times = 0\n    the_last_loss = the_current_loss    \nprint(\"Training Done!\")","bfd94071":"val_loss","bfe89986":"output = []\nmodel.eval()\nwith torch.no_grad():     \n    for X in test_loader:\n        output.append(model(X[0]).numpy()[0][0])","b7c24f35":"submission_df = pd.DataFrame()\nsubmission_df['Id'] = combined_id[len_train_df:]\nsubmission_df['SalePrice'] = output\nsubmission_df.to_csv(\"submission.csv\",index=False)","30092797":"submission_df.head()","14d0a0b9":"Will follow following techniques after data proprocessing\n1. Simple Deep Neural Network\n2. ML Techniques on all columns\n3. ML Techniques of some top correlated columns\n4. Using PCA to apply ML Techniques on few most columns having the most variance","bfd385ae":"# ML 5th Approach XG BOOST Feature Selection","28730e22":"# ML Models - 1st Approach","d37f28ad":"# Submission File","7bc3dfe8":"# ML - 2nd Approach: PCA\n","9a813e40":"# Imputation","861f2610":"# ML 4th Approach - XGBoost (Since, RF is is reducing variance, but we need to reduce bias as well) \n* (Best till now) Test Error: 0.13203 - without outlier removal and with threshold of 0.30 in categorical imputation; without feature selection\n* Test Error: 0.14456 - with outlier removal and with threshold of 0.30 in categorical imputation; without feature selection\n* Test Error: 0.13713 - without outlier removal and with threshold of 0.30 in categorical imputation; with feature selection\n* Todo : Hyperparamter Tuning - XGBoost Parameters + categorical imputation parameter + cross validation + Feature Engineering (Adding New Features)","bc7f5f2e":"# DL 6th Approach","9525252c":"# Outlier detection in training data set","3f566af1":"# ML - 3rd Approach: Feature Selection RF"}}