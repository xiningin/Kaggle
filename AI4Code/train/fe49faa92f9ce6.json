{"cell_type":{"64bff6ed":"code","bb833905":"code","3bc43b0f":"code","57074c31":"code","228c82af":"code","dcbc1a0e":"code","2dd55685":"code","b986aba2":"code","aad1a7d1":"code","c88e27aa":"code","29c557b9":"code","8b14cc9c":"code","e5bf2ce0":"code","51feee90":"code","cbef2dbb":"code","523da523":"code","11559da0":"code","1ca39ec4":"code","4d8e1257":"code","7d1a1d99":"code","42e3668a":"markdown","92ea7e35":"markdown","b8d4c9db":"markdown","11ebb3d2":"markdown","33e0be9d":"markdown","ea1e1a67":"markdown","84287f85":"markdown","42c4cd9e":"markdown","9a1dc917":"markdown","3747faad":"markdown","ca03c94f":"markdown","2fb3f593":"markdown","b0e76e48":"markdown","8206187e":"markdown","4a2a044d":"markdown","fed9802a":"markdown","cc8b7114":"markdown","5dfb27c2":"markdown","92140ec0":"markdown"},"source":{"64bff6ed":"import os\nos.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning\n\nimport numpy as np\nimport random\nimport sklearn\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport tensorflow as tf\nimport plotly.express as px\n\n!pip uninstall -y transformers\n!pip install transformers\n\nimport transformers\nimport tokenizers\n\n# Hugging Face new library for datasets (https:\/\/huggingface.co\/nlp\/)\n!pip install nlp\nimport nlp\n\nimport datetime\n\nstrategy = None","bb833905":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3bc43b0f":"original_train = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/train.csv\")\n\noriginal_train = sklearn.utils.shuffle(original_train)\noriginal_train = sklearn.utils.shuffle(original_train)\n\nvalidation_ratio = 0.2\nnb_valid_examples = max(1, int(len(original_train) * validation_ratio))\n\noriginal_valid = original_train[:nb_valid_examples]\noriginal_train = original_train[nb_valid_examples:]","57074c31":"print(f\"original - training: {len(original_train)} examples\")\noriginal_train.head(10)","228c82af":"print(f\"original - validation: {len(original_valid)} examples\")\noriginal_valid.head(10)","dcbc1a0e":"original_test = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/test.csv\")\nprint(f\"original - test: {len(original_test)} examples\")\noriginal_test.head(10)","2dd55685":"mnli = nlp.load_dataset(path='glue', name='mnli')","b986aba2":"print(mnli, '\\n')\n\nprint('The split names in MNLI dataset:')\nfor k in mnli:\n    print('   ', k)\n    \n# Get the datasets\nprint(\"\\nmnli['train'] is \", type(mnli['train']))\n\nmnli['train']","aad1a7d1":"print('The number of training examples in mnli dataset:', mnli['train'].num_rows)\nprint('The number of validation examples in mnli dataset - part 1:', mnli['validation_matched'].num_rows)\nprint('The number of validation examples in mnli dataset - part 2:', mnli['validation_mismatched'].num_rows, '\\n')\n\nprint('The class names in mnli dataset:', mnli['train'].features['label'].names)\nprint('The feature names in mnli dataset:', list(mnli['train'].features.keys()), '\\n')\n\nfor elt in mnli['train']:\n    \n    print('premise:', elt['premise'])\n    print('hypothesis:', elt['hypothesis'])\n    print('label:', elt['label'])\n    print('label name:', mnli['train'].features['label'].names[elt['label']])\n    print('idx', elt['idx'])\n    print('-' * 80)\n    \n    if elt['idx'] >= 10:\n        break","c88e27aa":"# convert to a dataframe and view\nmnli_train_df = pd.DataFrame(mnli['train'])\nmnli_valid_1_df = pd.DataFrame(mnli['validation_matched'])\nmnli_valid_2_df = pd.DataFrame(mnli['validation_mismatched'])\n\nmnli_train_df = mnli_train_df[['premise', 'hypothesis', 'label']]\nmnli_valid_1_df = mnli_valid_1_df[['premise', 'hypothesis', 'label']]\nmnli_valid_2_df = mnli_valid_2_df[['premise', 'hypothesis', 'label']]\n\nmnli_train_df['lang_abv'] = 'en'\nmnli_valid_1_df['lang_abv'] = 'en'\nmnli_valid_2_df['lang_abv'] = 'en'","29c557b9":"mnli_train_df.head(10)","8b14cc9c":"mnli_valid_1_df.head(10)","e5bf2ce0":"mnli_valid_2_df.head(10)","51feee90":"snli = nlp.load_dataset(path='snli')\n\nprint('The number of training examples in snli dataset:', snli['train'].num_rows)\nprint('The number of validation examples in snli dataset:', snli['validation'].num_rows, '\\n')\n\nprint('The class names in snli dataset:', snli['train'].features['label'].names)\nprint('The feature names in snli dataset:', list(snli['train'].features.keys()), '\\n')\n\nfor idx, elt in enumerate(snli['train']):\n    \n    print('premise:', elt['premise'])\n    print('hypothesis:', elt['hypothesis'])\n    print('label:', elt['label'])\n    print('label name:', snli['train'].features['label'].names[elt['label']])\n    print('-' * 80)\n    \n    if idx >= 10:\n        break","cbef2dbb":"# convert to a dataframe and view\nsnli_train_df = pd.DataFrame(snli['train'])\nsnli_valid_df = pd.DataFrame(snli['validation'])\n\nsnli_train_df = snli_train_df[['premise', 'hypothesis', 'label']]\nsnli_valid_df = snli_valid_df[['premise', 'hypothesis', 'label']]\n\nsnli_train_df['lang_abv'] = 'en'\nsnli_valid_df['lang_abv'] = 'en'","523da523":"snli_train_df.head(10)","11559da0":"snli_valid_df.head(10)","1ca39ec4":"xnli = nlp.load_dataset(path='xnli')\n\nprint('The number of validation examples in xnli dataset:', xnli['validation'].num_rows, '\\n')\n\nprint('The class names in xnli dataset:', xnli['validation'].features['label'].names)\nprint('The feature names in xnli dataset:', list(xnli['validation'].features.keys()), '\\n')\n\nfor idx, elt in enumerate(xnli['validation']):\n    \n    print('premise:', elt['premise'])\n    print('hypothesis:', elt['hypothesis'])\n    print('label:', elt['label'])\n    print('label name:', xnli['validation'].features['label'].names[elt['label']])\n    print('-' * 80)\n    \n    if idx >= 3:\n        break","4d8e1257":"# convert to a dataframe and view\nbuffer = {\n    'premise': [],\n    'hypothesis': [],\n    'label': [],\n    'lang_abv': []\n}\n\n\nfor x in xnli['validation']:\n    label = x['label']\n    for idx, lang in enumerate(x['hypothesis']['language']):\n        hypothesis = x['hypothesis']['translation'][idx]\n        premise = x['premise'][lang]\n        buffer['premise'].append(premise)\n        buffer['hypothesis'].append(hypothesis)\n        buffer['label'].append(label)\n        buffer['lang_abv'].append(lang)\n        \n# convert to a dataframe and view\nxnli_valid_df = pd.DataFrame(buffer)\nxnli_valid_df = xnli_valid_df[['premise', 'hypothesis', 'label', 'lang_abv']]","7d1a1d99":"xnli_valid_df.head(15 * 3)","42e3668a":"Again, the class names are\n```\n    ['entailment', 'neutral', 'contradiction'] \n```\nwhich corresponds to the original competition dataset.\n\nIn [SNLI](https:\/\/nlp.stanford.edu\/projects\/snli\/), we have the same premise with different hypotheses\/labels. With a first try, I got `nan` as the training loss value. So I won't use this dataset in the current notebook.","92ea7e35":"#### Convert SNLI to pandas.DataFrame","b8d4c9db":"#### Load a dataset - The Multi-Genre NLI Corpus (MNLI)\nFirst, let's load the [The Multi-Genre NLI Corpus (MultiNLI, MNLI)](https:\/\/cims.nyu.edu\/~sbowman\/multinli\/). It contains $433000$ sentence pairs annotated with textual entailment information.","11ebb3d2":"# Datasets","33e0be9d":"## Competition dataset","ea1e1a67":"#### Convert XNLI to pandas.DataFrame","84287f85":"### Let's use Hugging Face new library [nlp](https:\/\/huggingface.co\/nlp\/), to get more NLI datasets.","42c4cd9e":"### Load more extra datasets","9a1dc917":"## Extra datasets","3747faad":"#### check the loaded dataset\n\nLet's look some information about the MNLI dataset. The (default) return value of [nlp.load_dataset](https:\/\/huggingface.co\/nlp\/package_reference\/loading_methods.html#nlp.load_dataset) is a dictionary with split names as keys, usually they are `train`, `validation` and `test`, but not always. The values are [nlp.arrow_dataset.Dataset](https:\/\/huggingface.co\/nlp\/master\/package_reference\/main_classes.html#nlp.Dataset).\n\n","ca03c94f":"#### The Cross-Lingual NLI Corpus (XNLI)\n\nThe [MNLI](https:\/\/cims.nyu.edu\/~sbowman\/multinli\/) and [SNLI](https:\/\/nlp.stanford.edu\/projects\/snli\/) contain only english sentences. Let's load the [Cross-lingual NLI Corpus (XNLI)](https:\/\/cims.nyu.edu\/~sbowman\/xnli\/) dataset. It contains only validation and test dataset, not training examples.","2fb3f593":"Note that the class names are\n```\n    ['entailment', 'neutral', 'contradiction'] \n```\nwhich corresponds to the original competition dataset, described in [this competition data page](https:\/\/www.kaggle.com\/c\/contradictory-my-dear-watson\/data):\n\n> label: the classification of the relationship between the premise and hypothesis (0 for entailment, 1 for neutral, 2 for contradiction)","b0e76e48":"#### look inside 'nlp.arrow_dataset.Dataset'\n\nIn order to get the number of examples in a dataset, for example, `mnli['train']`, you can do\n```\n    mnli['train'].num_rows\n```\n\n\nYou can iterate a [nlp.arrow_dataset.Dataset](https:\/\/huggingface.co\/nlp\/master\/package_reference\/main_classes.html#nlp.Dataset) object like:\n```\n    for elt in mnli['train']:\n        ...\n```\nEach step, you get an example (which is a dictionary containing features - in a general sense).\n\nYou can also access the content of a [nlp.arrow_dataset.Dataset](https:\/\/huggingface.co\/nlp\/master\/package_reference\/main_classes.html#nlp.Dataset) object by specifying a feature name . For example, the training dataset in `mnli` has `premise`, `hypothesis`, `label` and `idx` as features.\n\nYou can either specify a feature name first (you get a list) followed by a slice, like\n```\n    # You get a `list` first, then slice it\n    mnli['train']['premise'][:3]\n```\nor use slice notation first to get a dictionary (which represents a sliced dataset) followed by a feature name.\n```\n    # You get a `dictionary` (of lists) first, then a list\n    mnli['train'][:3]['premise']\n```\n\nThe results will be the same.\n\nIn order to get the name of the classes, you can do\n\n```\nmnli['train'].features['label'].names\n```","8206187e":"#### The Stanford Natural Language Inference Corpus (SNLI)\n\nFirst, let's load the [The Stanford Natural Language Inference Corpus (SNLI)](https:\/\/nlp.stanford.edu\/projects\/snli\/). It contains $570000$ sentence pairs annotated with textual entailment information.","4a2a044d":"<center><img src=\"https:\/\/raw.githubusercontent.com\/chiapas\/kaggle\/master\/competitions\/contradictory-my-dear-watson\/header.png\" width=\"1000\"><\/center>\n<br>\n<center><h1>Detecting contradiction and entailment in multilingual text using TPUs<\/h1><\/center>\n<br>\n\n#### Natural Language Inferencing (NLI) is a classic NLP (Natural Language Processing) problem that involves taking two sentences (the _premise_ and the _hypothesis_ ), and deciding how they are related- if the premise entails the hypothesis, contradicts it, or neither.\n\n#### In this notebook, we will use more NLI datasets, including\n\n* [The Stanford Natural Language Inference Corpus (SNLI)](https:\/\/nlp.stanford.edu\/projects\/snli\/)\n* [The Multi-Genre NLI Corpus (MultiNLI, MNLI)](https:\/\/cims.nyu.edu\/~sbowman\/multinli\/)\n* [Cross-lingual NLI Corpus (XNLI)](https:\/\/cims.nyu.edu\/~sbowman\/xnli\/)\n\n#### We will also use Hugging Face recent library [nlp](https:\/\/huggingface.co\/nlp\/) to work with these datasets.\n\n## Update:\n### Since it is found that (some of) the test examples comes from these external datasets, I remove the training part in this notebook in order not to encourage using them.","fed9802a":"#### Convert MNLI to pandas.DataFrame","cc8b7114":"#### Import","5dfb27c2":"Let's use what we learned to check some training examples","92140ec0":"The class names are still\n```\n    ['entailment', 'neutral', 'contradiction'],\n```\nhowever, the features `premise` and `hypothesis` are no longer `string` but `dictionary` which contain sentences in different language! "}}