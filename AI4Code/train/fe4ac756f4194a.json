{"cell_type":{"c3b88237":"code","e4bb24dd":"code","b5276caf":"code","47eb635b":"code","852397ac":"code","1653576d":"code","95e3a1d7":"code","4497aeec":"code","83464b5a":"code","c317bc27":"code","3dba70b0":"code","ffc0b589":"code","792863f4":"code","3fd876cb":"code","d8d520c1":"code","83c7bb88":"code","8b40cc19":"code","a9a6bea0":"code","28cec814":"code","72fcd1ab":"code","4e3358de":"code","30a43e6e":"code","9df643d6":"code","43a5c8b2":"code","395f4eb3":"code","84df1240":"code","5d34d5f2":"code","a77622bb":"code","88d71c90":"code","7df48f7c":"code","2f360806":"code","cd5234a1":"code","3cda1900":"code","97f5c55d":"code","1feaa3c6":"code","351f4552":"code","f947864e":"code","82940dc7":"code","90d5140a":"code","768ea0a8":"code","54ca29ab":"code","d39d9f4f":"code","6ba630de":"code","fee1458c":"code","49c729af":"code","76e8f3fa":"code","ab9a11e1":"code","f9afb5df":"code","94b7a5f1":"code","153f328f":"code","1eea3d59":"code","1d2f0e71":"code","bf6c07bc":"code","344d42d7":"code","2d69f474":"code","78cfda55":"code","0088674b":"code","76a2c08e":"code","58891732":"code","f86bb53e":"code","55e5c53a":"code","7c632758":"code","469241b3":"code","0c654e4f":"code","b9de3ad8":"code","be4a8033":"code","4dfd4e6f":"markdown","e684d564":"markdown","a2265fb7":"markdown","078cd94d":"markdown","aba5a862":"markdown","ad81ece8":"markdown","0f417209":"markdown","d4d13eaf":"markdown","4d0bc494":"markdown","6e5ceb16":"markdown","25550aaf":"markdown","446a9194":"markdown","402979bf":"markdown","a6ff038d":"markdown","7769f9fe":"markdown","d0724a88":"markdown","baa8215b":"markdown","8b935814":"markdown","aa260817":"markdown","d30bd296":"markdown","612bb4b7":"markdown","9831a225":"markdown","ae36dee6":"markdown","3ae9b508":"markdown","a245d801":"markdown","e3d78e6b":"markdown","0462aaff":"markdown","e177f556":"markdown","ef22590f":"markdown","d8816432":"markdown","e78bdfd1":"markdown","5939c910":"markdown","663e7edf":"markdown","475ae46a":"markdown","5abc5b00":"markdown","ba5cfe9a":"markdown","823547bd":"markdown","576c0294":"markdown","eef7a65c":"markdown"},"source":{"c3b88237":"# lets import all the required libraries\n\n# for mathematical operations\nimport numpy as np\n# for dataframe operations\nimport pandas as pd\n\n# for data visualizations\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# for machine learning\nimport sklearn\nimport imblearn\n\n# setting up the size of the figures\nplt.rcParams['figure.figsize'] = (16, 5)\n# setting up the style of the plot\nplt.style.use('fivethirtyeight')","e4bb24dd":"import os\nprint(os.listdir(\"..\/input\"))","b5276caf":"# reading the datasets\n\ntrain = pd.read_csv('..\/input\/hr-ana\/train.csv')\ntest = pd.read_csv('..\/input\/hr-ana\/test.csv')","47eb635b":"# lets check the shape of the train and test datasets\nprint(\"Shape of the Training Data :\", train.shape)\nprint(\"Shape of the Test Data :\", test.shape)","852397ac":"# columns in Training Data\ntrain.columns","1653576d":"# columns in Testing Data\ntest.columns","95e3a1d7":"# lets check the head of the dataset\ntrain.head()","4497aeec":"# lets check the head of the test data\ntest.head()","83464b5a":"# lets also check the tail of the test data\ntrain.tail()","c317bc27":"# lets also check the tail of the test data\ntest.tail()","3dba70b0":"# values in Departments\n\ntrain['department'].value_counts()","ffc0b589":"# values in Region\n\ntrain['region'].value_counts()","792863f4":"# lets check descriptive statistics for numerical columns\ntrain.describe().style.background_gradient(cmap = 'copper')","3fd876cb":"# lets check descriptive statistics for categorical columns\ntrain.describe(include = 'object')","d8d520c1":"# lets check the Target Class Balance\n\nplt.rcParams['figure.figsize'] = (15, 5)\nplt.style.use('fivethirtyeight')\n\nplt.subplot(1, 2, 1)\nsns.countplot(train['is_promoted'],)\n\nplt.xlabel('Promoted or Not?', fontsize = 10)\n\nplt.subplot(1, 2, 2)\ntrain['is_promoted'].value_counts().plot(kind = 'pie', explode = [0, 0.1], autopct = '%.2f%%', startangle = 90,\n                                       labels = ['1','0'], shadow = True, pctdistance = 0.5)\nplt.axis('off')\n\nplt.suptitle('Target Class Balance', fontsize = 15)\nplt.show()","83c7bb88":"# missing values in training data set\n\n# lets calculate the total missing values in the dataset\ntrain_total = train.isnull().sum()\n\n# lets calculate the percentage of missing values in the dataset\ntrain_percent = ((train.isnull().sum()\/train.shape[0])*100).round(2)\n\n# lets calculate the total missing values in the dataset\ntest_total = test.isnull().sum()\n\n# lets calculate the percentage of missing values in the dataset\ntest_percent = ((test.isnull().sum()\/test.shape[0])*100).round(2)\n\n# lets make a dataset consisting of total no. of missing values and percentage of missing values in the dataset\ntrain_missing_data = pd.concat([train_total, train_percent, test_total, test_percent],\n                                axis=1, \n                                keys=['Train_Total', 'Train_Percent %','Test_Total', 'Test_Percent %'],\n                                sort = True)\n\n# lets check the head\ntrain_missing_data.style.bar(color = ['gold'])","8b40cc19":"# lets impute the missing values in the Training Data\n\ntrain['education'] = train['education'].fillna(train['education'].mode()[0])\ntrain['previous_year_rating'] = train['previous_year_rating'].fillna(train['previous_year_rating'].mode()[0])\n\n# lets check whether the Null values are still present or not?\nprint(\"Number of Missing Values Left in the Training Data :\", train.isnull().sum().sum())","a9a6bea0":"# lets impute the missing values in the Testing Data\n\ntest['education'] = test['education'].fillna(test['education'].mode()[0])\ntest['previous_year_rating'] = test['previous_year_rating'].fillna(test['previous_year_rating'].mode()[0])\n\n# lets check whether the Null values are still present or not?\nprint(\"Number of Missing Values Left in the Training Data :\", test.isnull().sum().sum())","28cec814":"# Lets first analyze the Numberical Columns\ntrain.select_dtypes('number').head()","72fcd1ab":"# lets check the boxplots for the columns where we suspect for outliers\nplt.rcParams['figure.figsize'] = (15, 5)\nplt.style.use('fivethirtyeight')\n\n# Box plot for average training score\nplt.subplot(1, 2, 1)\nsns.boxplot(train['avg_training_score'], color = 'red')\nplt.xlabel('Average Training Score', fontsize = 12)\nplt.ylabel('Range', fontsize = 12)\n\n# Box plot for length of service\nplt.subplot(1, 2, 2)\nsns.boxplot(train['length_of_service'], color = 'red')\nplt.xlabel('Length of Service', fontsize = 12)\nplt.ylabel('Range', fontsize = 12)\n\nplt.suptitle('Box Plot', fontsize = 20)\nplt.show()","4e3358de":"# lets remove the outliers from the length of service column\n\ntrain = train[train['length_of_service'] > 13]","30a43e6e":"# lets plot pie chart for the columns where we have very few categories\nplt.rcParams['figure.figsize'] = (16,5)\nplt.style.use('fivethirtyeight')\n\n# plotting a pie chart to represent share of Previous year Rating of the Employees\n\"\"\"\nplt.subplot(1, 3, 1)\nlabels = ['0','1']\nsizes = train['KPIs_met >80%'].value_counts()\ncolors = plt.cm.Wistia(np.linspace(0, 1, 5))\nexplode = [0, 0]\n\nplt.pie(sizes, labels = labels, colors = colors, explode = explode, shadow = True, startangle = 90)\nplt.title('KPIs Met > 80%', fontsize = 20)\n\"\"\"\n\n# plotting a pie chart to represent share of Previous year Rating of the Employees\nplt.subplot(1, 3, 2)\nlabels = ['1', '2', '3', '4', '5']\nsizes = train['previous_year_rating'].value_counts()\ncolors = plt.cm.Wistia(np.linspace(0, 1, 5))\nexplode = [0, 0, 0, 0, 0.1]\n\nplt.pie(sizes, labels = labels, colors = colors, explode = explode, shadow = True, startangle = 90)\nplt.title('Previous year Ratings', fontsize = 20)\n\n# plotting a pie chart to represent share of Previous year Rating of the Employees\nplt.subplot(1, 3, 3)\nlabels = ['0', '1']\nsizes = train['awards_won?'].value_counts()\ncolors = plt.cm.Wistia(np.linspace(0, 1, 5))\nexplode = [0,0.1]\n\nplt.pie(sizes, labels = labels, colors = colors, explode = explode, shadow = True, startangle = 90)\nplt.title('Awards Won?', fontsize = 20)\n\n\nplt.legend()\nplt.show()","9df643d6":"# lets check the distribution of trainings undertaken by the employees\n\nplt.rcParams['figure.figsize'] = (17, 4)\nsns.countplot(train['no_of_trainings'], palette = 'spring')\nplt.xlabel(' ', fontsize = 14)\nplt.title('Distribution of Trainings undertaken by the Employees')\nplt.show()","43a5c8b2":"# lets check the Age of the Employees\n\nplt.rcParams['figure.figsize'] = (8, 4)\nplt.hist(train['age'], color = 'black')\nplt.title('Distribution of Age among the Employees', fontsize = 15)\nplt.xlabel('Age of the Employees')\nplt.grid()\nplt.show()","395f4eb3":"# lets check different Departments\n\nplt.rcParams['figure.figsize'] = (12, 6)\nsns.countplot(y = train['department'], palette = 'cividis', orient = 'v')\nplt.xlabel('')\nplt.ylabel('Department Name')\nplt.title('Distribution of Employees in Different Departments', fontsize = 15)\nplt.grid()\n\nplt.show()","84df1240":"# lets check distribution of different Regions\n\nplt.rcParams['figure.figsize'] = (12,15)\nplt.style.use('fivethirtyeight')\nsns.countplot(y = train['region'], palette = 'inferno', orient = 'v')\nplt.xlabel('')\nplt.ylabel('Region')\nplt.title('Different Regions', fontsize = 15)\nplt.xticks(rotation = 90)\nplt.grid()\nplt.show()","5d34d5f2":"# lets plot pie chart for the columns where we have very few categories\nplt.rcParams['figure.figsize'] = (16,5)\n\n# plotting a pie chart to represent share of Previous year Rating of the Employees\nplt.subplot(1, 3, 1)\nlabels = train['education'].value_counts().index\nsizes = train['education'].value_counts()\ncolors = plt.cm.copper(np.linspace(0, 1, 5))\nexplode = [0, 0, 0.1]\n\nplt.pie(sizes, labels = labels, colors = colors, explode = explode, shadow = True, startangle = 90)\nplt.title('Education', fontsize = 20)\n\n# plotting a pie chart to represent share of Previous year Rating of the Employees\nplt.subplot(1, 3, 2)\nlabels = train['gender'].value_counts().index\nsizes = train['gender'].value_counts()\ncolors = plt.cm.copper(np.linspace(0, 1, 5))\nexplode = [0, 0]\n\nplt.pie(sizes, labels = labels, colors = colors, explode = explode, shadow = True, startangle = 90)\nplt.title('Gender', fontsize = 20)\n\n# plotting a pie chart to represent share of Previous year Rating of the Employees\nplt.subplot(1, 3, 3)\nlabels = train['recruitment_channel'].value_counts().index\nsizes = train['recruitment_channel'].value_counts()\ncolors = plt.cm.copper(np.linspace(0, 1, 5))\nexplode = [0,0,0.1]\n\nplt.pie(sizes, labels = labels, colors = colors, explode=explode,shadow = True, startangle = 90)\nplt.title('Recruitment Channel', fontsize = 20)\n\nplt.show()","a77622bb":"# Lets compare the Gender Gap in the promotion\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams['figure.figsize'] = (15, 3)\nx = pd.crosstab(train['gender'], train['is_promoted'])\ncolors = plt.cm.Wistia(np.linspace(0, 1, 5))\nx.div(x.sum(1).astype(float), axis = 0).plot(kind = 'bar', stacked = False, colors = colors)\nplt.title('Effect of Gender on Promotion', fontsize = 15)\nplt.xlabel(' ')\nplt.show()","88d71c90":"# lets compare the effect of different Departments and Promotion\n\nplt.rcParams['figure.figsize'] = (15,4)\nx = pd.crosstab(train['department'], train['is_promoted'])\ncolors = plt.cm.copper(np.linspace(0, 1, 3))\nx.div(x.sum(1).astype(float), axis = 0).plot(kind = 'area', stacked = False, colors = colors)\nplt.title('Effect of Department on Promotion', fontsize = 15)\nplt.xticks(rotation = 20)\nplt.xlabel(' ')\nplt.show()","7df48f7c":"# Effect of Age on the Promotion\n\nplt.rcParams['figure.figsize'] = (15,4)\nsns.boxenplot(train['is_promoted'], train['age'], palette = 'PuRd')\nplt.title('Effect of Age on Promotion', fontsize = 15)\nplt.xlabel('Is the Employee Promoted?', fontsize = 10)\nplt.ylabel('Age of the Employee', fontsize = 10)\nplt.show()","2f360806":"# Department Vs Average Training Score\n\nplt.rcParams['figure.figsize'] = (16, 7)\nsns.boxplot(train['department'], train['avg_training_score'], palette = 'autumn')\nplt.title('Average Training Scores from each Department', fontsize = 15)\nplt.ylabel('Promoted or not', fontsize = 10)\nplt.xlabel('Departments', fontsize = 10)\nplt.show()","cd5234a1":"# lets check the Heat Map for the Data with respect to correlation.\n\nplt.rcParams['figure.figsize'] = (15, 8)\nsns.heatmap(train.corr(), annot = True, linewidth = 0.5, cmap = 'Wistia')\nplt.title('Correlation Heat Map', fontsize = 15)\nplt.show()","3cda1900":"# lets check the relation of Departments and Promotions when they won awards ?\n\nplt.rcParams['figure.figsize'] = (16, 7)\nsns.barplot(train['department'], train['avg_training_score'], hue = train['gender'], palette = 'autumn')\nplt.title('Chances of Promotion in each Department when they have won some Awards too', fontsize = 15)\nplt.ylabel('Promoted or not', fontsize = 10)\nplt.xlabel('Departments', fontsize = 10)\nplt.show()","97f5c55d":"# lets create some extra features from existing features to improve our Model\n\n# creating a Metric of Sum\ntrain['sum_metric'] = train['awards_won?']+train['KPIs_met >80%'] + train['previous_year_rating']\ntest['sum_metric'] = test['awards_won?']+test['KPIs_met >80%'] + test['previous_year_rating']\n\n# creating a total score column\ntrain['total_score'] = train['avg_training_score'] * train['no_of_trainings']\ntest['total_score'] = test['avg_training_score'] * test['no_of_trainings']","1feaa3c6":"# lets remove some of the columns which are not very useful for predicting the promotion.\n\n# we already know that the recruitment channel is very least related to promotion of an employee, so lets remove this column\n# even the region seems to contribute very less, when it comes to promotion, so lets remove it too.\n# also the employee id is not useful so lets remove it.\n\ntrain = train.drop(['recruitment_channel', 'region', 'employee_id'], axis = 1)\ntest = test.drop(['recruitment_channel', 'region', 'employee_id'], axis = 1)\n\n# lets check the columns in train and test data set after feature engineering\ntrain.columns","351f4552":"'''\nlets check the no. of employee who did not get an award, did not acheive 80+ KPI, previous_year_rating as 1\nand avg_training score is less than 40\nbut, still got promotion.\n''' \n\ntrain[(train['KPIs_met >80%'] == 0) & (train['previous_year_rating'] == 1.0) & \n      (train['awards_won?'] == 0) & (train['avg_training_score'] < 60) & (train['is_promoted'] == 1)]","f947864e":"# lets remove the above two columns as they have a huge negative effect on our training data\n\n# lets check shape of the train data before deleting two rows\nprint(\"Before Deleting the above two rows :\", train.shape)\n\ntrain = train.drop(train[(train['KPIs_met >80%'] == 0) & (train['previous_year_rating'] == 1.0) & \n      (train['awards_won?'] == 0) & (train['avg_training_score'] < 60) & (train['is_promoted'] == 1)].index)\n\n# lets check the shape of the train data after deleting the two rows\nprint(\"After Deletion of the above two rows :\", train.shape)","82940dc7":"## Lets check the categorical columns present in the data\ntrain.select_dtypes('object').head()","90d5140a":"# lets check the value counts for the education column\ntrain['education'].value_counts()","768ea0a8":"# lets start encoding these categorical columns to convert them into numerical columns\n\n# lets encode the education in their degree of importance \ntrain['education'] = train['education'].replace((\"Master's & above\", \"Bachelor's\", \"Below Secondary\"),\n                                                (3, 2, 1))\ntest['education'] = test['education'].replace((\"Master's & above\", \"Bachelor's\", \"Below Secondary\"),\n                                                (3, 2, 1))\n\n# lets use Label Encoding for Gender and Department to convert them into Numerical\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ntrain['department'] = le.fit_transform(train['department'])\ntest['department'] = le.fit_transform(test['department'])\ntrain['gender'] = le.fit_transform(train['gender'])\ntest['gender'] = le.fit_transform(test['gender'])\n\n# lets check whether we still have any categorical columns left after encoding\nprint(train.select_dtypes('object').columns)\nprint(test.select_dtypes('object').columns)","54ca29ab":"# lets check the data after encoding\ntrain.head(3)","d39d9f4f":"# lets split the target data from the train data\n\ny = train['is_promoted']\nx = train.drop(['is_promoted'], axis = 1)\nx_test = test\n\n# lets print the shapes of these newly formed data sets\nprint(\"Shape of the x :\", x.shape)\nprint(\"Shape of the y :\", y.shape)\nprint(\"Shape of the x Test :\", x_test.shape)","6ba630de":"# It is very important to resample the data, as the Target class is Highly imbalanced.\n# Here We are going to use Over Sampling Technique to resample the data.\n# lets import the SMOTE algorithm to do the same.\n\nfrom imblearn.over_sampling import SMOTE\n\nx_resample, y_resample  = SMOTE().fit_sample(x, y.values.ravel())\n\n# lets print the shape of x and y after resampling it\nprint(x_resample.shape)\nprint(y_resample.shape)","fee1458c":"# lets also check the value counts of our target variable4\n\nprint(\"Before Resampling :\")\nprint(y.value_counts())\n\nprint(\"After Resampling :\")\ny_resample = pd.DataFrame(y_resample)\nprint(y_resample[0].value_counts())","49c729af":"# lets create a validation set from the training data so that we can check whether the model that we have created is good enough\n# lets import the train_test_split library from sklearn to do that\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_valid, y_train, y_valid = train_test_split(x_resample, y_resample, test_size = 0.2, random_state = 0)\n\n# lets print the shapes again \nprint(\"Shape of the x Train :\", x_train.shape)\nprint(\"Shape of the y Train :\", y_train.shape)\nprint(\"Shape of the x Valid :\", x_valid.shape)\nprint(\"Shape of the y Valid :\", y_valid.shape)\nprint(\"Shape of the x Test :\", x_test.shape)","76e8f3fa":"# It is very import to scale all the features of the dataset into the same scale\n# Here, we are going to use the standardization method, which is very commonly used.\n\n# lets import the standard scaler library from sklearn to do that\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_valid = sc.transform(x_valid)\nx_test = sc.transform(x_test)","ab9a11e1":"# Lets use Decision Trees to classify the data\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier()\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_valid)","f9afb5df":"from sklearn.metrics import confusion_matrix, classification_report\n\nprint(\"Training Accuracy :\", model.score(x_train, y_train))\nprint(\"Testing Accuracy :\", model.score(x_valid, y_valid))\n\ncm = confusion_matrix(y_valid, y_pred)\nplt.rcParams['figure.figsize'] = (3, 3)\nsns.heatmap(cm, annot = True, cmap = 'Wistia', fmt = '.8g')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.show()","94b7a5f1":"# lets take a look at the Classification Report\n\ncr = classification_report(y_valid, y_pred)\nprint(cr)","153f328f":"train.describe()","1eea3d59":"prediction = rfecv.predict(np.array([[2, #department code\n                                      3, #masters degree\n                                      1, #male\n                                      1, #1 training\n                                      30, #30 years old\n                                      5, #previous year rating\n                                      10, #length of service\n                                      1, #KPIs met >80%\n                                      1, #awards won\n                                      95, #avg training score\n                                      7, #sum of metric \n                                      700 #total score\n                                     ]]))\n\nprint(\"Whether the Employee should get a Promotion : 1-> Promotion, and 0-> No Promotion :\", prediction)","1d2f0e71":"max_age = data['age'].max()\nmin_age = data['age'].min()\nprint('Minimum age is {} and Maximum age is  {}'.format(min_age, max_age))","bf6c07bc":"# lets consider all the employees under 30 as young and otherwise as old\n\nyoung_employees = data[data['age'] <= 30]\nold_employees = data[data['age'] > 30]","344d42d7":"# Now Lets check the Percentage of Employees getting Promotion from Young and Old Group of Employees.\n\nyoung_employees['is_promoted'].value_counts()","2d69f474":"# Now Lets check the Percentage of Employees getting Promotion from Young and Old Group of Employees.\n\nold_employees['is_promoted'].value_counts()","78cfda55":"perc_young = 3909\/(40237+3909)  # young_employees[young_employees['is_promorted'] == 1]\/young_employees.shape[0]\nperc_old = 759\/(9909+759)\n\nprint(perc_young, perc_old)","0088674b":"# lets check the count of employees who got promotion \n\naward_wining_employees = data[data['awards_won?'] == 1]\naward_wining_employees['is_promoted'].value_counts()","76a2c08e":"# let check the Probability of the employees getting promoted after wining an award\n\n# probability = award_wining_employees[award_wining_employees['is_promoted'] == 1].shape[0]\/data.shape[0]\nprobability = 559\/(559+711)\nprint(\"The Probability of an Employee to get Promotion is : {0:.2f}%\".format(probability*100))","58891732":"award_wining_employees = data[data['awards_won?'] == 0]\naward_wining_employees['is_promoted'].value_counts()","f86bb53e":"x = 4109\/(4109+49429)\nx","55e5c53a":"promoted_employees = data[data['is_promoted'] == 1]\navg_training_score_promoted_emp = promoted_employees['avg_training_score'].mean()\nprint(\"The Average Training Score for the Employees who got Promotion is {0:.0f}\".format(avg_training_score_promoted_emp))","7c632758":"# lets check the gender gap in total employees\n\ndata['gender'].value_counts()","469241b3":"# lets check the Gender Gap in Promotion\n\npromoted_employees['gender'].value_counts()","0c654e4f":"m_prom = 3201\/38496\nf_prom = 1467\/16312\nprint(m_prom, f_prom)","b9de3ad8":"# lets consider the employees who have worked for less than equal to two years\n\nfreshers = data[(data['length_of_service'] <= 2) & (data['age'] <= 30)]\nfreshers['is_promoted'].value_counts()","be4a8033":"# lets check the Percentage also\n\nprob = 743\/(8057+743)\nprint(\"Probability of a Fresher being Promoted is {0:.2f}%\".format(prob*100))","4dfd4e6f":"## Feature Engineering\n\nFeature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself.\n\n* There are mutliple ways of performing feature engineering.\n* So many people in the Industry consider it the most important step to improve the Model Performance.\n* We should always understand the columns well to make some new features using the old existing features.\n* Let's discuss the ways how we can perform feature engineering\n    * We can perform Feature Engineering by Removing Unnecassary Columns\n    * We can do it by Extracting Features from the Date and Time Features.\n    * We can do it by Extracting Features from the Categorcial Features.\n    * We can do it by Binnning the Numerical and Categorical Features.\n    * We can do it by Aggregating Multiple Features together by using simple Arithmetic operations\n    \n* Here, we are only going to perform Feature Engineering by Aggregating some features together.","e684d564":"## Q3. What is the Average Training Score of those Employees who got Promotion?","a2265fb7":"## Q4. What is the Impact of Gender in Promotions?","078cd94d":"we can easily, see that the Target Class is Highly Imbalanced, and we must balance these classes of Target Class. Most of the Times, when we use Machine Learning Models with Imbalanced Classes, we have very poor Results which are completely biased towards the class having Higher Distribution.","aba5a862":"## Bivariate Analysis\n\nBivariate analysis is one of the simplest forms of quantitative analysis. It involves the analysis of two variables, for the purpose of determining the empirical relationship between them. Bivariate analysis can be helpful in testing simple hypotheses of association.\n\n* Types of Bivariate Analysis\n    * Categorical vs Categorical \n    * Categorical vs Numerical\n    * Numerical vs Numerical\n    \n* First, we will perform Categorical vs Categorical Analysis using Stacked and Grouped Bar Charts with the help of crosstab function.\n* Second, we will perform Categorical vs Numerical Analysis using Bar Charts, Box plots, Strip plots, Swarm plots, Boxen plots, Violin Plots, etc\n* Atlast, we will perform Numerical vs Numerical Analysis using Scatter plots.","ad81ece8":"We, can see that there are some pie charts, we have for representing KPIs, Previous year Ratings, and Awards Won?\n\nAlso, The one Big Pattern is that only some of the employees could reach above 80% of KPIs set.\nMost of the Employees have a very low rating for the previous year, and\nvery few employees, probably 2% of them could get awards for their work, which is normal.","0f417209":"## Q1. Does Older Employees getting more Promotion than Younger Employees?","d4d13eaf":"The abov Countplot, where are checking the distribution of trainings undertaken by the Employee, It is clearly visible that 80 % of the employees have taken the training only once, and there are negligible no. of employees, who took trainings more than thrice.","4d0bc494":"## <center>Predict whether the Employee of an Organization should get Promotion or Not?<\/center>\n\n&nbsp;\n\nYour client is a large MNC and they have 9 broad verticals across the organisation. One of the problem your client is facing is around identifying the right people for promotion (only for manager position and below) and prepare them in time. Currently the process, they are following is:\n    * They first identify a set of employees based on recommendations\/ past performance.\n    * Selected employees go through the separate training and evaluation program for each vertical. These programs are based on he required skill of each vertical\n    * At the end of the program, based on various factors such as training performance, an employee gets the promotion\n\n![image](https:\/\/corehr.files.wordpress.com\/2013\/02\/wrong-promotion1.jpg?w=290)","6e5ceb16":"Here, we can see some obvious results, that is Length of Service, and Age are Highly Correlated,\nAlso, KPIs, and Previous year rating are correlated to some extent, hinting that there is some relation.","25550aaf":"We can see from the above table, that Only two columns have missing values in Train and Test Dataset both. Also, the Percentage of Missing values is around 4 and 7% in education, and previous_year_rating respectively. So, do not have delete any missing values, we can simply impute the values using Mean, Median, and Mode Values. ","446a9194":"It is quite clear that we are not having Outliers in our Dataset, the average training score for most of the Employee lie between 40 to 100, which is a very good distribution, also th mean is 50.\n\nAlso, the Length of service, is not having very disruptive values, so we can keep them for model training. they are not going to harm us a lot.","402979bf":"## Splitting the Data\n\nThis is one of the most Important step to perform Machine Learning Prediction on a Dataset,\nWe have to separate the Target and Independent Columns.\n* We store the Target Variable in y, and then we store the rest of the columns in x, by deleting the target column from the data\n* Also, we are changing the name of test dataset to x_test for ease of understanding.","a6ff038d":"## Univariate Analysis\n\nUnivariate analysis is perhaps the simplest form of statistical analysis. Like other forms of statistics, it can be inferential or descriptive. The key fact is that only one variable is involved. Univariate analysis can yield misleading results in cases in which multivariate analysis is more appropriate.\n\n* This is an Essential step, to understand the variables present in the dataset one by one.\n* First, we will check the Univariate Analysis for Numerical Columns to check for Outliers by using Box plots.\n* Then, we will use Distribution plots to check the distribution of the Numerical Columns in the Dataset.\n* After that we will check the Univariate Analysis for Categorical Columns using Pie charts, and Count plots.\n* We Use Pie charts, when we have very few categories in the categorical column, and we use count plots we have more categorises in the dataset.","7769f9fe":"**Here comes to an end to this session, where we predicted whether an employee belonging to an organization should get a promotion or not using Logistic Regression and Decision Trees.**\n\n**We also used Feature Selection, and Hyper Parameter Tuning to Improve the Model**.\n\n**This is the end of this Project, Now lets understand how to present this project to a client or Managers using Presentations, Reports, Conclussions, and Visualizations**\n\n","d0724a88":"## Treatment of Missing Values\n\n* Treatment of Missing Values is very Important Step in any Machine Learning Model Creation \n* Missing Values can be cause due to varios reasons such as the filling incomplete forms, values not available, etc\n* There are so many types of Missing Values such as \n     * Missing values at Random\n     * Missing values at not Random\n     * Missing Values at Completely Random\n* What can we do to Impute or Treat Missing values to make a Good Machine Learning Model\n    * We can use Business Logic to Impute the Missing Values\n    * We can use Statistical Methods such as Mean, Median, and Mode.\n    * We can use ML Techniques to impute the Missing values\n    * We can delete the Missing values, when the Missing values percentage is very High.\n    \n* When to use Mean, and when to use Median?\n    * We use Mean, when we do not have Outliers in the dataset for the Numerical Variables.\n    * We use Median, when we have outliers in the dataset for the Numerical Variables.\n    * We use Mode, When we have Categorical Variables.","baa8215b":"## Day 5 \n\nIn the Started our Data Science Journey on Day 2, Today is the last day of our Data Science Journey. Today we will learn how to perform Real Time Predictions using the Model which we have created yesterday.\n\nSo, lets get started, \n* First, we we will check the descriptive summary of the data again, so that we can analyze the columns and values which we can provide to the Model as Input and expect the Model to return Output whether the Employee should get a promotion or not.\n\n* Then we will define the value for which we want the predction, and then finally we will predict the values.","8b935814":"\n##  Machine Learning Predictive Modelling\n\nPredictive modeling is a process that uses data and statistics to predict outcomes with data models. These models can be used to predict anything from sports outcomes and TV ratings to technological advances and corporate earnings. Predictive modeling is also often referred to as: Predictive analytics.","aa260817":"## Resampling\n\nResampling is the method that consists of drawing repeated samples from the original data samples. The method of Resampling is a nonparametric method of statistical inference.\n\n* Earlier, in this Problem we noticed that the Target column is Highly Imbalanced, we need to balance the data by using some Statistical Methods.\n* There are many Statistical Methods we can use for Resampling the Data such as:\n    * Over Samping\n    * Cluster based Sampling\n    * Under Sampling.\n    \nOversampling and undersampling in data analysis are techniques used to adjust the class distribution of a data set. These terms are used both in statistical sampling, survey design methodology and in machine learning. Oversampling and undersampling are opposite and roughly equivalent techniques\n    \n* We are going to use Over Sampling. \n* We will not use Under Sampling to avoid data loss.","d30bd296":"we imputed the missing values, using the Mode values, even for the previous year rating, it only seems to be numerical, but in real it's also categorical.\nAfter, Imputing the missing values in the training and testing data set we can see that there are no Null Values left in any of the datasets.\n\nSo, we are Done with the Treatment of the Missing Values.","612bb4b7":"## Outlier Detection\n\nThe presence of outliers in a classification or regression dataset can result in a poor fit and lower predictive modeling performance. Instead, automatic outlier detection methods can be used in the modeling pipeline and compared, just like other data preparation transforms that may be applied to the dataset.","9831a225":"## Real Time Prediction","ae36dee6":"Here, the Box plot, helps us to analyze the middle 50 percentile of the data, and we can clearly check the minimum, maximum, median, and outlier values.\n\nIn the Length of service attribute, we can see some points after the Max Value, which can be termed to be as Outliers. We do not need to remove these values, as the values are not very far and Huge.","3ae9b508":"## Feature Scaling\n\nFeature scaling is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step\n![image.png](attachment:image.png)","a245d801":"## Reading the Dataset\n\n* Here, we are having two datasets, i.e., Training and Testing Datasets\n* We will read both the datasets \n* Training Datasets is used to train the Machine learning Models\n* After learning the patterns from the Testing Datasets, We have to predict the Target Variable.","e3d78e6b":"As we have already seen that the Females are in Minority, but when it comes to Promotion, they are competing with their Men Counterparts neck-to-neck. That's a great Inference.","0462aaff":"From, the above pie charts displayed for representing Education, Gender, and Recruitment Channel.\n\nlets infer the Main Highlights\nVery Few employees are actually working only after their Secondary Education, \nObviously Females are again in Minority as compared to their Male Counterparts.\nand the Recruitment Channel, says that the Referred Employees are very less, i.e., most of the employees are recruited either by sourcing, or some other recruitment agencies, sources etc.","e177f556":"# lets perform some Real time predictions on top of the Model that we just created using Decision Tree Classifier\n\n# lets check the parameters we have in our Model\n'''\ndepartment -> The values are from 0 to 8, (Department does not matter a lot for promotion)\neducation -> The values are from 0 to 3 where Masters-> 3, Btech -> 2, and secondary ed -> 1\ngender -> the values are 0 for female, and 1 for male\nno_of_trainings -> the values are from 0 to 5\nage -> the values are from 20 to 60\npreviou_year_rating -> The values are from 1 to 5\nlength_of service -> The values are from 1 to 37\nKPIs_met >80% -> 0 for Not Met and 1 for Met\nawards_won> -> 0-no, and 1-yes\navg_training_score -> ranges from 40 to 99\nsum_metric -> ranges from 1 to 7\ntotal_score -> 40 to 710\n'''","ef22590f":"## Importing all the Required Libraries\n\n* We Import Numpy, Pandas, Matplot, and Seaborn for Data Analysis and Visualizations\n* We import ipywidgets, Sweetviz, ppscore for Exploratory Data Analysis\n* We Import Sklearn, Imblearn for Machine Learning Modelling","d8816432":"## <center>Data Description<\/center>\n\n<table>\n    <tr>\n        <td><b>Variable<\/b><\/td>\n        <td><b>Definition<\/b><\/td>\n    <\/tr>\n    <tr>\n        <td>employee_id<\/td>\n        <td>Unique ID for employee<td>\n    <\/tr>\n    <tr>\n        <td>department<\/td>\n        <td>Department of employee<\/td>\n    <\/tr>\n    <tr>\n        <td>region<\/td>\n        <td>Region of employment (unordered)<\/td>\n    <\/tr>\n    <tr>\n        <td>education<\/td>\n        <td>Education Level<\/td>\n    <\/tr>\n    <tr>\n        <td>gender<\/td>\n        <td>Gender of Employee<\/td>\n    <\/tr>\n    <tr>\n        <td>recruitment_channel<\/td>\n        <td>Channel of recruitment for employee<\/td>\n    <\/tr>\n    <tr>\n        <td>no_of_trainings<\/td>\n        <td>no of other trainings completed in previous year on soft skills, technical skills etc.<\/td>\n    <\/tr>\n    <tr>\n        <td>age<\/td>\n        <td>Age of Employee<\/td>\n    <\/tr>\n    <tr>\n        <td>previous_year_rating<\/td>\n        <td>Employee Rating for the previous year<\/td>\n    <\/tr>\n    <tr>\n        <td>length_of_service<\/td>\n        <td>Length of service in years<\/td>\n    <\/tr>\n    <tr>\n        <td>KPIs_met >80%<\/td>\n        <td>if Percent of KPIs(Key performance Indicators) >80% then 1 else 0<\/td>\n    <\/tr>\n    <tr>\n        <td>awards_won?<\/td>\n        <td>if awards won during previous year then 1 else 0<\/td>\n    <\/tr>\n    <tr>\n        <td>avg_training_score<\/td>\n        <td>Average score in current training evaluations<\/td>\n    <\/tr>\n    <tr>\n        <td>is_promoted\t(Target)<\/td>\n        <td>Recommended for promotion<\/td>\n    <\/tr>\n<\/table>","e78bdfd1":"## Dealing with Categorical Columns\n\nCategorical variables are known to hide and mask lots of interesting information in a data set. It\u2019s crucial to learn the methods of dealing with such variables. If you won\u2019t, many a times, you\u2019d miss out on finding the most important variables in a model. It has happened with me. Initially, I used to focus more on numerical variables. Hence, never actually got an accurate model. But, later I discovered my flaws and learnt the art of dealing with such variables.\n\n* There are various ways to encode categorical columns into Numerical columns\n* This is an Essential Step, as we Machine Learning Models only works with Numerical Values.\n* Here, we are going to use Business Logic to encode the education column\n* Then we will use the Label Encoder, to Department and Gender Columns","5939c910":"## Examining the Data\n\n* This is an Important Step in Data Science and Machine Learning to ensure about the columns, and rows present.\n* First, we will check the shape of the dataset\n* Second, we will check the head, tail, and sample of the datasets\n* Third, we will check the Data Description\n* Then, we will check the Data Types of the columns present in the data.\n* Atlast, we will check the Target Class Balance","663e7edf":"## Multivariate Analysis\n\nMultivariate analysis is based on the principles of multivariate statistics, which involves observation and analysis of more than one statistical outcome variable at a time.\n\n* First, we will use the Correlation Heatmap to check the correlation between the Numerical Columns\n* Then we will check the ppscore or the Predictive Score to check the correlation between all the columns present in the data.\n* Then, we will use Bubble Charts, split Violin plots, Hue with Bivariate Plots.","475ae46a":"### Q2. What is the Probability to get Promoted, If an employeed has won an award?","5abc5b00":"We, also check the Distribution of these attributes after checking the Box Plot so that we can be more clear about the Values present in these columns.","ba5cfe9a":"From, the above chart we can see that almost all the Departments have a very similar effect on Promotion. So, we can consider that all the Departments have a similar effect on the promotion. Also, this column comes out to be lesser important in making a Machine Learning Model, as it does not contribute at all when it comes to Predicting whether the Employee should get Promotion.","823547bd":"## Descriptive Statistics\n\n* Descriptive Statistics is one of the most Important Step to Understand the Data and take out Insights\n* First we will the Descriptive Statistics for the Numerical Columns\n* for Numerical Columns we check for stats such as Max, Min, Mean, count, standard deviation, 25 percentile, 50 percentile, and 75 percentile.\n* Then we will check for the Descriptive Statistics for Categorical Columns\n* for Categorical Columns we check for stats such as count, frequency, top, and unique elements.","576c0294":"## Q5. What is the Probability of Freshers getting Promoted?","eef7a65c":"\n\n### Decision Tree Classifier\n\nA decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.\n\n![image.png](attachment:image.png)"}}