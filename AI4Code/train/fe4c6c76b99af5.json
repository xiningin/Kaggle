{"cell_type":{"9225b187":"code","3206b08b":"code","914eff3b":"code","d7b92287":"code","a3cdebca":"code","a5397ddf":"code","72f3f651":"code","47a0c336":"code","0fd45b34":"code","c563d3b0":"code","1148d0f4":"code","9d6b40bb":"code","97fb012f":"code","d21b7326":"code","28f05890":"code","ec83d218":"code","6476c0ca":"code","38c7d80f":"code","5f2e7fd0":"code","8e4570f8":"code","88980827":"code","cd5f1b85":"code","c493ebfb":"code","c46ef0c9":"code","d4e8fc6a":"code","2a5959c5":"code","9afa9ae4":"code","73122803":"code","3354c5c7":"code","e4c97b74":"code","bf0f9bdb":"code","4f5224a1":"code","51c872ef":"code","97a56dea":"code","2ef26f12":"code","4604fe83":"code","a13ea874":"code","564cadfa":"code","a0201bda":"code","bc95fca5":"code","374d814e":"code","c0c82c36":"code","58a11e8f":"markdown","730398db":"markdown","f105ab5f":"markdown","f82d621f":"markdown","be4937f6":"markdown","0b1669ce":"markdown","f13f136e":"markdown","552badab":"markdown","8e1a2447":"markdown","a8da5907":"markdown","4d897b07":"markdown","a0647eb0":"markdown","1309155f":"markdown","517a6969":"markdown","9f6afee4":"markdown","4f5f92f1":"markdown","7360b962":"markdown","276f2f85":"markdown","fe4651c5":"markdown"},"source":{"9225b187":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport seaborn as sns\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split,KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import KNNImputer\n\nimport lightgbm as lgb\n\nfrom sklearn.feature_selection import mutual_info_classif, chi2, f_classif\nfrom sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score, auc, roc_curve, accuracy_score\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3206b08b":"df = pd.read_csv(\"\/kaggle\/input\/heart-failure-prediction\/heart.csv\")","914eff3b":"df.head()","d7b92287":"df.describe()","a3cdebca":"sns.histplot(x=\"Age\",hue=\"Sex\",data=df)\nprint(df.groupby(\"Sex\")[\"Age\"].count()*(100\/len(df)))\n","a5397ddf":"sns.histplot(x=\"Age\",hue=\"Sex\",data=df[df[\"HeartDisease\"]==1])\nprint(df[df[\"HeartDisease\"]==1].groupby(\"Sex\")[\"Age\"].count()*(100\/len(df[df[\"HeartDisease\"]==1])))\n","72f3f651":"plt.figure(figsize=(8,8))\nsns.heatmap(df.corr(),annot=True)\nplt.show()","47a0c336":"sns.catplot(x=\"Age\",y=\"Cholesterol\",hue=\"HeartDisease\",data=df,aspect=3)\nplt.xlabel(\"Age\",fontsize=20)\nplt.ylabel(\"Cholesterol\",fontsize=20)\nplt.show()","0fd45b34":"sns.regplot(x=\"Age\",y=\"Oldpeak\",data=df)","c563d3b0":"sns.regplot(x=\"Age\",y=\"MaxHR\",data=df)","1148d0f4":"sns.displot(x=\"Oldpeak\",data=df,hue=\"HeartDisease\")","9d6b40bb":"CPT_oh = pd.get_dummies(df[\"ChestPainType\"])\nSex_oh = pd.get_dummies(df[\"Sex\"])\nEx_oh = pd.get_dummies(df[\"ExerciseAngina\"])\nST_oh = pd.get_dummies(df[\"ST_Slope\"])\nrest_oh = pd.get_dummies(df[\"RestingECG\"])","97fb012f":"df.drop([\"ChestPainType\",\"Sex\",\"RestingECG\",\"ExerciseAngina\",\"ST_Slope\"],axis=1,inplace=True)","d21b7326":"df = pd.concat([df,CPT_oh,Sex_oh,ST_oh,Ex_oh,rest_oh],axis=1)","28f05890":"df.head()","ec83d218":"X = df.drop([\"HeartDisease\"],axis=1)\ny = df[\"HeartDisease\"]","6476c0ca":"from sklearn import preprocessing\nX = preprocessing.StandardScaler().fit(X).transform(X)","38c7d80f":"X_train, X_test, y_train, y_test = train_test_split(X,y,\n                                                    test_size=0.25,random_state=42)","5f2e7fd0":"# We will use this at the final to show best algorithm with best score\nmodels_accuracy_scores = {}","8e4570f8":"# Allows us to test parameters of classification algorithms and find the best one\nfrom sklearn.model_selection import GridSearchCV\n# Logistic Regression classification algorithm\nfrom sklearn.linear_model import LogisticRegression\n# Support Vector Machine classification algorithm\nfrom sklearn.svm import SVC\n# Decision Tree classification algorithm\nfrom sklearn.tree import DecisionTreeClassifier\n# K Nearest Neighbors classification algorithm\nfrom sklearn.neighbors import KNeighborsClassifier\n# Random Forest Algorithm\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\n","88980827":"parameters ={\"C\":[0.01,0.1,1],'penalty':['l2'], 'solver':['lbfgs']}\nlr=LogisticRegression()\nlogreg_cv = GridSearchCV(lr,parameters,cv=10) \nlogreg_cv.fit(X_train,y_train)","cd5f1b85":"print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\nprint(\"accuracy :\",logreg_cv.best_score_)","c493ebfb":"y_hat = logreg_cv.predict(X_test)\n","c46ef0c9":"print(classification_report(y_test,y_hat))\nplot_confusion_matrix(logreg_cv, X_test, y_test,cmap=\"binary\") \n","d4e8fc6a":"logreg_f1 = f1_score(y_test,y_hat,average=\"weighted\")\nlogreg_accuracy = logreg_cv.score(X_test,y_test)\nlogreg_recall = recall_score(y_test,y_hat,average=\"binary\")\nmodels_accuracy_scores[\"Logistic Regression\"] = [logreg_f1,logreg_accuracy,logreg_recall]\n","2a5959c5":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn import metrics\nKs = 40\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\n\nfor n in range(1,Ks):\n    \n    #Train Model and Predict  \n    kNN = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat=kNN.predict(X_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n\n    \n    std_acc[n-1]=np.std(yhat==y_test)\/np.sqrt(yhat.shape[0])\n\nmean_acc","9afa9ae4":"plt.plot(range(1,Ks),mean_acc,'g')\nplt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\nplt.fill_between(range(1,Ks),mean_acc - 3 * std_acc,mean_acc + 3 * std_acc, alpha=0.10,color=\"green\")\nplt.legend(('Accuracy ', '+\/- 1xstd','+\/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Neighbors (K)')\nplt.tight_layout()\nplt.show()","73122803":"kNN = KNeighborsClassifier(n_neighbors = 28).fit(X_train,y_train)\ny_hat = kNN.predict(X_test)\nprint(kNN.score(X_test,y_test))\nprint(metrics.accuracy_score(y_test, yhat))\n    \n\n","3354c5c7":"kNN = KNeighborsClassifier(n_neighbors = 28).fit(X_train,y_train)\ny_hat = kNN.predict(X_test)\nprint(classification_report(y_test,y_hat))\nplot_confusion_matrix(kNN, X_test, y_test,cmap=\"binary\") \n    ","e4c97b74":"kNN_f1 = f1_score(y_test,y_hat,average=\"weighted\")\nkNN_accuracy = kNN.score(X_test,y_test)\nkNN_recall = recall_score(y_test,y_hat,average=\"binary\")\nmodels_accuracy_scores[\"K-Nearest Neighbors\"] = [kNN_f1,kNN_accuracy,kNN_recall]","bf0f9bdb":"parameters = {'criterion': ['gini', 'entropy'],\n     'splitter': ['best', 'random'],\n     'max_depth': [2*n for n in range(1,10)],\n     'max_features': ['auto', 'sqrt'],\n     'min_samples_leaf': [1, 2, 4],\n     'min_samples_split': [2, 5, 10]}","4f5224a1":"tree = DecisionTreeClassifier()\ntree_cv = GridSearchCV(tree,parameters,cv=10)\ntree_cv.fit(X_train,y_train)","51c872ef":"print(\"tuned hpyerparameters :(best parameters) \",tree_cv.best_params_)\nprint(\"accuracy :\",tree_cv.best_score_)","97a56dea":"y_hat = tree_cv.predict(X_test)\nprint(classification_report(y_test,y_hat))\nplot_confusion_matrix(tree_cv, X_test, y_test,cmap=\"binary\") ","2ef26f12":"tree_f1 = f1_score(y_test,y_hat,average=\"weighted\")\ntree_accuracy = tree_cv.score(X_test,y_test)\ntree_recall = recall_score(y_test,y_hat,average=\"binary\")\nmodels_accuracy_scores[\"Decision Tree Classifier\"] = [tree_f1,tree_accuracy,tree_recall]","4604fe83":"rfc=RandomForestClassifier(n_estimators=400,random_state = 0,max_features='sqrt',n_jobs=-1)\nrfc.fit(X_train,y_train)","a13ea874":"y_hat = rfc.predict(X_test)\nprint(classification_report(y_test,y_hat))\nplot_confusion_matrix(rfc, X_test, y_test,cmap=\"binary\") ","564cadfa":"rfc_f1 = f1_score(y_test,y_hat,average=\"weighted\")\nrfc_accuracy = rfc.score(X_test,y_test)\nrfc_recall = recall_score(y_test,y_hat,average=\"binary\")\nmodels_accuracy_scores[\"Random Forest Classifier\"] = [rfc_f1,rfc_accuracy,rfc_recall]","a0201bda":"names = [\"F1-Score\",\"Accuracy\",\"Recall\"]\ndf_scores = pd.DataFrame(models_accuracy_scores.values(),columns = names,index=models_accuracy_scores.keys())","bc95fca5":"fig = plt.figure(figsize = (10, 5))\n# creating the bar plot\nsns.barplot(y=df_scores[\"Accuracy\"],x=df_scores.index)\n \nplt.xlabel(\"Methods\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Best Perfomed Method\")\n\nplt.show()","374d814e":"df_scores","c0c82c36":"df_scores[df_scores[\"Accuracy\"]==max(df_scores[\"Accuracy\"])]","58a11e8f":"According to the regplot above, Age affect MaxHR but not too much that as Age increases,MaxHR decreases.","730398db":"**78 percent of the data consists of men**","f105ab5f":"### Splitting Data","f82d621f":"## Logistic Regression","be4937f6":"# Data Exploring & Collection","0b1669ce":"**90 percent of the data -where \"HeartDisease\" is 1- consists of men**\n","f13f136e":"#### Finding Best K","552badab":"## EVALUATION PART","8e1a2447":"Attribute Information Categorical Variables\n\nSex: sex of the patient [M: Male, F: Female]\n\nChestPainType: chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n\nFastingBS: fasting blood sugar [1: if FastingBS > 120 mg\/dl, 0: otherwise]\n\nRestingECG: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n\nExerciseAngina: exercise-induced angina [Y: Yes, N: No]\n\nST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n\n(Target) HeartDisease: output class [1: heart disease, 0: Normal]\n\nNumeric Variables\n\nAge: age of the patient [years]\n\nRestingBP: resting blood pressure [mm Hg]\n\nCholesterol: serum cholesterol [mm\/dl]\n\nMaxHR: maximum heart rate achieved [Numeric value between 60 and 202]\n\nOldpeak: oldpeak = ST [Numeric value measured in depression]","a8da5907":"# Pre-processing: Feature selection\/extraction\n","4d897b07":"## K-Nearest Neighbors Classifier","a0647eb0":"    Age around 60 years are the most affected to Heart Disease. And Age doesnt affect our data on             Cholesterol levels much.","1309155f":"### Normalize Data\n","517a6969":"**According to the displot above; the higher the oldpeak, the higher the ratio of yellow in the bar. (Yellow indicate 1). So, Oldpeak noticeably affects HeartDisease**","9f6afee4":"# DATA MODELL\u0130NG","4f5f92f1":"Oldpeak column has the highest correlation with HeartDisease\n\nMaxHR column has the lowest correlation with HeartDisease","7360b962":"# EDA with Visulization","276f2f85":"## Random Forest Classifier","fe4651c5":"## Decision Tree Classifier"}}