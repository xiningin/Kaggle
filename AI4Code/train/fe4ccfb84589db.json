{"cell_type":{"8d0d9da2":"code","bc9b3194":"code","5e15bcd5":"code","061bfe88":"code","27ac481d":"code","cc4f4e5f":"code","4e5454c6":"code","37207cb0":"code","eb18936f":"code","4f04e3d6":"code","0e29ad5f":"code","5f4f0711":"code","3ee4b488":"code","7bfee117":"code","e701fefd":"code","2617df65":"code","6c97c98f":"code","69a0a01a":"code","c0a4fd80":"code","e01ac160":"markdown","d0b1011e":"markdown"},"source":{"8d0d9da2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bc9b3194":"df1=pd.read_csv(\"\/kaggle\/input\/unstructured-l0-nlp-hackathon\/data.csv\")","5e15bcd5":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport string\nimport nltk","061bfe88":"df1.head()","27ac481d":"df1['text'] =df1['text'].str.replace(\"[^a-zA-Z#]\", \" \")\nstopwords_list = stopwords.words('english')\npunctuations = list(set(string.punctuation))\n\ndef clean_text_initial(text):\n    text = ' '.join([x.lower() for x in word_tokenize(text) if x.lower() not in stopwords_list and len(x)>1])\n    text = ' '.join([x.lower() for x in word_tokenize(text) if x.lower() not in punctuations and len(x)>3])\n    text = ' '.join([x.lower() for x in word_tokenize(text) if nltk.pos_tag([x])[0][1].startswith(\"NN\") or nltk.pos_tag([x])[0][1].startswith(\"JJ\")])\n    return text.strip()\n\ndf1[\"clean_text\"]=df1.text.apply(lambda text:clean_text_initial(str(text)))\ndf1.head()","cc4f4e5f":"from gensim.corpora.dictionary import Dictionary\n\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\nfrom gensim.models import ldamodel","4e5454c6":"cleaned_text_list=df1.clean_text.apply(lambda clean_text:[lemmatizer.lemmatize(tokenized_text) for tokenized_text in word_tokenize(clean_text)])\n\ngensim_dict=Dictionary(cleaned_text_list)\n\ndoc_term_matrix = [gensim_dict.doc2bow(text) for text in cleaned_text_list]\n\nLDA = ldamodel.LdaModel","37207cb0":"from sklearn.model_selection import GridSearchCV\nfrom gensim.sklearn_api import LdaTransformer","eb18936f":"num_topics = 5\n\n# Define Search Param\nsearch_params = {'alpha':np.arange(0,1,0.1) , 'eta': np.arange(0,1,0.1)}\n\n# Init the Model\nlda = LdaTransformer(num_topics=num_topics,id2word=gensim_dict, iterations=10, random_state=1)\n\n# Init Grid Search Class\nmodel = GridSearchCV(lda, param_grid=search_params)\n\n# Do the Grid Search\nmodel.fit(doc_term_matrix)","4f04e3d6":"# Best Model\nbest_lda_model = model.best_estimator_\n\n# Model Parameters\nprint(\"Best Model's Params: \", model.best_params_)\n\n# Log Likelihood Score\nprint(\"Best Log Likelihood Score: \", model.best_score_)","0e29ad5f":"num_topics = 5\n\n# Running and Training LDA model on the document term matrix.\nlda_model = LDA(corpus=doc_term_matrix, num_topics=num_topics, id2word = gensim_dict, passes=10,random_state=1,alpha=0.1,eta=0.4)","5f4f0711":"def get_lda_topics(model, num_topics):\n    word_dict = {}\n    topics = model.show_topics(num_topics,10)\n    word_dict = {'Topic '+str(i):[x.split('*') for x in words.split('+')] \\\n                 for i,words in model.show_topics(num_topics,10)}\n    return pd.DataFrame.from_dict(word_dict)\n\nget_lda_topics(lda_model, 5)","3ee4b488":"df_doc_top = pd.DataFrame()\nfinal_list = []\nfor index in range(len(df1.clean_text)):\n    word_id_dict = dict(lda_model.get_document_topics(doc_term_matrix[index]))\n    word_score_list = []\n    for index in range(num_topics):\n        try:\n            value = word_id_dict[index]\n        except:\n            value = 0\n        word_score_list.append(value)\n    final_list.append(word_score_list)","7bfee117":"df_doc_top = pd.DataFrame(final_list)\ndf_doc_top.columns = ['Topic ' + str(i) for i in range(1, num_topics+1)]\ndf_doc_top.index = ['Document ' + str(i) for i in range(1, len(df1.clean_text)+1)]\ndf_doc_top.head()","e701fefd":"df_doc_top[\"Dominant_Topic\"] = df_doc_top.idxmax(axis=1).tolist()\ndf_doc_top[\"Topic_Probability\"] = df_doc_top.max(axis=1).tolist()\ndocument_df = df_doc_top.reset_index().rename(columns={\"index\":\"Document\"})[[\"Document\",\"Dominant_Topic\",\"Topic_Probability\"]]\ndocument_df","2617df65":"initial_submission=pd.concat([df1.Id,document_df.Dominant_Topic],axis=1)","6c97c98f":"initial_submission.Dominant_Topic=initial_submission.Dominant_Topic.replace({\"Topic 5\":\"Automobiles\",\"Topic 4\":\"room_rentals\",\n                                           \"Topic 3\":\"glassdoor_reviews\",\"Topic 2\":\"sports_news\",\n                                          \"Topic 1\":\"tech_news\"})","69a0a01a":"initial_submission=initial_submission.set_index(\"Id\").rename(columns={\"Dominant_Topic\":\"topic\"})","c0a4fd80":"initial_submission.to_csv(\"initial_submission.csv\")","e01ac160":"1 is sports_news, 2 is glassdoor_reviews, 0 might be tech_news, 1 is uncertain, 3 is house_posting, 4 is Automobile (Missing is Automobiles)","d0b1011e":"# Base Gensim LDA"}}