{"cell_type":{"9631a1d2":"code","13aaf91a":"code","015f1502":"code","51327d12":"code","640dd3e5":"code","359b6759":"code","3d6f4f16":"code","897b0082":"code","0be45f53":"code","1a77f9f7":"code","5d85800e":"code","172b5196":"code","a4e7976d":"code","5aac3279":"code","bb5dbedc":"code","e870b682":"code","8bc9205f":"code","168c4fb2":"code","2b33be0a":"code","20e75229":"code","2631bc62":"code","cdb00349":"code","dc175b01":"code","313d997d":"code","0101ded8":"code","d1112e96":"code","ba8deee8":"code","c958a2bd":"code","197197aa":"code","a2968057":"code","17e6ad69":"code","1232e897":"code","f28548f7":"code","7ead3937":"code","ee249dec":"code","27a47b06":"code","4b5cd533":"code","f0ba6a3b":"code","fffea236":"code","9b624b91":"code","64ca9912":"code","714a11df":"code","3dda043b":"code","73bcb11a":"code","d7a2d946":"code","a95e8fe0":"code","e7ee62b7":"code","ca18fdc2":"code","7392506b":"code","45b81f82":"code","8d606df8":"code","8f8447a3":"code","98511a8c":"code","c9c7ccfc":"code","e40b41b5":"code","f8cf9ce9":"code","cb8a54ef":"code","3b387815":"code","e79f9f36":"code","9c66bc43":"code","b7bfc312":"code","cb1183b9":"code","fc1162b1":"code","ed7c730a":"code","1cf9017a":"code","424d2afb":"code","c7a128e5":"code","0ef3cac2":"code","b8594f08":"code","f18b8073":"code","7cb19e55":"markdown","402476f1":"markdown","604ed147":"markdown","351c98b6":"markdown","64b6f6c3":"markdown","bbee7d97":"markdown","c43eae64":"markdown","09b5ff34":"markdown","e6dfef15":"markdown","bb22c823":"markdown","c6e8f4fa":"markdown","515c1dc7":"markdown","3346bd62":"markdown","bbc6ab6e":"markdown","df429f66":"markdown"},"source":{"9631a1d2":"import pandas as pd\nimport numpy as np\nimport os\n","13aaf91a":"# Objective of the problem : To identify the hatespeech in the twitter tweets","015f1502":"# Reading the train Data #\n\ndf = pd.read_csv(\"..\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv\")\ndf.head()","51327d12":"# Size of the data #\n\ndf.shape\n\n# 31,962 - records and 2 independent features and 1 target variable","640dd3e5":"# Reading the train Data #\n\ndf1 = pd.read_csv(\"..\/input\/twitter-sentiment-analysis-hatred-speech\/test.csv\")\ndf1.head()","359b6759":"# Size of the data #\n\ndf1.shape\n\n# 17,197- records and 2 independent features ","3d6f4f16":"# Types of sentiments : total classes available for target variable (labels) : 2 classes\n\ndf['label'].value_counts()","897b0082":"# Understanding the sentiments in the data #\n\ndf[df['label']==0].head(20) \n\n# Class 0 : shows the list of positive sentiments\n","0be45f53":"# Understanding the sentiments in the data #\n\ndf[df['label']==1].head(20) \n\n# Class 1  : Negative sentiments of the user ","1a77f9f7":"# Check the missing values # - Train\n\ndf.isnull().sum()\n\n# No missing values ","5d85800e":"# We can drop the ID column as it does not have any importance in builiding the model #\n\ndf.drop(['id'],axis=1,inplace=True)\ndf.head()","172b5196":"import seaborn as sns\nsns.heatmap(df.isnull(), yticklabels = False, cbar = False, cmap = 'Blues')\n\n# There no missing values #","a4e7976d":"!pip install WordCloud\n\n# Visualize the most frequent words #","5aac3279":"#  word cloud library #\nfrom wordcloud import WordCloud","bb5dbedc":"# Combining all tje records togather into a list #\n\nsentences = df['tweet'].tolist()\nlen(sentences)","e870b682":"#Joining sentences (combining all the sentences togather into one paragraph through string separation)\nsentences_as_single_string = \" \".join(sentences)","8bc9205f":"import matplotlib.pyplot as plt\nplt.figure(figsize=(20,20))\nplt.imshow(WordCloud().generate(sentences_as_single_string))\n\n# From this plort, we can understand that positive words are more as compared to the negative words, as we already know that the data is \n# data is imbalanced that we habe more records of positive sentiments (29720) against the negative sentiments (2242)\n\n# Size of words shows the frequency of this word in this document #","168c4fb2":"# Analyzing the positive words from this positive sentiments # \n\npositive = df[df['label']==0]['tweet']\npositive","2b33be0a":"# Understanding positive words from positive sentiments #\n\npos = positive.tolist()\nsentences_as_single_positive = \" \".join(pos)\nplt.figure(figsize=(20,20))\nplt.imshow(WordCloud().generate(sentences_as_single_positive))","20e75229":"# Analyzing the negative words from this negative sentiments # \n\nnegative = df[df['label']==1]['tweet']\nnegative","2631bc62":"# Understanding negative words from negative sentiments #\n\nneg = negative.tolist()\nsentences_as_single_negative = \" \".join(neg)\nplt.figure(figsize=(20,20))\nplt.imshow(WordCloud().generate(sentences_as_single_negative))","cdb00349":"# Data Cleaning : Removing the exclaimation, fullstops, commas, hashtags, symbols, hyphen, semicolon, etc. from this tweets\n\n# String.punctuation : All punctuation marks\nimport string\nstring.punctuation","dc175b01":"def clean(text):\n    remv_punc = [char for char in text.lower() if char not in string.punctuation]\n    remv_punc_join = ''.join(remv_punc)\n    \n    return remv_punc_join","313d997d":"clean(' @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run')\n","0101ded8":"tweets_df_clean = df['tweet'].apply(clean)","d1112e96":"tweets_df_clean[6]\n\n# Hence we are able to clean the text data correctly","ba8deee8":"tweets_df_clean","c958a2bd":"!pip install nltk","197197aa":"# Removal of StopWords fromt this text #\n\nimport nltk\nstopwords = nltk.corpus.stopwords.words('english')\nprint(stopwords[:10])\n# Belw is the list of StopWords in english language ","a2968057":"tweets_df = pd.DataFrame(tweets_df_clean)\ntweets_df.columns","17e6ad69":"# stopwords treatment and converting the data into lpwer case #\ndef stop(text):\n    remv_stop = [a for a in text.split() if a.lower() not in stopwords]\n    remv_stop_join = ' '.join(remv_stop)\n    return remv_stop_join","1232e897":"tweets_df['tweet'][0]","f28548f7":"stop(tweets_df['tweet'][0])","7ead3937":"tweets_df_stopwords = tweets_df['tweet'].apply(stop)","ee249dec":"tweets_df_stopwords[:2]\n\n# all stopwords have been removed from this document#","27a47b06":"tweets_df_stopwords = pd.DataFrame(tweets_df_stopwords)\ntweets_df_stopwords","4b5cd533":"# Applying the Stemming to reduce the word to its root form #\n\n#Porter Stemmer : It is used to stem all the words ( inclduing the stopwords as well) so we have removed the stopwords as well\nfrom nltk.stem import PorterStemmer\nst = PorterStemmer()\n\ndef steming(text):\n    ste = [st.stem(word) for word in text.split()]\n    ste_join = ' '.join(ste)\n    return ste_join","f0ba6a3b":"tweets_df_stem = tweets_df_stopwords['tweet'].apply(steming)\n\ntweets_df_stem[:2]\n\n# The dataset has been stemmed to its root word","fffea236":"tweets_df_stopwords['tweet'][0]","9b624b91":"\nfrom nltk.stem import WordNetLemmatizer\n\nwl = WordNetLemmatizer()\n","64ca9912":"import nltk\nnltk.download('wordnet')","714a11df":"\ndef lematize(text):\n    ste = [wl.lemmatize(word) for word in text.split()]\n    ste_join = ' '.join(ste)\n    return ste_join","3dda043b":"lematize('user father dysfunctional selfish drags kids dysfunction run')","73bcb11a":"tweets_df_stopwords.iloc[:2]","d7a2d946":"tweets_df_stem = pd.DataFrame(tweets_df_stem)\ntweets_df_stem.head()","a95e8fe0":"# Applying the Count Vectorizer \n\nfrom sklearn.feature_extraction.text import CountVectorizer \n\ncv = CountVectorizer(max_features=5000)\n\nsen = tweets_df_stem['tweet'].tolist()\nlen(sen)\n\n","e7ee62b7":"from pandas import DataFrame","ca18fdc2":"def document_matrix(text, vectorizer):\n    mat = vectorizer.fit_transform(text)\n    return DataFrame(mat.toarray())","7392506b":"m = document_matrix(sen,cv)\nm.head()","45b81f82":"# Tfidf Vectorizer #\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer \n\n\ntfidf_vec = TfidfVectorizer(max_features=2500)","8d606df8":"n = document_matrix(sen,tfidf_vec)\nn.head()","8f8447a3":"n.head()","98511a8c":"# splitting the data #\n\n# Independent and dependent vcariables #\n\ny= df['label']\ny.head()","c9c7ccfc":"# splitting the data #\n\n# 1st Model : We will use the Count Vectorizer document\n\n\n# Splitting the data into test and train #\n\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(m,y,test_size=0.33,random_state=25)","e40b41b5":"x_train.shape","f8cf9ce9":"y_train.shape","cb8a54ef":"#Optmization of the Model, through different hyper parameter tuning \n# Model 1 : Using the Count Vectorizer and stemming technique\n# Model 2 : Using the TFIDF Vectorizer and stemming technique","3b387815":"# Model Building #\n# Applying the Naive Bayes Algorithm #\n\nfrom sklearn.naive_bayes import MultinomialNB\nNaiveBclassifier = MultinomialNB()\nNaiveBclassifier.fit(x_train,y_train)","e79f9f36":"# Predicting train cases\ny_pred_train = NaiveBclassifier.predict(x_train)","9c66bc43":"from sklearn.metrics import accuracy_score","b7bfc312":"#Accuract Score #\n\nacc = accuracy_score(y_train, y_pred_train)\nacc","cb1183b9":"# Predicting test cases\ny_pred_test = NaiveBclassifier.predict(x_test)","fc1162b1":"# Confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred_test)\ncm","ed7c730a":"from sklearn.metrics import accuracy_score, confusion_matrix\n\nacc = accuracy_score(y_test, y_pred_test)\nacc","1cf9017a":"# splitting the data #\n\n# 2nd Model : We will use the TFIDF Vectorizer document\n\n# Splitting the data into test and train #\n\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(n,y,test_size=0.33,random_state=25)","424d2afb":"# Model Building #\n# 2nd Model : We will use the TFIDF Vectorizer document\n\n# Applying the Naive Bayes Algorithm #\n\nfrom sklearn.naive_bayes import MultinomialNB\nNaiveBclassifier = MultinomialNB()\nNaiveBclassifier.fit(x_train,y_train)","c7a128e5":"# Predicting train cases\ny_pred_train = NaiveBclassifier.predict(x_train)","0ef3cac2":"#Accuract Score #\n\nacc = accuracy_score(y_train, y_pred_train)\nacc","b8594f08":"# Predicting test cases\ny_pred_test = NaiveBclassifier.predict(x_test)","f18b8073":"acc = accuracy_score(y_test, y_pred_test)\nacc","7cb19e55":"# Model 2 : Using the TFIDF Vectorizer and stemming technique","402476f1":"# EDA - Understanding the twitter sentiment data #","604ed147":"# Model 1 :  Using the Count Vectorizer and stemming technique","351c98b6":"# Splitting the data into dependent and independent variables","64b6f6c3":"# Data Preprocessing Steps #","bbee7d97":"# Applying Stemming ","c43eae64":"# Removing stopwords","09b5ff34":"# Postive Words # \n\nSmile, good, great, beautiful, happy, love, etc.","e6dfef15":"# Reading the data and understanding the problem Statement ","bb22c823":"# Applying Lemmatization","c6e8f4fa":"# Negative Words - Hate Speech # \n\nbigot, racist,hate, black, condemn, etc.","515c1dc7":"# Advanced preprocessing techniques : CountVectorizer , TfidfVectorizer","3346bd62":"# Removing Punctuation","bbc6ab6e":"# Problem Statement : Classify the tweets into positive or a negative sentiments (hate speech)","df429f66":"# Model can give an accuracy of 94.92% on any randomly selected data "}}