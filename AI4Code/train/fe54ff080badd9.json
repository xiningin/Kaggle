{"cell_type":{"e2c289e9":"code","36428bae":"code","910d28c4":"code","763d6395":"code","2a26fb5b":"code","5f35cad9":"code","ee5c4ca4":"code","24d72dd1":"code","b6bd5401":"code","86a7a730":"code","e05a63f3":"code","892268c3":"code","cf49011e":"code","8c41d606":"code","0b433d8c":"code","31eb4710":"code","c2de08df":"markdown","e5c00b67":"markdown","2b242283":"markdown","de0ea8d8":"markdown","e73a6271":"markdown","97d7b76f":"markdown","49e054d4":"markdown","7a792835":"markdown","4c669df1":"markdown","0f85721f":"markdown","6ee62b0a":"markdown","7ddd68f7":"markdown","d514f840":"markdown"},"source":{"e2c289e9":"!pip install keras==2.2.4","36428bae":"import os\nimport sys\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport cv2\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom itertools import chain\nimport skimage\nfrom PIL import Image\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.util import crop, pad\nfrom skimage.morphology import label\nfrom skimage.color import rgb2gray, gray2rgb, rgb2lab, lab2rgb\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n\nfrom keras.models import Model, load_model,Sequential\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Dense, UpSampling2D, RepeatVector, Reshape\nfrom keras.layers.core import Dropout, Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras import backend as K\nimport keras\n\nimport tensorflow as tf","910d28c4":"IMG_WIDTH = 256\nIMG_HEIGHT = 256\nIMG_CHANNELS = 3\nINPUT_SHAPE=(IMG_HEIGHT, IMG_WIDTH, 1)\nTRAIN_PATH = '..\/input\/art-images-drawings-painting-sculpture-engraving\/dataset\/dataset_updated\/training_set\/drawings'\n\ntrain_ids = next(os.walk(TRAIN_PATH))[2]","763d6395":"all_images = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nmissing_count = 0\nprint('Getting train images ... ')\nsys.stdout.flush()\nfor n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n    path = TRAIN_PATH + id_+''\n    try:\n        img = imread(path)\n        img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n        all_images[n-missing_count] = img\n    except:\n        missing_count += 1\n\nall_images = all_images.astype('float32') \/ 255. #Normailize data v\u1ec1 kho\u1ea3ng 0-1\nprint(\"Total missing: \"+ str(missing_count))","2a26fb5b":"X_train, X_test = train_test_split(all_images, test_size=0.2)","5f35cad9":"X_train.shape","ee5c4ca4":"inceptionresnet = InceptionResNetV2(weights=None, include_top=True)\ninceptionresnet.load_weights('..\/input\/inception-resnet-v2-weights\/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5')\ninceptionresnet.graph = tf.compat.v1.get_default_graph()","24d72dd1":"def model():\n    \n    #L\u1ea5y 100 \u0111\u1eb7c tr\u01b0ng t\u1eeb InceptionResnet\n    embed_input = Input(shape=(1000,))\n    \n    #Encoder\n    en_input = Input(shape=(256, 256, 1,))\n    en_output = Conv2D(128, (3,3), activation='relu', padding='same',strides=1)(en_input)\n    en_output = MaxPooling2D((2, 2), padding='same')(en_output)\n    en_output = Conv2D(128, (4,4), activation='relu', padding='same')(en_output)\n    en_output = Conv2D(128, (3,3), activation='relu', padding='same',strides=1)(en_output)\n    en_output = MaxPooling2D((2, 2), padding='same')(en_output)\n    en_output = Conv2D(256, (4,4), activation='relu', padding='same')(en_output)\n    en_output = Conv2D(256, (3,3), activation='relu', padding='same',strides=1)(en_output)\n    en_output = MaxPooling2D((2, 2), padding='same')(en_output)\n    en_output = Conv2D(256, (4,4), activation='relu', padding='same')(en_output)\n    en_output = Conv2D(256, (3,3), activation='relu', padding='same')(en_output)\n    en_output = Conv2D(256, (3,3), activation='relu', padding='same')(en_output)\n    \n    #Fusion: k\u1ebft h\u1ee3p Encoder layer v\u1edbi c\u00e1c \u0111\u1eb7c tr\u01b0ng \u0111c tr\u00edch xu\u1ea5t t\u1eeb InceptionResnet\n    fu_output = RepeatVector(32 * 32)(embed_input) \n    fu_output = Reshape(([32, 32, 1000]))(fu_output) #Reshape \u0111\u1ec3 gi\u1ed1ng vector \u0111\u1eb7c tr\u01b0ng t\u1eeb InceptionResnet\n    fu_output = concatenate([en_output, fu_output], axis=3) #k\u1ebft h\u1ee3p\n    fu_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fu_output)\n    \n    #Decoder\n    de_output = Conv2D(128, (3,3), activation='relu', padding='same')(en_output)\n    de_output = Conv2D(64, (3,3), activation='relu', padding='same')(de_output)\n    de_output = UpSampling2D((2, 2))(de_output)\n    de_output = Conv2D(128, (3,3), activation='relu', padding='same')(de_output)\n    de_output = UpSampling2D((2, 2))(de_output)\n    de_output = Conv2D(64, (4,4), activation='relu', padding='same')(de_output)\n    de_output = Conv2D(64, (3,3), activation='relu', padding='same')(de_output)\n    de_output = Conv2D(32, (2,2), activation='relu', padding='same')(de_output)\n    de_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(de_output)\n    de_output = UpSampling2D((2, 2))(de_output)\n    \n    final_model = Model(inputs=[en_input, embed_input], outputs=de_output)\n    return final_model\n\nmodel = model()\nmodel.compile(optimizer='adam', loss='mean_squared_error')\nmodel.summary()","b6bd5401":"%%time\n\n#ImageDataGenerator takes the original images and transforms it and returns the transformed image. \n#It ensures that each image will never be the same which improves the learning rate\n#We are defining here how those are going to be transformed\nimage_generator = ImageDataGenerator(\n        shear_range=0.3,\n        zoom_range=0.3,\n        rotation_range=25,\n        horizontal_flip=True)\n\n#Create embedding\ndef img_embedding(gray_img):\n    def resize_(x):\n        #We are resizing because inception_resnet takes this shape as input\n        return resize(x, (299, 299, 3), mode='constant')\n    \n    gray_img_resized = np.array([resize_(x) for x in gray_img])\n    gray_img_resized = preprocess_input(gray_img_resized)\n    with inceptionresnet.graph.as_default():\n        #Here we are extracting features using inception_resnet\n        embed = inceptionresnet.predict(gray_img_resized)\n    return embed\n\n#Generate training data\ndef image_generate(dataset=X_train, batch_size = 20):\n    for batch in image_generator.flow(dataset, batch_size=batch_size):\n        \n        #We are converting images to LAB color space\n        X_batch = rgb2gray(batch)\n        gray_rgb = gray2rgb(X_batch)\n        lab_batch = rgb2lab(batch)\n        \n        #This is how we extract only grayscale part from the image (Luminance)\n        X_batch = lab_batch[:,:,:,0]\n        X_batch = X_batch.reshape(X_batch.shape+(1,))\n        \n        #we extract a and b channel from the image (AB channel of LAB)\n        #Wer are normalizing this by dividing 128\n        #Becasue A\/B channels ranges from -127 to +128\n        Y_batch = lab_batch[:,:,:,1:] \/ 128\n        yield [X_batch, img_embedding(gray_rgb)], Y_batch\n        ","86a7a730":"# It is for leatning rate optimizer\n# If no improvement is seen by monitoring loss for a 3 (patience) epochs, the learning rate is reduced.\nlearning_rate = ReduceLROnPlateau(monitor='loss', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5,\n                                            min_lr=0.0001)\nfilepath = \"colorized_model.h5\"\n\n#We are defining here to save that model only where the loss in minimum\nmodel_checkpoint = ModelCheckpoint(filepath,save_best_only=True,monitor='loss',mode='min')\n\n#This will be passed as callback while training\nmodel_callbacks = [learning_rate,model_checkpoint]","e05a63f3":"%%time\nbatch = 20\n\ngenerated_training_data = image_generate(X_train,batch)\n\nmodel.fit_generator(generated_training_data, epochs=30, verbose=1,\n                    steps_per_epoch=X_train.shape[0]\/batch, callbacks=model_callbacks\n                   )","892268c3":"model.save(filepath)\nmodel.save_weights(\"colorized_model_weights.h5\")","cf49011e":"model = load_model(\"..\/input\/colorized-model\/colorized_model.h5\")","8c41d606":"from math import log10, sqrt\ndef PSNR(original, reconstructed_image):\n    mse = np.mean((original - reconstructed_image) ** 2)\n    \n    if(mse == 0):       \n        return 100\n    \n    max_pixel_value = 255.0\n    psnr = 20 * log10(max_pixel_value \/ sqrt(mse))\n    \n    return psnr,mse","0b433d8c":"# Load the test images\nsample = X_test\noriginal_color_images = sample\n\n# T\u1ea1o embedding d\u1ef1a tr\u00ean InceptionRestnetv2\ncolor_me_embed = img_embedding(sample)\n\nrgb_to_lab = rgb2lab(sample)\n\n# L\u1ea5y k\u00eanh L\ncolor_me = rgb_to_lab[:,:,:,0]\ncolor_me = color_me.reshape(color_me.shape+(1,))\n\noutput = model.predict(color_me)\n# Scaling v\u1ec1 l\u1ea1i (Lab ch\u1ea1y t\u1eeb -128 \u0111\u1ebfn 127)\noutput = output * 128\n\ndecoded_imgs = np.zeros((len(output),256, 256, 3))\n\n\nfor i in range(len(output)):\n    resconstructed_image = np.zeros((256, 256, 3))\n    \n    #g\u1eafn k\u00eanh L\n    resconstructed_image[:,:,0] = color_me[i][:,:,0]\n    \n    #g\u1eafn k\u00eanh ab\n    resconstructed_image[:,:,1:] = output[i]\n    \n    # Combine k\u00eanh L v\u00e0 k\u00eanh ab \u0111\u1ec3 t\u1ea1o \u1ea3nh ho\u00e0n ch\u1ec9nh sau \u0111\u00f3 chuy\u1ec3n sang h\u1ec7 RGB\n    decoded_imgs[i] = lab2rgb(resconstructed_image)\n    \n    # \u0110\u00e1nh gi\u00e1\n    pnsr,mse = PSNR(X_test[i],decoded_imgs[i])\n    print(\"PNSR: \"+str(pnsr)+' MSE: '+str(mse))","31eb4710":"plt.figure(figsize=(20, 6))\nfor i in range(10):\n    # \u1ea2nh gray\n    plt.subplot(3, 10, i + 1)\n    plt.imshow(rgb2gray(X_test)[i].reshape(256, 256))\n    plt.gray()\n    plt.axis('off')\n \n    # Predict\n    plt.subplot(3, 10, i + 1 +10)\n    plt.imshow(decoded_imgs[i].reshape(256, 256,3))\n    plt.axis('off')\n    \n    # g\u1ed1c\n    plt.subplot(3, 10, i + 1 + 20)\n    plt.imshow(X_test[i].reshape(256, 256,3))\n    plt.axis('off')\n \nplt.tight_layout()\nplt.show()","c2de08df":"# Train the Model","e5c00b67":"# Data Preaparation for Training","2b242283":"# Load InceptionRestnet V2","de0ea8d8":"# Automatic Evaluation\nT\u00ednh Meand Square Error (MSE) v\u00e0 Peak Signal to Noise Ratio (PSNR) \n\n![image.png](attachment:cc3dbd4a-5593-491e-9816-0881303e8568.png)\n\n![image.png](attachment:d78e28af-59c1-4487-ab7a-69c663ea6100.png)","e73a6271":"# Sample the Results","97d7b76f":"# Checkpoints","49e054d4":"# Load model\n","7a792835":"# Get data","4c669df1":"# Import Necessary Library","0f85721f":"# Displaying the Result (Input-Prediction-Original)\n1. First Row Input\n2. Second Row Reconstructed Image\n3. Third Row Ground Truth","6ee62b0a":"We are saving here both model and model weights","7ddd68f7":"# Train Test Split for the whole dataset","d514f840":"# Define the CNN Model\n\n![image.png](attachment:9c41f66c-1099-4d38-a909-804461ea04e6.png)"}}