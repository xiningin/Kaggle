{"cell_type":{"f389c274":"code","2320f6ab":"code","e6ce7075":"code","f480a1b6":"code","509b06c7":"code","5028e66b":"code","b4886774":"code","796fbedf":"code","cef7a390":"code","f65650d0":"code","0c6cb66c":"code","ba919a0b":"code","cb43dbe2":"code","b42ce1a7":"code","1390260a":"code","ec224179":"code","3112d1a9":"code","cfed8290":"code","1318eff8":"code","8bf342bd":"code","bf09695e":"code","50e2d68a":"code","56a1a1fc":"code","2836d8a3":"markdown","be7f5607":"markdown","dc99ac0f":"markdown","395d236d":"markdown","08c8f1fe":"markdown","7c2a2c0a":"markdown","525e5ecc":"markdown","d77e1591":"markdown","12ea959e":"markdown","3ef05cbd":"markdown","baa25055":"markdown","6fd2ff43":"markdown","25319760":"markdown","4ac99ee6":"markdown","e897ee50":"markdown","e9bc35f5":"markdown","3e5c537c":"markdown"},"source":{"f389c274":"import pandas as pd\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import cohen_kappa_score\nfrom scipy.stats import mode\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\n\n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom matplotlib import pyplot\nimport shap\nfrom scipy import stats\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import RandomizedSearchCV, KFold","2320f6ab":"os.listdir('..\/input\/data-science-bowl-2019')\n","e6ce7075":"%%time\nkeep_cols = ['event_id', 'game_session', 'installation_id', 'event_count',\n             'event_code','title' ,'game_time', 'type', 'world','timestamp']\ntrain=pd.read_csv('..\/input\/data-science-bowl-2019\/train.csv',usecols=keep_cols)\ntrain_labels=pd.read_csv('..\/input\/data-science-bowl-2019\/train_labels.csv',\n                         usecols=['installation_id','game_session','accuracy_group'])\ntest=pd.read_csv('..\/input\/data-science-bowl-2019\/test.csv',usecols=keep_cols)\nsubmission=pd.read_csv('..\/input\/data-science-bowl-2019\/sample_submission.csv')","f480a1b6":"train.shape,train_labels.shape","509b06c7":"x=train_labels['accuracy_group'].value_counts()\nsns.barplot(x.index,x)","5028e66b":"not_req=(set(train.installation_id.unique()) - set(train_labels.installation_id.unique()))","b4886774":"train_new=~train['installation_id'].isin(not_req)\ntrain.where(train_new,inplace=True)\ntrain.dropna(inplace=True)\ntrain['event_code']=train.event_code.astype(int)","796fbedf":"def extract_time_features(df):\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    df['month'] = df['timestamp'].dt.month\n    df['hour'] = df['timestamp'].dt.hour\n    df['year'] = df['timestamp'].dt.year\n    df['dayofweek'] = df['timestamp'].dt.dayofweek\n    df['weekofyear'] = df['timestamp'].dt.weekofyear\n    return df","cef7a390":"time_features=['month','hour','year','dayofweek','weekofyear']\ndef prepare_data(df):\n    df=extract_time_features(df)\n    \n    df=df.drop('timestamp',axis=1)\n    #df['timestamp']=pd.to_datetime(df['timestamp'])\n    #df['hour_of_day']=df['timestamp'].map(lambda x : int(x.hour))\n    \n\n    join_one=pd.get_dummies(df[['event_code','installation_id','game_session']],\n                            columns=['event_code']).groupby(['installation_id','game_session'],\n                                                            as_index=False,sort=False).agg(sum)\n\n    agg={'event_count':sum,'game_time':['sum','mean'],'event_id':'count'}\n\n    join_two=df.drop(time_features,axis=1).groupby(['installation_id','game_session']\n                                                   ,as_index=False,sort=False).agg(agg)\n    \n    join_two.columns= [' '.join(col).strip() for col in join_two.columns.values]\n    \n\n    join_three=df[['installation_id','game_session','type','world','title']].groupby(\n                ['installation_id','game_session'],as_index=False,sort=False).first()\n    \n    join_four=df[time_features+['installation_id','game_session']].groupby(['installation_id',\n                'game_session'],as_index=False,sort=False).agg(mode)[time_features].applymap(lambda x: x.mode[0])\n    \n    join_one=join_one.join(join_four)\n    \n    join_five=(join_one.join(join_two.drop(['installation_id','game_session'],axis=1))). \\\n                        join(join_three.drop(['installation_id','game_session'],axis=1))\n    \n    return join_five\n\n","f65650d0":"\njoin_train=prepare_data(train)\ncols=join_train.columns.to_list()[2:-3]\njoin_train[cols]=join_train[cols].astype('int16')\n\n","0c6cb66c":"join_test=prepare_data(test)\ncols=join_test.columns.to_list()[2:-3]\njoin_test[cols]=join_test[cols].astype('int16')","ba919a0b":"cols=join_test.columns[2:-12].to_list()\ncols.append('event_id count')\ncols.append('installation_id')","cb43dbe2":"df=join_test[['event_count sum','game_time mean','game_time sum',\n    'installation_id']].groupby('installation_id',as_index=False,sort=False).agg('mean')\n\ndf_two=join_test[cols].groupby('installation_id',as_index=False,\n                               sort=False).agg('sum').drop('installation_id',axis=1)\n\ndf_three=join_test[['title','type','world','installation_id']].groupby('installation_id',\n         as_index=False,sort=False).last().drop('installation_id',axis=1)\n\ndf_four=join_test[time_features+['installation_id']].groupby('installation_id',as_index=False,sort=False). \\\n        agg(mode)[time_features].applymap(lambda x : x.mode[0])\n","b42ce1a7":"final_train=pd.merge(train_labels,join_train,on=['installation_id','game_session'],\n                                         how='left').drop(['game_session'],axis=1)\n\n#final_test=join_test.groupby('installation_id',as_index=False,sort=False).last().drop(['game_session','installation_id'],axis=1)\nfinal_test=(df.join(df_two)).join(df_three.join(df_four)).drop('installation_id',axis=1)","1390260a":"df=final_train[['event_count sum','game_time mean','game_time sum','installation_id']]. \\\n    groupby('installation_id',as_index=False,sort=False).agg('mean')\n\ndf_two=final_train[cols].groupby('installation_id',as_index=False,\n                                 sort=False).agg('sum').drop('installation_id',axis=1)\n\ndf_three=final_train[['accuracy_group','title','type','world','installation_id']]. \\\n        groupby('installation_id',as_index=False,sort=False). \\\n        last().drop('installation_id',axis=1)\n\ndf_four=join_train[time_features+['installation_id']].groupby('installation_id',as_index=False,sort=False). \\\n        agg(mode)[time_features].applymap(lambda x : x.mode[0])\n\n\n\nfinal_train=(df.join(df_two)).join(df_three.join(df_four)).drop('installation_id',axis=1)","ec224179":"final_train.shape,final_test.shape","3112d1a9":"len(set(final_train.columns) & set(final_test.columns))","cfed8290":"final=pd.concat([final_train,final_test])\nencoding=['type','world','title']\nfor col in encoding:\n    lb=LabelEncoder()\n    lb.fit(final[col])\n    final[col]=lb.transform(final[col])\n    \nfinal_train=final[:len(final_train)]\nfinal_test=final[len(final_train):]\n\n\n    \n","1318eff8":"X_train=final_train.drop('accuracy_group',axis=1)\ny_train=final_train['accuracy_group']","8bf342bd":"X = X_train\nY = final_test.drop('accuracy_group',axis=1)\ny = y_train","bf09695e":"clf_xgb = XGBClassifier(objective = 'multi:softmax', n_jobs = -1)\nparam_dist = {'n_estimators': stats.randint(150, 500),\n              'learning_rate': stats.uniform(0.01, 0.07),\n              'subsample': stats.uniform(0.5, 0.9),\n              'max_depth': [ 5, 6, 7, 8, 9],\n              'colsample_bytree': stats.uniform(0.5, 0.45),\n              'min_child_weight': [1, 2,4,5,6,8,10],\n              'reg_lambda':[1,10,20,30],\n              'reg_alpha': [1,10,20,30]\n             }\nclf = RandomizedSearchCV(clf_xgb, param_distributions = param_dist, n_iter = 10, error_score = 0, verbose = 3, n_jobs = -1)\n\nnumFolds = 5\nfolds = KFold(n_splits = numFolds, shuffle = True)\n\nestimators = []\nresults = np.zeros(len(X))\nscore = 0.0\nfor train_index, test_index in folds.split(X):\n    X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n    y_train, y_test = y.iloc[train_index].values.ravel(), y.iloc[test_index].values.ravel()\n    clf.fit(X_train, y_train)\n\n    estimators.append(clf.best_estimator_)\n    results[test_index] = clf.predict(X_test)\n    score += accuracy_score(y_test, results[test_index])\nscore \/= numFolds","50e2d68a":"pred=clf.predict(Y)\npred = np.array(pred)\npred = pred.astype(int)","56a1a1fc":"sub=pd.DataFrame({'installation_id':submission.installation_id,'accuracy_group':pred})\nsub.to_csv('submission.csv',index=False)\n","2836d8a3":"In this step,we will \n-  prepare train by merging  our train to train_labels.This will be our `final_train`.\n-  prepare the test by selecting last row of each installation_id ,game_session as we have only 1000 rows in `sample_submission`.The last accuracy group for each installation id is taken as the accuracy group of the child.\n","be7f5607":"## <font size=5 color='violet'> XGBoost with StratifiedKFold<\/font>","dc99ac0f":"Here we will use `StratifiedKFold` and `xgboost` model to train and make prediction.\n","395d236d":"After making our prediction we will make our submission to `submission.csv`.","08c8f1fe":"### <font size=4 color='violet'> Reading and understanding our data<\/font>","7c2a2c0a":"<font size=5 color='violet'>Evaluation<\/font>\n\nSubmissions are scored based on the quadratic weighted kappa, which measures the agreement between two outcomes. This metric typically varies from 0 (random agreement) to 1 (complete agreement). In the event that there is less agreement than expected by chance, the metric may go below 0.\n\n$$w_{i,j} = \\frac{\\left(i-j\\right)^2}{\\left(N-1\\right)^2}$$\n\nWe will use `cohen_kappa_score` which is available in `sklearn.metrics` to calculate the score.","525e5ecc":"- It seems that  we have to group dafaframe by `installation_id` to form a proper trainable dataframe.\n- We will apply the same to form out test set.","d77e1591":"## <font size=4 color='violet'> Label Encoding<\/font>\n- We will concat out final_train and final_test to form `final`.\n- We will label encode the categorical variables.\n- We will split them back to final_train and final_test.","12ea959e":"In this we will prepare the data and make it in a trainable form.For that we will do the following steps :\n- first,we will find the installation ids which are in `train.csv` and which are not in `train_labels.csv`.These installations won't  be of much use to us because `train_labels.csv` contains the the target label,ie `accuracy group`.We will first identify them and remove those rows.","3ef05cbd":"## <font size=5 color='violet'> Making our submission<\/font>","baa25055":"## <font size=5 color='violet'> Data Preparation<\/font>","6fd2ff43":"Just making sure that all the columns in our `final_train` and `final_test` is the same,except accuracy_group.The instersection should return `54`.","25319760":"YES ! It's done..","4ac99ee6":"Next,we will define a `prepare_data` funtion to prepare our train and test data.For the we will do the following steps:\n-   extract `hour_of_day` from timestamp and drop timestamp column,this indicated the hour of day in which is child playes the game.\n-   We will do an on_hot encoding on `event_code` and group the dataframe by installation_id and game_session.\n-   We will define an `agg` dictionary to define the the aggregate functions to be performed after grouping the dataframe\n-   For variables 'type','world' and 'title' we will the the first value,as it is unique for every installation_id,game_session pair.\n-   Atlast, we will join all these togethor and return the dataframe.\n","e897ee50":"We can see that this data contains full history of the installation,ie each time a child has played the game a unique game_session identifier is generated and the attributes related to the game is stored.The atttributes are:\n\nThe data provided in these files are as follows:\n- `event_id` - Randomly generated unique identifier for the event type. Maps to event_id column in specs table.\n- `game_session` - Randomly generated unique identifier grouping events within a single game or video play session.\n- `timestamp` - Client-generated datetime\n- `event_data` - Semi-structured JSON formatted string containing the events parameters. Default fields are: event_count, event_code, and game_time; otherwise - fields are determined by the event type.\n- `installation_id` - Randomly generated unique identifier grouping game sessions within a single installed application instance.\n- `event_count` - Incremental counter of events within a game session (offset at 1). Extracted from event_data.\n- `event_code` - Identifier of the event 'class'. Unique per game, but may be duplicated across games. E.g. event code '2000' always identifies the 'Start Game' event for all games. Extracted from event_data.\n- `game_time` - Time in milliseconds since the start of the game session. Extracted from event_data.\n- `title` - Title of the game or video.\n- `type` - Media type of the game or video. Possible values are: 'Game', 'Assessment', 'Activity', 'Clip'.\n- `world` - The section of the application the game or video belongs to. Helpful to identify the educational curriculum goals of the media.\n\n We will not consider `specs.csv`,it contains description of events in natural language.","e9bc35f5":"<font size=3 color='violet'>Extracting time features<\/font>\n","3e5c537c":"##  <font size=5 color='violet'> Importing required libraries<\/font>"}}