{"cell_type":{"4fbc1255":"code","766520ac":"code","7631665b":"code","771be74d":"code","96bdf928":"code","cd86fed8":"code","56e32804":"code","5fe35a36":"code","11713e1c":"code","b64bcca0":"code","e8024ab3":"code","99253a23":"code","e1633e81":"code","2adf17cf":"code","ed68e5db":"code","e72f85bd":"code","028c920b":"code","d7ecefda":"code","670fe09b":"code","a1c60413":"code","aff8edbd":"code","ceea5fec":"code","a31cb006":"code","51a34d22":"code","53fab252":"code","fdd0ca8f":"code","f5272a51":"code","12a7c4f9":"code","87a681da":"code","0322067b":"code","9d17a34d":"code","7d779497":"code","b4bb7fba":"code","53487250":"code","e027f9e2":"code","9f38fa3a":"code","e89c1bbc":"code","d817ce26":"code","982f52e1":"code","0840dd9f":"code","9f183ca9":"code","a15d3a6b":"code","fd97be04":"code","50b7075f":"code","16d64d2f":"code","9e339889":"code","9802a130":"code","fe80d66f":"code","fa59a438":"code","ad03489f":"code","3941fcec":"code","89d68934":"code","48df62a1":"code","063df29f":"code","c4e251ee":"code","13169b61":"code","cd742f78":"code","387eaf4f":"code","35129e64":"code","dde86723":"code","6fec3521":"code","b4d2e125":"code","57b9d5d1":"code","f6765e71":"code","888a8d0f":"markdown","916529c4":"markdown","2ccf2dc7":"markdown","91491337":"markdown","0c57918b":"markdown","128a61af":"markdown","5023adbe":"markdown","a35b0644":"markdown","9ebe089e":"markdown","73e03723":"markdown","fbde0763":"markdown","edf9bb2e":"markdown","4584e694":"markdown","b59414df":"markdown","3ad9a2e2":"markdown","512cafce":"markdown","537dbea6":"markdown","4f6ba612":"markdown","ab7ffc15":"markdown","9b7d5321":"markdown","64fbcdbc":"markdown","94cf5200":"markdown","b090700f":"markdown","3ac63af0":"markdown","763827d6":"markdown","aa66e47f":"markdown","b05eece7":"markdown","a31f8de8":"markdown","146d6cc9":"markdown","603c8d7b":"markdown","3d3389e2":"markdown","8f6a3f84":"markdown","949d9809":"markdown","dfaea6e7":"markdown","30a2599b":"markdown","237f5b35":"markdown","e62102eb":"markdown","216be00e":"markdown","951e32f4":"markdown","b22aacaa":"markdown","bf31f6be":"markdown","f3c2bfa6":"markdown","c1e6b772":"markdown","a45fdfb5":"markdown","b258921d":"markdown"},"source":{"4fbc1255":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","766520ac":"! python -m pip install tf-models-nightly --no-deps -q\n! python -m pip install tf-models-official==2.4.0 -q\n! python -m pip install tensorflow-gpu==2.4.1 -q\n! python -m pip install tensorflow-text==2.4.1 -q\n! python -m spacy download en_core_web_sm -q\n! python -m spacy validate ","7631665b":"# Preprocessing\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\nimport string\nfrom bs4 import BeautifulSoup as bs\n# Model Training\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.metrics import BinaryAccuracy\nfrom tensorflow.keras.losses import BinaryCrossentropy\nimport official.nlp.optimization\nfrom official.nlp.optimization import create_optimizer # AdamW optimizer\nfrom sklearn.metrics import roc_curve, confusion_matrix\n# Visualization\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\n# Version\nfrom platform import python_version\n\nprint(f'TensorFlow Version: {tf.__version__}')\nprint(f'Python Version: {python_version()}')","771be74d":"RANDOM_SEED = 123\nnlp = spacy.load('en_core_web_sm') \npd.set_option('display.max_colwidth', None) # Expand DataFrame column width\nrcParams['figure.figsize'] = (10, 6) # Custom plot dimensions\nsns.set_theme(palette='muted', style='whitegrid') # Seaborn plot theme","96bdf928":"path = '..\/input\/nlp-getting-started\/train.csv'\ndf = pd.read_csv(path)\nprint(df.shape)\ndf.head()","cd86fed8":"path_test = '..\/input\/nlp-getting-started\/test.csv'\ndf_test = pd.read_csv(path_test)\nprint(df_test.shape)\ndf_test.head()","56e32804":"df.info(verbose=False)","5fe35a36":"df_test.info(verbose=False)","11713e1c":"df['text'].describe()","b64bcca0":"df_test['text'].describe()","e8024ab3":"duplicates = df[df.duplicated(['text', 'target'], keep=False)]\nprint(f'Train Duplicate Entries (text, target): {len(duplicates)}')\nduplicates.head()","99253a23":"df.drop_duplicates(['text', 'target'], inplace=True, ignore_index=True)\nprint(df.shape, df_test.shape)","e1633e81":"new_duplicates = df[df.duplicated(['keyword', 'text'], keep=False)]\n\nprint(f'Train Duplicate Entries (keyword, text): {len(new_duplicates)}')\nnew_duplicates[['text', 'target']].sort_values(by='text')","2adf17cf":"# Drop the target label that is false for each duplicate pair\ndf.drop([4253, 4193, 2802, 4554, 4182, 3212, 4249, 4259, 6535, 4319, 4239, 606, 3936, 6018, 5573], inplace=True)","ed68e5db":"# Reset the dataframe index to account for missing numbers\ndf = df.reset_index(drop=True)\ndf","e72f85bd":"df['target'].value_counts() \/ len(df)","028c920b":"def null_table(data): \n    # Bool is True if values are not null\n    null_list = []\n\n    for i in data:\n        if data[i].notnull().any():\n            null_list.append(data[i].notnull().value_counts())\n    \n    return pd.DataFrame(pd.concat(null_list, axis=1).T)","d7ecefda":"null_table(df)","670fe09b":"null_table(df_test)","a1c60413":"text = df['text']\ntarget = df['target']\n\ntest_text = df_test['text']\n\n# Print random samples from the training text \nfor i in np.random.randint(500, size=5):\n    print(f'Tweet #{i}: ', text[i], '=> Target: ', target[i], end='\\n' * 2)","aff8edbd":"lookup_dict = {\n  'abt' : 'about',\n  'afaik' : 'as far as i know',\n  'bc' : 'because',\n  'bfn' : 'bye for now',\n  'bgd' : 'background',\n  'bh' : 'blockhead',\n  'br' : 'best regards',\n  'btw' : 'by the way',\n  'cc': 'carbon copy',\n  'chk' : 'check',\n  'dam' : 'do not annoy me',\n  'dd' : 'dear daughter',\n  'df': 'dear fiance',\n  'ds' : 'dear son',\n  'dyk' : 'did you know',\n  'em': 'email',\n  'ema' : 'email address',\n  'ftf' : 'face to face',\n  'fb' : 'facebook',\n  'ff' : 'follow friday', \n  'fotd' : 'find of the day',\n  'ftw': 'for the win',\n  'fwiw' : 'for what it is worth',\n  'gts' : 'guess the song',\n  'hagn' : 'have a good night',\n  'hand' : 'have a nice day',\n  'hotd' : 'headline of the day',\n  'ht' : 'heard through',\n  'hth' : 'hope that helps',\n  'ic' : 'i see',\n  'icymi' : 'in case you missed it',\n  'idk' : 'i do not know',\n  'ig': 'instagram',\n  'iirc' : 'if i remember correctly',\n  'imho' : 'in my humble opinion',\n  'imo' : 'in my opinion',\n  'irl' : 'in real life',\n  'iwsn' : 'i want sex now',\n  'jk' : 'just kidding',\n  'jsyk' : 'just so you know',\n  'jv' : 'joint venture',\n  'kk' : 'cool cool',\n  'kyso' : 'knock your socks off',\n  'lmao' : 'laugh my ass off',\n  'lmk' : 'let me know', \n  'lo' : 'little one',\n  'lol' : 'laugh out loud',\n  'mm' : 'music monday',\n  'mirl' : 'meet in real life',\n  'mrjn' : 'marijuana',\n  'nbd' : 'no big deal',\n  'nct' : 'nobody cares though',\n  'njoy' : 'enjoy',\n  'nsfw' : 'not safe for work',\n  'nts' : 'note to self',\n  'oh' : 'overheard',\n  'omg': 'oh my god',\n  'oomf' : 'one of my friends',\n  'orly' : 'oh really',\n  'plmk' : 'please let me know',\n  'pnp' : 'party and play', \n  'qotd' : 'quote of the day',\n  're' : 'in reply to in regards to',\n  'rtq' : 'read the question',\n  'rt' : 'retweet',\n  'sfw' : 'safe for work',\n  'smdh' : 'shaking my damn head', \n  'smh' : 'shaking my head',\n  'so' : 'significant other',\n  'srs' : 'serious',\n  'tftf' : 'thanks for the follow',\n  'tftt' : 'thanks for this tweet',\n  'tj' : 'tweetjack',\n  'tl' : 'timeline',\n  'tldr' : 'too long did not read',\n  'tmb' : 'tweet me back',\n  'tt' : 'trending topic',\n  'ty' : 'thank you',\n  'tyia' : 'thank you in advance',\n  'tyt' : 'take your time',\n  'tyvw' : 'thank you very much',\n  'w': 'with', \n  'wtv' : 'whatever',\n  'ygtr' : 'you got that right',\n  'ykwim' : 'you know what i mean',\n  'ykyat' : 'you know you are addicted to',\n  'ymmv' : 'your mileage may vary',\n  'yolo' : 'you only live once',\n  'yoyo' : 'you are on your own',\n  'yt': 'youtube',\n  'yw' : 'you are welcome',\n  'zomg' : 'oh my god to the maximum'\n}","ceea5fec":"def lemmatize_text(text, nlp=nlp):\n    doc = nlp(text)    \n    lemma_sent = [i.lemma_ for i in doc if not i.is_stop]    \n    \n    return ' '.join(lemma_sent)  \n\ndef abbrev_conversion(text):\n    words = text.split() \n    abbrevs_removed = [] \n    \n    for i in words:\n        if i in lookup_dict:\n            i = lookup_dict[i]\n        abbrevs_removed.append(i)\n            \n    return ' '.join(abbrevs_removed)\n\ndef standardize_text(text_data):    \n    entity_pattern = re.compile(r'(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)') \n    url_pattern = re.compile(r'(?:\\@|http?\\:\/\/|https?\\:\/\/|www)\\S+')\n    retweet_pattern = re.compile(r'^(RT|RT:)\\s+')\n    digit_pattern = re.compile(r'[\\d]+')\n    # From https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               u\"\\U0001f926-\\U0001f937\"\n                               u\"\\U00010000-\\U0010ffff\"\n                               u\"\\u2640-\\u2642\"\n                               u\"\\u2600-\\u2B55\"\n                               u\"\\u200d\"\n                               u\"\\u23cf\"\n                               u\"\\u23e9\"\n                               u\"\\u231a\"\n                               u\"\\ufe0f\"  # dingbats\n                               u\"\\u3030\"\n                               \"]+\", flags=re.UNICODE)\n    \n    # Remove urls\n    url_strip = text_data.apply(lambda x: re.sub(url_pattern, '', x) if pd.isna(x) != True else x)\n    # Parse the HTML\n    html_parse = url_strip.apply(lambda x: bs(x, 'html.parser').get_text() if pd.isna(x) != True else x)\n    # Remove rewteets\n    retweet_strip = html_parse.apply(lambda x: re.sub(retweet_pattern, '', x) if pd.isna(x) != True else x)\n    # Remove emojis\n    emoji_strip = retweet_strip.apply(lambda x: re.sub(emoji_pattern, '', x) if pd.isna(x) != True else x)\n    # Remove entities\n    entity_strip = emoji_strip.apply(lambda x: re.sub(entity_pattern, '', x) if pd.isna(x) != True else x)\n    # Lowercase the strings\n    lowercase = entity_strip.apply(lambda x: str.lower(x) if pd.isna(x) != True else x)               \n    # Remove punctuation\n    punct_strip = lowercase.apply(lambda x: re.sub(f'[{re.escape(string.punctuation)}]', '', x) if pd.isna(x) != True else x) \n    # Convert abbreviations to words\n    abbrev_converted = punct_strip.apply(lambda x: abbrev_conversion(x) if pd.isna(x) != True else x)\n    # Remove digits\n    digit_strip = abbrev_converted.apply(lambda x: re.sub(digit_pattern, '', x) if pd.isna(x) != True else x)    \n    # Lemmatize text and filter stopwords\n    lemma_and_stop = digit_strip.apply(lambda x: lemmatize_text(x) if pd.isna(x) != True else x)\n    \n    return lemma_and_stop","a31cb006":"clean_text = np.asarray(standardize_text(text))\ntest_clean_text = np.asarray(standardize_text(test_text))\n\n# Print random samples from the cleaned training text\nfor i in np.random.randint(500, size=5):\n    print(f'Tweet #{i}: ', clean_text[i], '=> Target: ', target[i], end='\\n' * 2)","51a34d22":"df['clean_text'] = pd.DataFrame(clean_text)\ndf_test['clean_text'] = pd.DataFrame(test_clean_text)","53fab252":"df['tweet_len'] = df['clean_text'].apply(lambda x: len(x))\n\ncount, bin_edges = np.histogram(df['tweet_len'])\nsns.histplot(data=df, x=df['tweet_len'], bins=bin_edges, hue=df['target'])\nplt.title('Tweet Length Frequency')\nplt.xlabel('Length of Tweets')\nplt.ylabel('Frequency')\nplt.show()","fdd0ca8f":"word_cloud_0 = WordCloud(collocations=False, background_color='white').generate(' '.join(df['clean_text'][df['target']==0]))\nplt.imshow(word_cloud_0, interpolation='bilinear')\nplt.title('Non-Disaster Wordcloud (0)')\nplt.axis('off')\nplt.show()","f5272a51":"word_cloud_1 = WordCloud(collocations=False, background_color='black').generate(' '.join(df['clean_text'][df['target']==1]))\nplt.imshow(word_cloud_1, interpolation='bilinear')\nplt.title('Disaster Wordcloud (1)')\nplt.axis('off')\nplt.show()","12a7c4f9":"pattern_new = re.compile(r'\\bnew\\b')\n\nprint('Training Counts of \\'new\\': ', len(re.findall(pattern_new, ' '.join(df['clean_text']))))\nprint('Test Counts of \\'new\\': ', len(re.findall(pattern_new, ' '.join(df_test['clean_text']))))","87a681da":"# Clean the word 'new' from the training and test data\ndf['clean_text'] = df['clean_text'].apply(lambda x: re.sub(pattern_new, '', x) if pd.isna(x) != True else x)\ndf_test['clean_text'] = df_test['clean_text'].apply(lambda x: re.sub(pattern_new, '', x) if pd.isna(x) != True else x)","0322067b":"print('Training Counts of \\'new\\': ', len(re.findall(pattern_new, ' '.join(df['clean_text']))))\nprint('Test Counts of \\'new\\': ', len(re.findall(pattern_new, ' '.join(df_test['clean_text']))))","9d17a34d":"# Load the sentence encoder\nsentence_enc = hub.load('https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4')","7d779497":"def extract_keywords(text, nlp=nlp):\n    potential_keywords = []\n    TOP_KEYWORD = -1\n    # Create a list for keyword parts of speech\n    pos_tag = ['ADJ', 'NOUN', 'PROPN']\n    doc = nlp(text)\n    \n    for i in doc:\n        if i.pos_ in pos_tag:\n            potential_keywords.append(i.text)\n\n    document_embed = sentence_enc([text])\n    potential_embed = sentence_enc(potential_keywords)    \n    \n    vector_distances = cosine_similarity(document_embed, potential_embed)\n    keyword = [potential_keywords[i] for i in vector_distances.argsort()[0][TOP_KEYWORD:]]\n\n    return keyword\n\ndef keyword_filler(keyword, text):\n    if pd.isnull(keyword):\n        try:\n            keyword = extract_keywords(text)[0]\n        except:\n            keyword = '' \n        \n    return keyword","b4bb7fba":"df['keyword_fill'] = pd.DataFrame(list(map(keyword_filler, df['keyword'], df['clean_text']))).astype(str)\ndf_test['keyword_fill'] = pd.DataFrame(list(map(keyword_filler, df_test['keyword'], df_test['clean_text']))).astype(str)\n\nprint('Null Training Keywords => ', df['keyword_fill'].isnull().any())\nprint('Null Test Keywords => ', df_test['keyword_fill'].isnull().any())","53487250":"df['keyword_fill'] = pd.DataFrame(standardize_text(df['keyword_fill']))\ndf_test['keyword_fill'] = pd.DataFrame(standardize_text(df_test['keyword_fill']))","e027f9e2":"df.head()","9f38fa3a":"df_test.head()","e89c1bbc":"keyword_count_0 = pd.DataFrame(df['keyword_fill'][df['target']==0].value_counts().reset_index())\nkeyword_count_1 = pd.DataFrame(df['keyword_fill'][df['target']==1].value_counts().reset_index())","d817ce26":"sns.barplot(data=keyword_count_0[:10], x='keyword_fill', y='index')\nplt.title('Non-Disaster Keyword Frequency (0)')\nplt.xlabel('Frequency')\nplt.ylabel('Top 10 Keywords')\nplt.show()","982f52e1":"sns.barplot(data=keyword_count_1[:10], x='keyword_fill', y='index')\nplt.title('Disaster Keyword Frequency (1)')\nplt.xlabel('Frequency')\nplt.ylabel('Top 10 Keywords')\nplt.show()","0840dd9f":"train_features = df[['clean_text','keyword_fill']]\ntest_features = df_test[['clean_text', 'keyword_fill']]","9f183ca9":"train_features[:5]","a15d3a6b":"test_features[:5]","fd97be04":"print(train_features.shape)\nprint(test_features.shape)","50b7075f":"train_x, val_x, train_y, val_y = train_test_split(\n    train_features,\n    target,\n    test_size=0.2,\n    random_state=RANDOM_SEED,\n)\n\nprint(train_x.shape)\nprint(train_y.shape)\nprint(val_x.shape)\nprint(val_y.shape)","16d64d2f":"# Create TensorFlow Datasets \ntrain_ds = tf.data.Dataset.from_tensor_slices((dict(train_x), train_y))\nval_ds = tf.data.Dataset.from_tensor_slices((dict(val_x), val_y))\ntest_ds = tf.data.Dataset.from_tensor_slices(dict(test_features))","9e339889":"AUTOTUNE = tf.data.experimental.AUTOTUNE\n\nBUFFER_SIZE = 1000\nBATCH_SIZE = 32\n\ndef configure_dataset(dataset, shuffle=False, test=False):\n    # Configure the tf dataset for cache, shuffle, batch, and prefetch\n    if shuffle:\n        dataset = dataset.cache()\\\n                        .shuffle(BUFFER_SIZE, seed=RANDOM_SEED, reshuffle_each_iteration=True)\\\n                        .batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n    elif test:\n        dataset = dataset.cache()\\\n                        .batch(BATCH_SIZE, drop_remainder=False).prefetch(AUTOTUNE)\n    else:\n        dataset = dataset.cache()\\\n                        .batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n    return dataset","9802a130":"# Configure the datasets\ntrain_ds = configure_dataset(train_ds, shuffle=True)\nval_ds = configure_dataset(val_ds)\ntest_ds = configure_dataset(test_ds, test=True)","fe80d66f":"# Print the dataset specifications\nprint(train_ds.element_spec)\nprint(val_ds.element_spec)\nprint(test_ds.element_spec)","fa59a438":"# BERT encoder w\/ preprocessor\nbert_preprocessor = hub.KerasLayer('https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3', name='BERT_preprocesser')\nbert_encoder = hub.KerasLayer('https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/4', trainable=True, name='BERT_encoder')\n# Keyword embedding layer\nnnlm_embed = hub.KerasLayer('https:\/\/tfhub.dev\/google\/nnlm-en-dim50\/2', name='embedding_layer')","ad03489f":"def build_model():\n    \n    # Construct text layers\n    text_input = layers.Input(shape=(), dtype=tf.string, name='clean_text') # Name matches df heading\n    encoder_inputs = bert_preprocessor(text_input)\n    encoder_outputs = bert_encoder(encoder_inputs)\n    # pooled_output returns [batch_size, hidden_layers]\n    pooled_output = encoder_outputs[\"pooled_output\"]          \n    bert_dropout = layers.Dropout(0.1, name='BERT_dropout')(pooled_output)   \n    \n    # Construct keyword layers\n    key_input = layers.Input(shape=(), dtype=tf.string, name='keyword_fill') # Name matches df heading\n    key_embed = nnlm_embed(key_input)\n    key_flat = layers.Flatten()(key_embed)\n    key_dense = layers.Dense(128, activation='elu', kernel_regularizer=regularizers.l2(1e-4))(key_flat)\n    key_dropout = layers.Dropout(0.5, name='dense_dropout')(key_dense)\n    \n    # Merge the layers and classify \n    merge = layers.concatenate([bert_dropout, key_dropout])\n    dense = layers.Dense(128, activation='elu', kernel_regularizer=regularizers.l2(1e-4))(merge)\n    dropout = layers.Dropout(0.5, name='merged_dropout')(dense)    \n    clf = layers.Dense(1, activation='sigmoid', name='classifier')(dropout)\n\n    return Model([text_input, key_input], clf, name='BERT_classifier')","3941fcec":"bert_classifier = build_model()\nbert_classifier.summary()","89d68934":"tf.keras.utils.plot_model(bert_classifier, show_shapes=False, dpi=96)","48df62a1":"EPOCHS = 2\nLEARNING_RATE = 5e-5\n\nSTEPS_PER_EPOCH = int(train_ds.unbatch().cardinality().numpy() \/ BATCH_SIZE)\nVAL_STEPS = int(val_ds.unbatch().cardinality().numpy() \/ BATCH_SIZE)\n# Calculate the train and warmup steps for the optimizer\nTRAIN_STEPS = STEPS_PER_EPOCH * EPOCHS\nWARMUP_STEPS = int(TRAIN_STEPS * 0.1)\n\nadamw_optimizer = create_optimizer(\n    init_lr=LEARNING_RATE,\n    num_train_steps=TRAIN_STEPS,\n    num_warmup_steps=WARMUP_STEPS\n)","063df29f":"bert_classifier.compile(\n    loss=BinaryCrossentropy(from_logits=True), \n    optimizer= adamw_optimizer,\n    metrics=[BinaryAccuracy(name='accuracy')]\n)\n\nhistory = bert_classifier.fit(\n    train_ds, \n    epochs=EPOCHS,\n    steps_per_epoch=STEPS_PER_EPOCH,    \n    validation_data= val_ds,\n    validation_steps=VAL_STEPS\n)","c4e251ee":"# Assign the loss and accuracy metrics\ntrain_loss = history.history['loss']\nval_loss = history.history['val_loss']\ntrain_acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']","13169b61":"# Plot the training and validation metrics\nfig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nsns.lineplot(ax=ax1, data = train_acc, label=f'Training Accuracy')\nsns.lineplot(ax=ax1, data = val_acc, label=f'Validation Accuracy')\nsns.lineplot(ax=ax2, data = train_loss, label=f'Training Loss')\nsns.lineplot(ax=ax2, data = val_loss, label=f'Validation Loss')\nax1.set_ylabel('Accuracy')\nax1.set_xlim(xmin=0)\nax2.set_ylabel('Loss')\nax2.set_xlabel('Epochs')\nax2.set_xlim(xmin=0)\nplt.suptitle('History')\nplt.show()","cd742f78":"# Get the array of labels from the validation Dataset\nval_target = np.asarray([i[1] for i in list(val_ds.unbatch().as_numpy_iterator())])\nprint(val_target.shape)\nval_target[:5]","387eaf4f":"# Get predictions from the validation Dataset\nval_predict = bert_classifier.predict(val_ds)","35129e64":"# Get the false positive and true positive rates\nfpr, tpr, _ = roc_curve(val_target, val_predict)\nplt.plot(fpr, tpr, color='orange')\nplt.plot([0,1], [0,1], linestyle='--')\nplt.title('Validation ROC Curve')\nplt.xlabel('False Positives (%)')\nplt.ylabel('True Positives (%)')\nplt.grid(True)    \nplt.show()","dde86723":"THRESHOLD = 0.5 # Default value\n# Get the true negative, false positive, false negative, and true positive values\ntn, fp, fn, tp = confusion_matrix(val_target, val_predict > THRESHOLD).flatten()\n# Construct the dataframe\ncm = pd.DataFrame(\n                    [[tn, fp], [fn, tp]], \n                    index=['No Disaster', 'Disaster'], \n                    columns=['No Disaster', 'Disaster']\n)\n# Plot the matrix\nsns.heatmap(cm, annot=True, fmt='g')    \nplt.title('Validation Confusion Matrix')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","6fec3521":"predictions = bert_classifier.predict(test_ds)\nprint(predictions.shape)\nprint(predictions[:5])","b4d2e125":"count, bin_edges = np.histogram(predictions)\nsns.histplot(predictions, bins=bin_edges, legend=False)\nplt.axvline(x=THRESHOLD, linestyle='--', color='black', label='Threshold')\nplt.title('Predicted Probability of Disaster')\nplt.xlabel('Probabilities')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()","57b9d5d1":"# Use the threshold to label predictions\npredictions = np.where(predictions > THRESHOLD, 1, 0)\ndf_predictions = pd.DataFrame(predictions)\ndf_predictions.columns = ['target']\n\nprint(df_predictions.shape)\ndf_predictions.head()","f6765e71":"# Concatenate the columns and convert submission to csv\nsubmission = pd.concat([df_test['id'], df_predictions], axis=1)\nsubmission.to_csv('submission.csv', index=False)","888a8d0f":"A look at the top keywords for each class demonstrates the importance of observing the tweet itself to find context. At a glance, these keywords could belong to either class. That being the case, keywords can still emphasize important parts of speech that may be overlooked by the model. ","916529c4":"### 4.1. Select the Clean Text and Filled Keyword Columns","2ccf2dc7":"# 8. Predictions on the Test Data\n\nNow that we have our ideal threshold value, let's go ahead and make predictions with our classifier. We'll receive an array of probabilities that we can then use to plot a histogram of our results and graph the threshold value.","91491337":"A tweet length histogram shows that longer tweets at the end of the distribution have a greater frequency of disaster. ","0c57918b":"### 3.4. Remove the Word *new* from Text\n\nA comparison between disaster and non-disaster wordclouds shows how often different words appear in the tweets. We can see that the word `new` appears quite frequently in both classes, which may add noise to the training process. This term will be dropped altogether so the model can focus on more meaningful words.","128a61af":"### 1.4. Load the Training and Test Files","5023adbe":"### 2.1. Remove Duplicate Data\n\nIt is shown in the descriptive summary that the unique count is less than the total count. This means we could have duplicate data that needs to be dropped for better training results. First we'll look at duplicates that have the same text and target followed by duplicates that share the same keywords and text.","a35b0644":"### 5.3. Construct the AdamW Optimizer","9ebe089e":"### 4.3. Create TensorFlow Datasets\n\nWe'll form these data splits into TensorFlow Datasets and configure them before building our model. Include the features as dictionaries so they can be selectively processed by our model.","73e03723":"# 2. Raw Data Analysis\n\nWe'll use the `info` method to get a summary for the training and test datasets, then we'll use `describe` to get statistics on the text column.","fbde0763":"### 7.2. Plot the ROC Curve\n\nAnother graphical method we'll use is the Receiver Operating Characteristic curve ([ROC](https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic)). It plots the true positive rate against the false positive rate at different classification thresholds to visualize the predictive ability of the model. Curves skewed towards the upper left corner of the plot reflect models with good performance as opposed to near linear curves which indicate poor performance.","edf9bb2e":"### 4.2. Create Data Splits\nA training split is produced leaving 20% of the data for validation purposes. ","4584e694":"The next task is to build a text standardization function that is specific to the content found in tweets. Aside from punctuation, there are urls, abbreviations, entities, retweets, digits, stopwords, and of course emojis. The words in each tweet will also be lemmatized or reduced to their root form using the spaCy library. We'll start by building a lookup dictionary with common twitter phrase abbreviations. Tweet terms that match keys in the lookup dictionary will be expanded to their non-abbreviated form.","b59414df":"### 7.1. Plot the Loss and Accuracy Metrics","3ad9a2e2":"# 7. Visualize Results\n\nNow that the model has been trained, we will plot the accuracy and loss metrics for the training and validation data.   ","512cafce":"### 3.1. Text Standardization Functions","537dbea6":"# 1. Introduction\n\nTwitter has shown value as a tool for communicating emergencies in real-time. However, due to figures of speech it is not always clear to a machine when a tweet is about a real disaster. The goal of this exercise is to use natural language processing to build a model that can discern tweets about real disasters from those using figures of speech. The dataset for this task is a collection of 10,000 manually classified tweets (`1` for distaster, `0` for non-disaster) containing keywords, locations, and text. The model selected for this classification is known as Bidirectional Encoder Representations from Transformers ([BERT](https:\/\/arxiv.org\/abs\/1810.04805)). \n\nMethods for this exercise:\n* Text preprocessing and keyword extraction using spaCy\n* Loading pre-trained models from TensorFlow Hub\n* Building a BERT classifier with the Keras functional API\n* Evaluating results with a confusion matrix and ROC curve","4f6ba612":"### 5.2. Model Composition\n\nTo build our classifier we will be using the TensorFlow functional API, this will reduce constraints on our model design. Two input branches will be merged into a classification layer. The first branch is a text input layer that feeds into the BERT preprocessor. This layer is passed to the BERT encoder and is returned as a pooled output. This output is then regularized with a dropout layer. \n\nOn the second branch, a keyword input is passed to a pre-trained word embedding layer. The embeddings are flattened and passed into a dense neural net, then fed into a dropout layer. \n\nThe layer outputs from each model are concatenated, passed into a dense neural net with dropout, then sent to a single unit dense classification layer with a sigmoid activation. The sigmoid activation function will return class probabilities that we can use to plot a Receiver Operating Characteristic ([ROC](https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic)) curve and a [Confusion Matrix](https:\/\/en.wikipedia.org\/wiki\/Confusion_matrix) to analyze our results. ","ab7ffc15":"Now that the missing keywords are filled we should standardize them to ensure our keywords are clean and ready for training.","9b7d5321":"### 1.2. Import Libraries","64fbcdbc":"# 5. Building the Classifier Model\n\nFor this task we will be using a pre-trained BERT model loaded from TensorFlow Hub. This model has 12 hidden layers, a hidden unit size of 768, and 12 attention heads. It has a companion preprocessor that is loaded from the same repository. This preprocessor takes text segments and converts them to numeric token ids accepted by the BERT encoder. These token ids are:\n* input_word_ids\n    - ids of the input sequences\n* input_mask\n    - represents all pre-padded input tokens as 1, and padded tokens as 0\n* input_type_ids\n    - contains indices for each input segment with padding locations indexed at 0","94cf5200":"### 7.3. Plot the Confusion Matrix\n\nA confusion matrix will compare the predicted values with the actual data from the validation set. This is interpreted through the use of true and false positives versus true and false negatives. A model that performs well will maximize true rates while minimizing false rates. A false positive is known as a Type 1 error and a false negative is known as a Type 2 error.","b090700f":"### 1.1. Install Packages","3ac63af0":"### 1.3. Configure Settings","763827d6":"The text is now in a cleaner format for training, but before taking the next step we should explore it with some visualizations.","aa66e47f":"### 3.7. Plot the Keyword Frequencies \nTo learn more about this data column let's visualize the top ten keywords from each target class. ","b05eece7":"### 3.6. Keyword Extract and Fill Functions","a31f8de8":"A check for null values in the data shows that the `location` column is missing a significant amount information. This could interfere with model performance, so that data will be excluded. In contrast, the `keyword` column has a more acceptable count of missing values so we can fill these with extracted keywords using spaCy.","146d6cc9":"# Disaster Tweet Classification: NLP with TensorFlow, spaCy, and BERT","603c8d7b":"### 2.3. Check for Null Values\n\nWe will check for null values in the training and test data so we can decide how to preprocess it for training and prediction.","3d3389e2":"For training the BERT model we'll use an Adam optimizer with weight decay ([AdamW](https:\/\/arxiv.org\/abs\/1711.05101)). This method differs from the standard Adam algorithm with its use of decoupled weight decay regularization. This is the optimizer that BERT was originally trained with. It has a linear warm-up period over the first 10% of training steps paired with a lower learning rate. One of the simpler ways to implement this [optimizer](https:\/\/github.com\/tensorflow\/models\/blob\/master\/official\/nlp\/optimization.py) is with the TensorFlow official models collection.","8f6a3f84":"### 5.1. Load the Pre-trained BERT Encoder ","949d9809":"### 8.3. Submit the Predictions","dfaea6e7":"Our data needs to be prepared for training our model which will receive two inputs, one for text and the other for keywords. Our approach is to select the `clean_text` and `keyword_fill` columns before splitting the data into training and validation sets.","30a2599b":"### 2.2. Examine the Target Data Balance\n\nThe balance of the target data shows that the minority class makes up just over 40% of the training samples. This is mild enough where we don't need any imbalanced data sampling techinques. ","237f5b35":"This distribution represents the binary nature of the prediction values. The threshold marks the line by which predictions will be considered either non-disasters (below threshold) or disasters (above threshold). The default value for a binary accuracy threshold is `0.5`. Finally we use this threshold to label and submit the predictions.","e62102eb":"### 3.2. Plot Tweet Length Histogram","216be00e":"Duplicates with the same keyword and text share both target classes. The count is low enough to manually review these tweets and drop those with the incorrect target label. ","951e32f4":"### 3.5. Fill Missing Keywords\n\nThere are fifty-six missing keywords in the training data set and twenty-six in the test set. We will use spaCy's part of speech tagging feature to fill the missing words. The process is as follows:\n1. Create a list of potential keywords by tagging nouns, pronouns, and adjectives\n2. Use a sentence encoder to embed the list of potential keywords and the comparison text\n3. Calculate vector distances using the cosine similarity function\n4. Sort the vectors and select the top keyword for each tweet\n\nThe sentence encoder used here is loaded from TensorFlow Hub. It is a pre-trained universal sentence encoder published by Google for use in natural language tasks.","b22aacaa":"### 3.3. Display Non-Disaster and Disaster WordClouds","bf31f6be":"# 4. Prepare Data for Training","f3c2bfa6":"# 6. Train the Classifier\nThis is a classification problem with two labels, so we'll compile with a binary crossentropy loss function and a binary accuracy metric with our optimizer. Included are the previously calculated `steps_per_epoch` and `validation_steps`.","c1e6b772":"# 3. Text Preprocessing and EDA\nWe'll start by viewing some raw text samples so we can apply the appropriate data cleaning methods. ","a45fdfb5":"### 8.2. Label the Predictions","b258921d":"### 8.1. Predictions Histogram"}}