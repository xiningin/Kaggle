{"cell_type":{"46bd4c41":"code","5fdde3cc":"code","95a2be9c":"code","a1c66fd3":"code","ec23bf0f":"code","3ae089bd":"code","0f0f782e":"code","28dbd885":"code","b1cc8b56":"code","0515d080":"code","a0aefec6":"code","ff2c1cc0":"code","82109ff8":"code","c9fa546f":"code","c1bd77f9":"code","5837a185":"code","88362404":"code","7474574a":"code","a54ad4f5":"code","238b6c89":"code","4d960cba":"code","879f0e14":"code","aae7c621":"code","135a7208":"code","66fce8fa":"code","a729eaca":"code","ac2dfc1a":"code","8bd691ff":"markdown","6896e23d":"markdown","6c7716ba":"markdown","b4f43693":"markdown","73535fd4":"markdown","df955a37":"markdown","b96807aa":"markdown","69eb1d9d":"markdown","38618336":"markdown","90c20b07":"markdown","458a951f":"markdown","81e2a617":"markdown","1fb123d4":"markdown"},"source":{"46bd4c41":"import numpy as np # linear algebra\n# np.random.seed(8)\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, scale\nfrom sklearn.metrics import roc_auc_score\n\nfrom keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, BatchNormalization, Input, Conv2D\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import metrics\nfrom keras.optimizers import Adam \nfrom keras import backend as K\nimport keras\nfrom keras.models import Model\nfrom keras import regularizers\nimport tensorflow as tf\nfrom keras.losses import binary_crossentropy\nimport gc\nimport scipy.special\nfrom tqdm import *\nfrom scipy.stats import norm, rankdata\n\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n\n\nBATCH_SIZE = 1024\nNUM_FEATURES = 1200","5fdde3cc":"train = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/test.csv')\ntrain_knowledge = pd.read_csv('..\/input\/santander-2019-distillation\/lgbm_train.csv')","95a2be9c":"y = train['target']\ny_knowledge = train_knowledge['target']\nid_code_train = train['ID_code']\nid_code_test = test['ID_code']\nfeatures = [c for c in train.columns if c not in ['ID_code', 'target']]","a1c66fd3":"for feature in features:\n    # train['mean_'+feature] = (train[feature].mean()-train[feature])\n    # train['z_'+feature] = (train[feature] - train[feature].mean())\/train[feature].std(ddof=0)\n    train['sq_'+feature] = (train[feature])**2\n    # train['sqrt_'+feature] = np.abs(train[feature])**(1\/2)\n    train['c_'+feature] = (train[feature])**3\n    # train['p4_'+feature] = (train[feature])**4\n    # train['r1_'+feature] = np.round(train[feature], 1)\n    train['r2_'+feature] = np.round(train[feature], 2)\n    ","ec23bf0f":"for feature in features:\n    # test['mean_'+feature] = (train[feature].mean()-test[feature])\n    # test['z_'+feature] = (test[feature] - train[feature].mean())\/train[feature].std(ddof=0)\n    test['sq_'+feature] = (test[feature])**2\n    # test['sqrt_'+feature] = np.abs(test[feature])**(1\/2)\n    test['c_'+feature] = (test[feature])**3\n    # test['p4_'+feature] = (test[feature])**4\n    # test['r1_'+feature] = np.round(test[feature], 1)\n    test['r2_'+feature] = np.round(test[feature], 2)","3ae089bd":"class GaussRankScaler():\n\n    def __init__( self ):\n        self.epsilon = 1e-9\n        self.lower = -1 + self.epsilon\n        self.upper =  1 - self.epsilon\n        self.range = self.upper - self.lower\n\n    def fit_transform( self, X ):\n\n        i = np.argsort( X, axis = 0 )\n        j = np.argsort( i, axis = 0 )\n\n        assert ( j.min() == 0 ).all()\n        assert ( j.max() == len( j ) - 1 ).all()\n\n        j_range = len( j ) - 1\n        self.divider = j_range \/ self.range\n\n        transformed = j \/ self.divider\n        transformed = transformed - self.upper\n        transformed = scipy.special.erfinv( transformed )\n        ############\n        # transformed = transformed - np.mean(transformed)\n\n        return transformed","0f0f782e":"SPLIT = len(train)\ntrain = train.append(test)\ndel test; gc.collect()\n# print(train.shape)\nscaler = GaussRankScaler()\nsc = StandardScaler()\nfor feat in tqdm(features):\n    # train[feat] = scaler.fit_transform(train[feat])\n    train[feat] = sc.fit_transform(train[feat].values.reshape(-1, 1))\n    train[feat+'_r'] = rankdata(train[feat]).astype('float32')\n    train[feat+'_n'] = norm.cdf(train[feat]).astype('float32')\n\nfeats = [c for c in train.columns if c not in (['ID_code', 'target'] + features)]\nfor feat in tqdm(feats):\n    train[feat] = sc.fit_transform(train[feat].values.reshape(-1, 1))\n\ntrain = train.drop(['target', 'ID_code'], axis=1)\ntest = train[SPLIT:].values\ntrain = train[:SPLIT].values\n# test = test.drop(['ID_code'], axis=1)\nprint('Done!!')\nprint(train.shape)\n# train.head()\n# train[0:5]","28dbd885":"train = np.reshape(train, (-1, NUM_FEATURES, 1))\ntest = np.reshape(test, (-1, NUM_FEATURES, 1))","b1cc8b56":"x_train, x_valid, y_train, y_valid, y_knowledge_train, y_knowledge_valid  = train_test_split(train, y, y_knowledge, stratify=y, test_size=0.2, random_state=8)","0515d080":"function = 'relu'\n# function = keras.layers.advanced_activations.LeakyReLU(alpha=.001)\n\ndef create_model(input_shape, n_out):\n    input_tensor = Input(shape=input_shape)\n    x = Dense(16, activation=function)(input_tensor)\n    x = Flatten()(x)\n    out_put = Dense(n_out, activation='sigmoid')(x)\n    model = Model(input_tensor, out_put)\n    \n    return model","a0aefec6":"def auc(y_true, y_pred):\n    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)","ff2c1cc0":"gamma = 2.0\nalpha=.25\nepsilon = K.epsilon()\n\ndef focal_loss(y_true, y_pred):\n    pt_1 = y_pred * y_true\n    pt_1 = K.clip(pt_1, epsilon, 1-epsilon)\n    CE_1 = -K.log(pt_1)\n    FL_1 = alpha* K.pow(1-pt_1, gamma) * CE_1\n    \n    pt_0 = (1-y_pred) * (1-y_true)\n    pt_0 = K.clip(pt_0, epsilon, 1-epsilon)\n    CE_0 = -K.log(pt_0)\n    FL_0 = (1-alpha)* K.pow(1-pt_0, gamma) * CE_0\n    \n    loss = K.sum(FL_1, axis=1) + K.sum(FL_0, axis=1)\n    return loss","82109ff8":"def mixup_data(x, y, alpha=1.0):\n    # y = np.array(y)\n    # print(y)\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n\n    sample_size = x.shape[0]\n    index_array = np.arange(sample_size)\n    np.random.shuffle(index_array)\n    \n    mixed_x = lam * x + (1 - lam) * x[index_array]\n    mixed_y = (lam * y) + ((1 - lam) * y[index_array])\n    # print((1 - lam) * y[index_array])\n    # print((lam * y).shape,((1 - lam) * y[index_array]).shape)\n    return mixed_x, mixed_y\n\ndef make_batches(size, batch_size):\n    nb_batch = int(np.ceil(size\/float(batch_size)))\n    return [(i*batch_size, min(size, (i+1)*batch_size)) for i in range(0, nb_batch)]\n\n\ndef batch_generator(X,y,batch_size=128,shuffle=True,mixup=False):\n    y = np.array(y)\n    # print(X.shape[0], y.shape[0])\n    sample_size = X.shape[0]\n    index_array = np.arange(sample_size)\n    \n    while True:\n        if shuffle:\n            np.random.shuffle(index_array)\n        batches = make_batches(sample_size, batch_size)\n        for batch_index, (batch_start, batch_end) in enumerate(batches):\n            batch_ids = index_array[batch_start:batch_end]\n            X_batch = X[batch_ids]\n            y_batch = y[batch_ids]\n            \n            if mixup:\n                # print('before', X_batch.shape, y_batch.shape)\n                X_batch,y_batch = mixup_data(X_batch,y_batch,alpha=1.0)\n            # print('*****************')    \n            yield X_batch,y_batch","c9fa546f":"# model = create_model((train.shape[1],), 1)\nmodel = create_model((NUM_FEATURES,1), 1)\n# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc])\nmodel.compile(loss='binary_crossentropy', optimizer='adam')\nmodel.summary()\n\ncheckpoint = ModelCheckpoint('feed_forward_model.h5', monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = True)\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, \n                                   verbose=1, mode='min', epsilon=0.0001)\nearly = EarlyStopping(monitor=\"val_loss\", \n                      mode=\"min\", \n                      patience=9)\ncallbacks_list = [checkpoint, reduceLROnPlat, early]\ntr_gen = batch_generator(x_train,y_train,batch_size=BATCH_SIZE,shuffle=True,mixup=True)\n\nhistory = model.fit_generator(# x_train,y_train,\n                                tr_gen,\n                                steps_per_epoch=np.ceil(float(len(x_train)) \/ float(BATCH_SIZE)),\n                                epochs=10,\n                                verbose=1,\n                                callbacks=callbacks_list,\n                                validation_data=(x_valid, y_valid))","c1bd77f9":"model.load_weights('feed_forward_model.h5')\nprediction = model.predict(x_valid, batch_size=512, verbose=1)\nroc_auc_score(y_valid, prediction)","5837a185":"y_train = np.vstack((y_train, y_knowledge_train)).T\ny_valid = np.vstack((y_valid, y_knowledge_valid)).T\n\nprint(y_train.shape)\ny_train[0]","88362404":"def knowledge_distillation_loss_withBE(y_true, y_pred, beta=0.1):\n\n    # Extract the groundtruth from dataset and the prediction from teacher model\n    y_true, y_pred_teacher = y_true[: , :1], y_true[: , 1:]\n    \n    # Extract the prediction from student model\n    y_pred, y_pred_stu = y_pred[: , :1], y_pred[: , 1:]\n\n    loss = beta*binary_crossentropy(y_true,y_pred) + (1-beta)*binary_crossentropy(y_pred_teacher, y_pred_stu)\n\n    return loss","7474574a":"def auc_2(y_true, y_pred):\n    y_true = y_true[:, :1]\n    y_pred = y_pred[:, :1]\n    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n\ndef auc_3(y_true, y_pred):\n    y_true = y_true[:, :1]\n    y_pred = y_pred[:, 1:]\n    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)","a54ad4f5":"# model = create_model((train.shape[1],), 2)\nmodel = create_model((NUM_FEATURES,1), 2)\nmodel.compile(loss=knowledge_distillation_loss_withBE, optimizer='adam', metrics=[auc_2])\n\ncheckpoint = ModelCheckpoint('student_model_BE.h5', monitor='val_auc_2', verbose=1, \n                             save_best_only=True, mode='max', save_weights_only = True)\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_auc_2', factor=0.5, patience=4, \n                                   verbose=1, mode='max', epsilon=0.0001)\nearly = EarlyStopping(monitor=\"val_auc_2\", \n                      mode=\"max\", \n                      patience=9)\ncallbacks_list = [checkpoint, reduceLROnPlat, early]\n\nhistory = model.fit(x_train,y_train,\n                    epochs=10,\n                    batch_size = BATCH_SIZE,\n                    validation_data=(x_valid, y_valid))","238b6c89":"def knowledge_distillation_loss_withFL(y_true, y_pred, beta=0.1):\n\n    # Extract the groundtruth from dataset and the prediction from teacher model\n    y_true, y_pred_teacher = y_true[: , :1], y_true[: , 1:]\n    \n    # Extract the prediction from student model\n    y_pred, y_pred_stu = y_pred[: , :1], y_pred[: , 1:]\n\n    loss = beta*focal_loss(y_true,y_pred) + (1-beta)*binary_crossentropy(y_pred_teacher, y_pred_stu)\n\n    return loss","4d960cba":"# model = create_model((train.shape[1],), 2)\nmodel = create_model((NUM_FEATURES,1), 2)\nmodel.compile(loss=knowledge_distillation_loss_withFL, optimizer='adam', metrics=[auc_2, auc_3])\n\ncheckpoint = ModelCheckpoint('student_model_FL.h5', monitor='val_auc_2', verbose=1, \n                             save_best_only=True, mode='max', save_weights_only = True)\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_auc_2', factor=0.5, patience=4, \n                                   verbose=1, mode='max', epsilon=0.0001)\nearly = EarlyStopping(monitor=\"val_auc_2\", \n                      mode=\"max\", \n                      patience=9)\ncallbacks_list = [checkpoint, reduceLROnPlat, early]\n\nhistory = model.fit(x_train,y_train,\n                    epochs=10,\n                    batch_size = BATCH_SIZE,\n                    callbacks=callbacks_list,\n                    validation_data=(x_valid, y_valid))","879f0e14":"from scipy.special import logit\n\ndef sigmoid(x, derivative=False):\n  return x*(1-x) if derivative else 1\/(1+np.exp(-x))\n\nTEMPERATURE = 2\n\ny_knowledge_logit = logit(y_knowledge)\ny_temperature = sigmoid(y_knowledge_logit\/TEMPERATURE)\n\n# del x_train, x_valid; gc.collect()\n\nx_train, x_valid, y_train, y_valid, y_knowledge_train, y_knowledge_valid  = train_test_split(train, y, y_temperature,\n                                                                                             stratify=y, test_size=0.2, random_state=8)\n\n","aae7c621":"y_train = np.vstack((y_train, y_knowledge_train)).T\ny_valid = np.vstack((y_valid, y_knowledge_valid)).T\n\nprint(y_train.shape)\ny_train[0]","135a7208":"# model = create_model((train.shape[1],), 2)\nmodel = create_model((NUM_FEATURES,1), 2)\nmodel.compile(loss=knowledge_distillation_loss_withFL, optimizer='adam', metrics=[auc_2,auc_3])\n\ncheckpoint = ModelCheckpoint('student_model_FL.h5', monitor='val_auc_2', verbose=1, \n                             save_best_only=True, mode='max', save_weights_only = True)\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_auc_2', factor=0.5, patience=4, \n                                   verbose=1, mode='max', epsilon=0.0001)\nearly = EarlyStopping(monitor=\"val_auc_2\", \n                      mode=\"max\", \n                      patience=9)\ncallbacks_list = [checkpoint, reduceLROnPlat, early]\n\nhistory = model.fit(x_train,y_train,\n                    epochs=10,\n                    batch_size = 1024,\n                    callbacks=callbacks_list,\n                    validation_data=(x_valid, y_valid))","66fce8fa":"# run k-fold\nnum_fold = 5\nfolds = list(StratifiedKFold(n_splits=num_fold, shuffle=True, random_state=7).split(train, y))\n# del x_train, x_valid; gc.collect()\n\ny_test_pred_log = np.zeros(len(train))\ny_train_pred_log = np.zeros(len(train))\nprint(y_test_pred_log.shape)\nprint(y_train_pred_log.shape)\nscore = []\n\nfor j, (train_idx, valid_idx) in enumerate(folds):\n    print('\\n===================FOLD=',j)\n    x_train, x_valid = train[train_idx], train[valid_idx]\n    y_train, y_valid = y[train_idx], y[valid_idx]\n    y_knowledge_train, y_knowledge_valid = y_temperature[train_idx], y_temperature[valid_idx]\n    \n    y_train = np.vstack((y_train, y_knowledge_train)).T\n    y_valid = np.vstack((y_valid, y_knowledge_valid)).T\n    \n    # model = create_model((train.shape[1],), 2)\n    model = create_model((NUM_FEATURES,1), 2)\n    model.compile(loss=knowledge_distillation_loss_withFL, optimizer='adam', metrics=[auc_2, auc_3])\n\n    checkpoint = ModelCheckpoint('student_model_FL.h5', monitor='val_auc_2', verbose=1, \n                                 save_best_only=True, mode='max', save_weights_only = True)\n    reduceLROnPlat = ReduceLROnPlateau(monitor='val_auc_2', factor=0.5, patience=4, \n                                       verbose=1, mode='max', epsilon=0.0001)\n    early = EarlyStopping(monitor=\"val_auc_2\", \n                          mode=\"max\", \n                          patience=9)\n    callbacks_list = [checkpoint, reduceLROnPlat, early]\n    history = model.fit(x_train,y_train,\n                        epochs=100,\n                        batch_size = BATCH_SIZE,\n                        callbacks=callbacks_list,\n                        validation_data=(x_valid, y_valid))\n    \n    model.load_weights('student_model_FL.h5')\n    prediction = model.predict(x_valid,\n                               batch_size=512,\n                               verbose=1)\n    # print(prediction.shape)\n    # prediction = np.sum(prediction, axis=1)\/2\n    score.append(roc_auc_score(y_valid[:,0], prediction[:,1]))\n    # score.append(roc_auc_score(y_valid[:,0], prediction))\n    prediction = model.predict(test,\n                               batch_size=512,\n                               verbose=1)\n    # y_test_pred_log += np.sum(prediction, axis=1)\/2\n    y_test_pred_log += np.squeeze(prediction[:, 1])\n    \n    prediction = model.predict(x_valid,\n                               batch_size=512,\n                               verbose=1)\n    # y_train_pred_log += np.sum(prediction, axis=1)\/2\n    y_train_pred_log[valid_idx] += np.squeeze(prediction[:, 1])\n    \n    del x_train, x_valid, y_train, y_valid, y_knowledge_train, y_knowledge_valid\n    gc.collect()\n\n\n","a729eaca":"print(\"OOF score: \", roc_auc_score(y, y_train_pred_log\/num_fold))\nprint(\"average {} folds score: \".format(num_fold), np.sum(score)\/num_fold)\n","ac2dfc1a":"# make submission\nsubmit = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/sample_submission.csv')\nsubmit['ID_code'] = id_code_test\nsubmit['target'] = y_test_pred_log\/num_fold\nsubmit.to_csv('submission.csv', index=False)\nsubmit.head()","8bd691ff":"## **Load the dataset, and the prediction of 5-fold LGBM**\n","6896e23d":"## Experment 3\nWe set the ratio between teacher's prediction and groundtruth is 1:9, and use the focal loss.","6c7716ba":"## Normalize and split data\n","b4f43693":"# Please upvote if you find this kernel interesting ^_^","73535fd4":"## Some necessary functions\n","df955a37":"## Define our student network\n","b96807aa":"## Knowledge distillation\nThe basic idea is that you feed both groundtruth and the prediction from the teacher model to the student network.\nSoft targets (the prediction of the teacher model) contains more information than the hard labels (groundtruth) due to the fact that they encode similarity measures between the classes.","69eb1d9d":"# Introduction\n**\"[Distilling the Knowledge in a Neural Network](http:\/\/arxiv.org\/abs\/1503.02531)\" was introduced by Geoffrey Hinton, Oriol Vinyals, Jeff Dean in Mar 2015. In this kernel, I would like to share some experiments to distill the knowledge from a [LGBM teacher](https:\/\/www.kaggle.com\/tanreinama\/lightgbm-minimize-leaves-with-gaussiannb) (LB:0.899) to a neural network. The student network has not surpassed the teacher model yet (LB:0.894). But, I hope I can make it happen before this competition ends.**\n\n","38618336":"## Adding some features, the credit belong to these kernels: https:\/\/www.kaggle.com\/karangautam\/keras-nn, https:\/\/www.kaggle.com\/ymatioun\/santander-linear-model-with-additional-features\n","90c20b07":"# Please upvote if you find this kernel interesting ^_^","458a951f":"## Experiment 1\nFirstly, we check the performance of simple feed forward neural network.","81e2a617":"## Experment 4\nTuning hyper parameter \"Temperature\".","1fb123d4":"## Experment 2\nWe set the ratio between teacher's prediction and groundtruth is 1:9, and use the basic binary crossentropy loss."}}