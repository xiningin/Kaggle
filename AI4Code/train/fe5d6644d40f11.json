{"cell_type":{"cb9134e3":"code","ab2dc9ad":"code","e9379113":"code","71d64b99":"code","d6ac9d64":"code","98a8ac25":"code","02cf0301":"code","8e793f4f":"code","1c59e633":"code","2aba8df2":"code","31e2243b":"code","f04beb42":"code","dd83a12b":"code","7f72c318":"code","ff3478e3":"code","3f6ad3ac":"code","bbe2dd9c":"code","cfce659a":"code","b491b528":"code","169ad9be":"code","97198894":"code","d9636372":"code","705ed496":"code","e0257983":"code","c47e26a9":"code","ca9fa28b":"code","a82a9458":"code","198c7b89":"code","04cedabe":"code","aed87852":"code","c1679749":"code","d9705035":"code","0ad60168":"code","2953bc8c":"code","c8837f71":"code","3b0a2269":"code","77f7ef53":"code","ee14e5b5":"code","f0423b8a":"markdown","dc5a1b5b":"markdown","d54f332e":"markdown","9c2f9b63":"markdown","649b01e1":"markdown","806c0a68":"markdown","504b471b":"markdown","fe94abbf":"markdown","4f7df663":"markdown","69feb091":"markdown","6247f3db":"markdown","a15dccff":"markdown","cd5bc793":"markdown","58cf1a72":"markdown","ccbb0d3d":"markdown","6b396d71":"markdown","55fe7a28":"markdown","4d89c82f":"markdown","562b582d":"markdown","adfe8696":"markdown","5d359c43":"markdown","32ebb6f5":"markdown","177f20c1":"markdown","aff68d71":"markdown","0edd7f70":"markdown","72054bd9":"markdown","106f1cb0":"markdown","857c6c20":"markdown"},"source":{"cb9134e3":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\nfull_data=pd.concat([data, pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")])\ndata.head()","ab2dc9ad":"data.describe(include='all')","e9379113":"#NaN entries in the training dataset\ndata.isnull().sum()","71d64b99":"#NaN entries in the both datasets in total\nfull_data.isnull().sum()","d6ac9d64":"from plotly.offline import plot, iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nimport plotly.express as px\n\nfig = px.strip(data, y=\"Sex\", x=\"PassengerId\", color= 'Survived', stripmode='overlay')\nfig.show()","98a8ac25":"fig = px.histogram(data, x=\"Survived\", facet_col='Pclass', color='Sex', category_orders={\"Pclass\": [1, 2, 3]}, barmode='group')\nfig.show()","02cf0301":"data['Survived']=data['Survived'].astype('category')\nfig = px.box(data, x='Survived', y='Age', facet_col=\"Pclass\", color=\"Sex\", points='all',  category_orders={\"Pclass\": [1, 2, 3]})\nfig.show()","8e793f4f":"surv_ages=data[data.Survived==1]['Age']\nprint(\"STD of survival age with 80yo survivor:\", surv_ages.std(),     \n\"\\nSTD of survival age without 80yo survivor:\",surv_ages.drop(index=630).std(), \"\\n\")\n\n#records of 15 persons with highest age\n#the oldest survivor was at the age of 63 (except for  Mr.Algernon Henry Barkworth, who was actually 47)\ndata.sort_values(by='Age',ascending=False).head(15)","1c59e633":"fig = px.histogram(data, x='Age', facet_col=\"Sex\", color=\"Survived\", marginal=\"rug\",\n                   hover_data=data.columns, height=500, opacity=0.6, barmode='stack')\nfig.show()","2aba8df2":"# excluding 80 years old passenger\ndata.drop(index=630, inplace=True)","31e2243b":"fig = px.scatter(\n        data, x=\"Fare\", y=\"Age\", \n        color='Survived', \n        facet_row=\"Pclass\", height=1000, opacity=0.6,  category_orders={\"Pclass\": [1, 2, 3]} , log_x=True) \nfig.update_traces(marker_size=10)\nfig.show()","f04beb42":"#show passenger with 0 Fares sorted by ticket\ndata.loc[data.Fare==0].sort_values(by='Ticket')","dd83a12b":"fig = px.histogram(data, x=\"Embarked\", facet_col='Pclass', facet_row = 'Sex', height=700, color='Survived', category_orders={\"Pclass\": [1, 2, 3]})\nfig.show()","7f72c318":"df=data.groupby(['SibSp', 'Parch']).count()\nx, y =[x[0] for x in df.index], [y[1] for y in df.index]\ndf=pd.DataFrame({'SibSp':x, 'Parch':y, 'Count': list(df.PassengerId)})\n\nfig = px.scatter(df, x='SibSp', y='Parch', color='Count', size=np.log10(df.Count), size_max=30, color_continuous_scale='RdBu')\nfig.show()","ff3478e3":"#get all titles in datasets\nfull_data[\"Title\"]=full_data['Name'].map(lambda x: x.split('.')[0].split()[-1])\nfull_data[\"Title\"].value_counts()","3f6ad3ac":"data[\"Title\"]=data['Name'].map(lambda x: x.split('.')[0].split()[-1])\ndata.loc[data.Sex=='male','Title']=data.loc[data.Sex=='male']['Title'].map(lambda x: 'Mr' if x in ['Dr', 'Rev', 'Col', 'Major', 'Don', 'Capt', 'Jonkheer', 'Sir'] else x)\ndata.loc[data.Sex=='female','Title']=data.loc[data.Sex=='female']['Title'].map(lambda x: 'Mrs' if x in ['Mrs', 'Dr', 'Mme', 'Dona', 'Major', 'Countess', 'Lady'] else 'Miss')\ndata[\"Title\"].value_counts()","bbe2dd9c":"fig = px.strip(data, x=\"Age\", y=\"Title\", color= 'Survived', stripmode='overlay')\nfig.show()","cfce659a":"data['Age']=data.groupby(['Title', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))","b491b528":"#descriptive statistics of the Age in the datasets\nfull_data['Age'].describe()","169ad9be":"# Children\ndata.loc[data.Age <=12, 'Age_group'] = 0\n# From childs to 1st quartile \ndata.loc[(data.Age >12)&(data.Age <=21), 'Age_group'] = 1\n# From 1st quartile to median\ndata.loc[(data.Age >21)&(data.Age <=28), 'Age_group'] = 2\n# From median to 3rd quartile \ndata.loc[(data.Age >28)&(data.Age <=39), 'Age_group'] = 3\n# From 3rd quartile to age of the oldest survivor \ndata.loc[(data.Age >39)&(data.Age <=63), 'Age_group'] = 4\n# After excluding 80 years old survivor, there are no survivors over 63 in the training set \ndata.loc[data.Age >63, 'Age_group'] = 5\n\nfig = px.histogram(data, x=\"Age_group\", color='Survived', nbins=6, barmode='group', facet_col='Sex')\nfig.update_layout(\n    title_text='Survivors by age groups',\n    bargap=0.2,\n    bargroupgap=0.1)\nfig.show()","97198894":"#Fill zero fares and NaNs\ndata['Fare']=data.groupby('Pclass')['Fare'].apply(lambda x: x.fillna(x.median()))\ndata['Fare']=data.groupby('Pclass')['Fare'].apply(lambda x: x.replace(0.0,x.median()))","d9636372":"print('The 3rd class fare median:', full_data.groupby('Pclass')['Fare'].median()[3])\nprint('The 2nd class fare median:', full_data.groupby('Pclass')['Fare'].median()[2])\nprint('The 3rd quartile of all fares:', full_data.Fare.quantile(0.75))","705ed496":"#encode fares\n#The passengers with the Fare less, then 3rd class median\ndata.loc[ data['Fare'] <= 8.05, 'Fare'] = 0\n#The passengers with the Fare above 3rd class median, but less, then 2rd class median\ndata.loc[(data['Fare'] > 8.05) & (data['Fare'] <= 15.0458), 'Fare'] = 1\n#The passengers with the Fare above 2rd class median, but less, then 3rd quartile of all fares\ndata.loc[(data['Fare'] > 15.0458) & (data['Fare'] <= 31.275), 'Fare']   = 2 \n#The passengers with the Fare above 3rd quartile of all fares\ndata.loc[ data['Fare'] > 31.275, 'Fare']  = 3","e0257983":"data['Relatives']=data.SibSp + data.Parch","c47e26a9":"data['Survived']=data['Survived'].astype(float)\nfig=px.line(data.groupby(['Relatives','Pclass']).mean().reset_index(), x=\"Relatives\", y='Survived', color='Pclass')\nfig.show()","ca9fa28b":"data['Relatives']=data['Relatives'].map(lambda x: x if x <6 else 6)","a82a9458":"data['Embarked'].fillna(data.Embarked.mode()[0], inplace=True)","198c7b89":"data.drop(['PassengerId', 'Name', 'Ticket', 'Age', 'Cabin', 'Title'], axis=1, inplace=True)\n\ndata['Embarked']=data['Embarked'].map({'C':1, 'Q':2, 'S':3})   \ndata.Sex=data.Sex.map({'male':0, 'female':1})","04cedabe":"fig, ax = plt.subplots (figsize = (10,8))\nax.set_title('The correlation matrix of the training dataset')\nsns.heatmap(data.corr(),square=True, annot=True, cmap=plt.cm.Blues, ax=ax)","aed87852":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nX_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\nX_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n#delete 80yo survived outlier\nX_train.drop(index=630, inplace=True)\n\ny_train=X_train.Survived\nX_train.drop('Survived', axis=1, inplace=True)\n\n#this full list will be used for calculation of the Fares medians\npassengers=pd.concat([X_train, X_test])","c1679749":"def fill_age(data):\n    \"\"\"Fills missing values of the age in the DataFrame by medians (based on classes and titles) \n    and performs the age binning\"\"\" \n    \n    #Replacing NaN ages with the medians based on class and title.\n    data['Age']=data.groupby(['Title', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\n    \n    # Children\n    data.loc[data.Age <=12, 'Age_group'] = 0\n    # From childs to 1st quartile \n    data.loc[(data.Age >12)&(data.Age <=21), 'Age_group'] = 1\n    # From 1st quartile to median\n    data.loc[(data.Age >21)&(data.Age <=28), 'Age_group'] = 2\n    # From median to 3rd quartile \n    data.loc[(data.Age >28)&(data.Age <=39), 'Age_group'] = 3\n    # From 3rd quartile to age of the oldest survivor \n    data.loc[(data.Age >39)&(data.Age <=63), 'Age_group'] = 4\n    # After excluding 80 years old survivor, there are no survivors over 63 in the training set \n    data.loc[data.Age >63, 'Age_group'] = 5\n    \n    return data\n\n\ndef fill_price(data, population):\n    \"\"\"Fills missing  and zero values of the fare in the DataFrame by medians (based on classes) \n    and performs the fare binning\"\"\" \n    \n    # median of the price for each class in the population (both datasets)\n    class1 = population.loc[population.Pclass==1]['Fare'].median()\n    class2 = population.loc[population.Pclass==2]['Fare'].median()\n    class3 = population.loc[population.Pclass==3]['Fare'].median()\n    \n    #replacing of zero values\n    data.loc[data.Fare==0, 'Fare']=data.loc[data.Fare==0, 'Pclass'].map({1:class1, 2:class2, 3:class3})\n    #replacing of NaN values\n    data.loc[data.Fare.isnull(),'Fare']=data.loc[data.Fare.isnull(), 'Pclass'].map({1:class1, 2:class2, 3:class3})\n    \n    #encode fares\n    #The passengers with the Fare less, then 3rd class median\n    data.loc[ data['Fare'] <= 8.05, 'Fare'] = 0\n    #The passengers with the Fare above 3rd class median, but less, then 2rd class median\n    data.loc[(data['Fare'] > 8.05) & (data['Fare'] <= 15.0458), 'Fare'] = 1\n    #The passengers with the Fare above 2rd class median, but less, then 3rd quartile of all fares\n    data.loc[(data['Fare'] > 15.0458) & (data['Fare'] <= 31.275), 'Fare']   = 2 \n    #The passengers with the Fare above 3rd quartile of all fares\n    data.loc[ data['Fare'] > 31.275, 'Fare']  = 3 \n        \n    return data\n\ndef prepare_data(data, population):\n    #get the titles and group them to the 4 categories   \n    data[\"Title\"]=data['Name'].map(lambda x: x.split('.')[0].split()[-1])\n    data.loc[data.Sex=='male','Title']=data.loc[data.Sex=='male']['Title'].map(lambda x: 'Mr' if x in ['Dr', 'Rev', 'Col', 'Major', 'Don', 'Capt', 'Jonkheer', 'Sir'] else x)\n    data.loc[data.Sex=='female','Title']=data.loc[data.Sex=='female']['Title'].map(lambda x: 'Mrs' if x in ['Mrs', 'Dr', 'Mme', 'Don', 'Major','Countess', 'Lady'] else 'Miss')\n    \n    data=fill_age(data)\n    data=fill_price(data, population)\n    \n    #get Relatives\n    data['Relatives']=data.SibSp + data.Parch\n    data['Relatives']=data['Relatives'].map(lambda x: x if x <6 else 6)\n     \n    # Fill and encode Embarked       \n    data['Embarked'].fillna(population.Embarked.mode()[0], inplace=True)\n    data['Embarked']=data['Embarked'].map({'C':1, 'Q':2, 'S':3})\n    \n    # encode Sex\n    data.Sex=data.Sex.map({'male':0, 'female':1})\n                  \n    #drop PassengerId, Name, Ticket, Fare and Cabin columns\n    data.drop(['PassengerId', 'Name', 'Title','Age','Ticket', 'Cabin'], axis=1, inplace=True)\n           \n    return data","d9705035":"from sklearn.preprocessing import StandardScaler\nss=StandardScaler()\n\nX_test=prepare_data(X_test, passengers)\nX_train=prepare_data(X_train, passengers)\n\nX_train=ss.fit_transform(X_train)\nX_test=ss.transform(X_test)","0ad60168":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\n\nsplit= np.linspace(0.5, 0.95, 10)\nclfs = [('Random Forest', RandomForestClassifier()), \n        (\"KNN\", KNeighborsClassifier()), \n        (\"SVC\", SVC()), \n        (\"Naive Bayes\", GaussianNB()), \n        (\"XGBoost\", XGBClassifier(use_label_encoder=False, eval_metric='error')), \n        (\"Logistic Regression\", LogisticRegression())]\n\nscores=pd.DataFrame()\nfor n in list(split):\n    X_tr, X_te, y_tr, y_te = train_test_split(X_train, y_train, train_size=n, random_state=42)\n    for name, clf in clfs:\n        clf.fit(X_tr,y_tr)\n        scores.loc[n, name]=clf.score(X_te,y_te)    \n               \nscores.plot(figsize=(15,8), title='Accuracy scores of the base models with the train test split', xlabel='Train size', ylabel='Score', grid=True)","2953bc8c":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\n\nclfs = [('Random Forest', RandomForestClassifier()), \n        (\"KNN\", KNeighborsClassifier()), \n        (\"SVC\", SVC()), \n        (\"Naive Bayes\", GaussianNB()), \n        (\"XGBoost\", XGBClassifier(use_label_encoder=False, eval_metric='error')), \n        (\"Logistic Regression\", LogisticRegression())]\n\nscores=pd.DataFrame()\nfor n in list(range(2,6)):\n    cv=StratifiedKFold(n, shuffle=True, random_state=42)\n    for name, clf in clfs:         \n        scores.loc[n, name]=cross_val_score(clf, X_train, y_train, cv=cv).mean() \n        \nscores.plot(figsize=(15,8), title='Mean cross validation scores of the base models with the number of the Kfold', \n            xlabel='Kfold number', ylabel='Mean CV score', xticks=list(range(2,6)), grid=True)","c8837f71":"from sklearn.model_selection import GridSearchCV\n\nparams = {'C':[1,10,100,1000],'gamma':[1e-2, 1e-3], 'kernel':['linear','rbf']}\ngrid = GridSearchCV(SVC(),params, verbose=2, cv=4, n_jobs=-1).fit(X_train,y_train)\n\nprint('The best estimator is:', grid.best_estimator_)","3b0a2269":"from sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import plot_precision_recall_curve\nfrom sklearn.metrics import classification_report\nfrom sklearn.svm import SVC\n\n#svc=grid.best_estimator_\nsvc=SVC(C=100, gamma=0.01, kernel ='rbf')\n\ndef model_eval(estimator, X, y, test_size=0.33, random_state=42):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n    \n    estimator.fit(X_train, y_train)\n    \n    y_pred=estimator.predict(X_test)\n    \n    target_names = ['Deceased (0)', 'Survived (1)']\n    \n    print(classification_report(y_test, y_pred, target_names=target_names))\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (16,6))\n    \n    ax1.set_title('Confusion matrix')\n    ax2.set_title('Precision-Recall curve')\n    \n    plot_confusion_matrix(estimator, X_test, y_test,cmap=plt.cm.Blues, ax=ax1)\n    plot_precision_recall_curve(estimator, X_test, y_test, ax=ax2 )\n      \n    plt.show()\n    \nmodel_eval(svc, X_train, y_train)","77f7ef53":"#svc=grid.best_estimator_\nsvc=SVC(C=100, gamma=0.01, kernel ='rbf')\npredictions=svc.fit(X_train,y_train).predict(X_test)","ee14e5b5":"submission=pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\nsubmission['Survived']=predictions\n\nsubmission.to_csv('predictions.csv', index=False)","f0423b8a":"## \ud83d\udd16 Fare feature\n* The value of `Fare` is the cost of the ticket in the data set and several people may travel by the one `Ticket`.\n* As was mentioned before, there are some passengers with the `Fare` of 0 in each `Pclass` and that values should be redefined. \n* The code below replaces the zero `Fares` and NaN (there are NaN values in test set) with median based on the passenger class.","dc5a1b5b":"### \ud83d\udccc Datasets preparation and scaling","d54f332e":"## \ud83d\udd16 Number of relatives\nThe new feature `Relatives` is obtained as sum of `SibSp` and `Parch` for every passenger.","9c2f9b63":"### \ud83d\udccc Correlation of the features with the Survival\nNow let's drop some of the features that will not be used by the model and encode the `Sex` and `Embarked` features.<br>\nYou can see how `Relatives`, which is a linear combination of `SibSp` and `Parch`, is highly correlated with them. ","649b01e1":"### \ud83d\udccc The classifier metrics\nThe confusion matrix shows that there is a place for improvements as the model has the bias to the non-survival label.","806c0a68":"### \ud83d\udccc Parch and Sibsp variations","504b471b":"# Intro\nIn the real world the outcome of survival in catastrophe may highly depend on luck or coincidence. Although the set allows you to explicitly identify variables that increase the chances of survival, that variables can not guarantee it. Also, do not forget about the nature of some passengers who did not seek to leave the ship. According to the [Wiki page](https:\/\/en.wikipedia.org\/wiki\/Sinking_of_the_Titanic#CITEREFBallard1987):\n\n> *Other couples refused to be separated. Ida Straus, the wife of Macy's department store co-owner and former member of the United States House of Representatives Isidor Straus, told her husband: \"We have been living together for many years. Where you go, I go.\" They sat down in a pair of deck chairs and waited for the end.The industrialist Benjamin Guggenheim changed out of his life vest and sweater into top hat and evening dress and declared his wish to go down with the ship like a gentleman*.\n\n","fe94abbf":"### \ud83d\udccc Let's look closer at passengers with 'free tickets' below\nIt's hard to say was it a really free ticket or just unknown `Fare` filled with zeros. In any case, it would be better to redefine that fares based on cost of passenger classes. After all, we dont want the model to know that there may be the 1st or 2nd class passenger with lower `Fare` than almost all 3rd class passengers.","4f7df663":"There are not many passengers in the dataset with more than five `Relatives`. So values of 6 and more `Relatives` will be binned into one, marked with 6 (which means 6+).","69feb091":"Looks like some records has NaN values and number of records without specified `Cabin` is crucial. We can suggest that location of the cabin can make a significant contribution to the survival outcome. But, based on the large number of missing records, it was decided not to take into account this feature, which, moreover, most likely correlated with the class number. ","6247f3db":"# Feature Engineering\nAs was discovered above, there are some missing data in the columns.\n## \ud83d\udd16 Age feature\nLet's try to specify the `Age`. The title in the person's name can help to 'guess' the age a little more better. For example, 'Master' points us to to an underage boy, and 'Mrs.' to a married woman. The `Title` feature is obtained below.","a15dccff":"### \ud83d\udccc Scatter plot for Fare and Age values\n* The plots below shows distribution of survival outcomes along the `Fare` and `Age` axes in different `Pclass` ;\n* There is no surprise that greater `Fare` guarantees higher chance of survival, as shown at one of the plots before, the mortality rate in the 3rd class exceeds the rates of other more expensive classes;\n* For `Fare` values, a logarithmic scale is used, which distributes the points more evenly. But in this case, those few points with `Fare` of 0 are not displayed.","cd5bc793":"### \ud83d\udccc Replacing NaN ages with the medians based on class and title","58cf1a72":"### \ud83d\udccc Baseline models evaluation\nLet's run an accuracy test using train test split of different sizes and cross-validate with number of folds from two to five.","ccbb0d3d":"### \ud83d\udccc Parameters tuning\nIt was decided to stick with the SVM as it has highest average cross-validation score with the selected features.","6b396d71":"## \ud83d\udd16 Embarked\nThe final step is to replace the null values of the `Embarked` variable with the mode, since the number of missing values is very small.\n","55fe7a28":"The graph below allows you to see the variations in the number of `Relatives` in each of the `Pclass`, as well as the chances of survival for these groups.","4d89c82f":"### \ud83d\udccc The place of embarkment","562b582d":"Now give the data a closer look and highlight some observations:\n* Only 38.38 % of passengers had survived;\n* Mean of `Pclass` equals to 2.3, so most of the passengers in set traveled in 3rd class;\n* There are passengers with a \"Fare\" value of 0;\n* Each person has the sex specified and there is 577 males and 314 females;\n* There is only 681 unique tickets for 891 passengers, so you can assume that related people has the same ticket number and, as follows from the table below, these is the ticket that was used by 7 passengers;\n* The `Cabin` is only provided for 204 records.","adfe8696":"### \ud83d\udccc Proportion of survivors \nAs said above, only 38.38 % of passengers in training set had survived. \n","5d359c43":"### \ud83d\udccc Several age groups are defined below <br> \nThe `Age` quartiles determines borders of the middle groups (1-4). <br>\nThe bar plots represents the probability of survival for males and females in each age group.","32ebb6f5":"### \ud83d\udccc Now we bin the titles into four basic categories","177f20c1":"# EDA\nAt first let's load required libraries and given training data. ","aff68d71":"# Model building and evaluation\nLet's prepare the training and test datasets for the model again from the beginning.\n","0edd7f70":"# Uploading the predictions","72054bd9":"### \ud83d\udccc Finally, the Fares will be categorized","106f1cb0":"### \ud83d\udccc The graphs of the distribution of the number and ages of passengers among classes \n* The upper plot below represents total number of passengers by class, gender and survival outcome. Looks like travel in 1st and 2nd classes highly increase the chance of survival for women.\n* The boxplot shows the distribution of ages by class, gender and survival outcome. There is a very interesting outlier in group of survived males in 1st class. \n\n\n","857c6c20":"Let's look closer to the record of age outlier mentioned above. It tells that age of this passenger is equal to eighty - perhaps this person is quite important or incredibly lucky. Only this single record increases standard deviation of the survivors age subset from 14.66 to 14.95 years. \n\nThere is an idea of excluding this record out of training sample. What if google this person? [Algernon Henry Barkworth](http:\/\/\/www.encyclopedia-titanica.org\/titanic-survivor\/algernon-barkworth.html) - now things make sense, seems like there was a mistake in our record, and this gentlemen has died in 1945 at the age of 80. At the moment of Titanic disaster he was 47 years old. Although this record could be manually corrected, let's still remove it from the set after all."}}