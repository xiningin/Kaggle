{"cell_type":{"cd074932":"code","53090655":"code","9174ba98":"code","b9a8adec":"code","5e2dda19":"code","2b957bc0":"code","3f7c084b":"code","29b5fa94":"code","8a2641eb":"code","8cdd00f9":"code","f73f05b6":"code","4e3afc5e":"code","b1928836":"code","452f0aed":"code","b074de2c":"code","24e04e9e":"code","2f98c98f":"code","ea952cd4":"code","df7e3728":"code","7cd55e0e":"code","5eb5c436":"code","283b5f89":"code","b5b6c852":"code","30c95656":"code","fbed47f6":"code","38cfa32d":"code","e2eee308":"code","8a1d278f":"code","18f6b02c":"code","9df32489":"code","0af3752a":"code","7ef9ae3d":"markdown","4c3c1598":"markdown","11dbb7f2":"markdown","fd20eee2":"markdown"},"source":{"cd074932":"import cv2\nimport math\nimport joblib\nimport numpy as np\nimport pandas as pd\n\nimport scikitplot\nimport seaborn as sns\nfrom matplotlib import pyplot\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report\n\nimport tensorflow as tf\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Flatten, Dense, Conv2D, MaxPooling2D, Concatenate\nfrom tensorflow.keras.layers import Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import plot_model\n\nfrom keras.utils import np_utils","53090655":"df = pd.read_csv('..\/input\/fer2013\/fer2013.csv')\nprint(df.shape)\ndf.head()","9174ba98":"df.emotion.unique()","b9a8adec":"emotion_label_to_text = {0:'anger', 1:'disgust', 2:'fear', 3:'happiness', 4: 'sadness', 5: 'surprise', 6: 'neutral'}","5e2dda19":"df.emotion.value_counts()","2b957bc0":"sns.countplot(df.emotion)\npyplot.show()","3f7c084b":"math.sqrt(len(df.pixels[0].split(' ')))","29b5fa94":"fig = pyplot.figure(1, (14, 14))\n\nk = 0\nfor label in sorted(df.emotion.unique()):\n    for j in range(7):\n        px = df[df.emotion==label].pixels.iloc[k]\n        px = np.array(px.split(' ')).reshape(48, 48).astype('float32')\n\n        k += 1\n        ax = pyplot.subplot(7, 7, k)\n        ax.imshow(px, cmap='gray')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(emotion_label_to_text[label])\n        pyplot.tight_layout()","8a2641eb":"INTERESTED_LABELS = [3, 4, 6]","8cdd00f9":"df = df[df.emotion.isin(INTERESTED_LABELS)]\ndf.shape","f73f05b6":"img_array = df.pixels.apply(lambda x: np.array(x.split(' ')).reshape(48, 48, 1).astype('float32'))\nimg_array = np.stack(img_array, axis=0)","4e3afc5e":"img_array.shape","b1928836":"le = LabelEncoder()\nimg_labels = le.fit_transform(df.emotion)\nimg_labels = np_utils.to_categorical(img_labels)\nimg_labels.shape","452f0aed":"le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\nprint(le_name_mapping)","b074de2c":"facial_landmarks = joblib.load(\"..\/input\/fer2013-facial-landmarks\/facial_landmarks.pkl\")\nfacial_landmarks.shape","24e04e9e":"print(\"extracting HOG features, this may take some minutes...\")\n\nhog_feats = []\nfor img in img_array:\n    img = img.astype(\"uint8\")\n    img_ = cv2.resize(img, (64,128))\n    # img_ = cv2.equalizeHist(img_)\n\n    hog = cv2.HOGDescriptor()\n    hog_descr = hog.compute(img_)\n    hog_feats.append(hog_descr)\n\n\nhog_feats = np.array(hog_feats)\nprint(hog_feats.shape)","2f98c98f":"Xtrain_img, Xvalid_img, Xtrain_fl, Xvalid_fl, Xtrain_hog, Xvalid_hog, y_train, y_valid = \\\ntrain_test_split(img_array, facial_landmarks, hog_feats, img_labels,\n                shuffle=True, stratify=img_labels, test_size=0.1,\n                random_state=42)\n\nprint(Xtrain_img.shape, Xtrain_fl.shape, Xtrain_hog.shape, y_train.shape)\nprint(Xvalid_img.shape, Xvalid_fl.shape, Xvalid_hog.shape, y_valid.shape)","ea952cd4":"img_width = Xtrain_img.shape[1]\nimg_height = Xtrain_img.shape[2]\nimg_depth = Xtrain_img.shape[3]\nnum_classes = y_train.shape[1]","df7e3728":"# Normalizing image arrays\nXtrain_img = Xtrain_img \/ 255.\nXvalid_img = Xvalid_img \/ 255.","7cd55e0e":"def dcnn_pipeline(input_shape):\n    model_in = Input(shape=input_shape, name=\"input_DCNN\")\n    \n    conv2d_1 = Conv2D(\n        filters=64,\n        kernel_size=(3,3),\n        activation='elu',\n        padding='same',\n        kernel_initializer='he_normal',\n        name='conv2d_1'\n    )(model_in)\n    batchnorm_1 = BatchNormalization(name='batchnorm_1')(conv2d_1)\n    conv2d_2 = Conv2D(\n        filters=64,\n        kernel_size=(3,3),\n        activation='elu',\n        padding='same',\n        kernel_initializer='he_normal',\n        name='conv2d_2'\n    )(batchnorm_1)\n    batchnorm_2 = BatchNormalization(name='batchnorm_2')(conv2d_2)\n    \n    maxpool2d_1 = MaxPooling2D(pool_size=(2,2), name='maxpool2d_1')(batchnorm_2)\n    dropout_1 = Dropout(0.4, name='dropout_1')(maxpool2d_1)\n\n    conv2d_3 = Conv2D(\n        filters=128,\n        kernel_size=(3,3),\n        activation='elu',\n        padding='same',\n        kernel_initializer='he_normal',\n        name='conv2d_3'\n    )(dropout_1)\n    batchnorm_3 = BatchNormalization(name='batchnorm_3')(conv2d_3)\n    conv2d_4 = Conv2D(\n        filters=128,\n        kernel_size=(3,3),\n        activation='elu',\n        padding='same',\n        kernel_initializer='he_normal',\n        name='conv2d_4'\n    )(batchnorm_3)\n    batchnorm_4 = BatchNormalization(name='batchnorm_4')(conv2d_4)\n    \n    maxpool2d_2 = MaxPooling2D(pool_size=(2,2), name='maxpool2d_2')(batchnorm_4)\n    dropout_2 = Dropout(0.4, name='dropout_2')(maxpool2d_2)\n\n    conv2d_5 = Conv2D(\n        filters=256,\n        kernel_size=(3,3),\n        activation='elu',\n        padding='same',\n        kernel_initializer='he_normal',\n        name='conv2d_5'\n    )(dropout_2)\n    batchnorm_5 = BatchNormalization(name='batchnorm_5')(conv2d_5)\n    conv2d_6 = Conv2D(\n        filters=256,\n        kernel_size=(3,3),\n        activation='elu',\n        padding='same',\n        kernel_initializer='he_normal',\n        name='conv2d_6'\n    )(batchnorm_5)\n    batchnorm_6 = BatchNormalization(name='batchnorm_6')(conv2d_6)\n    \n    maxpool2d_3 = MaxPooling2D(pool_size=(2,2), name='maxpool2d_3')(batchnorm_6)\n    dropout_3 = Dropout(0.4, name='dropout_3')(maxpool2d_3)\n\n    flatten = Flatten(name='flatten_dcnn')(dropout_3)\n        \n    dense_1 = Dense(\n        128,\n        activation='elu',\n        kernel_initializer='he_normal',\n        name='dense1_dcnn'\n    )(flatten)\n    batchnorm_7 = BatchNormalization(name='batchnorm_7')(dense_1)\n    \n    model_out = Dropout(0.6, name='dropout_4')(batchnorm_7)\n    \n    return model_in, model_out","5eb5c436":"def facial_landmarks_pipeline(input_shape):\n    model_in = Input(shape=input_shape, name=\"input_Facial_Landmarks\")\n    flatten = Flatten(name=\"flatten_fl\")(model_in)\n    dense1 = Dense(64, activation=\"relu\", name=\"dense1_fl\")(flatten)\n    model_out = Dropout(0.4, name='dropout1_fl')(dense1)\n    \n    return model_in, model_out","283b5f89":"def facial_HOG_pipeline(input_shape):\n    model_in = Input(shape=input_shape, name=\"input_Facial_HOG\")\n    flatten = Flatten(name=\"flatten_hog\")(model_in)\n    dense1 = Dense(256, activation=\"relu\", name=\"dense1_hog\")(flatten)\n    model_out = Dropout(0.4, name='dropout1_hog')(dense1)\n        \n    return model_in, model_out","b5b6c852":"def merge_models(models_in: list, models_out: list, num_classes: int, show_summary=False):\n    \n    concated = Concatenate()(models_out)\n    dropout_1 = Dropout(0.4, name='dropout1_model')(concated)\n\n    dense1 = Dense(256, activation=\"relu\", name=\"dense1\")(dropout_1)\n    dropout_2 = Dropout(0.4, name='dropout2_model')(dense1)\n    out = Dense(num_classes, activation=\"softmax\", name=\"out_layer\")(dropout_2)\n\n    model = Model(inputs=models_in, outputs=out, name=\"FER_Model\")\n\n    if show_summary:\n        model.summary()\n    \n    return model","30c95656":"dcnn_in, dcnn_out = dcnn_pipeline(input_shape=(48,48,1))\nfl_in, fl_out = facial_landmarks_pipeline(input_shape=(68,2))\nhog_in, hog_out = facial_HOG_pipeline(input_shape=(3780,1))","fbed47f6":"model = merge_models(\n    models_in=[dcnn_in, hog_in],\n    models_out=[dcnn_out, hog_out],\n    num_classes=3\n)\nplot_model(model, show_shapes=True, show_layer_names=True, expand_nested=True, dpi=50, to_file='model_1.png')","38cfa32d":"early_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    min_delta=0.0001,\n    patience=5,\n    verbose=1,\n    restore_best_weights=True,\n)\n\nlr_scheduler = ReduceLROnPlateau(\n    monitor='val_accuracy',\n    min_delta=0.00025,\n    factor=0.25,\n    patience=3,\n    min_lr=1e-6,\n    verbose=1,\n)\n\ncallbacks = [\n    early_stopping,\n    lr_scheduler,\n]","e2eee308":"def data_generator(Xtrain_img, Xtain_hog, ytrain, batch_size):\n    while True:\n        idx = np.random.permutation(Xtrain_img.shape[0])\n\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.15,\n            height_shift_range=0.15,\n            shear_range=0.15,\n            zoom_range=0.15,\n            horizontal_flip=True,\n        )\n\n        batches = datagen.flow(Xtrain_img[idx], ytrain[idx], batch_size=batch_size, shuffle=False)\n        idx0 = 0\n        for batch in batches:\n            idx1 = idx0 + batch[0].shape[0]\n\n            yield [batch[0], Xtain_hog[idx[ idx0:idx1 ]]], batch[1]\n\n            idx0 = idx1\n            if idx1 >= Xtrain_img.shape[0]:\n                break","8a1d278f":"batch_size = 32\nepochs = 45\nlr = 0.001\noptim = optimizers.Adam(learning_rate=lr)\n\nmodel.compile(\n        loss='categorical_crossentropy',\n        optimizer=optim,\n        metrics=['accuracy']\n)\n\nhistory = model.fit(\n    data_generator(Xtrain_img, Xtrain_hog, y_train, batch_size=batch_size,),\n    validation_data=([Xvalid_img, Xvalid_hog], y_valid),\n    steps_per_epoch=len(Xtrain_img) \/ batch_size,\n    epochs=epochs,\n    callbacks=callbacks,\n    use_multiprocessing=True\n)","18f6b02c":"sns.set()\nfig = pyplot.figure(0, (12, 4))\n\nax = pyplot.subplot(1, 2, 1)\nsns.lineplot(history.epoch, history.history['accuracy'], label='train')\nsns.lineplot(history.epoch, history.history['val_accuracy'], label='valid')\npyplot.title('Accuracy')\npyplot.tight_layout()\n\nax = pyplot.subplot(1, 2, 2)\nsns.lineplot(history.epoch, history.history['loss'], label='train')\nsns.lineplot(history.epoch, history.history['val_loss'], label='valid')\npyplot.title('Loss')\npyplot.tight_layout()\n\npyplot.savefig('epoch_history_multipipe_model.png')\npyplot.show()","9df32489":"model_yaml = model.to_yaml()\nwith open(\"model.yaml\", \"w\") as yaml_file:\n    yaml_file.write(model_yaml)\n    \nmodel.save(\"model.h5\")","0af3752a":"yhat_valid = model.predict([Xvalid_img, Xvalid_hog, Xvalid_fl])\nyhat_valid = np.argmax(yhat_valid, axis=1)\n\nscikitplot.metrics.plot_confusion_matrix(np.argmax(y_valid, axis=1), yhat_valid, figsize=(7,7))\npyplot.savefig(\"confusion_matrix.png\")\n\nprint(f'total wrong validation predictions: {np.sum(np.argmax(y_valid, axis=1) != yhat_valid)}\\n\\n')\nprint(classification_report(np.argmax(y_valid, axis=1), yhat_valid))","7ef9ae3d":"`Now I will make the data compatible for neural networks.`","4c3c1598":"`So majority classes belongs to 3:Happy, 4:Sad and 6:Neutral nd we are also intersted in these three classes only.`","11dbb7f2":"The below data generator is taken from [this](https:\/\/github.com\/keras-team\/keras\/issues\/3386) keras issue thread.","fd20eee2":"`Splitting the data into training and validation set.`"}}