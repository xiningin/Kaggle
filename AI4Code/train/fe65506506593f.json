{"cell_type":{"a8961f8a":"code","957ff204":"code","b570ffb4":"code","83e28612":"code","3f0ec25b":"code","be357baf":"code","05968268":"code","5cfdbcff":"code","c362b537":"code","abaccc56":"code","91fd74ee":"code","9494a5d0":"code","49b54331":"code","fac49d8f":"code","07d6af17":"code","4c8a74c3":"code","2ad3a7dd":"code","586d8276":"code","2d8732f2":"code","84d2f02b":"code","9c9995dc":"code","511bc496":"code","bbfed356":"code","1520bb6d":"code","525b08f2":"code","62145b47":"code","9837aa67":"markdown","1d5fc3c4":"markdown","5e6829f2":"markdown","14b855c7":"markdown","2c396746":"markdown","0d44c3fa":"markdown","c8a28585":"markdown","8f5e2bd7":"markdown","05f27fbf":"markdown","b5c8c4a8":"markdown","15bef3dc":"markdown","d17597d8":"markdown","1e42fc1e":"markdown","da7f1261":"markdown","ff94fa80":"markdown","5c4eb6cf":"markdown"},"source":{"a8961f8a":"%reload_ext autoreload\n%autoreload 2","957ff204":"# Repo with all the fastai wrappers and helpers\n\n! git clone https:\/\/github.com\/deepklarity\/fastai-bert-finetuning.git fastai_bert_finetuning\n! mv fastai_bert_finetuning\/* .\n! rm -fr fastai_bert_finetuning","b570ffb4":"!ls","83e28612":"! pip install -r requirements.txt","3f0ec25b":"import csv\nimport pandas as pd\nfrom pathlib import Path\nimport matplotlib.cm as cm\nfrom fastai import *\nfrom fastai.text import *\nfrom fastai.callbacks import *\nfrom fastai.metrics import *\nimport utils, bert_fastai, bert_helper","be357baf":"# Seed random generators so all model runs are reproducible\n\nutils.seed_everything()","05968268":"# https:\/\/www.microsoft.com\/en-us\/download\/details.aspx?id=52398\n# Microsoft Research Paraphrase Corpus\nTASK='MRPC'\nDATA_ROOT = Path(\".\")\nlabel_col = \"Quality\"\ntext_cols = [\"#1 String\", \"#2 String\"]\n","5cfdbcff":"# Execute script to download MRPC data and create train.csv and test.csv\n\n! python download_glue_data.py --data_dir='glue_data' --tasks=$TASK --test_labels=True","c362b537":"train_df = pd.read_csv(DATA_ROOT \/ \"glue_data\" \/ \"MRPC\" \/ \"train.tsv\", sep = '\\t', quoting=csv.QUOTE_NONE)\ntrain_df.head()","abaccc56":"test_df = pd.read_csv(DATA_ROOT \/ \"glue_data\" \/ \"MRPC\" \/ \"test.tsv\", sep = '\\t', quoting=csv.QUOTE_NONE)\ntest_df.head()","91fd74ee":"print(f\"Number of Training records={len(train_df)}\")\nprint(f\"Number of Test records={len(test_df)}\")","9494a5d0":"def sample_sentences(quality, n=5):\n    ctr = 0\n    for row in train_df.query(f'Quality=={quality}').itertuples():\n        print(f\"1. {row[4]}\\n2. {row[5]}\")\n        print(\"=\"*100)\n        ctr += 1\n        if n==ctr:\n            break\n","49b54331":"# Different sentences samples            \nsample_sentences(0)","fac49d8f":"# Similar sentences samples            \nsample_sentences(1)","07d6af17":"# Specify BERT configs\n\nconfig = utils.Config(\n    bert_model_name=\"bert-base-uncased\",\n    num_labels=2, # 0 or 1\n    max_lr=2e-5,\n    epochs=3,\n    batch_size=32,\n    max_seq_len=128\n)","4c8a74c3":"fastai_tokenizer = bert_fastai.FastAITokenizer(model_name=config.bert_model_name, max_seq_len=config.max_seq_len)","2ad3a7dd":"databunch = TextDataBunch.from_df(\".\", train_df=train_df, valid_df=test_df,\n                  tokenizer=fastai_tokenizer.bert_tokenizer(),\n                  vocab=fastai_tokenizer.fastai_bert_vocab(),\n                  include_bos=False,\n                  include_eos=False,\n                  text_cols=text_cols,\n                  label_cols=label_col,\n                  bs=config.batch_size,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )\n","586d8276":"# Show wordpiece tokenized data\n\nfor i in range(5): \n    print(f\"Original==> {train_df.loc[i][text_cols[0]]},{train_df.loc[i][text_cols[1]]}\\n\\nTokenized==>. {databunch.x[i]}\")\n    print(\"=\"*100)","2d8732f2":"from pytorch_pretrained_bert.modeling import BertConfig, BertForSequenceClassification\n\n\nbert_model = BertForSequenceClassification.from_pretrained(\n    config.bert_model_name, len(train_df))\n\nlearner = bert_fastai.BertLearner(databunch,\n                                  bert_model,\n                                  metrics=[accuracy])\n\nlearner.callbacks.append(ShowGraph(learner))\n","84d2f02b":"preds, pred_values, true_labels = learner.get_predictions()\nlearner.print_metrics(preds, pred_values, true_labels)","9c9995dc":"txt_ci = TextClassificationInterpretation.from_learner(learner)","511bc496":"utils.custom_show_top_losses(txt_ci, test_df, text_cols, 5)","bbfed356":"learner.fit_one_cycle(config.epochs, max_lr=config.max_lr)","1520bb6d":"preds, pred_values, true_labels = learner.get_predictions()\nlearner.print_metrics(preds, pred_values, true_labels)","525b08f2":"txt_ci = TextClassificationInterpretation.from_learner(learner)","62145b47":"utils.custom_show_top_losses(txt_ci, test_df, text_cols, 10)","9837aa67":"# Model","1d5fc3c4":"Refer [A Tutorial to Fine-Tuning BERT with Fast AI](http:\/\/mlexplained.com\/2019\/05\/13\/a-tutorial-to-fine-tuning-bert-with-fast-ai\/) for deeper understanding of the code changes required for FastAI","5e6829f2":"**Acknowledgement**:\n\nThanks to [Keita Kurita](https:\/\/github.com\/keitakurita) for this excellent starter: [A Tutorial to Fine-Tuning BERT with Fast AI](http:\/\/mlexplained.com\/2019\/05\/13\/a-tutorial-to-fine-tuning-bert-with-fast-ai\/)","14b855c7":"Interpret the results","2c396746":"Now, we can build the databunch using the tokenizer we build above. Notice we're passing the `include_bos=False` and `include_eos=False` options. This is to prevent fastai from adding its own Start-Of-Sentence (SOS)\/End-Of-Sentence (EOS) tokens that will interfere with BERT's SOS\/EOS tokens.","0d44c3fa":"Now with the data in place, we will prepare the model. Again, the pytorch-pretrained-bert package gives us a sequence classifier based on BERT straight out of the box. We also build FastAI `Learner`.","c8a28585":"### Dataset\n\nMRPC dataset a text file containing 5800 pairs of sentences which have been extracted from news sources on the web, along with human annotations indicating whether each pair captures a paraphrase\/semantic equivalence relationship. The column named **Quality** indicates whether the sentences are similar (1) or not (0). **\"#1 String\" and \"#2 String\"** columns contain the sentences.","8f5e2bd7":"### Pre-trained Model\n\nWe will use op-for-op PyTorch reimplementation of Google's BERT model provided by pytorch-pretrained-BERT library. Refer the Github repo for more info: https:\/\/github.com\/huggingface\/pytorch-pretrained-BERT ","05f27fbf":"### Using BERT with fastai\n\nThere are three things we need to be careful of when using BERT with fastai:\n\n   1. BERT uses its own wordpiece tokenizer.\n    \n   [WordPiece](https:\/\/stackoverflow.com\/questions\/55382596\/how-is-wordpiece-tokenization-helpful-to-effectively-deal-with-rare-words-proble\/55416944#55416944) is a commonly used technique to segment words into subword-level in NLP tasks. In this approach an out of vocabulary word is progressively split into subwords and the word is then represented by a group of subwords. Since the subwords are part of the vocabulary, we have learned representations n context for these subwords and the context of the word is simply the combination of the context of the subwords. For more details regarding this approach please refer [Neural Machine Translation of Rare Words with Subword Units](https:\/\/arxiv.org\/pdf\/1508.07909)\n       \n   **How does this help?**\n\n  Imagine that the model sees the word walking. Unless this word occurs at least a few times in the\ntraining corpus, the model can't learn to deal with this word very well. However, it may have the\nwords walked, walker, walks, each occurring only a few times. Without subword segmentation, all these\nwords are treated as completely different words by the model.\nHowever, if these get segmented as walk@@ ing, walk@@ ed, etc., notice that all of them will now have\nwalk@@ in common, which will occur much frequently while training, and the model might be able to\nlearn more about it.\n   \n   2. BERT needs [CLS] and [SEP] tokens added to each sequence.\n    \n  For classification tasks we need to add [CLS] token before every input sequence.\n[SEP] token is used to separate sentences.\n    \n   3. BERT uses its own pre-built vocabulary.\n","b5c8c4a8":"#### Github Repo: https:\/\/github.com\/DeepakSinghRawat\/fastai-bert-finetuning","15bef3dc":"### Setup code for training","d17597d8":"## Objective\n\nIn this notebook we will finetune pre-trained BERT model on The Microsoft Research Paraphrase Corpus (MRPC). MRPC is a paraphrase identification dataset, where systems aim to identify if two sentences are paraphrases of each other.","1e42fc1e":"#### Next Steps\n\n- There's major improvement in the accuracy and loss by just running for few epochs. We can improve the model further to acc = 0.8504901960784313 and f1 = 0.8974789915966387 by using the techniques mentioned in the original BERT paper. Refer to https:\/\/github.com\/huggingface\/pytorch-pretrained-BERT#mrpc for details.\n- Use BertAdam Optimizer [BertAdam](https:\/\/github.com\/huggingface\/pytorch-pretrained-BERT\/blob\/694e2117f33d752ae89542e70b84533c52cb9142\/README.md#optimizers)\n[Is BertAdam better](https:\/\/github.com\/huggingface\/pytorch-pretrained-BERT\/issues\/420)\n\n- Finetune BERT on Quora insincere questions.\n- Explore LAMB Optimizer: https:\/\/forums.fast.ai\/t\/lamb-optimizer\/43582\/19\n- Compare with ULMFiT, gpt-2","da7f1261":"Let's print the accuracy and f1_score of the pre-trained model on this dataset","ff94fa80":"## BERT\n\n### Overview: \n1. Trained on BookCorpus and English Wikipedia (800M and 2,500M words respectively).\n2. Training time approx. about a week using 64 GPUs.\n3. State-Of-The-Art (SOTA) results on SQuAD v1.1 and all 9 GLUE benchmark tasks.\n\n### Architecture:\n\n#### Embedding Layers\n\n<img src=\"https:\/\/i2.wp.com\/mlexplained.com\/wp-content\/uploads\/2019\/01\/Screen-Shot-2019-01-04-at-3.32.48-PM.png?w=1128\">\n\n- Token embeddings, Segment embeddings and Position embeddings. \n- [SEP] token to mark the end of a sentence.\n- [CLS] token at the beginning of the input sequence, to be used only if classifying.\n\n- The convention in BERT is:\n\n(a) For sequence pairs:\n tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n \n type_ids:   0   0  0    0    0     0      0   0    1  1  1   1  1   1\n \n(b) For single sequences:\n tokens:   [CLS] the dog is hairy . [SEP]\n \n type_ids:   0   0   0   0  0     0   0\n\nWhere \"type_ids\" are used to indicate whether this is the first\nsequence or the second sequence. The embedding vectors for `type=0` and\n`type=1` were learned during pre-training and are added to the wordpiece\nembedding vector (and position vector). This is not *strictly* necessary\nsince the [SEP] token unambigiously separates the sequences, but it makes\nit easier for the model to learn the concept of sequences.\n        \n- For classification tasks, the first vector (corresponding to [CLS]) is\nused as as the \"sentence vector\". The  first  token  of  every  sequence  is  always  the  special  classification  embedding([CLS]). The  final  hidden  state  (i.e.,  out-put of Transformer) corresponding to this token  is  used  as  the  aggregate  sequence  rep-resentation for classification tasks. For non-classification tasks, this vector is ignored.\n\n**Note that this only makes sense because the entire model is fine-tuned.** \n\nBERT is bidirectional, the [CLS] is encoded including all representative information of all tokens through the multi-layer encoding procedure. The representation of [CLS] is individual in different sentences. [CLS] is later fine-tuned on the downstream task. Only after fine-tuning, [CLS] aka the first token can be a meaningful representation of the whole sentence. [Link](https:\/\/github.com\/google-research\/bert\/issues\/196)\n\n#### Encoders\n\n<img src=\"http:\/\/jalammar.github.io\/images\/bert-base-bert-large-encoders.png\">\n\nBERT BASE (12 encoders) and BERT LARGE (24 encoders)\n\n#### Training\n\n1. **Masked LM (MLM)**: Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a [MASK] token.\n\n    Training the language model in BERT is done by predicting 15% of the tokens in the input, that were randomly picked. \n\n    These tokens are pre-processed as follows\u200a\u2014\u200a80% are replaced with a \u201c[MASK]\u201d token, 10% with a random word, and 10% use the original word. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence.\n\n2. **Next Sentence Prediction (NSP)**: In the BERT training process, the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. ","5c4eb6cf":"### This notebook requires Python >= 3.6 and fastai==1.0.52"}}