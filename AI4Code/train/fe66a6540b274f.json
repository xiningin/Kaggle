{"cell_type":{"6bc16444":"code","8216db9b":"code","eb4dd3df":"code","f88e0da7":"code","3ce44c1f":"code","3eabe32c":"code","28273531":"code","c50a9d57":"code","f6813f23":"code","f7870491":"code","21b2f95f":"code","aa3e0a41":"code","b4dd3ae3":"code","f0369ea8":"code","4bdb0856":"code","1a0430bb":"code","b96fec25":"code","ce937d34":"code","d15c8624":"code","35e53897":"code","87620d6b":"code","f2a151e7":"code","75b0bebf":"code","6f11c30f":"code","02df9469":"code","715657f3":"code","677b7ba2":"code","ea2f5322":"code","e67ff38e":"code","7c006936":"code","38ee4c85":"code","96a8711d":"code","ef7a4d28":"code","2962013c":"code","399ba151":"code","2a178acf":"code","a3016617":"code","1e3dbff9":"code","9268da7c":"code","e5be150f":"code","c6872e18":"markdown","7a8af1ab":"markdown","c3d7f7b0":"markdown","00e639ec":"markdown","c8948d36":"markdown","cbb611a8":"markdown","39b3db65":"markdown","ad970f08":"markdown","7a13039a":"markdown","4956c74a":"markdown","f7d1106e":"markdown","444d33c5":"markdown","39d3417c":"markdown","1956cd5b":"markdown","3dc802e3":"markdown","d90ae859":"markdown","0e1fd52c":"markdown","61ca2114":"markdown","0ac3715e":"markdown","5aaea512":"markdown","eb96eee6":"markdown","eb58c50b":"markdown","210de44c":"markdown","1eeeaa1f":"markdown","9a3cc95c":"markdown"},"source":{"6bc16444":"import numpy as np #linear algebra \nimport pandas as pd #creating and manipulating dataframes\nimport matplotlib.pyplot as plt #visuals\nimport seaborn as sns #visuals\n\nfrom sklearn.cluster import KMeans #K-Means\nfrom sklearn.cluster import DBSCAN #DBSCAN\n\nfrom sklearn.preprocessing import StandardScaler #scaler","8216db9b":"#read the data\nblobs = pd.read_csv('..\/input\/clustering-datasets\/cluster_blobs.csv')\nmoons = pd.read_csv('..\/input\/clustering-datasets\/cluster_moons.csv')\ncircles = pd.read_csv('..\/input\/clustering-datasets\/cluster_circles.csv')","eb4dd3df":"#scatter plot\nplt.figure(figsize = (8,4), dpi = 100)\nsns.scatterplot(data=blobs,x='X1',y='X2')\nplt.show()","f88e0da7":"#scatter plot\nplt.figure(figsize = (8,4), dpi = 100)\nsns.scatterplot(data=moons,x='X_1',y='X_2')\nplt.show()","3ce44c1f":"#scatter plot\nplt.figure(figsize = (8,4), dpi = 100)\nsns.scatterplot(data=circles,x='X1',y='X2')\nplt.show()","3eabe32c":"#rename moons columns \nmoons.rename({\"X_1\": 'X1', 'X_2': 'X2'}, axis = 1, inplace = True)","28273531":"#define the function \ndef display_categories(model,data):\n    labels = model.fit_predict(data)\n    plt.figure(figsize = (8,4), dpi = 100)\n    sns.scatterplot(data=data,x='X1',y='X2',hue=labels,palette='Set1')\n    plt.show()","c50a9d57":"#intiate the model\nmodel = KMeans(n_clusters = 2)\ndisplay_categories(model,moons)","f6813f23":"#intiate the model\nmodel = KMeans(n_clusters = 3)\ndisplay_categories(model,blobs)","f7870491":"#intiate the model\nmodel = KMeans(n_clusters = 2)\ndisplay_categories(model,circles)","21b2f95f":"#intiate the model\nmodel = DBSCAN(eps=0.6)\ndisplay_categories(model,blobs)","aa3e0a41":"#intiate the model\nmodel = DBSCAN(eps=0.15)\ndisplay_categories(model,moons)","b4dd3ae3":"#intiate the model\nmodel = DBSCAN(eps=0.15)\ndisplay_categories(model,circles)","f0369ea8":"# lets read another dataframe\ntwo_blobs = pd.read_csv('..\/input\/clustering-datasets\/cluster_two_blobs.csv')\ntwo_blobs_outliers = pd.read_csv('..\/input\/clustering-datasets\/cluster_two_blobs_outliers.csv')","4bdb0856":"#scatter plot\nplt.figure(figsize = (8,4), dpi = 100)\nsns.scatterplot(data=two_blobs,x='X1',y='X2')\nplt.show()","1a0430bb":"#scatter plot\nplt.figure(figsize = (8,4), dpi = 100)\nsns.scatterplot(data=two_blobs_outliers,x='X1',y='X2')\nplt.show()","b96fec25":"#the model\ndbscan = DBSCAN()\ndisplay_categories(dbscan,two_blobs)","ce937d34":"#the model\ndisplay_categories(dbscan,two_blobs_outliers)","d15c8624":"#understanding the effect of changing epsilon \ndbscan = DBSCAN(eps = 0.001) #the default 0.5 \ndisplay_categories(dbscan,two_blobs_outliers)","35e53897":"#understanding the effect of changing epsilon \ndbscan = DBSCAN(eps = 10) #the default 0.5 \ndisplay_categories(dbscan,two_blobs_outliers)","87620d6b":"#understanding the effect of changing epsilon \ndbscan = DBSCAN(eps = 1) #the default 0.5 \ndisplay_categories(dbscan,two_blobs_outliers)","f2a151e7":"# the number of outliers\nnp.sum(dbscan.labels_ == -1)","75b0bebf":"# the percent of outliers\n100 * np.sum(dbscan.labels_ == -1) \/ len(dbscan.labels_)","6f11c30f":"#Create an Elbo values \noutlier_percent = []\nnumber_of_outliers = []\n\nfor eps in np.linspace(0.001,10,100):\n    \n    # Create Model\n    dbscan = DBSCAN(eps=eps)\n    dbscan.fit(two_blobs_outliers)\n    \n    # Log Number of Outliers\n    number_of_outliers.append(np.sum(dbscan.labels_ == -1))\n    \n    # Log percentage of points that are outliers\n    perc_outliers = 100 * np.sum(dbscan.labels_ == -1) \/ len(dbscan.labels_)\n    \n    outlier_percent.append(perc_outliers)","02df9469":"#create the Elbo Graph \nplt.figure(figsize = (8,4), dpi = 100)\nsns.lineplot(x=np.linspace(0.001,10,100),y=number_of_outliers)\nplt.ylabel(\"Percentage of Points Classified as Outliers\")\nplt.xlabel(\"Epsilon Value\")","715657f3":"#create the Elbo Graph \nplt.figure(figsize = (8,4), dpi = 100)\nsns.lineplot(x=np.linspace(0.001,10,100),y=number_of_outliers)\n\n#X, Y limits\nplt.xlim(0,2)\nplt.ylim(0,50)\n\n#H-Line \nplt.hlines(y=3,xmin=0,xmax=2,colors='red',ls='--')\n\n#labels \nplt.ylabel(\"Percentage of Points Classified as Outliers\")\nplt.xlabel(\"Epsilon Value\")","677b7ba2":"#read the data\ndf = pd.read_csv('..\/input\/clustering-datasets\/wholesome_customers_data.csv')","ea2f5322":"#top 5 rows \ndf.head()","e67ff38e":"#EDA\nplt.figure(figsize = (8,4), dpi = 100)\nsns.scatterplot(data=df,x='Milk',y='Grocery',hue='Channel')\nplt.show()","7c006936":"#EDA\nplt.figure(figsize = (8,4), dpi = 100)\nsns.histplot(df,x='Milk',hue='Channel',multiple=\"stack\")\nplt.show()","38ee4c85":"#an annotated clustermap of the correlations between spending on different cateogires.\nprint('Correlation Between Spending Categories')\nsns.clustermap(df.drop(['Region','Channel'],axis=1).corr(),annot=True);","96a8711d":"#PairPlot of the dataframe, colored by Region.\nsns.pairplot(df,hue='Region',palette='Set1')","ef7a4d28":"#Scale the data \nscaler = StandardScaler()\nscaled_X = scaler.fit_transform(df)","2962013c":"outlier_percent = []\n\nfor eps in np.linspace(0.001,3,50):\n    \n    # Create Model\n    dbscan = DBSCAN(eps=eps,min_samples=2*scaled_X.shape[1])\n    dbscan.fit(scaled_X)\n   \n     \n    # Log percentage of points that are outliers\n    perc_outliers = 100 * np.sum(dbscan.labels_ == -1) \/ len(dbscan.labels_)\n    \n    outlier_percent.append(perc_outliers)","399ba151":"#a line plot of the percentage of outlier points versus the epsilon value choice.\nplt.figure(figsize = (8,4), dpi = 100)\nsns.lineplot(x=np.linspace(0.001,3,50),y=outlier_percent)\nplt.ylabel(\"Percentage of Points Classified as Outliers\")\nplt.xlabel(\"Epsilon Value\")\nplt.show()","2a178acf":"#fit DBSCAN with eps = 2\ndbscan = DBSCAN(eps=2)\ndbscan.fit(scaled_X)","a3016617":"# plot the result \nplt.figure(figsize = (8,4), dpi = 100)\nsns.scatterplot(data=df,x='Grocery',y='Milk',hue=dbscan.labels_)\nplt.show()","1e3dbff9":"# plot the result \nplt.figure(figsize = (8,4), dpi = 100)\nsns.scatterplot(data=df,x='Detergents_Paper',y='Milk',hue=dbscan.labels_)\nplt.show()","9268da7c":"#Create a new column on the original dataframe called \"Labels\" consisting of the DBSCAN labels.\ndf['Labels'] = dbscan.labels_","e5be150f":"#Compare the statistical mean of the clusters and outliers for the spending amounts on the categories.\ncats = df.drop(['Channel','Region'],axis=1)\ncats.groupby('Labels').mean()","c6872e18":"# Aim of the project \n\nThis project aims to explore the difference in performance between the two algorithms, we will text each algorithm on 3 datasets with different levels of complexity.","7a8af1ab":"> We know that we have 3 outliers in our dataset, so we set a h-line on 3 and we figured out the corresponding value of Epsilon that gives us this particular number of outliers is some where around 0.75 - In general we are searching for this turning point. May be for example you are interested for 1 percent of points as outliers, you can definitely play with that. ","c3d7f7b0":"> Create a new column on the original dataframe called \"Labels\" consisting of the DBSCAN labels.","00e639ec":"> Setting Epsilon = 1 gives us the desired outcome as shown in the above graph ","c8948d36":"As we can see the DBSCAN is much more accurate. It is able to capture complex relationships between features. Further more, the algorithms was able to spot outliers (Labeled as -1 in the blobs graph). You can have more insights about the way the algorithms work through this [link](https:\/\/www.naftaliharris.com\/blog\/visualizing-dbscan-clustering\/)\n\nHowever, one might think that these points are not outliers, here we should change the default parameters of DBSCAN to take that into account. The parameter that is responsible for that called \"epsilon\" which deside the range of which n pints is to be considered as neighbors.","cbb611a8":"# Introduction to K-Means\n\nK-means is a centroid-based algorithm, or a distance-based algorithm, where we calculate the distances to assign a point to a cluster. In K-Means, each cluster is associated with a centroid.\n\nThe main objective of the K-Means algorithm is to minimize the sum of distances between the points and their respective cluster centroid.\n","39b3db65":"> Compare the statistical mean of the clusters and outliers for the spending amounts on the categories.","ad970f08":"### But, how to automatically search for reasonable values for epsilon?","7a13039a":"> Using DBSCAN and a for loop to create a variety of models testing different epsilon values. Set min_samples equal to 2 times the number of features. During the loop, keep track of and log the percentage of points that are outliers. For reference the solutions notebooks uses the following range of epsilon values for testing:","4956c74a":"# Hyperparameter tuning","f7d1106e":"# Introduction to DBSCAN\nDensity-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, J\u00f6rg Sander and Xiaowei Xu in 1996. It is a density-based clustering non-parametric algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). \n\nDBSCAN is one of the most common clustering algorithms and also most cited in scientific literature. In 2014, the algorithm was awarded the test of time award (an award given to algorithms which have received substantial attention in theory and practice) ","444d33c5":"> We are interested in reflection points where we go from extreme number of outliers to some extreme number of no outliers","39d3417c":"> Change Epsilon to 10: While the color is still the same, It is now considering every point as in the same cluster (0 not -1). The distance range is so large, so every point is fitting in the same cluster. ","1956cd5b":"## The Data\nSource: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Wholesale+customers\n\nAbstract: The data set refers to clients of a wholesale distributor. It includes the annual spending in monetary units (m.u.) on diverse product categories.\n\nAttribute Information:\n\n    1) FRESH: annual spending (m.u.) on fresh products (Continuous);\n    2) MILK: annual spending (m.u.) on milk products (Continuous);\n    3) GROCERY: annual spending (m.u.)on grocery products (Continuous);\n    4) FROZEN: annual spending (m.u.)on frozen products (Continuous)\n    5) DETERGENTS_PAPER: annual spending (m.u.) on detergents and paper products (Continuous)\n    6) DELICATESSEN: annual spending (m.u.)on and delicatessen products (Continuous);\n    7) CHANNEL: customers  Channel - Horeca (Hotel\/Restaurant\/Caf\u00c3\u00a9) or Retail channel (Nominal)\n    8) REGION: customers  Region Lisnon, Oporto or Other (Nominal)","3dc802e3":"# DBSCAN Results","d90ae859":"# Real Life Example","0e1fd52c":"> **Conclusion:** We clearly see that because K-Means is distance based algorithms, it is unable to capture complex relationships between features. ","61ca2114":"> Setting Epsilon to 0.001: Epsilon is the maximum distance between to samples for a one to be considered as the in the same neighborhood of the other. So, as Epsilon gets smaller there is a really tiny maximum distance which means pretty much every thing should be an outlier. No point is discovering any other point in the Epsioln distancw which means that every thing is outlier.\n","0ac3715e":"> We need to adjust epsilon to make the algorithm understand that the 3 red dots near the green cluster are not actually outliers","5aaea512":"> We can see that the outliers have the most extreme spending values. The algorithm did a very good job with detecting these outliers.\n\n> Thanks and see you in the next project ","eb96eee6":"# Imports ","eb58c50b":"# Label Discovery Function","210de44c":"> Based on the plot created in the previous task, retrain a DBSCAN model with a reasonable epsilon value. Note: For reference, the solutions use eps=2.","1eeeaa1f":"# Kmeans Results","9a3cc95c":"# Visalize the 3 datasets "}}