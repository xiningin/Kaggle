{"cell_type":{"39970b82":"code","15e2095a":"code","19028456":"code","f87462ad":"code","7834291c":"code","f916ae23":"code","2bcf2eed":"code","e9d3f049":"code","485afb72":"code","ce1fbaaa":"code","d1dace6b":"code","3cdbfb1f":"code","02504483":"code","6afb0df4":"code","7c6fc002":"code","2d09ba06":"code","ee1bfbde":"code","dab8848c":"code","1d5e409b":"code","c1b06961":"code","595994ab":"code","b4a34ff1":"code","b7b279a6":"code","e0c6d831":"code","934553b9":"code","b6ad5bf4":"code","5d826897":"code","e5dd7707":"code","3845ce7a":"code","3be51cec":"code","dd95894d":"code","4ce96c4a":"code","73419a05":"code","412028eb":"code","a15e0b19":"code","4a08e6e5":"code","19cd1632":"code","05a49680":"markdown","a0f87ca5":"markdown","3b5001f6":"markdown","e8c9a533":"markdown","9c0e1a3a":"markdown","0bcbedcf":"markdown","fbfb04f6":"markdown","cfe8ca28":"markdown"},"source":{"39970b82":"import numpy as np\nimport pandas as pd\nimport os\nimport string\nimport re\n\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","15e2095a":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom spacy.lemmatizer import Lemmatizer\nfrom spacy.lookups import Lookups\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom wordcloud import WordCloud\nstop_words = stopwords.words('english')\n\nfrom gensim.models.ldamodel import LdaModel","19028456":"DATA_PATH = '\/kaggle\/input\/nlp-topic-modelling'","f87462ad":"data = pd.read_csv(os.path.join(DATA_PATH, 'Reviews.csv'))\nprint(\"Data has {} rows and {} columns\".format(data.shape[0], data.shape[1]))","7834291c":"text = data.sample(5000).reset_index()['Text']","f916ae23":"# Looking at sample text\nprint(text[1])\nprint(text[132])","2bcf2eed":"print(word_tokenize(text[1]))\nprint(text[1].split(\" \"))","e9d3f049":"translator=str.maketrans('','',string.punctuation) # To remove punctuation\nlemmatizer = WordNetLemmatizer()\ndef clean_text(sent):\n    return ' '.join([lemmatizer.lemmatize(word.lower()) for word in word_tokenize(sent.translate(translator))\\\n     if word not in stop_words and len(word)>2 and word.isalpha()])","485afb72":"cleanText = text.apply(lambda x : clean_text(x))","ce1fbaaa":"print(text[23])\nprint(\"-------\")\nprint(cleanText[23])","d1dace6b":"tfidfVectorizer = TfidfVectorizer(max_features=500)\ntfidfVectors = tfidfVectorizer.fit_transform(cleanText)","3cdbfb1f":"tfidfVectors.shape","02504483":"print(tfidfVectors[234:244, 198:212].todense())","6afb0df4":"tfidfVectors.shape","7c6fc002":"lsa = TruncatedSVD(n_components=5, algorithm='randomized', n_iter=3, random_state=18)\nlsa_out = lsa.fit_transform(tfidfVectors)","2d09ba06":"lsa.explained_variance_ratio_","ee1bfbde":"lsa.singular_values_","dab8848c":"lsa_out.shape","1d5e409b":"for i, topic in enumerate(lsa_out[0]):\n    print(\"Topic \", i, \" : \", topic*100)","c1b06961":"lsa.components_.shape","595994ab":"vocab = tfidfVectorizer.get_feature_names()\ntopic_content = []","b4a34ff1":"for v in lsa.components_:\n    sorted_vocab = sorted(zip(vocab, v), key=lambda x : x[1], reverse=True)\n    topic_content.append({x:y for x, y in sorted_vocab})","b7b279a6":"print(\"Top 5 words in topic 1 : \", list(topic_content[0].keys())[:5])\nprint(\"Top 5 words in topic 2 : \", list(topic_content[1].keys())[:5])\nprint(\"Top 5 words in topic 3 : \", list(topic_content[2].keys())[:5])\nprint(\"Top 5 words in topic 4 : \", list(topic_content[3].keys())[:5])\nprint(\"Top 5 words in topic 5 : \", list(topic_content[4].keys())[:5])","e0c6d831":"wc1= WordCloud(background_color=\"black\", max_words=500)\nwc1.generate_from_frequencies(topic_content[0])\n\nfig = plt.figure(1, figsize=(15, 15))\nplt.imshow(wc1, interpolation=\"bilinear\")\nplt.title(\"Topic 1\")\nplt.axis(\"off\")\nplt.show()","934553b9":"wc2= WordCloud(background_color=\"black\", max_words=500)\nwc2.generate_from_frequencies(topic_content[1])\n\nfig = plt.figure(1, figsize=(15, 15))\nplt.imshow(wc2, interpolation=\"bilinear\")\nplt.title(\"Topic 2\")\nplt.axis(\"off\")\nplt.show()","b6ad5bf4":"wc3= WordCloud(background_color=\"black\", max_words=500)\nwc3.generate_from_frequencies(topic_content[2])\n\nfig = plt.figure(1, figsize=(15, 15))\nplt.imshow(wc3, interpolation=\"bilinear\")\nplt.title(\"Topic 3\")\nplt.axis(\"off\")\nplt.show()","5d826897":"wc4= WordCloud(background_color=\"black\", max_words=500)\nwc4.generate_from_frequencies(topic_content[3])\n\nfig = plt.figure(1, figsize=(15, 15))\nplt.imshow(wc4, interpolation=\"bilinear\")\nplt.title(\"Topic 4\")\nplt.axis(\"off\")\nplt.show()","e5dd7707":"wc5= WordCloud(background_color=\"black\", max_words=500)\nwc5.generate_from_frequencies(topic_content[4])\n\nfig = plt.figure(1, figsize=(15, 15))\nplt.imshow(wc5, interpolation=\"bilinear\")\nplt.title(\"Topic 5\")\nplt.axis(\"off\")\nplt.show()","3845ce7a":"from gensim import corpora, models\nimport gensim","3be51cec":"splitText = cleanText.apply(lambda x:word_tokenize(x))","dd95894d":"dictionary = corpora.Dictionary(splitText)\ncorpus = [dictionary.doc2bow(text) for text in splitText]","4ce96c4a":"lda = LdaModel(corpus, num_topics=5)","73419a05":"import pyLDAvis.gensim\nprint(lda.print_topics(num_topics=5, num_words=3))","412028eb":"lda.print_topics()[0]","a15e0b19":"import pyLDAvis.gensim\npyLDAvis.enable_notebook()\nnews = pyLDAvis.gensim.prepare(lda,corpus, dictionary)","4a08e6e5":"news","19cd1632":"# https:\/\/github.com\/cemoody\/lda2vec\n# https:\/\/github.com\/huseinzol05\/NLP-Models-Tensorflow\/tree\/master\/topic-model","05a49680":"### Data preprocessing\nFrom looking at the sample data, the text looks relatively better.\n\nThe preprocessing I will be doing is\n* Converting text to lower\n* Removing punctuation\n* Removing stopwords, numbers and words with length less than 3\n* Lemmatization of words","a0f87ca5":"### Importing libraries","3b5001f6":"### Loading data","e8c9a533":"Explaining the above function:\n* word_tokenize : Split the text into tokens(i.e. words in this case)\n* translate : Removes punctuation","9c0e1a3a":"Most popular algorihms for topic modelling are LSA(a.k.a LSI in NLP) and LDA.","0bcbedcf":"#### Latent Semantic Analysis (LSA)\n\nLSA uses SVD for dimensionality reduction of word X document matrix \n","fbfb04f6":"### LDA","cfe8ca28":"Since we need only reviews data for topic modelling, I am taking only text column and limiting the number of reviews to 5000 because of time and computational constraints."}}