{"cell_type":{"33108a09":"code","8184e4e7":"code","6430d41c":"code","832431ec":"code","c11b33c8":"code","1ef8d68d":"code","b87faa0e":"code","4eec498c":"code","0c12a9b3":"code","abc61bdd":"code","74dcd8fc":"code","f55fd488":"code","e57dfe5e":"code","fcd4608f":"code","57e8f1a1":"code","9d8c290c":"code","589a962d":"code","413e729f":"code","194b7923":"code","59cc1c60":"code","ebbbf99d":"code","f284c6fc":"code","906e418d":"code","ce03f164":"code","6b82b1e9":"code","5571a25d":"code","e44f1782":"code","7dc0ea42":"code","d2dde583":"code","c4f814c8":"code","82adb75a":"code","45c20b30":"code","b6fa6bb3":"code","b51847cb":"code","6cefe6af":"code","682249f9":"code","4a5d4546":"code","3fde0e79":"markdown","33faa4e6":"markdown","44105fe5":"markdown","18fd34a5":"markdown","a49a7260":"markdown","af0e80d6":"markdown","1e252113":"markdown","b92b42f6":"markdown","fcb5c765":"markdown","3e9f41f4":"markdown","318af1ed":"markdown","481db111":"markdown","ff2992dd":"markdown","e64ef3dd":"markdown","ae2fb354":"markdown","e629b30b":"markdown","0e22644a":"markdown","4ea4272b":"markdown","3b78ab8a":"markdown","73dd27dd":"markdown","d5ec78e1":"markdown","c938d550":"markdown","28ec5fb1":"markdown","69f0b6b4":"markdown","37ab1130":"markdown","d0b6d919":"markdown","8027955c":"markdown","db629558":"markdown","b21dface":"markdown","6db7648f":"markdown","4a298fe8":"markdown","7db07bde":"markdown","ac26420a":"markdown","2348275c":"markdown","053ef7ba":"markdown","1c62ae23":"markdown","3ccc9462":"markdown","71ba40f9":"markdown","4a999fb4":"markdown","076330f4":"markdown","5e5ffae4":"markdown","f8f51356":"markdown","d5a2be7c":"markdown"},"source":{"33108a09":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8184e4e7":"import pandas as pd\nimport numpy as np\n\nimport altair as alt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as matplotlib\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures, PowerTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom sklearn.metrics import confusion_matrix\n\n\nalt.data_transformers.disable_max_rows()\nsns.set_context('notebook')\nsns.set_theme(style=\"whitegrid\")\nsns.set_palette('Set2')","6430d41c":"#Author: Dennis Trimarchi\ndef make_confusion_matrix(cf,\n                          group_names=None,\n                          categories='auto',\n                          count=True,\n                          percent=True,\n                          cbar=True,\n                          xyticks=True,\n                          xyplotlabels=True,\n                          sum_stats=True,\n                          figsize=None,\n                          cmap='Blues',\n                          title=None):\n    '''\n    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n    Arguments\n    ---------\n    cf:            confusion matrix to be passed in\n    group_names:   List of strings that represent the labels row by row to be shown in each square.\n    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n    count:         If True, show the raw number in the confusion matrix. Default is True.\n    normalize:     If True, show the proportions for each category. Default is True.\n    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n                   Default is True.\n    xyticks:       If True, show x and y ticks. Default is True.\n    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n    sum_stats:     If True, display summary statistics below the figure. Default is True.\n    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n                   See http:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html\n                   \n    title:         Title for the heatmap. Default is None.\n    '''\n\n\n    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n    blanks = ['' for i in range(cf.size)]\n\n    if group_names and len(group_names)==cf.size:\n        group_labels = [\"{}\\n\".format(value) for value in group_names]\n    else:\n        group_labels = blanks\n\n    if count:\n        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n    else:\n        group_counts = blanks\n\n    if percent:\n        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()\/np.sum(cf)]\n    else:\n        group_percentages = blanks\n\n    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n\n\n    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n    if sum_stats:\n        #Accuracy is sum of diagonal divided by total observations\n        accuracy  = np.trace(cf) \/ float(np.sum(cf))\n\n        #if it is a binary confusion matrix, show some more stats\n        if len(cf)==2:\n            #Metrics for Binary Confusion Matrices\n            precision = cf[1,1] \/ sum(cf[:,1])\n            recall    = cf[1,1] \/ sum(cf[1,:])\n            f1_score  = 2*precision*recall \/ (precision + recall)\n            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n                accuracy,precision,recall,f1_score)\n        else:\n            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n    else:\n        stats_text = \"\"\n\n\n    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n    if figsize==None:\n        #Get default figure size if not set\n        figsize = plt.rcParams.get('figure.figsize')\n\n    if xyticks==False:\n        #Do not show categories if xyticks is False\n        categories=False\n\n\n    # MAKE THE HEATMAP VISUALIZATION\n    plt.figure(figsize=figsize)\n    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n\n    if xyplotlabels:\n        plt.ylabel('True label')\n        plt.xlabel('Predicted label' + stats_text)\n    else:\n        plt.xlabel(stats_text)\n    \n    if title:\n        plt.title(title)\n","832431ec":"df_train = pd.read_csv('\/kaggle\/input\/adult-pmr3508\/train_data.csv')\ndf_train.head()\n\n","c11b33c8":"df_train.shape","1ef8d68d":"df_test = pd.read_csv('\/kaggle\/input\/adult-pmr3508\/test_data.csv')\ndf_test.head()","b87faa0e":"df_test.shape","4eec498c":"df_train.info()","0c12a9b3":"df_train = df_train.drop(columns = ['Id'])","abc61bdd":"cat_vars = [i for i in df_train.columns if df_train.dtypes[i] == 'object']\nnum_vars = [i for i in df_train.columns if df_train.dtypes[i] == 'float64' or df_train.dtypes[i] == 'int64']","74dcd8fc":"sns.set(font_scale= 4)\nfig, axes = plt.subplots(2, 3, figsize=(35,25), sharey=False)\nfig.suptitle('Distribution for all numerical variables', size = 40)\nfig.tight_layout(h_pad = 2, w_pad = 2 )\n\nfor i, num_var in enumerate(num_vars):\n    ax = sns.histplot(df_train[num_var], ax = axes[i\/\/3, i%3], kde = 1)\n    \nplt.show()","f55fd488":"sns.set(font_scale= 5)\nfig, axes = plt.subplots(3, 3, figsize=(70,60), sharey=False)\nfig.tight_layout(h_pad = 8, w_pad = 2 )\n\nfor i, cat_var in enumerate(cat_vars):\n    \n    ax = sns.countplot(x = cat_var, hue= 'income', data=df_train, ax = axes[i\/\/3, i%3])\n    ax.tick_params(axis='x', labelrotation=90)\n\nplt.show()","e57dfe5e":"df_train.education.value_counts()","fcd4608f":"dict_order = {'Preschool':0, '1st-4th':1, '5th-6th':2, '7th-8th':3, '9th':4, '10th': 5,\n              '11th': 6, '12th':7, 'HS-grad':8, 'Prof-school':9, 'Assoc-acdm':10, 'Assoc-voc':11,\n              'Some-college':12,  'Bachelors':13, 'Masters':14,  'Doctorate':15}\n\ndf_train['education'] = df_train['education'].apply(lambda x: dict_order[x])\ndf_test['education'] = df_test['education'].apply(lambda x: dict_order[x])","57e8f1a1":"cat_vars = [i for i in df_train.columns if df_train.dtypes[i] == 'object']\nnum_vars = [i for i in df_train.columns if df_train.dtypes[i] == 'float64' or df_train.dtypes[i] == 'int64']","9d8c290c":"df_train['marital.status'].value_counts()","589a962d":"dict_assemble_variables_marital = {'Separated': 'Divorced', 'Married-AF-spouse':'Other',\n                           'Married-spouse-absent': 'Other', 'Widowed': 'Other' }\n\ndef preprocessing_assemble_variables(x, dict_assemble_variables):\n    if x in dict_assemble_variables:\n        return dict_assemble_variables[x]\n    return x\n\ndf_train['marital.status'] = df_train['marital.status'].apply(lambda x: preprocessing_assemble_variables(x,dict_assemble_variables_marital))\ndf_test['marital.status'] = df_test['marital.status'].apply(lambda x: preprocessing_assemble_variables(x,dict_assemble_variables_marital))\n\ndf_train['marital.status'].value_counts()","413e729f":"df_train.race.value_counts()","194b7923":"dict_assemble_variables_race = {'Amer-Indian-Eskimo': 'Other'}\n\ndf_train['race'] = df_train['race'].apply(lambda x: preprocessing_assemble_variables(x,dict_assemble_variables_race))\ndf_test['race'] = df_test['race'].apply(lambda x: preprocessing_assemble_variables(x,dict_assemble_variables_race))\n\ndf_train['race'].value_counts()","59cc1c60":"df_train['native.country'].value_counts()","ebbbf99d":"def preprocess_country(x):\n    if x in ['United-States','?','Mexico']:\n        return x\n    return 'Other'\n\n\ndf_train['native.country'] = df_train['native.country'].apply(lambda x: preprocess_country(x))\ndf_test['native.country'] = df_test['native.country'].apply(lambda x: preprocess_country(x))\n\ndf_train['native.country'].value_counts()","f284c6fc":"sns.set_context('notebook')\nsns.pairplot(df_train, hue = 'income')","906e418d":"fig, ax = plt.subplots(figsize=(10,10))         # Sample figsize in inches\nsns.heatmap(df_train[num_vars].corr(), annot = True)","ce03f164":"df_train = df_train.drop(columns = ['education'])\ndf_test = df_test.drop(columns = ['education'])","6b82b1e9":"X_train, X_val, y_train, y_val = train_test_split(df_train.drop('income',axis=1),\n                                                  df_train['income'], train_size=0.8, random_state=42\n                                                 )","5571a25d":"cat_vars_complete = [i for i in df_train.columns if df_train.dtypes[i] == 'object']\nnum_vars_complete = [i for i in df_train.columns if df_train.dtypes[i] == 'float64' or df_train.dtypes[i] == 'int64']\n\nignore_variables = ['fnlwgt','education']\ncat_vars = [element for element in cat_vars_complete if (element != 'income' and element not in ignore_variables)]\nspecific_vars = []\nnum_vars = [element for element in num_vars_complete if (element not in specific_vars + ignore_variables)]\n\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler()),\n#     ('polynomial_features',PolynomialFeatures(interaction_only=True)),\n#     ('pca', PCA())\n])\n\nspecific_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler()),\n    ('power_transform', PowerTransformer(method='yeo-johnson')),\n#     ('polynomial_features',PolynomialFeatures(interaction_only=False)),\n\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n#     ('truncated_svd', TruncatedSVD())\n#     ('sparse_pca', sparse_pca),\n#     ('to_dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)\n    ])\n\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, num_vars),\n        ('specific', specific_transformer, specific_vars),\n        ('cat', categorical_transformer, cat_vars),\n    ])","e44f1782":"knn_classifier = KNeighborsClassifier()\n\nparam_dist = {\"n_neighbors\": [28, 30, 33, 35],\n              \"weights\": ['uniform','distance'],\n              \"metric\": ['euclidean','manhattan']\n             }\n\nknn_grid_search = GridSearchCV(knn_classifier, param_grid=param_dist\n                               ,scoring='accuracy',cv=5)\n\nmodel = Pipeline(steps = [('preprocessor', preprocessor),\n                            ('model', knn_grid_search\n                            )\n                         ])","7dc0ea42":"model.fit(X_train, y_train)\ny_pred = model.predict(X_val)","d2dde583":"model['model'].best_params_","c4f814c8":"model['model'].best_score_","82adb75a":"cf_matrix = confusion_matrix(y_val, y_pred, labels=[\"<=50K\", \">50K\"])\nlabels = ['True Neg','False Pos','False Neg','True Pos']\ncategories = ['<=50K', '>50K']\nmake_confusion_matrix(cf_matrix, \n                      group_names=labels,\n                      categories=categories, \n                      cmap='binary')","45c20b30":"model.fit(df_train.drop(columns = ['income']),df_train['income'])","b6fa6bb3":"model['model'].best_params_","b51847cb":"model['model'].best_score_","6cefe6af":"y_pred = model.predict(df_test.drop(columns = ['Id']))","682249f9":"final_pred = pd.DataFrame(columns = [\"Id\",\"income\"])\n\nfinal_pred.Id = df_test.Id\nfinal_pred.income = y_pred","4a5d4546":"final_pred.to_csv(\"submission.csv\", index=False)","3fde0e79":"We see that the `education` variable we created seems to be completely correlated with the `education.num` variable, so we may want to drop one of them, since a correlation between variables could harm our model. (less probable in KNN but in general this is true). Let's drop education, since it is the one we created and is probably less trustworthy. Moreover, `hours.per.week` and `fnlwgt` seem not to be really important to our problem. So we might consider not putting that on our final model. ","33faa4e6":"## 1.2 Excluding Id column","44105fe5":"# 0. Setup code\nIn this section, we will import some packages, set up our data visualization environment and read our data from the CSVs. ","18fd34a5":"- `marital.status`: here, we also may try to put some variables together, such as divorced and separated. Moreover, we notice most people with more than 50k of income are actually in the category Married-civ-spouse. Furthermore, we see that relationship is kind of a less noisy version of this variable, so we may want to not use it afterwards, in order to decrease the dimension of the problem. \n\nLet's pre-treat this variable, by putting separated and divorced together and by putting less common categories in a common \"Other\" category. ","a49a7260":"## 1.4 Understanding the distribution of the variables and preprocessing the variables","af0e80d6":"## 0.1 Importing packages ","1e252113":"[1] Hastie, Trevor, Trevor Hastie, Robert Tibshirani, and J H. Friedman. **The Elements of Statistical Learning: Data Mining, Inference, and Prediction**. New York: Springer, 2001.\n\n[2] Larry Wasserman. **All of statistics: a concise course in statistical inference**. Springer Science & Business Media,2013.","b92b42f6":"Now let's take a look at our categorical variables and how they are related to our target variable income. ","fcb5c765":"Now let's actually see our results comparing `y_val` and `y_pred`. To do that, we will plot a confusion matrix, that will be a lot more useful than just knowing the accuracy. We will also have the values for precision, recall and the F1 Score, which are really important metrics. \n\nWe recall here the meaning of this metrics: \n\n$$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN} = \\text{the % got right}$$\n\n$$Precision = \\frac{TP}{TP + FP} = \\text{% of instances classified as Positive that were right}$$\n\n$$Recall = \\frac{TP}{TP + FN} = \\text{% of Positive instances that we actually captured}$$\n\n$$F1_{score} = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} = \\text{harmonic mean of Precision and Recall}$$\n\nwhere `T` means true, `F` means False, saying if our classification corresponded to the reality or not and `P` means positive and `N` means negative, regarding the classification of our model. Therefore, a `FP`, for instance, is something our model thought was positive, but the real label was negative. \n\nHere, our `positive = '>50K'` of income.  \n\n","3e9f41f4":"## 1.3 Separating the types of variables","318af1ed":"## 2.1 Splitting the data","481db111":"The first thing we want to do is to split the training data in a train and validation data set. Here, we won't call it a test data set, because the actual test data set is the one on Kaggle that we are going to test at the really end. Here, we will use the `X_train` and `y_train` to train and optimize our hyperparameters, using cross validation. Then, we will use `X_val` and `y_val` to test our optimized model. \n\nWe will also set a random_state, just to be sure we will get the same split every time we run the code.","ff2992dd":"In order to build our model, we will use the `Pipeline` function from sklearn, which makes our life much easier. Moreover, we will separate our variables in four types of variables: \n\n- `ignore_variables`: variables we will not use, since they are not really useful to our final model and are basically just fitting some noise;\n- `cat_vars`: categorical variables that are not in the ignore_variables list. \n- `specific_vars`: list of variables that have high skewness where it might be worth trying some different preprocessing techniques, such as the PowerTransformer. \n- `num_vars`: numerical variables that are not in the ignore_variables, neither on the specific_vars list. \n\n\nSeparating the variables like that make it possible for us to treat the variables differently. For instance, for the numerical variables, we want to scale the features, since KNN highly depends on the distances, so if the variables have different scales, it won't work well. On the other hand, for categorical variables, we will use the one hot encoder, for instance. \n\nAfter some iteration and testing (using for instance the Backward stepwise feature selection method), we decide that we want to ignore the variables `fnlwgt` and `education`. Moreover, our specific_transformer was not working particularly well for this problem, so we might just say that we will not put any variables on the specific variable features. \n","e64ef3dd":"## 1.1 Exploring types of variables\/missing values","ae2fb354":"- `sex`: males tend to have higher income than females. Moreover, we have more data from males than females. \n\n- `native.country`: we see that most the data we have come from the USA. We also have some '?' variables, which we may keep, since they are quite representative. Also, we seem to have a significat amount of data for Mexico. However, we can put the rest of the countries together in a 'Other' category in order to decrease the dimension of the problem.  \n\nLet's do that:","e629b30b":"Here, we will try to understand the distribution of our numerical variables. In order to do that, we will use the function `sns.histplot` from the seaborn package, here imported as _sns_. This function is really useful, since, besides plotting an histogram of the data, it also plots the `Kernel Density Estimation` or KDE, which is smoother and converges faster to the true density than the histograms. The KDE non-parametric statistic method and is really useful in practice. \n\nA kernel [2] is basically a smooth function K, such that $K(X) \\ge 0$ , $\\int K(x) dx = 1$, $\\int x K(x) dx = 0$ and $\\sigma_K^2 \\equiv \\int x^2 K(x) dx > 0$. The kernel used in the seaborn package is the Gaussian or Normal kernel, which is written as $K(x) = (2\\pi)^{-1\/2}e^{-x^2\/2}$. Then, the Kernel Density Estimator itself is written as: \n\n$$\\hat{f}(x) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K(\\frac{x-X_i}{h}) $$\n\nwhere $h$ is a positive number called the bandwidth. There are a lot of theories on how one may choose $h$, but this is out of the escpope of this notebook. Let's now take a look at how our data is distributed. ","0e22644a":"## 0.3 Reading data from csv","4ea4272b":"We can separate the different types of variables by the following code. ","3b78ab8a":"# 4. References ","73dd27dd":"# PMR3508 - KNN for Dataset Adult\n\n#### Author: Caio Iglesias\n\nThe goal of this notebook is to predict if the income of people is bigger than 50k or smaller than that, based on features such as their age, their education, their country, etc. To do that, we will use the Adult Dataset from the `UCI Machine Lerning Repository`, that can be found here: https:\/\/archive.ics.uci.edu\/ml\/index.php . Moreover, the goal here is not necessarily to obtain the best possible performance (here measured in accuracy) but to develop a solid framework that solves this problem, based on the theory learned on the course. We will use mostly `sklearn` to develop the Pipelines of our model, `pandas` to manipulate the data and `matplotlib` and `seaborn` for data visualization.","d5ec78e1":"## 2.4 Evaluating our model","c938d550":"# 3. Exporting final model","28ec5fb1":"## 0.2 Importing function to plot confusion matrix","69f0b6b4":"## 2.3 Fitting the model","37ab1130":"Here, we will use `pandas` to read the training data and test data. ","d0b6d919":"Below, we can see the types of the variables we are working with. Moreover, we see that we do not have any missing values (null values), which is great. ","8027955c":"The first thing we can do is drop the variable `Id`, since there is no reason for it to influence the income of someone and we would just be fitting some noise, by putting it in our classifier.","db629558":"We notice that we have `15 features and 1 target variable (income)`. We also see that we have a lot of categorical data, that we will need to treat them carefully in order to not harm the performance of our KNN model (more on that on section 2). Let's take a look at the shape of our train and test data sets. ","b21dface":"# 2. Developing our KNN model","6db7648f":"## 1.5 Understanding the correlation between the variables","4a298fe8":"Let's recompute our cat_vars and num_vars to take that into account. ","7db07bde":"We can see that our `accuracy` is around 84.9%, our `precision` is around 71.8% and our `recall` is around 64.9%. Our `F1 Score` is thus near 59.3%. We can see that our worst metric is thus the Recall and that we are not being able of identifying almost 40% of the true labels. ","ac26420a":"We can conclude a lot of different interesting things. Summing up, we can conclude that: \n\n- `workclass`: there are actually some camouflaged null values here, with the '?'. However, we may not remove this data, since it might contain some information, i.e. there might be a pattern in the people we don't know the workclassk. We also notice that most people with high income have workclass = Private. \n\n- `education`: besides being a categorical variable, we may want to ordenate them, since the degree of education naturally follows a sequence. This may help us when transforming this variables into numbers, since we won't need to use One Hot Enconding, which would increase the dimension of our problem by a lot. Moreover, we see that people with lower levels of education don't usually have a big income, which matches our intuition.\n\nIn order to do that, we will need to order the variables manually. Let's try to do that: ","2348275c":"After doing some iterations and try some different combination of parameters in our Grid Search, we finally get that the best parameters we have for our problem are `{'metric': 'euclidean', 'n_neighbors': 28, 'weights': 'uniform'}`. Let's check the best CV score of our model: ","053ef7ba":"Before diving into our modelling, let's review what `KNN classification` means. Firstly, we know [1] that: \n\n$$ \\hat{G}(x) = \\mathcal{G}_k \\text{ if } \\mathbb{P}(\\mathcal{G}_k \\mid X = x) = \\max_{g\\in\\mathcal{G}}\\,\\mathbb{P}(g \\mid X = x) $$\n\nThis classifier is called the _`Bayes Classifier`_ and is basically saying that we will classify the most probable classifier, given the data. This is the classifier that minimizes the Expected Prediction Error and its error rate is called the _`Bayes rate`_. Here the problem is a little bit simpler, since we only have two classes. \n\nWhat is interesting to note is that the KNN classifier is actually trying to directly approximate this solution: the probability is estimated by training-samples proportion and the conditional probability in a point is relaxed to a conditional probability on a neighborhood of a point. Therefore, it seems that, when having a big training set, we would be able to approximate the theorical optimal classifier. \n\nHowever, the problem here is that our intuition might not work well in high dimensions. That is what is called the _`curse of dimensionality`_. For instance, if we want to take 1% of all the data in a unit hyper cube, we need an edge of 0.63 (since $0.63^{10} \\approx 0.01$). This is counterintuitive but it is why we need to be hyper careful when pre processing our data, specially our categorical variables (since we could easily use a One Hot Encoding for every variable and increase the dimension hugely). ","1c62ae23":"Now that everything is ready, we call finally fit our model and predict what we think are the labels for the `X_val` data. ","3ccc9462":"After building the preprocessor Pipeline, we can finally move into our model. For this specific challenge, we are only allowed to use the KNN classifier, so that's what we will try to do. We will use here a `GridSearchCV`, which will basically try every possible combination of hyperparameters on the dictionary `param_dist`, doing cross validations each time. By doing that, our final model will have the best possible combination that reduces our CV Errors. Moreover, since `GridSearchCV` is initialized with `refit = True`, once it finds the best estimator, it retrains it on the whole training set, which is great. ","71ba40f9":"As we can see, we have `32560 data points for the training set` and `16280 for the test set`. Moreover, the test set does not contain the target variable, so in order to train and test our model we will need to use the training data set. ","4a999fb4":"- `occupation`: we also see some camouflaged null values with the '?' symbol. Again, we might not want to not consider them because they may contain some information. Otherwise, this feature seems pretty useful and don't need a lot of pre processing. \n\n- `relationship`: we see that this variable seems to be a combination of the marital.status variable and sex variable. Therefore, we might want to drop some of them in order to decrease the dimension of the problem. For instance, we see that Husbands tend to have bigger income, which we could also conclude by looking at the sex variable + the marital.status variable. \n\n- `race`: we see that most people with high income are white. However, there are not many Ameri-Indian-Eskimo, so we may want to put them together with the other group. \n\nLet's do that: ","076330f4":"In this section, we will try to understand if the variables are correlated or not, and try to see what variables are more important to predict our target variable. ","5e5ffae4":"# 1. Exploring the data","f8f51356":"We notice that many of the distributions are quite skewed, so we might want to try to do some pre processing steps that fit well this type of that. Moreover, `capital.gain` and `capital.loss` seem to be quite concentrated around 0. ","d5a2be7c":"## 2.2 Making our pipeline"}}