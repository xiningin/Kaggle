{"cell_type":{"28e0409e":"code","f38a468e":"code","03d58b12":"code","2d85abfb":"code","9b4eb187":"code","59abc4f5":"code","af450295":"code","0bc9fda0":"code","2e599c08":"code","71a98372":"code","219a5153":"code","c7a27c4c":"code","f8e0327a":"code","dcb3f092":"markdown"},"source":{"28e0409e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom pathlib import Path\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport IPython.display as ipd\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom glob import glob\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.optimizers import Adam\n\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import LayerNormalization, Input, LSTM, GRU, TimeDistributed\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU, MaxPooling2D, Flatten, Dropout, GlobalAveragePooling2D, Dense, Softmax, Bidirectional, GlobalAveragePooling1D\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, ReduceLROnPlateau\n\nfrom keras.layers.merge import add\n\nfrom tensorflow.keras.metrics import AUC\n","f38a468e":"!mkdir logs\n!mkdir models","03d58b12":"data = np.load('..\/input\/seti-breakthrough-listen\/train\/0\/00034abb3629.npy')\nprint(data.shape)","2d85abfb":"df_train = pd.read_csv('..\/input\/seti-breakthrough-listen\/train_labels.csv')","9b4eb187":"df_train.head()","59abc4f5":"df_train['target'].value_counts()","af450295":"TRAIN_DIRECTORY = \"\/kaggle\/input\/seti-breakthrough-listen\/train\/\"\nTEST_DIRECTORY = \"\/kaggle\/input\/seti-breakthrough-listen\/test\/\"\n\n\nMODEL_NAME = \"SIMPLE_LSTM\"\nEPOCHS = 10\nTRAINING_SET_SIZE = 0.8\nBATCH_SIZE = 64\nMULTI_THREAD = True","0bc9fda0":"class SignalGenerator(tf.keras.utils.Sequence):\n    def __init__(self, df, directory, batch_size=32, shuffle=True, training=True):\n        self.directory = directory\n        self.df = df\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.training = training\n        \n        self.on_epoch_end()\n    \n    def __len__(self):\n        return np.ceil(len(self.df) \/ self.batch_size).astype(int)\n\n    def __getitem__(self, index):\n        batch = self.df[index * self.batch_size:(index + 1) * self.batch_size]\n        \n        signals = []\n        \n        signals_1 = np.empty((len(batch), 3, 256, 273), dtype=np.float32)\n        signals_2 = np.empty((len(batch), 3, 256, 273), dtype=np.float32)\n        \n        i = 0\n        \n        for filename in batch.id:\n            # Get the path : directory\/first_char\/filename.npy\n            path = os.path.join(self.directory, filename[0], filename + \".npy\")\n            data = np.load(path)\n            \n            # Transpose the dimenssion\n            data = np.transpose(data, (0, 2, 1)).astype('float32')\n            \n            \n            #\u00a0signals = np.transpose(np.stack(signals), (0, 1, 3, 2)).astype('float32')\n            #\u00a0signals.append(data)\n        \n            signals_1[i, 0,] = data[0]\n            signals_1[i, 1,] = data[2]\n            signals_1[i, 2,] = data[4]\n        \n            signals_2[i, 0,] = data[1]\n            signals_2[i, 1,] = data[3]\n            signals_2[i, 2,] = data[5]\n            \n            i += 1\n        \n        \n        # Transform the array to the correct input shape\n        # signals = np.stack(signals)\n        \n        # Add the two signal to one array\n        inp = [signals_1, signals_2]\n        \n        if self.training:\n            #\u00a0return signals, batch.target.values\n            return inp, batch.target.values\n        else:\n            # return signals\n            return inp\n    \n    def on_epoch_end(self):\n        if self.shuffle:\n            np.random.seed(42)\n            self.df = self.df.sample(frac=1).reset_index(drop=True)\n            \n            \n# Split our training set\nsplit = int(len(df_train) * TRAINING_SET_SIZE)\ntrain_df = df_train[:split]\nvalid_df = df_train[split:]\n\n# Create our generator\ntrain_generator = SignalGenerator(train_df, TRAIN_DIRECTORY, batch_size=BATCH_SIZE)\nvalid_generator = SignalGenerator(valid_df, TRAIN_DIRECTORY, batch_size=BATCH_SIZE, shuffle=False)","2e599c08":"#\u00a0https:\/\/stackoverflow.com\/questions\/53743729\/when-to-use-globalaveragepooling1d-and-when-to-use-globalmaxpooling1d-while-usin\n\ndef get_model_multiple_input():\n    \n    # Signal ABACAD\n    # Try to have two inputs, one for A and another for the rest.\n    # See if some improvment can be made by that\n    \n    inp_1 = Input((3, 256, 273))\n    x = TimeDistributed(Bidirectional(LSTM(128, return_sequences=True)))(inp_1)\n    x = Dropout(0.5)(x)\n    #\u00a0x = TimeDistributed(GlobalAveragePooling1D())(x)\n    \n    inp_2 = Input((3, 256, 273))\n    y = TimeDistributed(Bidirectional(LSTM(128, return_sequences=True)))(inp_2)\n    y = Dropout(0.5)(y)\n    #\u00a0y = TimeDistributed(GlobalAveragePooling1D())(y)\n    \n    \n    # TODO :: Maybe choose a different merge method ?\n    z = add([x, y])\n    z = TimeDistributed(GlobalAveragePooling1D())(z)\n    \n    z = Flatten()(z)\n    z = Dense(128, activation='relu', name='dense')(z)\n    z = Dropout(0.5)(z)\n    output = Dense(1, activation='sigmoid', name=\"output_layer\")(z)\n    \n    model = Model(inputs=[inp_1, inp_2], outputs=output)\n    \n    return model\n\ndef get_model():\n    \n    model = Sequential()\n    model.add(Input((6, 273, 256)))\n    \n    model.add(TimeDistributed(Bidirectional(LSTM(128, return_sequences=True))))\n    model.add(Dropout(0.5))\n    \n    model.add(TimeDistributed(Bidirectional(LSTM(128, return_sequences=True))))\n    model.add(Dropout(0.5))\n    \n    model.add(Flatten())\n    \n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    return model\n\n\n#\u00a0model = get_model()\nmodel = get_model_multiple_input()\nmodel.summary()","71a98372":"csv_path = os.path.join('logs', '{}_history.csv'.format(MODEL_NAME))\ncsv_logger = CSVLogger(csv_path, append=False)\n\n\nmodel_checkpoint = ModelCheckpoint('models\/{}.h5'.format(MODEL_NAME), \n                                   monitor='val_loss', \n                                   save_best_only=True, \n                                   save_weights_only=False, \n                                   mode='auto', \n                                   save_freq='epoch', \n                                   verbose=1)\n\n#\u00a0reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.001, verbose=1)\n\n\n\nmodel.compile(optimizer=Adam(),\n                 loss='binary_crossentropy',\n                 metrics=[AUC()])\n\nmodel.fit(train_generator, \n          validation_data=valid_generator,\n          use_multiprocessing=MULTI_THREAD,\n          epochs=EPOCHS, \n          verbose=1, \n          callbacks=[csv_logger, model_checkpoint])","219a5153":"df_sub = pd.read_csv('..\/input\/seti-breakthrough-listen\/sample_submission.csv')\n\ntest_generator = SignalGenerator(df_sub, TEST_DIRECTORY, batch_size=BATCH_SIZE, shuffle=False, training=False)\n\npredictions = model.predict(test_generator, \n                            use_multiprocessing=MULTI_THREAD,\n                            verbose=2)","c7a27c4c":"df_sub['target'] = predictions\n\n# Don't know if we have to keep the probabilities\n# df_sub['target'] = (predictions > 0.5).astype(int)","f8e0327a":"df_sub.to_csv('submission.csv', index=False)","dcb3f092":"# Generate our data for the testing"}}