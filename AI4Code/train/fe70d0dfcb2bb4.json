{"cell_type":{"9623b573":"code","c85b75aa":"code","24c8f4a1":"code","d7464feb":"code","0b3759f8":"code","1f5a0114":"code","53b4304f":"code","771d2672":"code","57071adf":"code","826b8f94":"code","23c57f3d":"markdown","b8aa2880":"markdown","6bdf01c1":"markdown","92ea9cad":"markdown","33dad900":"markdown","8e0290d3":"markdown","63e633fc":"markdown","d6b40648":"markdown"},"source":{"9623b573":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # used to visualise data\nimport seaborn as sns           # used to visualise data\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c85b75aa":"# Read in raw_csv and visulaise first 5 rows of data\nraw_data = pd.read_csv(\"..\/input\/london-bike-sharing-dataset\/london_merged.csv\")\nraw_data.head(5)","24c8f4a1":"raw_data.dtypes","d7464feb":"# First call raw_data and change to different variable name.\ndata_time = raw_data\n\n# We will then convert our timestamp to a datetime value\ndata_time['timestamp'] = pd.to_datetime(data_time['timestamp'])\n\n# Add two new columns for time of day and day of week. \ndata_time['time'] = data_time['timestamp']\ndata_time['day'] = data_time['timestamp']\n\n# View data to see columns were added\ndata_time.head(5)","0b3759f8":"# We can now check our dtypes again\ndata_time.dtypes","1f5a0114":"# Import datetime library for conversion of timestamp\nimport datetime\n\n# Convert time value\ndata_time['time'] = data_time['time'].dt.hour\n\n# Convert day value\ndata_time['day'] = data_time['day'].dt.weekday_name\n\ndata_time.head(5)","53b4304f":"# Create groupby function for time of day\ndata_time_time = data_time.groupby('time').mean()\n\n# Plot values calculated above\nplt.figure()\nplt.bar(data_time_time.index, data_time_time['cnt'])\nplt.xlabel(\"Hour of Day\")\nplt.ylabel(\"Average Number of BikeShares\")\nplt.xticks([0,2,4,6,8,10,12,14,16,18,20,22])\nplt.suptitle(\"Bikeshares by Time of Day\")\nplt.show()","771d2672":"# Create groupby function for the day of the week\nday_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\ndata_time_day = data_time.groupby('day').mean()\ndata_time_day = data_time_day.reindex(index = day_order)\n\n# Plot values calculated above\nplt.figure()\nplt.bar(data_time_day.index, data_time_day['cnt'])\nplt.xlabel(\"Day of Week\")\nplt.ylabel(\"Average Number of BikeShares\")\nplt.suptitle(\"Bikeshares by Day of the Week\")\nplt.xticks(rotation=90)\nplt.ylim([900, 1300])\nplt.show()","57071adf":"# Create a plot with 5 axes.\nfig,(ax1, ax2, ax3, ax4, ax5)= plt.subplots(nrows=5)\nfig.set_size_inches(18,25)\n\n# Create all the subplots\nsns.pointplot(data=data_time, x='time', y='cnt', ax=ax1)\nsns.pointplot(data=data_time, x='time', y='cnt', hue='is_holiday', ax=ax2)\nsns.pointplot(data=data_time, x='time', y='cnt', hue='is_weekend', ax=ax3)\nsns.pointplot(data=data_time, x='time', y='cnt', hue='season', ax=ax4)\nsns.pointplot(data=data_time, x='time', y='cnt', hue='weather_code',ax=ax5)","826b8f94":"# Create a correlation matrix\ncorrmat = data_time.corr()\nf, ax = plt.subplots(figsize = (10,10))\nsns.heatmap(corrmat, vmax=1, annot=True);","23c57f3d":"From this we can see that the early hours of the day have minimal usage. Which obviously makes sense based on work hours and so on. We also have a couple of major peaks throughout the day, at around 8am and again at 5-6pm. These are times when people are starting or finishing work, so again makes sense. So let's now look at the days of the week. \n\n***","b8aa2880":"We can see that Monday through Friday is when the majority of bikeshares are taken. Therefore, this must be a use of transport for those that are working or going to university etc. Saturday and Sunday usage drops below 1000, indicating that people either stay home or opt for different transportation methods.\n\n***\n\nLet's have a look and see if the number of shares change throughout the day in relation to some of our other information in the dataset.","6bdf01c1":"Great, so now we have converted our values, let's plot the count of bikeshares throughout the day. \n\n***\n\n# Data Visulisation","92ea9cad":"# London Bike Sharing Dataset\n## A dataset from Kaggle\n\nThis dataset can be downloaded directly from Kaggle [here](https:\/\/www.kaggle.com\/hmavrodiev\/london-bike-sharing-dataset). \n\n### Summary of Dataset\nAs a summary, this data provides historical data of London bike sharing. This data can be used to help predict future bike sharing in London City. \n\nI have read in all the libraries I will use in the set up code chunk above. ","33dad900":"From the above matrix, we can see that humidity has the strongest relationship, although negative with the number of bikeshares. Time and temperatures also have appear to have an impact on the number of bikeshares, but to a smaller degree. \n\nNone of these correlations are really that strong though and so inclusion of most of these values would help us to provide a much stronger prediction.\nBut let's leave the prediction for another day! ","8e0290d3":"We can see from the header of the data that we have 10 columns of data in this dataset. They all relate to the following information: \n- *timestamp* - is the date and time of day, used for grouping the data. \n- *cnt* - This is the cound of new bike shares.\n- *t1* - The real temperature in C.\n- *t2* - The feels like temperature in C. \n- *hum* - The humidity as a percentage.\n- *wind_speed* - The wind speed in km\/h.\n- *weather_code* - The category of the weather.\n- *is_holiday* - A boolean field representing holiday (1) or non-holiday (0).\n- *is_weekend* - A boolean field representing the weekend (1) or a weekday (0).\n- *season* - Categorical field representing the season:\n    - Spring - 0\n    - Summer - 1\n    - Fall - 2\n    - Winter - 3\n\nWeather codes relate to:\n- 1 = Clear or mostly clear. \n- 2 = Scattered Clouds or Few Clouds. \n- 3 = Broken Clouds.\n- 4 = Cloudy. \n- 7 = Rain or Light Rain.\n- 10 = Rain with Thunderstorms.\n- 26 = Snowfall. \n- 94 = Freezing Fog.\n\n\nNow we have the housekeeping out of the way, let's get in to actually exploring this data. \n\n\n***\n\n# 1. Bikeshares by Time of Day or Day of Week\n\nThe first thing we will explore is when bike shares most commonly occur as a time of day. \nSo let's do a little data manipulation and then visualise this. ","63e633fc":"We can see that holidays and weekends follow similar trends, with a much more distinct curve that is consistant throughout the day. Whilst, weekdays have to distinctive spikes around the time of commuters beginning work for a day. \nThe seasons have a similar trend, with slightly different absolute values. Winter in particular, indicates a big drop in the amount of bikeshares taken through the day. \n\n***\n\n# Correlation Matrix\n\nNow, what things are most related to eachother in this dataset?  \nWe can create a correlation matrix to take a look. ","d6b40648":"Perfect, so now that we have two new columns, which are in datetime format, let's convert their timestamps in to a time and day value for grouping. \n\n***"}}