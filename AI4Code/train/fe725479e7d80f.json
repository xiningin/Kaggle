{"cell_type":{"b98cf86b":"code","9a6b141d":"code","28c94439":"code","abfbddf9":"code","eeed9ccc":"code","1798f1e3":"code","a240f23a":"code","0af622ae":"code","85dbc110":"code","da702d06":"code","a8f5bb50":"code","4bcfa292":"code","01a5a164":"code","ad5418e0":"code","715d3783":"code","a013aec3":"code","607b6d10":"code","7e183ab0":"code","07ed2aa8":"code","a9d687c5":"code","ce3c022a":"code","0df31f8a":"code","294dbcf6":"code","7d763109":"code","8a04de78":"code","18866bab":"code","3dfd08a0":"code","cfe18f76":"code","c36a3849":"markdown"},"source":{"b98cf86b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\npath_list=[]\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        path_list.append(os.path.join(dirname, filename))\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9a6b141d":"# Load csv and Drop columns that have Na Values more than 90% of their data\ndf = pd.read_csv(path_list[2])\nfor column in df.columns:\n    if df[column].isna().sum()>len(df[column])*0.9:\n        df.drop([column], axis=1, inplace=True)","28c94439":"# Class for preprocessing of data\nclass Maker():\n    def __init__(self, df, column_info = None, label_dict = None):\n        self.ob_df, self.nu_df = pd.DataFrame(), pd.DataFrame()\n        self.column_info = column_info\n        self.label_dict = label_dict\n        \n        if column_info is not None:\n            df = df[column_info]\n        \n        # Split dataframes to object and numeric\n        for column in df.columns:\n            if df[column].dtype == 'object':\n                self.ob_df[column] = df[column]\n            else:\n                self.nu_df[column] = df[column]\n        \n        # column_info will be used as meaningful labels\n        # Only use column_info is none with test set \n        if column_info is not None and label_dict is not None:\n            self.clear_na()\n            self.set_label()\n        else:    \n            self.drop_huge()\n            self.clear_na()\n            self.set_label()\n            self.drop_unuseful()\n    \n    # Object DataFrame naver have unnormally huge data, so Only use it for Numeric\n    def drop_huge(self):\n        for column in self.nu_df.columns:\n            index = self.nu_df.loc[self.nu_df[column]>float(self.nu_df[column].quantile(q=0.99)+self.nu_df[column].std()),\n                                    [column]].index\n            # Set to have same index number\n            self.nu_df.drop(index=index, axis=0, inplace = True)\n            self.ob_df.drop(index=index, axis=0, inplace = True)\n    \n    # This is only for object DataFrame\n    def set_label(self):\n        label_dict = {}\n        for column in self.ob_df.columns:\n            data_list = []\n            for i,j in self.ob_df[column].items():\n                if j not in data_list:\n                    data_list.append(str(j))\n            data_list.sort(key=str)\n\n            if self.label_dict is None:\n                label_dict[column] = {str(data_list[i]) : i for i in range(len(data_list))}  \n\n                for num in range(len(data_list)):\n                    self.ob_df.loc[self.ob_df[column]==data_list[num], [column]] = num\n            else:\n                for data in data_list:\n                    if data in self.label_dict[column].keys():\n                        self.ob_df.loc[self.ob_df[column]==data, [column]] = self.label_dict[column][data]\n                    else:\n                        self.ob_df.loc[self.ob_df[column]==data, [column]] = len(data_list)+1\n                        print(column)\n                        print(data)\n                        \n            self.ob_df[column] = self.ob_df[column].astype(int)        \n                \n        self.label_dict = label_dict\n                    \n                    \n    \n    # Make dataset clearly. if Na values are less then 30% of data, convert to that by using mean of data.\n    # and If data type is object, chnage it to Numeric type( int or float )\n    def clear_na(self):\n        for column in self.nu_df.columns:\n            if int(self.nu_df[column].isna().sum())<len(self.nu_df[column])*0.3:\n                self.nu_df.loc[self.nu_df[column].isna(),[column]] = int(self.nu_df[column].mean())\n            else:\n                self.nu_df.drop([column], axis=1, inplace=True)\n                \n        for column in self.ob_df.columns:\n            if int(self.ob_df[column].isna().sum())<len(self.ob_df[column])*0.3:\n                self.ob_df.loc[self.ob_df[column].isna(), [column]] = str(self.ob_df[column].mode()[0])\n            else:\n                self.ob_df.drop([column], axis=1, inplace=True)\n    \n    # Drop data that 75% of itself has same values\n    def drop_unuseful(self):\n        for column in self.nu_df.columns:\n            if float(self.nu_df[column].min())==self.nu_df[column].quantile(q=0.75):\n                self.nu_df.drop([column], axis=1, inplace=True)\n        \n        for column in self.ob_df.columns:\n            if float(self.ob_df[column].min())==self.ob_df[column].quantile(q=0.75):\n                self.ob_df.drop([column], axis=1, inplace=True)\n    \n    # Set Test Set\n    def set_test_df(self):\n        test_df = pd.DataFrame()\n        temp = pd.concat([self.nu_df,self.ob_df], axis=1)\n        temp = temp[self.column_info]\n        \n        column_list = list(temp.columns)\n        column_list.sort(key=str)\n        \n        for column in column_list:\n            test_df[column] = temp.pop(column)\n        \n        return test_df\n        \n    # Set Train Set    \n    def set_train_df(self):\n        train_df = pd.DataFrame()\n        temp = pd.concat([self.nu_df,self.ob_df], axis=1)\n        column_list = list(temp.columns)\n        column_list.sort(key=str)\n        \n        for column in column_list:\n            train_df[column] = temp.pop(column)\n            \n        return train_df, self.label_dict ","abfbddf9":"# Make Train set\ntrain_maker = Maker(df)\ntrain_df, label_info = train_maker.set_train_df()","eeed9ccc":"# Check the train_df\ntrain_df","1798f1e3":"print(label_info)","a240f23a":"# Visualize data that has connection with saleprice\nfor column in train_df.columns:\n    if column != 'SalePrice'and abs(train_df['SalePrice'].corr(train_df[column]))>0.1:\n        sns.regplot(data=train_df, x=column, y='SalePrice')\n        plt.show()","0af622ae":"# Drop columns has connection less then 0.1\nfor column in train_df.columns:\n    if abs(train_df['SalePrice'].corr(train_df[column]))<0.1:\n        train_df.drop([column], axis=1, inplace = True)","85dbc110":"# Set target values and input\ny_true = train_df.pop('SalePrice')\nusing_columns = train_df.columns\nprint(using_columns)","da702d06":"# Normalize\ndef norm(z):\n    return (z-z.mean())\/z.std()","a8f5bb50":"input_df = norm(train_df)\ntarget = y_true","4bcfa292":"inputs = np.array(input_df, dtype=np.float32)\noutputs = np.array(target, dtype=np.float32).reshape(-1,1)\nprint(f\"inputs have shape {inputs.shape}, outputs have shape {outputs.shape}\")","01a5a164":"import tensorflow as tf","ad5418e0":"dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs)).cache().batch(32).prefetch(1)\nprint(dataset)","715d3783":"class Build_model(tf.keras.Model):\n    def __init__(self):\n        super(Build_model, self).__init__()\n        self.dense_1 = tf.keras.layers.Dense(32, activation='relu')\n        \n        self.dense_2 = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer='l2')\n        self.drop_2 = tf.keras.layers.Dropout(0.2)\n        self.batch_2 = tf.keras.layers.BatchNormalization()\n        \n        self.dense_3 = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer='l2')\n        self.drop_3 = tf.keras.layers.Dropout(0.2)\n        \n        self.dense_4 = tf.keras.layers.Dense(128, activation='relu', kernel_regularizer='l2')\n        self.batch_4 = tf.keras.layers.BatchNormalization()\n        \n        self.dense_5 = tf.keras.layers.Dense(32, activation='relu')\n        \n        self.last = tf.keras.layers.Dense(1)\n        \n    def call(self, x):\n        x = self.dense_1(x)\n        \n        x = self.dense_2(x)\n        x = self.drop_2(x)\n        x = self.batch_2(x)\n        \n        x = self.dense_3(x)\n        x = self.drop_3(x)\n        \n        x = self.dense_4(x)\n        x = self.batch_4(x)\n        \n        x = self.dense_5(x)\n        \n        return self.last(x)\n\nmodel = Build_model()","a013aec3":"loss_object = tf.keras.losses.MeanSquaredLogarithmicError()\noptimizer = tf.keras.optimizers.RMSprop()\n\ntrain_score = tf.keras.metrics.MeanSquaredLogarithmicError()\n\n\n# Set function for training\n@tf.function\ndef train_step(features,labels):\n    with tf.GradientTape() as tape:\n        predictions = model(features)\n        loss = loss_object(labels, predictions)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    train_score.update_state(labels, predictions)\n    \n    return loss","607b6d10":"EPOCHS = 4500\nfor epoch in range(EPOCHS):\n    for feature, label in dataset:\n        loss_score = train_step(feature, label)\n    if (epoch+1) % int(EPOCHS\/10) == 0:\n        print(\"EPOCH : {}, Loss : {:.5f}, Score : {:.3f}\".format(epoch+1, loss_score,\n                                                                float(train_score.result())))","7e183ab0":"forecast = model.predict(dataset)\nforecast = forecast.flatten()","07ed2aa8":"# Set values for Checking Error_rate\nreal_value = y_true.ravel(order='C')\nerror_rate = abs(real_value-forecast)*100\/real_value\n\n# Check the error\nprint(\"Mean Error rate is {:.3f}%\".format(np.mean(error_rate)))","a9d687c5":"plt.figure()\nplt.scatter(forecast, real_value)\nplt.plot(real_value, real_value, color='r', marker='o')\nplt.show()","ce3c022a":"tdf = pd.read_csv(path_list[3])\ntid = tdf['Id']","0df31f8a":"test_maker = Maker(tdf, column_info=using_columns, label_dict=label_info)\ntest_df = test_maker.set_test_df()\nprint(f'test id {len(tid)} test df {len(test_df)}')","294dbcf6":"test_df","7d763109":"norm_test = norm(test_df)","8a04de78":"test_array = np.array(norm_test, dtype=np.float32)\nprint(f'test array shape : {test_array.shape}')","18866bab":"test_dataset = tf.data.Dataset.from_tensor_slices(test_array).batch(32)\ntest_fore = model.predict(test_dataset)\ntest_fore = test_fore.flatten()","3dfd08a0":"gu = pd.DataFrame([i for i in test_fore],index=tid, columns=['SalePrice'])\nprint(gu)","cfe18f76":"gu.to_csv(\"predictions.csv\", mode='w')","c36a3849":"# T E S T"}}