{"cell_type":{"daa03f7e":"code","32ded59f":"code","0e5283b8":"code","6c5319c6":"code","42e10397":"code","91264077":"code","7ef0e63e":"code","ca86af9a":"code","6ea3b381":"code","9326e0ab":"code","bc64ba11":"code","31bec6b3":"code","db714c98":"code","94629f4e":"code","2c0687a6":"code","f93ca1b9":"code","04615c7d":"code","b7356b43":"code","33fd5e32":"code","3e147b1a":"code","a788028b":"code","4553d47f":"code","037e03f1":"code","b8767349":"code","8de7f2c2":"code","d61b4ed0":"code","7755c2be":"code","fea6aea7":"code","0bc3fa4b":"code","6107046f":"code","a4cb8316":"code","f6907f09":"code","dede0dc7":"code","5f62de6c":"code","d0ce257d":"code","6d042c06":"code","84c617ca":"code","09da6dea":"code","8a438879":"code","2e739ebc":"code","e41e8f5d":"code","217bf055":"code","82702e40":"code","5dd24ee3":"code","7d131f92":"code","b0d3634e":"code","51dcde7a":"code","7a31e180":"code","f4dd44f1":"code","bfe07e34":"code","fe084deb":"code","0f9612da":"code","2f0bf89a":"code","a2c26783":"code","942d54d1":"code","3ff20242":"code","e675a92c":"code","07af2b32":"code","0989cb39":"code","bacb603b":"code","bfc608f4":"code","4f9d579c":"code","2ef068fb":"code","8efc3a2b":"code","8f6e2781":"markdown","d6eac564":"markdown","bbdbb99f":"markdown","ad173460":"markdown","faf2a49f":"markdown","7aff2011":"markdown","2642eab5":"markdown","26a86cfe":"markdown","dc6387b7":"markdown","e3c164af":"markdown","1df23215":"markdown","0e298604":"markdown","236fff15":"markdown","aa521034":"markdown","da565b1e":"markdown","27957b9c":"markdown","da8e4249":"markdown","1275448f":"markdown","5ff75888":"markdown","543b79ea":"markdown","204ab9d6":"markdown","57675471":"markdown","7c981e46":"markdown","8a87b845":"markdown","19cacea2":"markdown","f23b423a":"markdown","dfd85867":"markdown","921a1ae8":"markdown","456363d3":"markdown","b5b5d7b4":"markdown","e9645630":"markdown","e50fca19":"markdown","da5ebac6":"markdown"},"source":{"daa03f7e":"#Import necessary Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n%matplotlib inline\npd.set_option('display.max_columns',500)\npd.set_option('display.max_rows',500)\npd.set_option('display.width', 500)\n#to display all the columns of dataframe","32ded59f":"#read train.csv file\ndf = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')","0e5283b8":"df","6c5319c6":"df.describe()","42e10397":"# Check for Corelation between Features\nplt.figure(figsize=(20, 10))\nsb.heatmap(df.corr(),yticklabels=True,cbar=True,cmap='viridis')","91264077":"#Let's check which feature has maximum corelation with our dependent feature- SalePrice\ndf.corr()[\"SalePrice\"].sort_values(ascending = False)","7ef0e63e":"sb.scatterplot(data = df, x = \"OverallQual\", y = \"SalePrice\");","ca86af9a":"sb.scatterplot(data = df, x = \"GrLivArea\", y = \"SalePrice\");","6ea3b381":"sb.scatterplot(data = df, x = \"GarageCars\", y = \"SalePrice\");","9326e0ab":"sb.scatterplot(data = df, x = \"GarageArea\", y = \"SalePrice\");","bc64ba11":"df.info()","31bec6b3":"def percent_missing_data(df):\n    missing_count = df.isna().sum().sort_values(ascending = False)\n    missing_percent = 100 * df.isna().sum().sort_values(ascending = False) \/ len(df)\n    \n    missing_count = pd.DataFrame(missing_count[missing_count > 0])\n    missing_percent = pd.DataFrame(missing_percent[missing_percent > 0])\n    \n    missing_table = pd.concat([missing_count,missing_percent], axis = 1)\n    missing_table.columns = [\"missing_count\", \"missing_percent\"]\n    \n    return missing_table","db714c98":"missing_values = percent_missing_data(df)\nmissing_values","94629f4e":"df_object = df.select_dtypes(include = \"object\")\ndf_numeric = df.select_dtypes(exclude = \"object\")\ndf_object.shape , df_numeric.shape","2c0687a6":"# list of variables that contain year information\nyear_feature = [feature for feature in df_numeric if 'Yr' in feature or 'Year' in feature]\nprint(\"Temporial feature Count : {}\".format(len(year_feature)))\nyear_feature","f93ca1b9":"## Visualising the Temporal Datetime Variables\n## We will check whether there is a relation between year the house is sold and the sales price\n\ndf.groupby('YrSold')['SalePrice'].median().plot()\nplt.xlabel('Year Sold')\nplt.ylabel('Median House Price')\nplt.title(\"House Price vs YearSold\")","04615c7d":"#Discrete Features\ndiscrete_feature=[feature for feature in df_numeric if len(df[feature].unique())<25 and feature not in year_feature+['Id']]\nprint(\"Discrete Variables Count: {}\".format(len(discrete_feature)))","b7356b43":"#Continous Features\ncontinuous_feature=[feature for feature in df_numeric if feature not in discrete_feature+year_feature+['Id']]\nprint(\"Continuous feature Count: {}\".format(len(continuous_feature)))","33fd5e32":"for feature in continuous_feature:\n    data=df.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature]=np.log(data[feature])\n        data.boxplot(column=feature)\n        plt.ylabel(feature)\n        plt.title(feature)\n        plt.show()","3e147b1a":"missing_values = percent_missing_data(df)\nmissing_values","a788028b":"plt.figure(figsize=(10,4), dpi = 100)\nsb.barplot(x=missing_values.index,y='missing_percent', data=missing_values)\nplt.xticks(rotation=90)\nplt.show()","4553d47f":"#Extracting Features that have less than 1% missing values\nmissing_values[missing_values['missing_percent']<1]","037e03f1":"#Electrical has only 1 missing value.It can be filled with mode\ndf['Electrical'].mode()","b8767349":"df['Electrical'] = df['Electrical'].fillna('SBrkr')\ndf['Electrical'].isna().sum()","8de7f2c2":"df['MasVnrArea'].value_counts(), df['MasVnrType'].value_counts()","d61b4ed0":"df['MasVnrArea']= df['MasVnrArea'].fillna(0)\ndf['MasVnrType']= df['MasVnrType'].fillna('None')","7755c2be":"missing_values = percent_missing_data(df)\nmissing_values","fea6aea7":"# basement string features ==> fill with none\nbsmt_str_cols =  ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\ndf[bsmt_str_cols] = df[bsmt_str_cols].fillna('None')\n\n# basement numeric features ==> fill with 0\nbsmt_num_cols = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']\ndf[bsmt_num_cols] = df[bsmt_num_cols].fillna(0)\n","0bc3fa4b":"missing_values = percent_missing_data(df)\nmissing_values","6107046f":"# Garage string features ==> fill with Mode                                           \ndf['GarageType']= df['GarageType'].fillna('Attchd')    \ndf['GarageCond']= df['GarageCond'].fillna('TA') \ndf['GarageFinish']= df['GarageFinish'].fillna('Unf') \ndf['GarageQual']= df['GarageQual'].fillna('TA') \n\n# basement numeric features ==> fill with Mean\ndf['GarageYrBlt']= df['GarageYrBlt'].fillna(df.GarageYrBlt.mean()) \n","a4cb8316":"missing_values = percent_missing_data(df)\nmissing_values","f6907f09":"# Dropping columns with more than 80% missing values.\ndf = df.drop([\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\"], axis = 1)","dede0dc7":"missing_values = percent_missing_data(df)\nmissing_values","5f62de6c":"df['FireplaceQu'].value_counts() , df['LotFrontage'].value_counts()","d0ce257d":"df['FireplaceQu']= df['FireplaceQu'].fillna('None')    \ndf['LotFrontage']= df['LotFrontage'].fillna(df.LotFrontage.median())    ","6d042c06":"missing_values = percent_missing_data(df)\nmissing_values","84c617ca":"df_numeric","09da6dea":"num_features=['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\n\nfor feature in num_features:\n    df[feature]=np.log(df[feature])","8a438879":"df_numeric","2e739ebc":"df.head()","e41e8f5d":"for feature in df.select_dtypes(include = \"object\"):\n    labels_ordered=df.groupby([feature])['SalePrice'].mean().sort_values().index\n    labels_ordered={k:i for i,k in enumerate(labels_ordered,0)}\n    df[feature]=df[feature].map(labels_ordered)","217bf055":"df.head()","82702e40":"df.shape","5dd24ee3":"X = df.drop(['Id','SalePrice'],axis=1)\nY = df['SalePrice']","7d131f92":"# Train test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=101)","b0d3634e":"# Standard scaling our data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train) \nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","51dcde7a":"X_train.shape , X_test.shape","7a31e180":"# Create the Ridge model\nfrom sklearn.linear_model import Ridge\nrid_reg = Ridge(alpha = 100)\nrid_reg.fit(X_train, Y_train)\n\nY_pred = rid_reg.predict(X_test)\n\n# testing the model\nfrom sklearn.metrics import r2_score,mean_absolute_error\nprint(\"MAE : \",mean_absolute_error(Y_test, Y_pred))\nprint('R2 SCORE : ',r2_score(Y_test, Y_pred))\n","f4dd44f1":"# Now, let's find best values for alpha and train model again\n\nalpha_list = []\nmse_list = []\nfor alpha_val in np.arange(0.01, 200):\n    ridge1 = Ridge(alpha = alpha_val)\n    ridge1.fit(X_train, Y_train)\n    alpha_list.append(alpha_val)\n    \n    # testing the model\n    Y_predict = ridge1.predict(X_test)\n    mse = mean_absolute_error(Y_test, Y_predict)\n    mse_list.append(mse)\n    \nalpha_list = pd.DataFrame(alpha_list)\nmse_list = pd.DataFrame(mse_list)\nalpha_mse = pd.concat([alpha_list, mse_list], axis = 1)\nalpha_mse.columns = [\"alpha_list\", \"mse_list\"]\n\nalpha_mse[alpha_mse[\"mse_list\"] == alpha_mse[\"mse_list\"].min()]","bfe07e34":"# Create the Ridge model using best alpha value:\nfrom sklearn.linear_model import Ridge\nrid_reg = Ridge(alpha = 8.01)\nrid_reg.fit(X_train, Y_train)\n\nY_pred_ridge = rid_reg.predict(X_test)\n\n# testing the model\nfrom sklearn.metrics import r2_score,mean_absolute_error\nridge_mae = mean_absolute_error(Y_test, Y_pred_ridge)\nridge_r2_score= r2_score(Y_test, Y_pred_ridge)\n\nprint(\"MAE for Ridge : \",ridge_mae)\nprint('R2 SCORE for Ridge: ',ridge_r2_score)\n","fe084deb":"Y_pred.min()","0f9612da":"plt.figure(figsize=(10,8))\nsb.regplot(Y_pred_ridge,Y_test);","2f0bf89a":"# Create Lasso model\nfrom sklearn.linear_model import Lasso\nls = Lasso(alpha = 0.8)\nls.fit(X_train, Y_train)\n\nY_pred = ls.predict(X_test)\n\n# testing the model\nfrom sklearn.metrics import mean_absolute_error\nprint(\"MAE : \",mean_absolute_error(Y_test, Y_pred))\n\nfrom sklearn.model_selection import cross_val_score\nprint('R2 SCORE : ',r2_score(Y_test, Y_pred))\n","a2c26783":"# Now, let's find best values for alpha and train model again\n\nalpha_list = []\nmse_list = []\nfor alpha_val in np.arange(0.01, 200):\n    ls1 = Lasso(alpha = alpha_val)\n    ls1.fit(X_train, Y_train)\n    alpha_list.append(alpha_val)\n    \n    # testing the model\n    Y_predict = ls1.predict(X_test)\n    mse = mean_absolute_error(Y_test, Y_predict)\n    mse_list.append(mse)\n    \nalpha_list = pd.DataFrame(alpha_list)\nmse_list = pd.DataFrame(mse_list)\nalpha_mse = pd.concat([alpha_list, mse_list], axis = 1)\nalpha_mse.columns = [\"alpha_list\", \"mse_list\"]\n\nalpha_mse[alpha_mse[\"mse_list\"] == alpha_mse[\"mse_list\"].min()]","942d54d1":"# Create the Lasso model using best alpha value:\n\nls = Lasso(alpha = 0.01)\nls.fit(X_train, Y_train)\n\nY_pred_lasso = ls.predict(X_test)\n\n# testing the model\nlasso_mae = mean_absolute_error(Y_test, Y_pred_lasso)\nlasso_r2_score= r2_score(Y_test, Y_pred_lasso)\n\nprint(\"MAE for Lasso : \",lasso_mae)\nprint('R2 SCORE for Lasso : ',lasso_r2_score)\n","3ff20242":"Y_pred_lasso.min()","e675a92c":"plt.figure(figsize=(10,8))\nsb.regplot(x = Y_pred_lasso, y = Y_test)","07af2b32":"#Import the poly conerter \nfrom sklearn.preprocessing import PolynomialFeatures\npolynomial_converter = PolynomialFeatures(degree=2,include_bias=False)\n\n#convert X data \npoly_features_train = polynomial_converter.fit_transform(X_train)\npoly_features_test = polynomial_converter.fit_transform(X_test)","0989cb39":"#import elastic net \nfrom sklearn.linear_model import ElasticNetCV\nelastic_model = ElasticNetCV(l1_ratio= 1,tol=0.01)\nelastic_model.fit(poly_features_train,Y_train)","bacb603b":"Y_pred_poly = elastic_model.predict(poly_features_test)","bfc608f4":"#testing model\npoly_mae = mean_absolute_error(Y_test, Y_pred_poly)\npoly_r2_score = r2_score(Y_test, Y_pred_poly)\nprint(\"MAE for Polynomial: \",poly_mae)\nprint('R2 SCORE for Polynomial: ',poly_r2_score)","4f9d579c":"Y_pred.min()","2ef068fb":"plt.figure(figsize=(10,8))\nsb.regplot(Y_pred_poly,Y_test)","8efc3a2b":"models = pd.DataFrame({\n    'Regression Model': ['Ridge','Lasso','Polynomial'],\n    'MAE Score': [\n        ridge_mae, \n        lasso_mae,\n        poly_mae ],\n    'R2 Score': [\n        ridge_r2_score, \n        lasso_r2_score,\n        poly_r2_score   \n    ]})\nprint(\"--- MODEL EVALUATION---\")\nmodels.sort_values(by='MAE Score', ascending=True)","8f6e2781":"### Check for Missing Values:","d6eac564":"# House Price Prediction - Advanced Regression Techniques","bbdbb99f":"- We can see column 'PoolQC' has maximum missing values.We can either drop it or decide to keep it if the feature is important for our model.\n- But first, we can deal with features that have less missing values.ie. less than 1% missing values.\n","ad173460":"-- Numerical variables are usually of 3 types:\n-- Continous variable \n-- Discrete Variables and \n-- Temporal(Date-Time Features) Variables","faf2a49f":"- Here all the Garage related features has around 5% missing values.\n- We can fill those categories using mean and mode.(mean for numerical features and mode for categorical features)\n","7aff2011":"- 'FireplaceQu' is categorical column, so missing values can be filled with 'None'.\n- But, 'LotFrontage' is numerical column and it also has outliers, so it can be filled with median.","2642eab5":"# ![](https:\/\/miro.medium.com\/max\/804\/1*D6s2K1y7kjE14swcgITB1w.png)\n### House prices increase every year, so there is a need for a system to predict house prices in the future. House price prediction can help the developer determine the selling price of a house and can help the customer to arrange the right time to purchase a house.","26a86cfe":"### Ridge Regression :","dc6387b7":"- Here we can see that 'OverAllQual' is most corelated with 'SalePrice'.\n- Lets explore highly correlated features with 'Sale Price'.","e3c164af":"### Checking for Outliers","1df23215":"## 1. Exploratory Data Analysis    ","0e298604":"- Let's Convert categorical features into numerical.","236fff15":"- We can see that **Ridge Regression Model is best suitable** here.","aa521034":"## Lasso Regression","da565b1e":"### Yayy!! There is no missing values now!","27957b9c":"## Consider **UPVOTING** if you find it useful.....","da8e4249":"#### Categorical Features:","1275448f":"## Feature Scaling","5ff75888":"- numerical variables are skewed,so we can perform log normal distribution to prevent negative predictions.\n- We will only perform log normal distribution to columns which do not have any zero values.","543b79ea":"1. Exploratory Data Analysis\n2. Feature Engineering\n3. Model Building & Evaluation\n","204ab9d6":"- All the Basement related features have 2% missing values.\n- By going through data,we can find that Nan actually means that the house do not has a basement.So we can replace Nan values with 'None' which means no basement.\n- For Basement related numeric columns we will replace Nan values with zero.","57675471":"## Polynomial Regression","7c981e46":"### Handling Missing Values","8a87b845":"- By going through that data,we can see that 'MasVnrArea' has values with 0, so missing values can be filled with '0'.\n- 'MasType' has category for None, so missing values can be filled with 'None'.\n","19cacea2":"\n#### In this Notebook, I have used Advanced Regression Techniques like **Ridge, Lasso & Polynomial Regression** in most simplystic manner with EDA.\n#### The speciality of this notebook is the detailed & bit by bit **feature engineering** done and its impact on model performance.","f23b423a":"### Please share your valuable feedbacks and suggestions in comments.","dfd85867":"- 'MasVnrArea' & 'MasVnrType' also have less than 1% missing values.\n","921a1ae8":"- Here some of the columns has more than 80% missing values. It is best option to drop them.","456363d3":"- We can see that House sales prices are actually decreasing over the time.","b5b5d7b4":"# 2. Feature Engineering","e9645630":"- We can see that there are houses with higher(10\/10) quality but have very low prices.\n- There are high prices for larger living areas, but we can see some of the outliers also.Same goes for Garage Area too.\n- Number of garage cars also tend to follow the trend where higher the amount of cars is propotional to higher Sales Price.(Highest number 4 can be counted as exceptional)","e50fca19":"# 3.Model Building & Evaluation","da5ebac6":"#### Numerical Features"}}