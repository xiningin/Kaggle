{"cell_type":{"aae97edd":"code","861a23f0":"code","8e76e39b":"code","fd46dce9":"code","6d8a1c98":"code","dd904221":"code","70e8f67b":"code","e8694d73":"code","601a1d19":"code","e94d37c8":"code","8115d17f":"code","75e05c87":"code","fb2e4f36":"code","12b957af":"code","4cfcfab4":"code","2ec9fc61":"code","5281d6d3":"code","da74b07f":"code","db825970":"code","e06fdca4":"code","fcc577be":"code","65543db5":"code","19548121":"code","6425b00e":"code","d339685f":"markdown","e3edfb52":"markdown","586faace":"markdown","a414e08a":"markdown","6dd22522":"markdown","44ae4843":"markdown","4b13ab11":"markdown","be4ef786":"markdown","ada84a4b":"markdown","518001b2":"markdown","3e963921":"markdown","7267e7a9":"markdown","30cb39f0":"markdown","a0c7a953":"markdown","6e46b797":"markdown","5cdcd584":"markdown","5c0c5986":"markdown","b08c95a7":"markdown","bb80e00e":"markdown","fda631ff":"markdown","f814aa30":"markdown"},"source":{"aae97edd":"import time\nimport pickle\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np \nimport pandas as pd \nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures, QuantileTransformer\nfrom sklearn.mixture import GaussianMixture, BayesianGaussianMixture\n\nfrom scipy.stats import rankdata\nfrom sklearn.svm import SVC, NuSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis","861a23f0":"NFOLDS=5          # The number of folds\nRS=42             # The random seed\ndebug=0          # The debuging mode switch: 1-debugging on; 0-debugging off \n\nparams_2={'n_components' : 2,              # The parameters of the auxiliary \n          'init_params': 'random',         # GMM classifier computing clusters'\n          'covariance_type': 'full',       # stats (means and covariance)\n          'tol':0.001, \n          'reg_covar': 0.001, \n          'max_iter': 55, \n          'n_init': 10,\n          'random_state': RS,\n         }\n\nparams_2_qda={'n_components' : 2,              # The parameters of the auxiliary \n          'init_params': 'random',         # GMM classifier computing clusters'\n          'covariance_type': 'full',       # stats (means and covariance)\n          'tol':0.001, \n          'reg_covar': 0.001, \n          'max_iter': 50, \n          'n_init': 10,\n          'random_state': RS,\n         }\n\n#PARAMETERS FOR THE GMM CLASSIFIER (2 clusters per class; 4 clusters in total)\nparams_4={'n_components' : 4, \n          'init_params': 'random', \n          'covariance_type': 'full', \n          'tol':0.001, \n          'reg_covar': 0.001, \n          'max_iter': 55, \n          'n_init': 10, \n          'random_state': RS,\n         }\n\n#PARAMETERS FOR THE QDA CLASSIFIER (2 clusters per class; 4 clusters in total)\nparams_qda={'reg_param' : 0.111,\n         }\n\n#PARAMETERS TO BE USED FOR PSEUDOLABELING GMM\nlow=0.0067\nhigh=1-low\n\n#PARAMETERS TO BE USED FOR PSEUDOLABELING QDA\n# low_vals = [None, 0.01, 0.01, 0.01, 0.001, 0.0001]\n# high_vals = [None, 0.99, 0.99, 0.99, 0.999, 0.9999]\n# low_vals = [0.01, 0.01, 0.01, 0.001, 0.0001]\n# high_vals = [0.99, 0.99, 0.99, 0.999, 0.9999]\n\nrp_values = [0.8, 0.7, 0.6, 0.6]\n#rp_values = [0.8]#[0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85]\n\nlow_vals = [0.01, 0.01, 0.01, 0.01]\nhigh_vals = [1-low for low in low_vals]\n\n# low_vals.insert(0, None)\n# high_vals.insert(0, None)\n\nprint(low_vals, high_vals)","8e76e39b":"%%time\n\npath = Path('..\/input')\n\ntrain = pd.read_csv(path\/'train.csv')\ntest = pd.read_csv(path\/'test.csv')\nsub = pd.read_csv(path\/'sample_submission.csv')","fd46dce9":"if debug:\n    magic_max=2\n    magic_min=0\n    NFOLDS=2\nelse:\n    magic_max=train['wheezy-copper-turtle-magic'].max()\n    magic_min=train['wheezy-copper-turtle-magic'].min()","6d8a1c98":"def preprocess(train=train, test=test):\n       \n    prepr = {} \n    \n    #PREPROCESS 512 SEPARATE MODELS\n    for i in range(magic_min, magic_max+1):\n\n        # EXTRACT SUBSET OF DATASET WHERE WHEEZY-MAGIC EQUALS i     \n        X = train[train['wheezy-copper-turtle-magic']==i].copy()\n        Y = X.pop('target').values\n        X_test = test[test['wheezy-copper-turtle-magic']==i].copy()\n        idx_train = X.index \n        idx_test = X_test.index\n        X.reset_index(drop=True,inplace=True)\n\n        cols = np.array([c for c in X.columns if c not in ['id', 'wheezy-copper-turtle-magic']])\n\n        l=len(X)\n        X_all = pd.concat([X[cols], X_test[cols]], ignore_index=True)\n        \n        sel = VarianceThreshold(threshold=2)\n        X_vt = sel.fit_transform(X_all)               # np.ndarray\n        \n        prepr['vt_' + str(i)] = X_vt\n        prepr['n_vt' + str(i)] = X_vt.shape[1]\n        prepr['feats_vt' + str(i)] = cols[sel.get_support(indices=True)]        \n        prepr['train_size_' + str(i)] = l\n        prepr['idx_train_' + str(i)] = idx_train\n        prepr['idx_test_' + str(i)] = idx_test\n        prepr['target_' + str(i)] = Y\n        \n    return prepr","dd904221":"%%time\n\ndata = preprocess()","70e8f67b":"def get_data(i, data):\n    \n    l = data['train_size_' + str(i)]\n    \n    X_all = data['vt_' + str(i)]                \n\n    X = X_all[:l, :]\n    X_test = X_all[l:, :]\n\n    Y = data['target_' + str(i)]\n\n    idx_train = data['idx_train_' + str(i)]\n    idx_test = data['idx_test_' + str(i)]\n    \n    return X, X_test, Y, idx_train, idx_test","e8694d73":"def initialize_cv():\n    \n    auc = np.array([])\n    oof = np.zeros(len(train))\n    preds = np.zeros(len(test)) \n    return preds, oof, auc","601a1d19":"def report_results(oof, auc_all, clf_name='GMM'):\n    \n    # PRINT VALIDATION CV AUC FOR THE CLASSFIER\n    print(f'The result summary for the {clf_name} classifier:')\n    auc_combo = roc_auc_score(train['target'].values, oof)\n    auc_av = np.mean(auc_all)\n    std = np.std(auc_all)\/(np.sqrt(NFOLDS)*np.sqrt(magic_max+1))\n\n    print(f'The combined CV score is {round(auc_combo, 5)}.')    \n    print(f'The folds average CV score is {round(auc_av, 5)}.')\n    print(f'The standard deviation is {round(std, 5)}.\\n')","e94d37c8":"def clusters_stats(X_train, Y_train, params=params_2):\n    \n    X_train_0 = X_train[Y_train==0]\n    Y_train_0 = Y_train[Y_train==0].reshape(-1, 1)\n\n    X_train_1 = X_train[Y_train==1]\n    Y_train_1 = Y_train[Y_train==1].reshape(-1, 1)\n\n    clf_0 = GaussianMixture(**params)\n\n    clf_0.fit(X_train_0)\n    means_0 = clf_0.means_\n    covs_0 = clf_0.covariances_\n    ps_0 = [np.linalg.inv(m) for m in covs_0]\n\n    clf_1 = GaussianMixture(**params)\n\n    clf_1.fit(X_train_1)\n    means_1 = clf_1.means_\n    covs_1 = clf_1.covariances_\n    ps_1 = [np.linalg.inv(m) for m in covs_1]\n\n    #SAVING CLUSTERS' MEANS AND COVARIANCES       \n    ms = np.stack((means_0[0], means_0[1], means_1[0], means_1[1]))\n    ps = np.stack((ps_0[0], ps_0[1], ps_1[0], ps_1[1]))\n    \n    return ms, ps","8115d17f":"def pseudolabeling(X_train, X_test, Y_train, Y_pseudo, \n                   idx_test, test=test, low=low, high=high):\n    \n    assert len(test) == len(Y_pseudo), \"The length of test does not match that of Y_pseudo!\"\n    \n    #SELECT ONLY THE PSEUDOLABLES CORRESPONDING TO THE CURRENT VALUES OF 'wheezy-copper-turtle-magic'\n    Y_aug = np.copy(Y_pseudo[idx_test])\n    \n    assert len(Y_aug) == len(X_test), \"The length of Y_aug does not match that of X_test!\"\n\n    Y_aug[Y_aug > high] = 1\n    Y_aug[Y_aug < low] = 0\n    \n    mask = (Y_aug == 1) | (Y_aug == 0)\n    \n    Y_useful = Y_aug[mask]\n    X_test_useful = X_test[mask]\n    \n    X_train_aug = np.vstack((X_train, X_test_useful))\n    Y_train_aug = np.vstack((Y_train.reshape(-1, 1), Y_useful.reshape(-1, 1)))\n    \n    return X_train_aug, Y_train_aug","75e05c87":"def train_classifier(Y_pseudo, params=params_4):\n    \n    preds, oof, auc_all = initialize_cv()\n\n    print(f\"Computing centroids and covariances for the four clusters (two per class).\")\n\n    # BUILD 512 SEPARATE NON-LINEAR MODELS\n    for i in tqdm(range(magic_min, magic_max+1)):   \n\n        X, X_test, Y, idx_train, idx_test = get_data(i=i, data=data) \n\n        # STRATIFIED K FOLD\n        auc_folds=np.array([])\n\n        folds = StratifiedKFold(n_splits=NFOLDS, random_state=RS)\n\n        for train_index, val_index in folds.split(X, Y):\n\n            X_train, Y_train = X[train_index, :], Y[train_index]\n            X_val, Y_val = X[val_index, :], Y[val_index]\n            \n            if Y_pseudo is None:\n                params['means_init'], params['precisions_init'] = clusters_stats(X_train, Y_train)\n            else:\n                X_aug, Y_aug = pseudolabeling(X_train, X_test, Y_train, Y_pseudo, idx_test)\n                params['means_init'], params['precisions_init'] = clusters_stats(X_aug, Y_aug.ravel())  \n            \n            #INSTANTIATING THE MAIN CLASSIFIER\n            clf = GaussianMixture(**params) \n            \n            clf.fit(np.concatenate([X_train, X_test], axis = 0))\n\n            oof[idx_train[val_index]] = np.sum(clf.predict_proba(X_val)[:, 2:], axis=1)\n            preds[idx_test] += np.sum(clf.predict_proba(X_test)[:,2: ], axis=1)\/NFOLDS\n\n            auc = roc_auc_score(Y_val, oof[idx_train[val_index]])\n            auc_folds = np.append(auc_folds, auc)\n\n        auc_all = np.append(auc_all, np.mean(auc_folds))\n\n    report_results(oof, auc_all)\n    \n    return preds, oof, auc_all","fb2e4f36":"Y_pseudo, oof, auc_all = train_classifier(Y_pseudo=None)","12b957af":"preds_gmm, oof_gmm, auc_gmm = train_classifier(Y_pseudo=Y_pseudo)\n\nsub['target'] = preds_gmm\nsub.to_csv('submission_gmm.csv',index=False)","4cfcfab4":"def get_labels(X_train, Y_train, params=params_2_qda):\n    \n    X_train_0 = X_train[Y_train==0]\n    X_train_1 = X_train[Y_train==1]\n\n    clf_0 = GaussianMixture(**params)\n    labels_0 = clf_0.fit_predict(X_train_0).reshape(-1, 1)\n\n    clf_1 = GaussianMixture(**params)\n    labels_1 = clf_1.fit_predict(X_train_1).reshape(-1, 1)\n    \n    labels_1[labels_1==0] = 2\n    labels_1[labels_1==1] = 3\n\n    #CREATE LABELED DATA \n    \n    X_l = np.vstack((X_train_0, X_train_1))\n    Y_l = np.vstack((labels_0, labels_1))\n    \n    perm = np.random.permutation(len(X_l))\n    \n    X_l = X_l[perm]\n    Y_l = Y_l[perm]\n    \n    return X_l, Y_l","2ec9fc61":"def pseudolabeling_qda(X_train, X_test, Y_train, Y_pseudo, \n                       idx_test, low, high, test=test):\n    \n    assert len(test) == len(Y_pseudo), \"The length of test does not match that of Y_pseudo!\"\n    \n    #SELECT ONLY THE PSEUDOLABLES CORRESPONDING TO THE CURRENT VALUES OF 'wheezy-copper-turtle-magic'\n    Y_aug = np.copy(Y_pseudo[idx_test])\n    \n    assert len(Y_aug) == len(X_test), \"The length of Y_aug does not match that of X_test!\"\n\n    Y_aug[Y_aug > high] = 1\n    Y_aug[Y_aug < low] = 0\n    \n    mask = (Y_aug == 1) | (Y_aug == 0)\n    \n    Y_useful = Y_aug[mask]\n    X_test_useful = X_test[mask]\n    \n    X_train_aug = np.vstack((X_train, X_test_useful))\n    Y_train_aug = np.vstack((Y_train.reshape(-1, 1), Y_useful.reshape(-1, 1)))\n    \n    return X_train_aug, Y_train_aug","5281d6d3":"def train_qda(Y_pseudo, low, high, params=params_qda):\n    \n    preds, oof, auc_all = initialize_cv()\n\n    print(f\"Computing centroids and covariances for the four clusters (two per class).\")\n\n    # BUILD 512 SEPARATE NON-LINEAR MODELS\n    for i in tqdm(range(magic_min, magic_max+1)):   \n\n        X, X_test, Y, idx_train, idx_test = get_data(i=i, data=data) \n\n        # STRATIFIED K FOLD\n        auc_folds=np.array([])\n\n        folds = StratifiedKFold(n_splits=NFOLDS, random_state=RS)\n\n        for train_index, val_index in folds.split(X, Y):\n\n            X_train, Y_train = X[train_index, :], Y[train_index]\n            X_val, Y_val = X[val_index, :], Y[val_index]\n            \n            #INSTANTIATING THE MAIN CLASSIFIER\n            clf = QuadraticDiscriminantAnalysis(**params)  \n            \n            if Y_pseudo is None:\n                X_l, Y_l = get_labels(X_train, Y_train)\n            else:\n                X_aug, Y_aug = pseudolabeling_qda(X_train, X_test, Y_train, Y_pseudo, idx_test, low, high)\n                X_l, Y_l = get_labels(X_aug, Y_aug.ravel()) \n                \n            clf.fit(X_l, Y_l.ravel())\n                \n            oof[idx_train[val_index]] = np.sum(clf.predict_proba(X_val)[:, 2:], axis=1)\n            preds[idx_test] += np.sum(clf.predict_proba(X_test)[:,2: ], axis=1)\/NFOLDS\n\n            auc = roc_auc_score(Y_val, oof[idx_train[val_index]])\n            auc_folds = np.append(auc_folds, auc)\n\n        auc_all = np.append(auc_all, np.mean(auc_folds))\n\n    report_results(oof, auc_all, clf_name='QDA')\n    \n    return preds, oof, auc_all","da74b07f":"Y_pseudo=preds_gmm#None\n\nfor rp, low, high in zip(rp_values, low_vals, high_vals):\n    parmas_qda = {'reg_param': rp}\n    Y_pseudo, oof_qda, auc_qda = train_qda(Y_pseudo=Y_pseudo, low=low, high=high, params=params_qda)\n\n#THE LAST PREDICTIONS \npreds_qda = Y_pseudo\n\nsub['target'] = preds_qda\nsub.to_csv('submission_qda.csv',index=False)","db825970":"preds_highest = preds_gmm\noof_highest = oof_gmm\n\nmask = (auc_qda > auc_gmm)\n\nprint(f\"The number of models where QDA's predictions are better is {sum(mask)}.\")","e06fdca4":"for i in tqdm(range(magic_min, magic_max+1)):\n    \n    if mask[i]:\n        _, _, _, idx_train, idx_test = get_data(i=i, data=data)\n        oof_highest[idx_train] = oof_qda[idx_train]\n        preds_highest[idx_test] = preds_qda[idx_test]\n        \nauc = roc_auc_score(train['target'].values, oof_highest)\nprint(f\"The 'highest' ROC AUC score is {auc}.\")","fcc577be":"sub['target'] = preds_highest\nsub.to_csv('submission_highest.csv',index=False)","65543db5":"oof_all = pd.DataFrame()\npreds_all = pd.DataFrame()\n\noof_all['gmm'] = rankdata(oof_gmm)\/len(oof_gmm)\noof_all['qda'] = rankdata(oof_qda)\/len(oof_qda)\npreds_all['gmm'] = rankdata(preds_gmm)\/len(preds_gmm)\npreds_all['qda'] = rankdata(preds_qda)\/len(preds_qda)\n\nlr = LogisticRegression()\n\nlr.fit(oof_all.values, train['target'].values)\npreds_lr = lr.predict_proba(preds_all.values)[:,1]\n\npreds_train = lr.predict_proba(oof_all)[:,1]\n\nauc = roc_auc_score(train['target'].values, preds_train)\nprint(f\"The final ROC AUC score is {auc}.\")","19548121":"sub['target'] = preds_lr\nsub.to_csv('submission_lr.csv',index=False)","6425b00e":"w = 0.02\nmask = (preds_gmm < (0.5 + w))&(preds_gmm > (0.5 - w))\n\npreds = rankdata(preds_gmm)\/len(preds_gmm)\n\npreds[mask] = preds_lr[mask]\n\nsub['target'] = preds\nsub.to_csv('submission_picking.csv',index=False)","d339685f":"## Computations with QDA\n\n### Identifying Clusters and Predicting Classes\n\nIn what follows, I will assume that there are 2 clusters per class in the data set. To identify these clusters we will run GMM on positive and negative instances separately. In each case, our goal is to label instances that belong to two different clusters. Here is a handy function that computes the means and covariances for all 4 clusters (2 clusters per each class):","e3edfb52":"## Stacking GMM and QDA with Logistic Regression","586faace":"Now, let's actually train the QDA classifier:","a414e08a":"### Handle Debuging\n\nChecking and handling the debuging mode (low values of magic_max and NFOLDS save a lot of time; the latter breaks cross-validation):","6dd22522":"### Loading Data","44ae4843":"Here is another very useful function initializing storage arrays for our cross-validation results: AUC, out-of-fold predictions, and test set prediction.","4b13ab11":"And another useful function to report the result of the cross-validation procedure.","be4ef786":"### Loading Libraries","ada84a4b":"QDA pseudolabeling function:","518001b2":"Here is the function for training the QDA classifier.","3e963921":"### Parameters","7267e7a9":"Now, let's actually train our first classifier:","30cb39f0":"And here is a handy function to get data for any value of `i`.","a0c7a953":"And, finally, here is our main function for training our classifiers.","6e46b797":"And repeat with pseudolabeling:","5cdcd584":"## Identifying Clusters and Predicting Classes with GMM\n\nIn what follows, I will assume that there are 2 clusters per class in the data set. To identify these clusters we will run `GMM` on positive and negative instances separately. In each case, our goal is to label instances that belong to two different clusters. Here is a handy function that computes the means and covariances for all 4 clusters (2 clusters per each class):","5c0c5986":"### Some Useful Functions and Preprocessing\n\nIn this part, we will collect and preprocess data from all 512 model (one model per one value of the `'wheezy-copper-turtle-magic'` categorical variable as was explained by Chris Deotte [here](https:\/\/www.kaggle.com\/cdeotte\/support-vector-machine-0-925). Later we will be able to load all these data from a single dictionary.","b08c95a7":"### Picking the Final Predictions\n\nIn our final submission, we will keep the GMM prediction results that were made with high degree of certainty. For the least certain predictions we will use the stacking results.","bb80e00e":"### Creating the submission file","fda631ff":"## Selecting the Strongest GMM\/QDA Results\n\nIdea: For each value of the 'magic' variable, let's keep the predictions of the model (GMM or QDA) that has the largest AUC for this value of the variable. ","f814aa30":"Here is another function that we will be using for pseudolabeling. "}}