{"cell_type":{"88d42990":"code","a60be5c6":"code","ad7dd2b3":"code","278f886e":"code","01434e0b":"code","734e4308":"code","a5840d85":"code","e8ce7d05":"code","ba7257c4":"code","8c6269c9":"code","3ec9a0f2":"code","ae711e60":"code","4d166d69":"code","9080192a":"code","f9144abe":"code","86f4e2fd":"code","2e626adc":"code","9f6ad703":"code","a403e515":"code","b3e3cbff":"code","452d057b":"code","d8cd0269":"markdown","20b50653":"markdown","dc569dff":"markdown","8567c4ff":"markdown","846e7b26":"markdown","70441435":"markdown","14787c31":"markdown","1e7cfc38":"markdown","c0fbe024":"markdown","3c66785a":"markdown","4361996f":"markdown","d357ef10":"markdown","4e03a50d":"markdown","3f7a69b8":"markdown","4cf7b490":"markdown"},"source":{"88d42990":"import pandas as pd\nimport numpy as np\nimport tqdm\nimport matplotlib.pyplot as plt\n\n#TF stuff\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\n\n#XGBoost\nfrom xgboost import XGBClassifier\n\n#Scikit\nfrom sklearn.metrics import log_loss\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform","a60be5c6":"import numpy as np\nfrom joblib import Parallel, delayed\nfrom scipy.interpolate import interp1d\nfrom scipy.special import erf, erfinv\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils.validation import FLOAT_DTYPES, check_array, check_is_fitted\n\n\nclass GaussRankScaler(BaseEstimator, TransformerMixin):\n    \"\"\"Transform features by scaling each feature to a normal distribution.\n    Parameters\n        ----------\n        epsilon : float, optional, default 1e-4\n            A small amount added to the lower bound or subtracted\n            from the upper bound. This value prevents infinite number\n            from occurring when applying the inverse error function.\n        copy : boolean, optional, default True\n            If False, try to avoid a copy and do inplace scaling instead.\n            This is not guaranteed to always work inplace; e.g. if the data is\n            not a NumPy array, a copy may still be returned.\n        n_jobs : int or None, optional, default None\n            Number of jobs to run in parallel.\n            ``None`` means 1 and ``-1`` means using all processors.\n        interp_kind : str or int, optional, default 'linear'\n           Specifies the kind of interpolation as a string\n            ('linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic',\n            'previous', 'next', where 'zero', 'slinear', 'quadratic' and 'cubic'\n            refer to a spline interpolation of zeroth, first, second or third\n            order; 'previous' and 'next' simply return the previous or next value\n            of the point) or as an integer specifying the order of the spline\n            interpolator to use.\n        interp_copy : bool, optional, default False\n            If True, the interpolation function makes internal copies of x and y.\n            If False, references to `x` and `y` are used.\n        Attributes\n        ----------\n        interp_func_ : list\n            The interpolation function for each feature in the training set.\n        \"\"\"\n\n    def __init__(self, epsilon=1e-4, copy=True, n_jobs=None, interp_kind='linear', interp_copy=False):\n        self.epsilon = epsilon\n        self.copy = copy\n        self.interp_kind = interp_kind\n        self.interp_copy = interp_copy\n        self.fill_value = 'extrapolate'\n        self.n_jobs = n_jobs\n\n    def fit(self, X, y=None):\n        \"\"\"Fit interpolation function to link rank with original data for future scaling\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The data used to fit interpolation function for later scaling along the features axis.\n        y\n            Ignored\n        \"\"\"\n        X = check_array(X, copy=self.copy, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n\n        self.interp_func_ = Parallel(n_jobs=self.n_jobs)(delayed(self._fit)(x) for x in X.T)\n        return self\n\n    def _fit(self, x):\n        x = self.drop_duplicates(x)\n        rank = np.argsort(np.argsort(x))\n        bound = 1.0 - self.epsilon\n        factor = np.max(rank) \/ 2.0 * bound\n        scaled_rank = np.clip(rank \/ factor - bound, -bound, bound)\n        return interp1d(\n            x, scaled_rank, kind=self.interp_kind, copy=self.interp_copy, fill_value=self.fill_value)\n\n    def transform(self, X, copy=None):\n        \"\"\"Scale the data with the Gauss Rank algorithm\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The data used to scale along the features axis.\n        copy : bool, optional (default: None)\n            Copy the input X or not.\n        \"\"\"\n        check_is_fitted(self, 'interp_func_')\n\n        copy = copy if copy is not None else self.copy\n        X = check_array(X, copy=copy, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n\n        X = np.array(Parallel(n_jobs=self.n_jobs)(delayed(self._transform)(i, x) for i, x in enumerate(X.T))).T\n        return X\n\n    def _transform(self, i, x):\n        return erfinv(self.interp_func_[i](x))\n\n    def inverse_transform(self, X, copy=None):\n        \"\"\"Scale back the data to the original representation\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data used to scale along the features axis.\n        copy : bool, optional (default: None)\n            Copy the input X or not.\n        \"\"\"\n        check_is_fitted(self, 'interp_func_')\n\n        copy = copy if copy is not None else self.copy\n        X = check_array(X, copy=copy, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n\n        X = np.array(Parallel(n_jobs=self.n_jobs)(delayed(self._inverse_transform)(i, x) for i, x in enumerate(X.T))).T\n        return X\n\n    def _inverse_transform(self, i, x):\n        inv_interp_func = interp1d(self.interp_func_[i].y, self.interp_func_[i].x, kind=self.interp_kind,\n                                   copy=self.interp_copy, fill_value=self.fill_value)\n        return inv_interp_func(erf(x))\n\n    @staticmethod\n    def drop_duplicates(x):\n        is_unique = np.zeros_like(x, dtype=bool)\n        is_unique[np.unique(x, return_index=True)[1]] = True\n        return x[is_unique]","ad7dd2b3":"# A convenient plotting function as we train models\n\ndef plot_hist(hist, last = None):\n    if last == None:\n        last = len(hist.history[\"loss\"])\n    plt.plot(hist.history[\"loss\"][-last:])\n    plt.plot(hist.history[\"val_loss\"][-last:])\n    plt.title(\"model accuracy\")\n    plt.ylabel(\"accuracy\")\n    plt.xlabel(\"epoch\")\n    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n    plt.show()","278f886e":"\n\"\"\"This file includes multilabel cross validators based on an implementation of\nthe Iterative Stratification algorithm described in the following paper:\nSechidis K., Tsoumakas G., Vlahavas I. (2011) On the Stratification of Multi-\nLabel Data. In: Gunopulos D., Hofmann T., Malerba D., Vazirgiannis M. (eds)\nMachine Learning and Knowledge Discovery in Databases. ECML PKDD 2011. Lecture\nNotes in Computer Science, vol 6913. Springer, Berlin, Heidelberg.\nFrom scikit-learn 0.19.0, StratifiedKFold, RepeatedStratifiedKFold, and\nStratifiedShuffleSplit were copied and modified, retaining compatibility\nwith scikit-learn.\nAttribution to authors of scikit-learn\/model_selection\/_split.py under BSD 3 clause:\n    Alexandre Gramfort <alexandre.gramfort@inria.fr>,\n    Gael Varoquaux <gael.varoquaux@normalesup.org>,\n    Olivier Grisel <olivier.grisel@ensta.org>,\n    Raghav RV <rvraghav93@gmail.com>\n\"\"\"\n\n# Author: Trent J. Bradberry <trentjason@hotmail.com>\n# License: BSD 3 clause\n\nimport numpy as np\n\nfrom sklearn.utils import check_random_state\nfrom sklearn.utils.validation import _num_samples, check_array\nfrom sklearn.utils.multiclass import type_of_target\n\nfrom sklearn.model_selection._split import _BaseKFold, _RepeatedSplits, \\\n    BaseShuffleSplit, _validate_shuffle_split\n\n\ndef IterativeStratification(labels, r, random_state):\n    \"\"\"This function implements the Iterative Stratification algorithm described\n    in the following paper:\n    Sechidis K., Tsoumakas G., Vlahavas I. (2011) On the Stratification of\n    Multi-Label Data. In: Gunopulos D., Hofmann T., Malerba D., Vazirgiannis M.\n    (eds) Machine Learning and Knowledge Discovery in Databases. ECML PKDD\n    2011. Lecture Notes in Computer Science, vol 6913. Springer, Berlin,\n    Heidelberg.\n    \"\"\"\n\n    n_samples = labels.shape[0]\n    test_folds = np.zeros(n_samples, dtype=int)\n\n    # Calculate the desired number of examples at each subset\n    c_folds = r * n_samples\n\n    # Calculate the desired number of examples of each label at each subset\n    c_folds_labels = np.outer(r, labels.sum(axis=0))\n\n    labels_not_processed_mask = np.ones(n_samples, dtype=bool)\n\n    while np.any(labels_not_processed_mask):\n        # Find the label with the fewest (but at least one) remaining examples,\n        # breaking ties randomly\n        num_labels = labels[labels_not_processed_mask].sum(axis=0)\n\n        # Handle case where only all-zero labels are left by distributing\n        # across all folds as evenly as possible (not in original algorithm but\n        # mentioned in the text). (By handling this case separately, some\n        # code redundancy is introduced; however, this approach allows for\n        # decreased execution time when there are a relatively large number\n        # of all-zero labels.)\n        if num_labels.sum() == 0:\n            sample_idxs = np.where(labels_not_processed_mask)[0]\n\n            for sample_idx in sample_idxs:\n                fold_idx = np.where(c_folds == c_folds.max())[0]\n\n                if fold_idx.shape[0] > 1:\n                    fold_idx = fold_idx[random_state.choice(fold_idx.shape[0])]\n\n                test_folds[sample_idx] = fold_idx\n                c_folds[fold_idx] -= 1\n\n            break\n\n        label_idx = np.where(num_labels == num_labels[np.nonzero(num_labels)].min())[0]\n        if label_idx.shape[0] > 1:\n            label_idx = label_idx[random_state.choice(label_idx.shape[0])]\n\n        sample_idxs = np.where(np.logical_and(labels[:, label_idx].flatten(), labels_not_processed_mask))[0]\n\n        for sample_idx in sample_idxs:\n            # Find the subset(s) with the largest number of desired examples\n            # for this label, breaking ties by considering the largest number\n            # of desired examples, breaking further ties randomly\n            label_folds = c_folds_labels[:, label_idx]\n            fold_idx = np.where(label_folds == label_folds.max())[0]\n\n            if fold_idx.shape[0] > 1:\n                temp_fold_idx = np.where(c_folds[fold_idx] ==\n                                         c_folds[fold_idx].max())[0]\n                fold_idx = fold_idx[temp_fold_idx]\n\n                if temp_fold_idx.shape[0] > 1:\n                    fold_idx = fold_idx[random_state.choice(temp_fold_idx.shape[0])]\n\n            test_folds[sample_idx] = fold_idx\n            labels_not_processed_mask[sample_idx] = False\n\n            # Update desired number of examples\n            c_folds_labels[fold_idx, labels[sample_idx]] -= 1\n            c_folds[fold_idx] -= 1\n\n    return test_folds\n\n\nclass MultilabelStratifiedKFold(_BaseKFold):\n    \"\"\"Multilabel stratified K-Folds cross-validator\n    Provides train\/test indices to split multilabel data into train\/test sets.\n    This cross-validation object is a variation of KFold that returns\n    stratified folds for multilabel data. The folds are made by preserving\n    the percentage of samples for each label.\n    Parameters\n    ----------\n    n_splits : int, default=3\n        Number of folds. Must be at least 2.\n    shuffle : boolean, optional\n        Whether to shuffle each stratification of the data before splitting\n        into batches.\n    random_state : int, RandomState instance or None, optional, default=None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Unlike StratifiedKFold that only uses random_state\n        when ``shuffle`` == True, this multilabel implementation\n        always uses the random_state since the iterative stratification\n        algorithm breaks ties randomly.\n    Examples\n    --------\n    >>> from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n    >>> import numpy as np\n    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n    >>> mskf = MultilabelStratifiedKFold(n_splits=2, random_state=0)\n    >>> mskf.get_n_splits(X, y)\n    2\n    >>> print(mskf)  # doctest: +NORMALIZE_WHITESPACE\n    MultilabelStratifiedKFold(n_splits=2, random_state=0, shuffle=False)\n    >>> for train_index, test_index in mskf.split(X, y):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    TRAIN: [0 3 4 6] TEST: [1 2 5 7]\n    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n    Notes\n    -----\n    Train and test sizes may be slightly different in each fold.\n    See also\n    --------\n    RepeatedMultilabelStratifiedKFold: Repeats Multilabel Stratified K-Fold\n    n times.\n    \"\"\"\n\n    def __init__(self, n_splits=3, shuffle=False, random_state=None):\n        super(MultilabelStratifiedKFold, self).__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n\n    def _make_test_folds(self, X, y):\n        y = np.asarray(y, dtype=bool)\n        type_of_target_y = type_of_target(y)\n\n        if type_of_target_y != 'multilabel-indicator':\n            raise ValueError(\n                'Supported target type is: multilabel-indicator. Got {!r} instead.'.format(type_of_target_y))\n\n        num_samples = y.shape[0]\n\n        rng = check_random_state(self.random_state)\n        indices = np.arange(num_samples)\n\n        if self.shuffle:\n            rng.shuffle(indices)\n            y = y[indices]\n\n        r = np.asarray([1 \/ self.n_splits] * self.n_splits)\n\n        test_folds = IterativeStratification(labels=y, r=r, random_state=rng)\n\n        return test_folds[np.argsort(indices)]\n\n    def _iter_test_masks(self, X=None, y=None, groups=None):\n        test_folds = self._make_test_folds(X, y)\n        for i in range(self.n_splits):\n            yield test_folds == i\n\n    def split(self, X, y, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n            Note that providing ``y`` is sufficient to generate the splits and\n            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n            ``X`` instead of actual training data.\n        y : array-like, shape (n_samples, n_labels)\n            The target variable for supervised learning problems.\n            Multilabel stratification is done based on the y labels.\n        groups : object\n            Always ignored, exists for compatibility.\n        Returns\n        -------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        Notes\n        -----\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting ``random_state``\n        to an integer.\n        \"\"\"\n        y = check_array(y, ensure_2d=False, dtype=None)\n        return super(MultilabelStratifiedKFold, self).split(X, y, groups)\n\n\nclass RepeatedMultilabelStratifiedKFold(_RepeatedSplits):\n    \"\"\"Repeated Multilabel Stratified K-Fold cross validator.\n    Repeats Mulilabel Stratified K-Fold n times with different randomization\n    in each repetition.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of folds. Must be at least 2.\n    n_repeats : int, default=10\n        Number of times cross-validator needs to be repeated.\n    random_state : None, int or RandomState, default=None\n        Random state to be used to generate random state for each\n        repetition as well as randomly breaking ties within the iterative\n        stratification algorithm.\n    Examples\n    --------\n    >>> from iterstrat.ml_stratifiers import RepeatedMultilabelStratifiedKFold\n    >>> import numpy as np\n    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n    >>> rmskf = RepeatedMultilabelStratifiedKFold(n_splits=2, n_repeats=2,\n    ...     random_state=0)\n    >>> for train_index, test_index in rmskf.split(X, y):\n    ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...     X_train, X_test = X[train_index], X[test_index]\n    ...     y_train, y_test = y[train_index], y[test_index]\n    ...\n    TRAIN: [0 3 4 6] TEST: [1 2 5 7]\n    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n    TRAIN: [0 1 4 5] TEST: [2 3 6 7]\n    TRAIN: [2 3 6 7] TEST: [0 1 4 5]\n    See also\n    --------\n    RepeatedStratifiedKFold: Repeats (Non-multilabel) Stratified K-Fold\n    n times.\n    \"\"\"\n    def __init__(self, n_splits=5, n_repeats=10, random_state=None):\n        super(RepeatedMultilabelStratifiedKFold, self).__init__(\n            MultilabelStratifiedKFold, n_repeats=n_repeats, random_state=random_state,\n            n_splits=n_splits)\n\n\nclass MultilabelStratifiedShuffleSplit(BaseShuffleSplit):\n    \"\"\"Multilabel Stratified ShuffleSplit cross-validator\n    Provides train\/test indices to split data into train\/test sets.\n    This cross-validation object is a merge of MultilabelStratifiedKFold and\n    ShuffleSplit, which returns stratified randomized folds for multilabel\n    data. The folds are made by preserving the percentage of each label.\n    Note: like the ShuffleSplit strategy, multilabel stratified random splits\n    do not guarantee that all folds will be different, although this is\n    still very likely for sizeable datasets.\n    Parameters\n    ----------\n    n_splits : int, default 10\n        Number of re-shuffling & splitting iterations.\n    test_size : float, int, None, optional\n        If float, should be between 0.0 and 1.0 and represent the proportion\n        of the dataset to include in the test split. If int, represents the\n        absolute number of test samples. If None, the value is set to the\n        complement of the train size. By default, the value is set to 0.1.\n        The default will change in version 0.21. It will remain 0.1 only\n        if ``train_size`` is unspecified, otherwise it will complement\n        the specified ``train_size``.\n    train_size : float, int, or None, default is None\n        If float, should be between 0.0 and 1.0 and represent the\n        proportion of the dataset to include in the train split. If\n        int, represents the absolute number of train samples. If None,\n        the value is automatically set to the complement of the test size.\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Unlike StratifiedShuffleSplit that only uses\n        random_state when ``shuffle`` == True, this multilabel implementation\n        always uses the random_state since the iterative stratification\n        algorithm breaks ties randomly.\n    Examples\n    --------\n    >>> from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n    >>> import numpy as np\n    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n    >>> msss = MultilabelStratifiedShuffleSplit(n_splits=3, test_size=0.5,\n    ...    random_state=0)\n    >>> msss.get_n_splits(X, y)\n    3\n    >>> print(mss)       # doctest: +ELLIPSIS\n    MultilabelStratifiedShuffleSplit(n_splits=3, random_state=0, test_size=0.5,\n                                     train_size=None)\n    >>> for train_index, test_index in msss.split(X, y):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n    TRAIN: [2 3 6 7] TEST: [0 1 4 5]\n    TRAIN: [1 2 5 6] TEST: [0 3 4 7]\n    Notes\n    -----\n    Train and test sizes may be slightly different from desired due to the\n    preference of stratification over perfectly sized folds.\n    \"\"\"\n\n    def __init__(self, n_splits=10, test_size=\"default\", train_size=None,\n                 random_state=None):\n        super(MultilabelStratifiedShuffleSplit, self).__init__(\n            n_splits=n_splits, test_size=test_size, train_size=train_size, random_state=random_state)\n\n    def _iter_indices(self, X, y, groups=None):\n        n_samples = _num_samples(X)\n        y = check_array(y, ensure_2d=False, dtype=None)\n        y = np.asarray(y, dtype=bool)\n        type_of_target_y = type_of_target(y)\n\n        if type_of_target_y != 'multilabel-indicator':\n            raise ValueError(\n                'Supported target type is: multilabel-indicator. Got {!r} instead.'.format(\n                    type_of_target_y))\n\n        n_train, n_test = _validate_shuffle_split(n_samples, self.test_size,\n                                                  self.train_size)\n\n        n_samples = y.shape[0]\n        rng = check_random_state(self.random_state)\n        y_orig = y.copy()\n\n        r = np.array([n_train, n_test]) \/ (n_train + n_test)\n\n        for _ in range(self.n_splits):\n            indices = np.arange(n_samples)\n            rng.shuffle(indices)\n            y = y_orig[indices]\n\n            test_folds = IterativeStratification(labels=y, r=r, random_state=rng)\n\n            test_idx = test_folds[np.argsort(indices)] == 1\n            test = np.where(test_idx)[0]\n            train = np.where(~test_idx)[0]\n\n            yield train, test\n\n    def split(self, X, y, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n            Note that providing ``y`` is sufficient to generate the splits and\n            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n            ``X`` instead of actual training data.\n        y : array-like, shape (n_samples, n_labels)\n            The target variable for supervised learning problems.\n            Multilabel stratification is done based on the y labels.\n        groups : object\n            Always ignored, exists for compatibility.\n        Returns\n        -------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        Notes\n        -----\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting ``random_state``\n        to an integer.\n        \"\"\"\n        y = check_array(y, ensure_2d=False, dtype=None)\n        return super(MultilabelStratifiedShuffleSplit, self).split(X, y, groups)","01434e0b":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sys\n\nimport os\nimport gc\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\ntf.random.set_seed(42)\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nimport tensorflow_addons as tfa\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom scipy.optimize import minimize\nfrom tqdm.notebook import tqdm\nfrom time import time\n\nprint(\"Tensorflow version \" + tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE","734e4308":"MIXED_PRECISION = False\nXLA_ACCELERATE = True\n\nif MIXED_PRECISION:\n    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n    if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n    mixed_precision.set_policy(policy)\n    print('Mixed precision enabled')\n\nif XLA_ACCELERATE:\n    tf.config.optimizer.set_jit(True)\n    print('Accelerated Linear Algebra enabled')\n    \n# [Fast Numpy Log Loss] https:\/\/www.kaggle.com\/gogo827jz\/optimise-blending-weights-4-5x-faster-log-loss\ndef log_loss_metric(y_true, y_pred):\n    loss = 0\n    y_pred_clip = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    for i in range(y_pred.shape[1]):\n        loss += - np.mean(y_true[:, i] * np.log(y_pred_clip[:, i]) + (1 - y_true[:, i]) * np.log(1 - y_pred_clip[:, i]))\n    return loss \/ y_pred.shape[1]","a5840d85":"def register_keras_custom_object(cls):\n    tf.keras.utils.get_custom_objects()[cls.__name__] = cls\n    return cls\n\n\ndef glu(x, n_units=None):\n    \"\"\"Generalized linear unit nonlinear activation.\"\"\"\n    if n_units is None:\n        n_units = tf.shape(x)[-1] \/\/ 2\n\n    return x[..., :n_units] * tf.nn.sigmoid(x[..., n_units:])\n\n\n\"\"\"\nCode replicated from https:\/\/github.com\/tensorflow\/addons\/blob\/master\/tensorflow_addons\/activations\/sparsemax.py\n\"\"\"\n\n\n@register_keras_custom_object\n@tf.function\ndef sparsemax(logits, axis):\n    \"\"\"Sparsemax activation function [1].\n    For each batch `i` and class `j` we have\n      $$sparsemax[i, j] = max(logits[i, j] - tau(logits[i, :]), 0)$$\n    [1]: https:\/\/arxiv.org\/abs\/1602.02068\n    Args:\n        logits: Input tensor.\n        axis: Integer, axis along which the sparsemax operation is applied.\n    Returns:\n        Tensor, output of sparsemax transformation. Has the same type and\n        shape as `logits`.\n    Raises:\n        ValueError: In case `dim(logits) == 1`.\n    \"\"\"\n    logits = tf.convert_to_tensor(logits, name=\"logits\")\n\n    # We need its original shape for shape inference.\n    shape = logits.get_shape()\n    rank = shape.rank\n    is_last_axis = (axis == -1) or (axis == rank - 1)\n\n    if is_last_axis:\n        output = _compute_2d_sparsemax(logits)\n        output.set_shape(shape)\n        return output\n\n    # If dim is not the last dimension, we have to do a transpose so that we can\n    # still perform softmax on its last dimension.\n\n    # Swap logits' dimension of dim and its last dimension.\n    rank_op = tf.rank(logits)\n    axis_norm = axis % rank\n    logits = _swap_axis(logits, axis_norm, tf.math.subtract(rank_op, 1))\n\n    # Do the actual softmax on its last dimension.\n    output = _compute_2d_sparsemax(logits)\n    output = _swap_axis(output, axis_norm, tf.math.subtract(rank_op, 1))\n\n    # Make shape inference work since transpose may erase its static shape.\n    output.set_shape(shape)\n    return output\n\n\ndef _swap_axis(logits, dim_index, last_index, **kwargs):\n    return tf.transpose(\n        logits,\n        tf.concat(\n            [\n                tf.range(dim_index),\n                [last_index],\n                tf.range(dim_index + 1, last_index),\n                [dim_index],\n            ],\n            0,\n        ),\n        **kwargs,\n    )\n\n\ndef _compute_2d_sparsemax(logits):\n    \"\"\"Performs the sparsemax operation when axis=-1.\"\"\"\n    shape_op = tf.shape(logits)\n    obs = tf.math.reduce_prod(shape_op[:-1])\n    dims = shape_op[-1]\n\n    # In the paper, they call the logits z.\n    # The mean(logits) can be substracted from logits to make the algorithm\n    # more numerically stable. the instability in this algorithm comes mostly\n    # from the z_cumsum. Substacting the mean will cause z_cumsum to be close\n    # to zero. However, in practise the numerical instability issues are very\n    # minor and substacting the mean causes extra issues with inf and nan\n    # input.\n    # Reshape to [obs, dims] as it is almost free and means the remanining\n    # code doesn't need to worry about the rank.\n    z = tf.reshape(logits, [obs, dims])\n\n    # sort z\n    z_sorted, _ = tf.nn.top_k(z, k=dims)\n\n    # calculate k(z)\n    z_cumsum = tf.math.cumsum(z_sorted, axis=-1)\n    k = tf.range(1, tf.cast(dims, logits.dtype) + 1, dtype=logits.dtype)\n    z_check = 1 + k * z_sorted > z_cumsum\n    # because the z_check vector is always [1,1,...1,0,0,...0] finding the\n    # (index + 1) of the last `1` is the same as just summing the number of 1.\n    k_z = tf.math.reduce_sum(tf.cast(z_check, tf.int32), axis=-1)\n\n    # calculate tau(z)\n    # If there are inf values or all values are -inf, the k_z will be zero,\n    # this is mathematically invalid and will also cause the gather_nd to fail.\n    # Prevent this issue for now by setting k_z = 1 if k_z = 0, this is then\n    # fixed later (see p_safe) by returning p = nan. This results in the same\n    # behavior as softmax.\n    k_z_safe = tf.math.maximum(k_z, 1)\n    indices = tf.stack([tf.range(0, obs), tf.reshape(k_z_safe, [-1]) - 1], axis=1)\n    tau_sum = tf.gather_nd(z_cumsum, indices)\n    tau_z = (tau_sum - 1) \/ tf.cast(k_z, logits.dtype)\n\n    # calculate p\n    p = tf.math.maximum(tf.cast(0, logits.dtype), z - tf.expand_dims(tau_z, -1))\n    # If k_z = 0 or if z = nan, then the input is invalid\n    p_safe = tf.where(\n        tf.expand_dims(\n            tf.math.logical_or(tf.math.equal(k_z, 0), tf.math.is_nan(z_cumsum[:, -1])),\n            axis=-1,\n        ),\n        tf.fill([obs, dims], tf.cast(float(\"nan\"), logits.dtype)),\n        p,\n    )\n\n    # Reshape back to original size\n    p_safe = tf.reshape(p_safe, shape_op)\n    return p_safe\n\n\n\"\"\"\nCode replicated from https:\/\/github.com\/tensorflow\/addons\/blob\/master\/tensorflow_addons\/layers\/normalizations.py\n\"\"\"\n\n\n@register_keras_custom_object\nclass GroupNormalization(tf.keras.layers.Layer):\n    \"\"\"Group normalization layer.\n    Group Normalization divides the channels into groups and computes\n    within each group the mean and variance for normalization.\n    Empirically, its accuracy is more stable than batch norm in a wide\n    range of small batch sizes, if learning rate is adjusted linearly\n    with batch sizes.\n    Relation to Layer Normalization:\n    If the number of groups is set to 1, then this operation becomes identical\n    to Layer Normalization.\n    Relation to Instance Normalization:\n    If the number of groups is set to the\n    input dimension (number of groups is equal\n    to number of channels), then this operation becomes\n    identical to Instance Normalization.\n    Arguments\n        groups: Integer, the number of groups for Group Normalization.\n            Can be in the range [1, N] where N is the input dimension.\n            The input dimension must be divisible by the number of groups.\n        axis: Integer, the axis that should be normalized.\n        epsilon: Small float added to variance to avoid dividing by zero.\n        center: If True, add offset of `beta` to normalized tensor.\n            If False, `beta` is ignored.\n        scale: If True, multiply by `gamma`.\n            If False, `gamma` is not used.\n        beta_initializer: Initializer for the beta weight.\n        gamma_initializer: Initializer for the gamma weight.\n        beta_regularizer: Optional regularizer for the beta weight.\n        gamma_regularizer: Optional regularizer for the gamma weight.\n        beta_constraint: Optional constraint for the beta weight.\n        gamma_constraint: Optional constraint for the gamma weight.\n    Input shape\n        Arbitrary. Use the keyword argument `input_shape`\n        (tuple of integers, does not include the samples axis)\n        when using this layer as the first layer in a model.\n    Output shape\n        Same shape as input.\n    References\n        - [Group Normalization](https:\/\/arxiv.org\/abs\/1803.08494)\n    \"\"\"\n\n    def __init__(\n            self,\n            groups: int = 2,\n            axis: int = -1,\n            epsilon: float = 1e-3,\n            center: bool = True,\n            scale: bool = True,\n            beta_initializer=\"zeros\",\n            gamma_initializer=\"ones\",\n            beta_regularizer=None,\n            gamma_regularizer=None,\n            beta_constraint=None,\n            gamma_constraint=None,\n            **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.supports_masking = True\n        self.groups = groups\n        self.axis = axis\n        self.epsilon = epsilon\n        self.center = center\n        self.scale = scale\n        self.beta_initializer = tf.keras.initializers.get(beta_initializer)\n        self.gamma_initializer = tf.keras.initializers.get(gamma_initializer)\n        self.beta_regularizer = tf.keras.regularizers.get(beta_regularizer)\n        self.gamma_regularizer = tf.keras.regularizers.get(gamma_regularizer)\n        self.beta_constraint = tf.keras.constraints.get(beta_constraint)\n        self.gamma_constraint = tf.keras.constraints.get(gamma_constraint)\n        self._check_axis()\n\n    def build(self, input_shape):\n\n        self._check_if_input_shape_is_none(input_shape)\n        self._set_number_of_groups_for_instance_norm(input_shape)\n        self._check_size_of_dimensions(input_shape)\n        self._create_input_spec(input_shape)\n\n        self._add_gamma_weight(input_shape)\n        self._add_beta_weight(input_shape)\n        self.built = True\n        super().build(input_shape)\n\n    def call(self, inputs, training=None):\n        # Training=none is just for compat with batchnorm signature call\n        input_shape = tf.keras.backend.int_shape(inputs)\n        tensor_input_shape = tf.shape(inputs)\n\n        reshaped_inputs, group_shape = self._reshape_into_groups(\n            inputs, input_shape, tensor_input_shape\n        )\n\n        normalized_inputs = self._apply_normalization(reshaped_inputs, input_shape)\n\n        outputs = tf.reshape(normalized_inputs, tensor_input_shape)\n\n        return outputs\n\n    def get_config(self):\n        config = {\n            \"groups\": self.groups,\n            \"axis\": self.axis,\n            \"epsilon\": self.epsilon,\n            \"center\": self.center,\n            \"scale\": self.scale,\n            \"beta_initializer\": tf.keras.initializers.serialize(self.beta_initializer),\n            \"gamma_initializer\": tf.keras.initializers.serialize(\n                self.gamma_initializer\n            ),\n            \"beta_regularizer\": tf.keras.regularizers.serialize(self.beta_regularizer),\n            \"gamma_regularizer\": tf.keras.regularizers.serialize(\n                self.gamma_regularizer\n            ),\n            \"beta_constraint\": tf.keras.constraints.serialize(self.beta_constraint),\n            \"gamma_constraint\": tf.keras.constraints.serialize(self.gamma_constraint),\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def _reshape_into_groups(self, inputs, input_shape, tensor_input_shape):\n\n        group_shape = [tensor_input_shape[i] for i in range(len(input_shape))]\n        group_shape[self.axis] = input_shape[self.axis] \/\/ self.groups\n        group_shape.insert(self.axis, self.groups)\n        group_shape = tf.stack(group_shape)\n        reshaped_inputs = tf.reshape(inputs, group_shape)\n        return reshaped_inputs, group_shape\n\n    def _apply_normalization(self, reshaped_inputs, input_shape):\n\n        group_shape = tf.keras.backend.int_shape(reshaped_inputs)\n        group_reduction_axes = list(range(1, len(group_shape)))\n        axis = -2 if self.axis == -1 else self.axis - 1\n        group_reduction_axes.pop(axis)\n\n        mean, variance = tf.nn.moments(\n            reshaped_inputs, group_reduction_axes, keepdims=True\n        )\n\n        gamma, beta = self._get_reshaped_weights(input_shape)\n        normalized_inputs = tf.nn.batch_normalization(\n            reshaped_inputs,\n            mean=mean,\n            variance=variance,\n            scale=gamma,\n            offset=beta,\n            variance_epsilon=self.epsilon,\n        )\n        return normalized_inputs\n\n    def _get_reshaped_weights(self, input_shape):\n        broadcast_shape = self._create_broadcast_shape(input_shape)\n        gamma = None\n        beta = None\n        if self.scale:\n            gamma = tf.reshape(self.gamma, broadcast_shape)\n\n        if self.center:\n            beta = tf.reshape(self.beta, broadcast_shape)\n        return gamma, beta\n\n    def _check_if_input_shape_is_none(self, input_shape):\n        dim = input_shape[self.axis]\n        if dim is None:\n            raise ValueError(\n                \"Axis \" + str(self.axis) + \" of \"\n                                           \"input tensor should have a defined dimension \"\n                                           \"but the layer received an input with shape \" + str(input_shape) + \".\"\n            )\n\n    def _set_number_of_groups_for_instance_norm(self, input_shape):\n        dim = input_shape[self.axis]\n\n        if self.groups == -1:\n            self.groups = dim\n\n    def _check_size_of_dimensions(self, input_shape):\n\n        dim = input_shape[self.axis]\n        if dim < self.groups:\n            raise ValueError(\n                \"Number of groups (\" + str(self.groups) + \") cannot be \"\n                                                          \"more than the number of channels (\" + str(dim) + \").\"\n            )\n\n        if dim % self.groups != 0:\n            raise ValueError(\n                \"Number of groups (\" + str(self.groups) + \") must be a \"\n                                                          \"multiple of the number of channels (\" + str(dim) + \").\"\n            )\n\n    def _check_axis(self):\n\n        if self.axis == 0:\n            raise ValueError(\n                \"You are trying to normalize your batch axis. Do you want to \"\n                \"use tf.layer.batch_normalization instead\"\n            )\n\n    def _create_input_spec(self, input_shape):\n\n        dim = input_shape[self.axis]\n        self.input_spec = tf.keras.layers.InputSpec(\n            ndim=len(input_shape), axes={self.axis: dim}\n        )\n\n    def _add_gamma_weight(self, input_shape):\n\n        dim = input_shape[self.axis]\n        shape = (dim,)\n\n        if self.scale:\n            self.gamma = self.add_weight(\n                shape=shape,\n                name=\"gamma\",\n                initializer=self.gamma_initializer,\n                regularizer=self.gamma_regularizer,\n                constraint=self.gamma_constraint,\n            )\n        else:\n            self.gamma = None\n\n    def _add_beta_weight(self, input_shape):\n\n        dim = input_shape[self.axis]\n        shape = (dim,)\n\n        if self.center:\n            self.beta = self.add_weight(\n                shape=shape,\n                name=\"beta\",\n                initializer=self.beta_initializer,\n                regularizer=self.beta_regularizer,\n                constraint=self.beta_constraint,\n            )\n        else:\n            self.beta = None\n\n    def _create_broadcast_shape(self, input_shape):\n        broadcast_shape = [1] * len(input_shape)\n        broadcast_shape[self.axis] = input_shape[self.axis] \/\/ self.groups\n        broadcast_shape.insert(self.axis, self.groups)\n        return broadcast_shape\n\nclass TransformBlock(tf.keras.Model):\n\n    def __init__(self, features,\n                 norm_type,\n                 momentum=0.9,\n                 virtual_batch_size=None,\n                 groups=2,\n                 block_name='',\n                 **kwargs):\n        super(TransformBlock, self).__init__(**kwargs)\n\n        self.features = features\n        self.norm_type = norm_type\n        self.momentum = momentum\n        self.groups = groups\n        self.virtual_batch_size = virtual_batch_size\n\n        self.transform = tf.keras.layers.Dense(self.features, use_bias=False, name=f'transformblock_dense_{block_name}')\n\n        if norm_type == 'batch':\n            self.bn = tf.keras.layers.BatchNormalization(axis=-1, momentum=momentum,\n                                                         virtual_batch_size=virtual_batch_size,\n                                                         name=f'transformblock_bn_{block_name}')\n\n        else:\n            self.bn = GroupNormalization(axis=-1, groups=self.groups, name=f'transformblock_gn_{block_name}')\n\n    def call(self, inputs, training=None):\n        x = self.transform(inputs)\n        x = self.bn(x, training=training)\n        return x\n\n\nclass TabNet(tf.keras.Model):\n\n    def __init__(self, feature_columns,\n                 feature_dim=64,\n                 output_dim=64,\n                 num_features=None,\n                 num_decision_steps=5,\n                 relaxation_factor=1.5,\n                 sparsity_coefficient=1e-5,\n                 norm_type='group',\n                 batch_momentum=0.98,\n                 virtual_batch_size=None,\n                 num_groups=2,\n                 epsilon=1e-5,\n                 **kwargs):\n        \"\"\"\n        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https:\/\/arxiv.org\/abs\/1908.07442)\n        # Hyper Parameter Tuning (Excerpt from the paper)\n        We consider datasets ranging from \u223c10K to \u223c10M training points, with varying degrees of fitting\n        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n        selection:\n            - Most datasets yield the best results for Nsteps \u2208 [3, 10]. Typically, larger datasets and\n            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n            overfitting and yield poor generalization.\n            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n            - An optimal choice of \u03b3 can have a major role on the overall performance. Typically a larger\n            Nsteps value favors for a larger \u03b3.\n            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n            much smaller than the batch size.\n            - Initially large learning rate is important, which should be gradually decayed until convergence.\n        Args:\n            feature_columns: The Tensorflow feature columns for the dataset.\n            feature_dim (N_a): Dimensionality of the hidden representation in feature\n                transformation block. Each layer first maps the representation to a\n                2*feature_dim-dimensional output and half of it is used to determine the\n                nonlinearity of the GLU activation where the other half is used as an\n                input to GLU, and eventually feature_dim-dimensional output is\n                transferred to the next layer.\n            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n                later mapped to the final classification or regression output.\n            num_features: The number of input features (i.e the number of columns for\n                tabular data assuming each feature is represented with 1 dimension).\n            num_decision_steps(N_steps): Number of sequential decision steps.\n            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n                feature at different decision steps. When it is 1, a feature is enforced\n                to be used only at one decision step and as it increases, more\n                flexibility is provided to use a feature at multiple decision steps.\n            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n                Sparsity may provide a favorable inductive bias for convergence to\n                higher accuracy for some datasets where most of the input features are redundant.\n            norm_type: Type of normalization to perform for the model. Can be either\n                'batch' or 'group'. 'group' is the default.\n            batch_momentum: Momentum in ghost batch normalization.\n            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n                overall batch size should be an integer multiple of virtual_batch_size.\n            num_groups: Number of groups used for group normalization.\n            epsilon: A small number for numerical stability of the entropy calculations.\n        \"\"\"\n        super(TabNet, self).__init__(**kwargs)\n\n        # Input checks\n        if feature_columns is not None:\n            if type(feature_columns) not in (list, tuple):\n                raise ValueError(\"`feature_columns` must be a list or a tuple.\")\n\n            if len(feature_columns) == 0:\n                raise ValueError(\"`feature_columns` must be contain at least 1 tf.feature_column !\")\n\n            if num_features is None:\n                num_features = len(feature_columns)\n            else:\n                num_features = int(num_features)\n\n        else:\n            if num_features is None:\n                raise ValueError(\"If `feature_columns` is None, then `num_features` cannot be None.\")\n\n        if num_decision_steps < 1:\n            raise ValueError(\"Num decision steps must be greater than 0.\")\n\n        if feature_dim < output_dim:\n            raise ValueError(\"To compute `features_for_coef`, feature_dim must be larger than output dim\")\n\n        feature_dim = int(feature_dim)\n        output_dim = int(output_dim)\n        num_decision_steps = int(num_decision_steps)\n        relaxation_factor = float(relaxation_factor)\n        sparsity_coefficient = float(sparsity_coefficient)\n        batch_momentum = float(batch_momentum)\n        num_groups = max(1, int(num_groups))\n        epsilon = float(epsilon)\n\n        if relaxation_factor < 0.:\n            raise ValueError(\"`relaxation_factor` cannot be negative !\")\n\n        if sparsity_coefficient < 0.:\n            raise ValueError(\"`sparsity_coefficient` cannot be negative !\")\n\n        if virtual_batch_size is not None:\n            virtual_batch_size = int(virtual_batch_size)\n\n        if norm_type not in ['batch', 'group']:\n            raise ValueError(\"`norm_type` must be either `batch` or `group`\")\n\n        self.feature_columns = feature_columns\n        self.num_features = num_features\n        self.feature_dim = feature_dim\n        self.output_dim = output_dim\n\n        self.num_decision_steps = num_decision_steps\n        self.relaxation_factor = relaxation_factor\n        self.sparsity_coefficient = sparsity_coefficient\n        self.norm_type = norm_type\n        self.batch_momentum = batch_momentum\n        self.virtual_batch_size = virtual_batch_size\n        self.num_groups = num_groups\n        self.epsilon = epsilon\n\n        # if num_decision_steps > 1:\n            # features_for_coeff = feature_dim - output_dim\n            # print(f\"[TabNet]: {features_for_coeff} features will be used for decision steps.\")\n\n        if self.feature_columns is not None:\n            self.input_features = tf.keras.layers.DenseFeatures(feature_columns, trainable=True)\n\n            if self.norm_type == 'batch':\n                self.input_bn = tf.keras.layers.BatchNormalization(axis=-1, momentum=batch_momentum, name='input_bn')\n            else:\n                self.input_bn = GroupNormalization(axis=-1, groups=self.num_groups, name='input_gn')\n\n        else:\n            self.input_features = None\n            self.input_bn = None\n\n        self.transform_f1 = TransformBlock(2 * self.feature_dim, self.norm_type,\n                                           self.batch_momentum, self.virtual_batch_size, self.num_groups,\n                                           block_name='f1')\n\n        self.transform_f2 = TransformBlock(2 * self.feature_dim, self.norm_type,\n                                           self.batch_momentum, self.virtual_batch_size, self.num_groups,\n                                           block_name='f2')\n\n        self.transform_f3_list = [\n            TransformBlock(2 * self.feature_dim, self.norm_type,\n                           self.batch_momentum, self.virtual_batch_size, self.num_groups, block_name=f'f3_{i}')\n            for i in range(self.num_decision_steps)\n        ]\n\n        self.transform_f4_list = [\n            TransformBlock(2 * self.feature_dim, self.norm_type,\n                           self.batch_momentum, self.virtual_batch_size, self.num_groups, block_name=f'f4_{i}')\n            for i in range(self.num_decision_steps)\n        ]\n\n        self.transform_coef_list = [\n            TransformBlock(self.num_features, self.norm_type,\n                           self.batch_momentum, self.virtual_batch_size, self.num_groups, block_name=f'coef_{i}')\n            for i in range(self.num_decision_steps - 1)\n        ]\n\n        self._step_feature_selection_masks = None\n        self._step_aggregate_feature_selection_mask = None\n\n    def call(self, inputs, training=None):\n        if self.input_features is not None:\n            features = self.input_features(inputs)\n            features = self.input_bn(features, training=training)\n\n        else:\n            features = inputs\n\n        batch_size = tf.shape(features)[0]\n        self._step_feature_selection_masks = []\n        self._step_aggregate_feature_selection_mask = None\n\n        # Initializes decision-step dependent variables.\n        output_aggregated = tf.zeros([batch_size, self.output_dim])\n        masked_features = features\n        mask_values = tf.zeros([batch_size, self.num_features])\n        aggregated_mask_values = tf.zeros([batch_size, self.num_features])\n        complementary_aggregated_mask_values = tf.ones(\n            [batch_size, self.num_features])\n\n        total_entropy = 0.0\n        entropy_loss = 0.\n\n        for ni in range(self.num_decision_steps):\n            # Feature transformer with two shared and two decision step dependent\n            # blocks is used below.=\n            transform_f1 = self.transform_f1(masked_features, training=training)\n            transform_f1 = glu(transform_f1, self.feature_dim)\n\n            transform_f2 = self.transform_f2(transform_f1, training=training)\n            transform_f2 = (glu(transform_f2, self.feature_dim) +\n                            transform_f1) * tf.math.sqrt(0.5)\n\n            transform_f3 = self.transform_f3_list[ni](transform_f2, training=training)\n            transform_f3 = (glu(transform_f3, self.feature_dim) +\n                            transform_f2) * tf.math.sqrt(0.5)\n\n            transform_f4 = self.transform_f4_list[ni](transform_f3, training=training)\n            transform_f4 = (glu(transform_f4, self.feature_dim) +\n                            transform_f3) * tf.math.sqrt(0.5)\n\n            if (ni > 0 or self.num_decision_steps == 1):\n                decision_out = tf.nn.relu(transform_f4[:, :self.output_dim])\n\n                # Decision aggregation.\n                output_aggregated += decision_out\n\n                # Aggregated masks are used for visualization of the\n                # feature importance attributes.\n                scale_agg = tf.reduce_sum(decision_out, axis=1, keepdims=True)\n\n                if self.num_decision_steps > 1:\n                    scale_agg = scale_agg \/ tf.cast(self.num_decision_steps - 1, tf.float32)\n\n                aggregated_mask_values += mask_values * scale_agg\n\n            features_for_coef = transform_f4[:, self.output_dim:]\n\n            if ni < (self.num_decision_steps - 1):\n                # Determines the feature masks via linear and nonlinear\n                # transformations, taking into account of aggregated feature use.\n                mask_values = self.transform_coef_list[ni](features_for_coef, training=training)\n                mask_values *= complementary_aggregated_mask_values\n                mask_values = sparsemax(mask_values, axis=-1)\n\n                # Relaxation factor controls the amount of reuse of features between\n                # different decision blocks and updated with the values of\n                # coefficients.\n                complementary_aggregated_mask_values *= (\n                        self.relaxation_factor - mask_values)\n\n                # Entropy is used to penalize the amount of sparsity in feature\n                # selection.\n                total_entropy += tf.reduce_mean(\n                    tf.reduce_sum(\n                        -mask_values * tf.math.log(mask_values + self.epsilon), axis=1)) \/ (\n                                     tf.cast(self.num_decision_steps - 1, tf.float32))\n\n                # Add entropy loss\n                entropy_loss = total_entropy\n\n                # Feature selection.\n                masked_features = tf.multiply(mask_values, features)\n\n                # Visualization of the feature selection mask at decision step ni\n                # tf.summary.image(\n                #     \"Mask for step\" + str(ni),\n                #     tf.expand_dims(tf.expand_dims(mask_values, 0), 3),\n                #     max_outputs=1)\n                mask_at_step_i = tf.expand_dims(tf.expand_dims(mask_values, 0), 3)\n                self._step_feature_selection_masks.append(mask_at_step_i)\n\n            else:\n                # This branch is needed for correct compilation by tf.autograph\n                entropy_loss = 0.\n\n        # Adds the loss automatically\n        self.add_loss(self.sparsity_coefficient * entropy_loss)\n\n        # Visualization of the aggregated feature importances\n        # tf.summary.image(\n        #     \"Aggregated mask\",\n        #     tf.expand_dims(tf.expand_dims(aggregated_mask_values, 0), 3),\n        #     max_outputs=1)\n\n        agg_mask = tf.expand_dims(tf.expand_dims(aggregated_mask_values, 0), 3)\n        self._step_aggregate_feature_selection_mask = agg_mask\n\n        return output_aggregated\n\n    @property\n    def feature_selection_masks(self):\n        return self._step_feature_selection_masks\n\n    @property\n    def aggregate_feature_selection_mask(self):\n        return self._step_aggregate_feature_selection_mask\n\n\nclass TabNetClassifier(tf.keras.Model):\n\n    def __init__(self, feature_columns,\n                 num_classes,\n                 num_features=None,\n                 feature_dim=64,\n                 output_dim=64,\n                 num_decision_steps=5,\n                 relaxation_factor=1.5,\n                 sparsity_coefficient=1e-5,\n                 norm_type='group',\n                 batch_momentum=0.98,\n                 virtual_batch_size=None,\n                 num_groups=1,\n                 epsilon=1e-5,\n                 multi_label=False, \n                 **kwargs):\n        \"\"\"\n        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https:\/\/arxiv.org\/abs\/1908.07442)\n        # Hyper Parameter Tuning (Excerpt from the paper)\n        We consider datasets ranging from \u223c10K to \u223c10M training points, with varying degrees of fitting\n        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n        selection:\n            - Most datasets yield the best results for Nsteps \u2208 [3, 10]. Typically, larger datasets and\n            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n            overfitting and yield poor generalization.\n            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n            - An optimal choice of \u03b3 can have a major role on the overall performance. Typically a larger\n            Nsteps value favors for a larger \u03b3.\n            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n            much smaller than the batch size.\n            - Initially large learning rate is important, which should be gradually decayed until convergence.\n        Args:\n            feature_columns: The Tensorflow feature columns for the dataset.\n            num_classes: Number of classes.\n            feature_dim (N_a): Dimensionality of the hidden representation in feature\n                transformation block. Each layer first maps the representation to a\n                2*feature_dim-dimensional output and half of it is used to determine the\n                nonlinearity of the GLU activation where the other half is used as an\n                input to GLU, and eventually feature_dim-dimensional output is\n                transferred to the next layer.\n            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n                later mapped to the final classification or regression output.\n            num_features: The number of input features (i.e the number of columns for\n                tabular data assuming each feature is represented with 1 dimension).\n            num_decision_steps(N_steps): Number of sequential decision steps.\n            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n                feature at different decision steps. When it is 1, a feature is enforced\n                to be used only at one decision step and as it increases, more\n                flexibility is provided to use a feature at multiple decision steps.\n            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n                Sparsity may provide a favorable inductive bias for convergence to\n                higher accuracy for some datasets where most of the input features are redundant.\n            norm_type: Type of normalization to perform for the model. Can be either\n                'group' or 'group'. 'group' is the default.\n            batch_momentum: Momentum in ghost batch normalization.\n            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n                overall batch size should be an integer multiple of virtual_batch_size.\n            num_groups: Number of groups used for group normalization.\n            epsilon: A small number for numerical stability of the entropy calculations.\n        \"\"\"\n        super(TabNetClassifier, self).__init__(**kwargs)\n\n        self.num_classes = num_classes\n\n        self.tabnet = TabNet(feature_columns=feature_columns,\n                    num_features=num_features,\n                    feature_dim=feature_dim,\n                    output_dim=output_dim,\n                    num_decision_steps=num_decision_steps,\n                    relaxation_factor=relaxation_factor,\n                    sparsity_coefficient=sparsity_coefficient,\n                    norm_type=norm_type,\n                    batch_momentum=batch_momentum,\n                    virtual_batch_size=virtual_batch_size,\n                    num_groups=num_groups,\n                    epsilon=epsilon,\n                    **kwargs)\n        \n        if multi_label:\n            \n            self.clf = tf.keras.layers.Dense(num_classes, activation='sigmoid', use_bias=False, name='classifier')\n            \n        else:\n            \n            self.clf = tf.keras.layers.Dense(num_classes, activation='softmax', use_bias=False, name='classifier')\n\n    def call(self, inputs, training=None):\n        self.activations = self.tabnet(inputs, training=training)\n        out = self.clf(self.activations)\n\n        return out\n\n    def summary(self, *super_args, **super_kwargs):\n        super().summary(*super_args, **super_kwargs)\n        self.tabnet.summary(*super_args, **super_kwargs)\n\n\nclass TabNetRegressor(tf.keras.Model):\n\n    def __init__(self, feature_columns,\n                 num_regressors,\n                 num_features=None,\n                 feature_dim=64,\n                 output_dim=64,\n                 num_decision_steps=5,\n                 relaxation_factor=1.5,\n                 sparsity_coefficient=1e-5,\n                 norm_type='group',\n                 batch_momentum=0.98,\n                 virtual_batch_size=None,\n                 num_groups=1,\n                 epsilon=1e-5,\n                 **kwargs):\n        \"\"\"\n        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https:\/\/arxiv.org\/abs\/1908.07442)\n        # Hyper Parameter Tuning (Excerpt from the paper)\n        We consider datasets ranging from \u223c10K to \u223c10M training points, with varying degrees of fitting\n        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n        selection:\n            - Most datasets yield the best results for Nsteps \u2208 [3, 10]. Typically, larger datasets and\n            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n            overfitting and yield poor generalization.\n            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n            - An optimal choice of \u03b3 can have a major role on the overall performance. Typically a larger\n            Nsteps value favors for a larger \u03b3.\n            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n            much smaller than the batch size.\n            - Initially large learning rate is important, which should be gradually decayed until convergence.\n        Args:\n            feature_columns: The Tensorflow feature columns for the dataset.\n            num_regressors: Number of regression variables.\n            feature_dim (N_a): Dimensionality of the hidden representation in feature\n                transformation block. Each layer first maps the representation to a\n                2*feature_dim-dimensional output and half of it is used to determine the\n                nonlinearity of the GLU activation where the other half is used as an\n                input to GLU, and eventually feature_dim-dimensional output is\n                transferred to the next layer.\n            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n                later mapped to the final classification or regression output.\n            num_features: The number of input features (i.e the number of columns for\n                tabular data assuming each feature is represented with 1 dimension).\n            num_decision_steps(N_steps): Number of sequential decision steps.\n            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n                feature at different decision steps. When it is 1, a feature is enforced\n                to be used only at one decision step and as it increases, more\n                flexibility is provided to use a feature at multiple decision steps.\n            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n                Sparsity may provide a favorable inductive bias for convergence to\n                higher accuracy for some datasets where most of the input features are redundant.\n            norm_type: Type of normalization to perform for the model. Can be either\n                'group' or 'group'. 'group' is the default.\n            batch_momentum: Momentum in ghost batch normalization.\n            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n                overall batch size should be an integer multiple of virtual_batch_size.\n            num_groups: Number of groups used for group normalization.\n            epsilon: A small number for numerical stability of the entropy calculations.\n        \"\"\"\n        super(TabNetRegressor, self).__init__(**kwargs)\n\n        self.num_regressors = num_regressors\n\n        self.tabnet = TabNet(feature_columns=feature_columns,\n                             num_features=num_features,\n                             feature_dim=feature_dim,\n                             output_dim=output_dim,\n                             num_decision_steps=num_decision_steps,\n                             relaxation_factor=relaxation_factor,\n                             sparsity_coefficient=sparsity_coefficient,\n                             norm_type=norm_type,\n                             batch_momentum=batch_momentum,\n                             virtual_batch_size=virtual_batch_size,\n                             num_groups=num_groups,\n                             epsilon=epsilon,\n                             **kwargs)\n\n        self.regressor = tf.keras.layers.Dense(num_regressors, use_bias=False, name='regressor')\n\n    def call(self, inputs, training=None):\n        self.activations = self.tabnet(inputs, training=training)\n        out = self.regressor(self.activations)\n        return out\n\n    def summary(self, *super_args, **super_kwargs):\n        super().summary(*super_args, **super_kwargs)\n        self.tabnet.summary(*super_args, **super_kwargs)\n\n\n# Aliases\nTabNetClassification = TabNetClassifier\nTabNetRegression = TabNetRegressor\n\nclass StackedTabNet(tf.keras.Model):\n\n    def __init__(self, feature_columns,\n                 num_layers=1,\n                 feature_dim=64,\n                 output_dim=64,\n                 num_features=None,\n                 num_decision_steps=5,\n                 relaxation_factor=1.5,\n                 sparsity_coefficient=1e-5,\n                 norm_type='group',\n                 batch_momentum=0.98,\n                 virtual_batch_size=None,\n                 num_groups=2,\n                 epsilon=1e-5,\n                 **kwargs):\n        \"\"\"\n        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https:\/\/arxiv.org\/abs\/1908.07442)\n        Stacked variant of the TabNet model, which stacks multiple TabNets into a singular model.\n        # Hyper Parameter Tuning (Excerpt from the paper)\n        We consider datasets ranging from \u223c10K to \u223c10M training points, with varying degrees of fitting\n        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n        selection:\n            - Most datasets yield the best results for Nsteps \u2208 [3, 10]. Typically, larger datasets and\n            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n            overfitting and yield poor generalization.\n            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n            - An optimal choice of \u03b3 can have a major role on the overall performance. Typically a larger\n            Nsteps value favors for a larger \u03b3.\n            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n            much smaller than the batch size.\n            - Initially large learning rate is important, which should be gradually decayed until convergence.\n        Args:\n            feature_columns: The Tensorflow feature columns for the dataset.\n            num_layers: Number of TabNets to stack together.\n            feature_dim (N_a): Dimensionality of the hidden representation in feature\n                transformation block. Each layer first maps the representation to a\n                2*feature_dim-dimensional output and half of it is used to determine the\n                nonlinearity of the GLU activation where the other half is used as an\n                input to GLU, and eventually feature_dim-dimensional output is\n                transferred to the next layer. Can be either a single int, or a list of\n                integers. If a list, must be of same length as the number of layers.\n            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n                later mapped to the final classification or regression output.\n                Can be either a single int, or a list of\n                integers. If a list, must be of same length as the number of layers.\n            num_features: The number of input features (i.e the number of columns for\n                tabular data assuming each feature is represented with 1 dimension).\n            num_decision_steps(N_steps): Number of sequential decision steps.\n            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n                feature at different decision steps. When it is 1, a feature is enforced\n                to be used only at one decision step and as it increases, more\n                flexibility is provided to use a feature at multiple decision steps.\n            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n                Sparsity may provide a favorable inductive bias for convergence to\n                higher accuracy for some datasets where most of the input features are redundant.\n            norm_type: Type of normalization to perform for the model. Can be either\n                'batch' or 'group'. 'group' is the default.\n            batch_momentum: Momentum in ghost batch normalization.\n            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n                overall batch size should be an integer multiple of virtual_batch_size.\n            num_groups: Number of groups used for group normalization.\n            epsilon: A small number for numerical stability of the entropy calculations.\n        \"\"\"\n        super(StackedTabNet, self).__init__(**kwargs)\n\n        if num_layers < 1:\n            raise ValueError(\"`num_layers` cannot be less than 1\")\n\n        if type(feature_dim) not in [list, tuple]:\n            feature_dim = [feature_dim] * num_layers\n\n        if type(output_dim) not in [list, tuple]:\n            output_dim = [output_dim] * num_layers\n\n        if len(feature_dim) != num_layers:\n            raise ValueError(\"`feature_dim` must be a list of length `num_layers`\")\n\n        if len(output_dim) != num_layers:\n            raise ValueError(\"`output_dim` must be a list of length `num_layers`\")\n\n        self.num_layers = num_layers\n\n        layers = []\n        layers.append(TabNet(feature_columns=feature_columns,\n                             num_features=num_features,\n                             feature_dim=feature_dim[0],\n                             output_dim=output_dim[0],\n                             num_decision_steps=num_decision_steps,\n                             relaxation_factor=relaxation_factor,\n                             sparsity_coefficient=sparsity_coefficient,\n                             norm_type=norm_type,\n                             batch_momentum=batch_momentum,\n                             virtual_batch_size=virtual_batch_size,\n                             num_groups=num_groups,\n                             epsilon=epsilon))\n\n        for layer_idx in range(1, num_layers):\n            layers.append(TabNet(feature_columns=None,\n                                 num_features=output_dim[layer_idx - 1],\n                                 feature_dim=feature_dim[layer_idx],\n                                 output_dim=output_dim[layer_idx],\n                                 num_decision_steps=num_decision_steps,\n                                 relaxation_factor=relaxation_factor,\n                                 sparsity_coefficient=sparsity_coefficient,\n                                 norm_type=norm_type,\n                                 batch_momentum=batch_momentum,\n                                 virtual_batch_size=virtual_batch_size,\n                                 num_groups=num_groups,\n                                 epsilon=epsilon))\n\n        self.tabnet_layers = layers\n\n    def call(self, inputs, training=None):\n        x = self.tabnet_layers[0](inputs, training=training)\n\n        for layer_idx in range(1, self.num_layers):\n            x = self.tabnet_layers[layer_idx](x, training=training)\n\n        return x\n\n    @property\n    def tabnets(self):\n        return self.tabnet_layers\n\n    @property\n    def feature_selection_masks(self):\n        return [tabnet.feature_selection_masks\n                for tabnet in self.tabnet_layers]\n\n    @property\n    def aggregate_feature_selection_mask(self):\n        return [tabnet.aggregate_feature_selection_mask\n                for tabnet in self.tabnet_layers]\n\n\nclass StackedTabNetClassifier(tf.keras.Model):\n\n    def __init__(self, feature_columns,\n                 num_classes,\n                 num_layers=1,\n                 feature_dim=64,\n                 output_dim=64,\n                 num_features=None,\n                 num_decision_steps=5,\n                 relaxation_factor=1.5,\n                 sparsity_coefficient=1e-5,\n                 norm_type='group',\n                 batch_momentum=0.98,\n                 virtual_batch_size=None,\n                 num_groups=2,\n                 epsilon=1e-5,\n                 multi_label = False, \n                 **kwargs):\n        \"\"\"\n        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https:\/\/arxiv.org\/abs\/1908.07442)\n        Stacked variant of the TabNet model, which stacks multiple TabNets into a singular model.\n        # Hyper Parameter Tuning (Excerpt from the paper)\n        We consider datasets ranging from \u223c10K to \u223c10M training points, with varying degrees of fitting\n        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n        selection:\n            - Most datasets yield the best results for Nsteps \u2208 [3, 10]. Typically, larger datasets and\n            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n            overfitting and yield poor generalization.\n            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n            - An optimal choice of \u03b3 can have a major role on the overall performance. Typically a larger\n            Nsteps value favors for a larger \u03b3.\n            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n            much smaller than the batch size.\n            - Initially large learning rate is important, which should be gradually decayed until convergence.\n        Args:\n            feature_columns: The Tensorflow feature columns for the dataset.\n            num_classes: Number of classes.\n            num_layers: Number of TabNets to stack together.\n            feature_dim (N_a): Dimensionality of the hidden representation in feature\n                transformation block. Each layer first maps the representation to a\n                2*feature_dim-dimensional output and half of it is used to determine the\n                nonlinearity of the GLU activation where the other half is used as an\n                input to GLU, and eventually feature_dim-dimensional output is\n                transferred to the next layer. Can be either a single int, or a list of\n                integers. If a list, must be of same length as the number of layers.\n            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n                later mapped to the final classification or regression output.\n                Can be either a single int, or a list of\n                integers. If a list, must be of same length as the number of layers.\n            num_features: The number of input features (i.e the number of columns for\n                tabular data assuming each feature is represented with 1 dimension).\n            num_decision_steps(N_steps): Number of sequential decision steps.\n            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n                feature at different decision steps. When it is 1, a feature is enforced\n                to be used only at one decision step and as it increases, more\n                flexibility is provided to use a feature at multiple decision steps.\n            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n                Sparsity may provide a favorable inductive bias for convergence to\n                higher accuracy for some datasets where most of the input features are redundant.\n            norm_type: Type of normalization to perform for the model. Can be either\n                'batch' or 'group'. 'group' is the default.\n            batch_momentum: Momentum in ghost batch normalization.\n            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n                overall batch size should be an integer multiple of virtual_batch_size.\n            num_groups: Number of groups used for group normalization.\n            epsilon: A small number for numerical stability of the entropy calculations.\n        \"\"\"\n        super(StackedTabNetClassifier, self).__init__(**kwargs)\n\n        self.num_classes = num_classes\n\n        self.stacked_tabnet = StackedTabNet(feature_columns=feature_columns,\n                                            num_layers=num_layers,\n                                            feature_dim=feature_dim,\n                                            output_dim=output_dim,\n                                            num_features=num_features,\n                                            num_decision_steps=num_decision_steps,\n                                            relaxation_factor=relaxation_factor,\n                                            sparsity_coefficient=sparsity_coefficient,\n                                            norm_type=norm_type,\n                                            batch_momentum=batch_momentum,\n                                            virtual_batch_size=virtual_batch_size,\n                                            num_groups=num_groups,\n                                            epsilon=epsilon)\n        if multi_label:\n            \n            self.clf = tf.keras.layers.Dense(num_classes, activation='sigmoid', use_bias=False)\n        \n        else:\n            \n            self.clf = tf.keras.layers.Dense(num_classes, activation='softmax', use_bias=False)\n\n    def call(self, inputs, training=None):\n        self.activations = self.stacked_tabnet(inputs, training=training)\n        out = self.clf(self.activations)\n\n        return out\n\n\nclass StackedTabNetRegressor(tf.keras.Model):\n\n    def __init__(self, feature_columns,\n                 num_regressors,\n                 num_layers=1,\n                 feature_dim=64,\n                 output_dim=64,\n                 num_features=None,\n                 num_decision_steps=5,\n                 relaxation_factor=1.5,\n                 sparsity_coefficient=1e-5,\n                 norm_type='group',\n                 batch_momentum=0.98,\n                 virtual_batch_size=None,\n                 num_groups=2,\n                 epsilon=1e-5,\n                 **kwargs):\n        \"\"\"\n        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https:\/\/arxiv.org\/abs\/1908.07442)\n        Stacked variant of the TabNet model, which stacks multiple TabNets into a singular model.\n        # Hyper Parameter Tuning (Excerpt from the paper)\n        We consider datasets ranging from \u223c10K to \u223c10M training points, with varying degrees of fitting\n        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n        selection:\n            - Most datasets yield the best results for Nsteps \u2208 [3, 10]. Typically, larger datasets and\n            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n            overfitting and yield poor generalization.\n            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n            - An optimal choice of \u03b3 can have a major role on the overall performance. Typically a larger\n            Nsteps value favors for a larger \u03b3.\n            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n            much smaller than the batch size.\n            - Initially large learning rate is important, which should be gradually decayed until convergence.\n        Args:\n            feature_columns: The Tensorflow feature columns for the dataset.\n            num_regressors: Number of regressors.\n            num_layers: Number of TabNets to stack together.\n            feature_dim (N_a): Dimensionality of the hidden representation in feature\n                transformation block. Each layer first maps the representation to a\n                2*feature_dim-dimensional output and half of it is used to determine the\n                nonlinearity of the GLU activation where the other half is used as an\n                input to GLU, and eventually feature_dim-dimensional output is\n                transferred to the next layer. Can be either a single int, or a list of\n                integers. If a list, must be of same length as the number of layers.\n            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n                later mapped to the final classification or regression output.\n                Can be either a single int, or a list of\n                integers. If a list, must be of same length as the number of layers.\n            num_features: The number of input features (i.e the number of columns for\n                tabular data assuming each feature is represented with 1 dimension).\n            num_decision_steps(N_steps): Number of sequential decision steps.\n            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n                feature at different decision steps. When it is 1, a feature is enforced\n                to be used only at one decision step and as it increases, more\n                flexibility is provided to use a feature at multiple decision steps.\n            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n                Sparsity may provide a favorable inductive bias for convergence to\n                higher accuracy for some datasets where most of the input features are redundant.\n            norm_type: Type of normalization to perform for the model. Can be either\n                'batch' or 'group'. 'group' is the default.\n            batch_momentum: Momentum in ghost batch normalization.\n            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n                overall batch size should be an integer multiple of virtual_batch_size.\n            num_groups: Number of groups used for group normalization.\n            epsilon: A small number for numerical stability of the entropy calculations.\n        \"\"\"\n        super(StackedTabNetRegressor, self).__init__(**kwargs)\n\n        self.num_regressors = num_regressors\n\n        self.stacked_tabnet = StackedTabNet(feature_columns=feature_columns,\n                                            num_layers=num_layers,\n                                            feature_dim=feature_dim,\n                                            output_dim=output_dim,\n                                            num_features=num_features,\n                                            num_decision_steps=num_decision_steps,\n                                            relaxation_factor=relaxation_factor,\n                                            sparsity_coefficient=sparsity_coefficient,\n                                            norm_type=norm_type,\n                                            batch_momentum=batch_momentum,\n                                            virtual_batch_size=virtual_batch_size,\n                                            num_groups=num_groups,\n                                            epsilon=epsilon)\n\n        self.regressor = tf.keras.layers.Dense(num_regressors, use_bias=False)\n\n    def call(self, inputs, training=None):\n        self.activations = self.tabnet(inputs, training=training)\n        out = self.regressor(self.activations)\n        return outl","e8ce7d05":"# Meta-Variables\n\nFINAL_VERSION=True # To run final version or bootcamp version\n\nDO_RANKGAUSS = False # Whether to normalize the data using Rank Gauss \nVAR_THRE = 0.6 # Threeshold of variance\n\nDO_PCA = True # Whether to run PCA to add extra features\nNCOMPO_GEN_PCA = 30 # Number of components for PCA for genes\nNCOMPO_CEL_PCA = 20 # Number of components for PCA for cells\n\nNUM_FEATURES = 875 + NCOMPO_GEN_PCA + NCOMPO_CEL_PCA\n\nN_SPLITS = 10 # Number of splits for k-fold\nN_STARTS = 1 # Number of seeds to use for TabNet","ba7257c4":"train_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntrain_targets = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ntest_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')","8c6269c9":"# Prepare data\ndata_all = pd.concat([train_features, test_features],\n                     ignore_index = True)\ncols_numeric = [feat for feat in list(data_all.columns)\n                if feat not in [\"sig_id\", \"cp_type\", \"cp_time\", \"cp_dose\"]]\nmask = (data_all[cols_numeric].var() >= VAR_THRE).values\ntmp = data_all[cols_numeric].loc[:, mask]\n\ndata_all = pd.concat([data_all[[\"sig_id\", \"cp_type\", \"cp_time\", \"cp_dose\"]], tmp],\n                     axis = 1)\ncols_numeric = [feat for feat in list(data_all.columns)\n                if feat not in [\"sig_id\", \"cp_type\", \"cp_time\", \"cp_dose\"]]","3ec9a0f2":"# Using a Rank Gauss Process for scaling has been shown\n# to be quite efficient in other notebooks compared\n# to more standard scaling\n\nif DO_RANKGAUSS:\n\n    scaler = GaussRankScaler()\n    data_all[cols_numeric] = scaler.fit_transform(data_all[cols_numeric])","ae711e60":"# As in the bootcamp, we use a PCA to add extra-features\n# We just separate the genes and cells components\n\nif DO_PCA:\n\n    GENES = [col for col in data_all.columns if col.startswith(\"g-\")]\n    CELLS = [col for col in data_all.columns if col.startswith(\"c-\")]\n\n    pca_genes = PCA(n_components = NCOMPO_GEN_PCA,\n                    random_state = 1010).fit_transform(data_all[GENES])\n    pca_cells = PCA(n_components = NCOMPO_CEL_PCA,\n                    random_state = 1010).fit_transform(data_all[CELLS])\n\n    pca_genes = pd.DataFrame(pca_genes, columns = [f\"pca_g-{i}\" for i in range(NCOMPO_GEN_PCA)])\n    pca_cells = pd.DataFrame(pca_cells, columns = [f\"pca_c-{i}\" for i in range(NCOMPO_CEL_PCA)])\n    data_all = pd.concat([data_all, pca_genes, pca_cells], axis = 1)","4d166d69":"data_all = pd.get_dummies(data_all, columns = [\"cp_time\", \"cp_dose\"])","9080192a":"GENES = [col for col in data_all.columns if col.startswith(\"g-\")]\nCELLS = [col for col in data_all.columns if col.startswith(\"c-\")]\n\nfor stats in tqdm([\"sum\", \"mean\", \"std\", \"kurt\", \"skew\"]):\n    data_all[\"g_\" + stats] = getattr(data_all[GENES], stats)(axis = 1)\n    data_all[\"c_\" + stats] = getattr(data_all[CELLS], stats)(axis = 1)    \n    data_all[\"gc_\" + stats] = getattr(data_all[GENES + CELLS], stats)(axis = 1)","f9144abe":"# Final steps: remove id, separe train and test and \n# convert to numpy\n\n# Drop features\n\nfeatures_to_drop = [\"sig_id\", \"cp_type\"]\ntry:\n    data_all.drop(features_to_drop, axis = 1, inplace = True)\nexcept:\n    pass\ntry:\n    train_targets.drop(\"sig_id\", axis = 1, inplace = True)\nexcept:\n    pass\n\n# Separe train and test\n\ntrain_df = data_all[: train_features.shape[0]]\ntrain_df.reset_index(drop = True, inplace = True)\n\ntest_df = data_all[train_df.shape[0]: ]\ntest_df.reset_index(drop = True, inplace = True)\n\n# Convert to numpy\n\nX = train_df.values\nX_final = test_df.values\n\ny = train_targets.values","86f4e2fd":"if FINAL_VERSION: \n\n    def l1_model(input_shape, no_classes, lr):\n        inputs = tf.keras.Input(shape=input_shape)\n        x = layers.Dense(128, activation='sigmoid')(inputs)\n        x = layers.BatchNormalization()(x)\n        x = layers.Dropout(0.2)(x)\n        outputs = layers.Dense(no_classes, activation='sigmoid')(x)\n        model = tf.keras.Model(inputs, outputs)\n        model.compile(loss='binary_crossentropy',\n                      optimizer=tf.keras.optimizers.Adam(learning_rate = lr),\n                      metrics=['binary_crossentropy'])\n        return model\n\n    # Store losses and models\n    losses_NN=[]\n    l1_models=[]\n\n    kf = KFold(n_splits=N_SPLITS)\n    tf.random.set_seed(1010)\n    np.random.seed(1010)\n\n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n        control_vehicle_mask = X_train[:,-2] == 0\n        X_train = X_train[~control_vehicle_mask,:]\n        y_train = y_train[~control_vehicle_mask]\n\n        nnclf = l1_model((NUM_FEATURES,),206,0.0005)\n        hist = nnclf.fit(X_train, y_train, batch_size=512, epochs=50,\n                         validation_data=(X_test, y_test), verbose=0)\n\n        # Plot loss evolution\n        # plot_hist(hist, last=20)\n\n        preds = nnclf.predict(X_test) # list of preds per class\n\n        control_mask = X_test[:,-2]==0\n        preds[control_mask] = 0\n\n        loss = log_loss(np.ravel(y_test), np.ravel(preds))\n        print('Loss: '+str(loss))\n\n        # Add loss and model\n        losses_NN.append(loss)\n        l1_models.append(nnclf)\n\n    print('Average Loss: '+str(np.average(losses_NN))) \n    \n# Bootcamp Average Loss: 0.016532726385804736\n# New Average Loss: 0.016319222364238613","2e626adc":"if FINAL_VERSION: \n\n    def l2_model(input_shape, no_classes, lr):\n        inputs = tf.keras.Input(shape=input_shape)\n        x = layers.Dense(128, activation='sigmoid')(inputs)\n        x = layers.BatchNormalization()(x)\n        x = layers.Dropout(0.2)(x)\n        x = layers.Dense(128, activation='sigmoid')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Dropout(0.2)(x)\n        x = layers.Dense(128, activation='sigmoid')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Dropout(0.2)(x)\n        outputs = layers.Dense(no_classes, activation='sigmoid')(x)\n        model = tf.keras.Model(inputs, outputs)\n        model.compile(loss='binary_crossentropy',\n                      optimizer=tf.keras.optimizers.Adam(learning_rate = lr),\n                      metrics=['binary_crossentropy'])\n        return model\n\n    # Store losses and models\n    losses_NN=[]\n    l2_models=[]\n\n    kf = KFold(n_splits=N_SPLITS)\n    tf.random.set_seed(1010)\n    np.random.seed(1010)\n\n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n        control_vehicle_mask = X_train[:,-2] == 0\n        X_train = X_train[~control_vehicle_mask,:]\n        y_train = y_train[~control_vehicle_mask]\n\n        nnclf = l2_model((NUM_FEATURES,),206,0.0005)\n        hist = nnclf.fit(X_train, y_train, batch_size=512, epochs=50,\n                         validation_data=(X_test, y_test), verbose=0)\n\n        # Plot loss evolution\n        # plot_hist(hist, last = 20)\n\n        preds = nnclf.predict(X_test) # list of preds per class\n\n        control_mask = X_test[:,-2]==0\n        preds[control_mask] = 0\n\n        loss = log_loss(np.ravel(y_test), np.ravel(preds))\n        print('Loss: '+str(loss))\n\n        # Add loss and model\n        losses_NN.append(loss)\n        l2_models.append(nnclf)\n\n    print('Average Loss: '+str(np.average(losses_NN))) \n    \n# Bootcamp Average Loss: 0.016114999312747662\n# New Average Loss: 0.0.016157238843075565","9f6ad703":"\n\ndef l3_model(input_shape, no_classes, lr):\n    inputs = tf.keras.Input(shape=input_shape)\n    x = layers.Dense(128, activation='sigmoid')(inputs)\n    x = layers.BatchNormalization()(x)\n    b_1 = layers.Dropout(0.2)(x)\n    x = layers.Dense(128, activation='sigmoid')(b_1)\n    x = layers.BatchNormalization()(x)\n    b_2 = layers.Dropout(0.2)(x)\n    x = layers.Dense(128, activation='sigmoid')(b_2)\n    x = layers.BatchNormalization()(x)\n    b_3 = layers.Dropout(0.2)(x)\n    tot_op = tf.keras.layers.add([b_1, b_2, b_3])\n    outputs = layers.Dense(no_classes, activation='sigmoid')(tot_op)\n    model = tf.keras.Model(inputs, outputs)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=tf.keras.optimizers.Adam(learning_rate = lr),\n                  metrics=['binary_crossentropy'])\n    return model\n\n\n# Store losses and models\nlosses_NN=[]\nl3_models=[]\n\nkf = KFold(n_splits=N_SPLITS)\ntf.random.set_seed(1010)\nnp.random.seed(1010)\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    control_vehicle_mask = X_train[:,-2] == 0\n    X_train = X_train[~control_vehicle_mask,:]\n    y_train = y_train[~control_vehicle_mask]\n\n    nnclf = l3_model((NUM_FEATURES,),206,0.0005)\n    hist = nnclf.fit(X_train, y_train, batch_size=512, epochs=50,\n                     validation_data=(X_test, y_test), verbose=0)\n    \n    # Plot loss evolution\n    #plot_hist(hist, last=20)\n\n    preds = nnclf.predict(X_test) # list of preds per class\n\n    control_mask = X_test[:,-2]==0\n    preds[control_mask] = 0\n\n    loss = log_loss(np.ravel(y_test), np.ravel(preds))\n    print('Loss: '+str(loss))\n    \n    # Add loss and model\n    losses_NN.append(loss)\n    l3_models.append(nnclf)\n\nprint('Average Loss: '+str(np.average(losses_NN))) \n\n# Bootcamp Average Loss: 0.015595737874931456\n# New Average Loss: 0.015815207593072523","a403e515":"res = train_targets.copy()\ntabnet_pred=sample_submission.copy()\n\ntabnet_pred.loc[:, train_targets.columns] = 0\nres.loc[:, train_targets.columns] = 0\n\nfor seed in range(N_STARTS):\n    \n    for n, (tr, te) in enumerate(MultilabelStratifiedKFold(n_splits = N_SPLITS,\n                                                           random_state = 12,\n                                                           shuffle = True).split(train_targets, train_targets)):\n        \n        start_time = time()\n        x_tr, x_val = X[tr], X[te]\n        y_tr, y_val = train_targets.astype(float).values[tr], train_targets.astype(float).values[te]\n        x_tt = X_final.copy()\n        \n        model = StackedTabNetClassifier(feature_columns = None, num_classes = 206, num_layers = 2, \n                                        feature_dim = 128, output_dim = 64, num_features = NUM_FEATURES,\n                                        num_decision_steps = 1, relaxation_factor = 1.5,\n                                        sparsity_coefficient = 1e-5, batch_momentum = 0.98,\n                                        virtual_batch_size = None, norm_type = 'group',\n                                        num_groups = -1, multi_label = True)\n\n        model.compile(optimizer = tfa.optimizers.Lookahead(tf.optimizers.Adam(1e-3), sync_period = 10), \n                      loss = 'binary_crossentropy')\n        \n        rlr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 3, verbose = 0, \n                                min_delta = 1e-4, mode = 'min')\n        ckp = ModelCheckpoint(f'TabNet_{seed}_{n}.hdf5', monitor = 'val_loss', verbose = 0, \n                              save_best_only = True, save_weights_only = True, mode = 'min')\n        es = EarlyStopping(monitor = 'val_loss', min_delta = 1e-4, patience = 10, mode = 'min', \n                           baseline = None, restore_best_weights = True, verbose = 0)\n        \n        model.fit(x_tr, y_tr, validation_data = (x_val, y_val), epochs = 100, batch_size = 128,\n                  callbacks = [rlr, ckp, es], verbose = 0)\n        \n        model.load_weights(f'TabNet_{seed}_{n}.hdf5')\n        sample_submission.loc[:, train_targets.columns] += model.predict(x_tt, batch_size = x_tt.shape[0]) \/ (N_SPLITS * N_STARTS)\n        fold_pred = model.predict(x_val, batch_size = x_val.shape[0])\n        res.loc[te, train_targets.columns] += fold_pred \/ N_STARTS\n        fold_score = log_loss_metric(train_targets.loc[te].values, fold_pred)\n        print(f'[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] TabNet: Seed {seed}, Fold {n}:', fold_score)\n        \n        K.clear_session()\n        del model\n        x = gc.collect()","b3e3cbff":"# Final model from the bootcamp submission\n\nif not(FINAL_VERSION): \n    \n\n    # Prediction with most accurate model\n    prediction = pd.DataFrame(l3_models[4].predict(X_final),\n                                  columns = sample_submission.columns[1:])\n\n    # Adding back the sig_id column for submission\n    prediction.insert(0, sample_submission.columns[0],\n                         test_features[sample_submission.columns[0]])\n\n    # Save final output\n    prediction.to_csv('submission.csv', index=False)\n","452d057b":"# Final Submission, adding more ensembling,models and \n# data engineering\n\n\n# Ensembling weights\nl1_weight = 1\nl2_weight = 3\nl3_weight = 10\ntabnet_weight = 5\n\ntotal_weights = l1_weight+l2_weight+l3_weight+tabnet_weight\n\nif FINAL_VERSION: \n\n    # Ensembling for Final output\n\n    prediction = pd.DataFrame(0,index=sample_submission.index,\n                              columns=sample_submission.columns[1:])\n    \n    # Models 1\n    for model in l1_models:\n\n        # Using individual models predictions\n        pred_model = pd.DataFrame(model.predict(X_final),\n                                  columns = sample_submission.columns[1:])\n\n        # Summing weighted individual model predictions\n        prediction+=pred_model*l1_weight\/(total_weights*N_SPLITS)\n        \n    # Models 2\n    for model in l2_models:\n\n        # Using individual models predictions\n        pred_model = pd.DataFrame(model.predict(X_final),\n                                  columns = sample_submission.columns[1:])\n\n        # Summing weighted individual model predictions\n        prediction+=pred_model*l2_weight\/(total_weights*N_SPLITS)\n        \n    # Models 3\n    for model in l3_models:\n        \n        # Using individual models predictions\n        pred_model = pd.DataFrame(model.predict(X_final),\n                                  columns = sample_submission.columns[1:])\n\n        # Summing weighted individual model predictions\n        prediction+=pred_model*l3_weight\/(total_weights*N_SPLITS)\n\n        \n    # Models 4: Tabnet\n    \n    del tabnet_pred['sig_id']\n    prediction+=tabnet_pred*(tabnet_weight\/total_weights)   \n        \n    \n    # Adding back the sig_id column for submission\n    prediction.insert(0, sample_submission.columns[0],\n                         test_features[sample_submission.columns[0]])\n\n\n    # Save final output\n    prediction.to_csv('submission.csv', index=False)","d8cd0269":"## I - Preliminary Steps","20b50653":"Compared to bootcamp version, this submission adds:\n* More feature engineering (PCA, RankGauss Scaling, Statistical features...)\n* New models (TabNet)\n* Ensembling of bootcamp and new models\n\nNotebook used for inspiration:\n* https:\/\/www.kaggle.com\/gogo827jz\/moa-stacked-tabnet-baseline-tensorflow-2-0?scriptVersionId=42840807\n* https:\/\/www.kaggle.com\/optimo\/tabnetregressor-2-0-train-infer \n* https:\/\/www.kaggle.com\/liuhdme\/moa-competition\/data\n* https:\/\/www.kaggle.com\/kushal1506\/moa-pytorch-0-01859-rankgauss-pca-nn\/data?select=train_targets_scored.csv\n\nNote that some of the code of modules had to be copied in the notebook as the competition\ndidn't allow to access internet.","dc569dff":"#### One-hot encoding","8567c4ff":"## TabNet","846e7b26":"### Tabnet","70441435":"### Iterative Stratification","14787c31":"### Import data","1e7cfc38":"#### Scaling","c0fbe024":"#### PCA","3c66785a":"## II - Preprocessing and Feature Engineering\n","4361996f":"# Team 4 Improved Submission","d357ef10":"### Import modules","4e03a50d":"#### Adding statistical features","3f7a69b8":"### Auxiliary functions","4cf7b490":"#### Models"}}