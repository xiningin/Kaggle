{"cell_type":{"022e5e07":"code","a64b0598":"code","5b68de75":"code","93ed37e3":"code","547490c6":"code","ef36f6e4":"code","720c0b36":"code","ff9ea592":"code","4d6d5495":"code","935c8e4d":"code","6634f3d1":"code","e6a0a727":"code","7c513aa7":"code","7fcf55a5":"code","873de06f":"code","60ba217f":"code","5bfac597":"code","17c97143":"code","96d30cff":"code","2c007edd":"code","49c24ba5":"code","37cb6c7a":"code","7b665c5e":"code","66e173b8":"code","7b998017":"code","874ce3f4":"code","b447ca28":"code","64f46d62":"code","ee2bc472":"code","076b1f89":"code","ae83dcd4":"code","dc8e057f":"code","99d98124":"code","f7b9c545":"markdown","8cfe043a":"markdown","2975ecfe":"markdown","6578c998":"markdown","eae5b68d":"markdown","9e57c6ad":"markdown","30747928":"markdown","338309c1":"markdown","935aecb7":"markdown","7b184649":"markdown","0cdb6c02":"markdown","dbda5fa0":"markdown","070927b9":"markdown","03f234ee":"markdown","ffbd633a":"markdown","4e9e81a8":"markdown","71da5cb5":"markdown","9811d758":"markdown","78a5cb82":"markdown","d45f0c6a":"markdown","28ed799f":"markdown","515a7b9f":"markdown","e0a66cbb":"markdown","174eb02f":"markdown","66c83ab8":"markdown","c92c9d49":"markdown","2dc5aea4":"markdown","faf90f61":"markdown"},"source":{"022e5e07":"# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n# data visualisation and manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\nimport missingno as msno\nimport pandas_profiling as pdp\n\n#configure\n# sets matplotlib to inline and displays graphs below the corressponding cell.\n%matplotlib inline  \nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid',color_codes=True)\n\n#import the necessary modelling algos.\n\n#classifiaction.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC,SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport xgboost as xgb\nimport lightgbm as lgbm\nimport catboost as cb\nfrom sklearn.ensemble import AdaBoostClassifier\n\n#model selection\nfrom sklearn.model_selection import train_test_split,cross_validate\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n#preprocessing\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\n\n#evaluation metrics\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error # for regression\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score  # for classification\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score","a64b0598":"df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf.head()","5b68de75":"df.shape","93ed37e3":"df.info()","547490c6":"df.isnull().sum()","ef36f6e4":"df.dropna(inplace = True)","720c0b36":"df.isnull().sum()","ff9ea592":"for i in ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']:\n    print(df[i].unique())","4d6d5495":"sns.barplot(x = 'stroke', y = 'smoking_status', data = df)","935c8e4d":"replace_values = {'Unknown': 'never smoked'}\n\ndf = df.replace({'smoking_status': replace_values})\ndf.head()","6634f3d1":"fig, axes = plt.subplots(nrows = 5, ncols = 2, figsize = (12, 20))\nsns.boxplot(x = 'avg_glucose_level', data = df, ax=axes[0][0])\nsns.boxplot(x = 'age', data = df, ax=axes[1][0])\nsns.boxplot(x = 'bmi', data = df, ax=axes[2][0])\nsns.boxplot(x = 'smoking_status', y = 'age', hue = 'stroke', data = df, ax=axes[3][0])\nsns.boxplot(x = 'hypertension',y = 'age', hue = 'stroke', data = df, ax=axes[3][1])\nsns.boxplot(x = 'heart_disease', y= 'age', hue = 'stroke', data = df, ax=axes[0][1])\nsns.boxplot(x = 'ever_married',y = 'age', hue = 'stroke', data = df, ax = axes[1][1])\nsns.boxplot(x = 'work_type',y = 'age',hue = 'stroke', data = df, ax = axes[2][1])\nsns.boxplot(x = 'Residence_type',y = 'age',hue = 'stroke', data = df, ax = axes[4][0])\nsns.boxplot(x = 'smoking_status',y = 'age', hue = 'stroke', data = df, ax = axes[4][1])","e6a0a727":"### Thanks for the function https:\/\/www.kaggle.com\/ankitak46\n\ndef remove_outliers(data):\n    arr=[]\n    #print(max(list(data)))\n    q1=np.percentile(data,25)\n    q3=np.percentile(data,75)\n    iqr=q3-q1\n    mi=q1-(1.5*iqr)\n    ma=q3+(1.5*iqr)\n    #print(mi,ma)\n    for i in list(data):\n        if i<mi:\n            i=mi\n            arr.append(i)\n        elif i>ma:\n            i=ma\n            arr.append(i)\n        else:\n            arr.append(i)\n    #print(max(arr))\n    return arr","7c513aa7":"df['bmi'] = remove_outliers(df['bmi'])\ndf['avg_glucose_level'] = remove_outliers(df['avg_glucose_level'])\nprint('Outliers successfully removed')","7fcf55a5":"fig, axes = plt.subplots(nrows = 2, ncols = 2, figsize = (10, 5))\nsns.boxplot(x = 'bmi', data = df, ax=axes[1][0])\nsns.boxplot(x = 'avg_glucose_level', data = df, ax=axes[0][0])","873de06f":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf['gender'] = le.fit_transform(df['gender'])\ndf['ever_married'] = le.fit_transform(df['ever_married'])\ndf = pd.get_dummies(df, columns = ['work_type'])\ndf = pd.get_dummies(df, columns = ['Residence_type'])\ndf = pd.get_dummies(df, columns = ['smoking_status'])\ndf = df.drop('id', axis = 1)\nprint('Encoding was successful ')","60ba217f":"X = df.drop('stroke', axis = 1)\ny = df.stroke","5bfac597":"### before oversampling\n\nsns.countplot(x = y, data = df)","17c97143":"from imblearn.over_sampling import SMOTE\n\nsm = SMOTE()\nX_res, y_res = sm.fit_resample(X, y)\n\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y==0)))\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_res.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_res.shape))\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_res==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_res==0)))","96d30cff":"### after oversampling\n\nsns.countplot(x = y_res, data = df)","2c007edd":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size = 0.2, random_state = 42)","49c24ba5":"dtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\ndtc_pred = dtc.predict(X_test)\nprint(confusion_matrix(dtc_pred, y_test))\nprint('-----')\nprint(classification_report(dtc_pred, y_test))","37cb6c7a":"rf = RandomForestClassifier()\nrf.fit(X_train, y_train)\nrf_pred = rf.predict(X_test)\nprint(confusion_matrix(rf_pred, y_test))\nprint('-----')\nprint(classification_report(rf_pred, y_test))","7b665c5e":"lr = LogisticRegression()\nlr.fit(X_train, y_train)\nlr_pred = lr.predict(X_test)\nprint(confusion_matrix(lr_pred, y_test))\nprint('-----')\nprint(classification_report(lr_pred, y_test))","66e173b8":"svc = SVC()\nsvc.fit(X_train, y_train)\nsvc_pred = svc.predict(X_test)\nprint(confusion_matrix(svc_pred, y_test))\nprint('-----')\nprint(classification_report(svc_pred, y_test))","7b998017":"knn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nknn_pred = knn.predict(X_test)\nprint(confusion_matrix(knn_pred, y_test))\nprint('-----')\nprint(classification_report(knn_pred, y_test))","874ce3f4":"cross_valid_scores = {}","b447ca28":"%%time\nparameters = {\n    \"max_depth\": [3, 5, 7, 9, 11, 13],\n}\n\nmodel_dtc = DecisionTreeClassifier(\n    random_state=42,\n    class_weight='balanced',\n)\n\nmodel_dtc = GridSearchCV(\n    model_dtc, \n    parameters, \n    cv=5,\n)\n\nmodel_dtc.fit(X_train, y_train)\nmodel_dtc_pred = model_dtc.predict(X_test)\nprint(classification_report(model_dtc_pred, y_test))\n\nprint('-----')\nprint(f'Best parameters {model_dtc.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: ' + \\\n    f'{model_dtc.best_score_:.3f}'\n)\ncross_valid_scores['desicion_tree'] = model_dtc.best_score_\nprint('-----')\n","64f46d62":"%%time\nparameters = {\n    \"n_estimators\": [5, 10, 15, 20, 25], \n    \"max_depth\": [3, 5, 7, 9, 11, 13],\n}\n\nmodel_rf = RandomForestClassifier(\n    random_state=42,\n    class_weight='balanced',\n)\n\nmodel_rf = GridSearchCV(\n    model_rf, \n    parameters, \n    cv=5,\n)\n\nmodel_rf.fit(X_train, y_train)\nmodel_rf_pred = model_rf.predict(X_test)\nprint(classification_report(model_rf_pred, y_test))\n\nprint('-----')\nprint(f'Best parameters {model_rf.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: '+ \\\n    f'{model_rf.best_score_:.3f}'\n)\ncross_valid_scores['random_forest'] = model_rf.best_score_\nprint('-----')","ee2bc472":"%%time\nparameters = {\n    'max_depth': [3, 5, 7, 9], \n    'n_estimators': [5, 10, 15, 20, 25, 50, 100],\n    'learning_rate': [0.01, 0.05, 0.1]\n}\n\nmodel_xgb = xgb.XGBClassifier(\n    random_state=42, verbosity = 0\n)\n\nmodel_xgb = GridSearchCV(\n    model_xgb, \n    parameters, \n    cv=5,\n    scoring='accuracy',\n)\n\nmodel_xgb.fit(X_train, y_train)\nmodel_xgb_pred = model_xgb.predict(X_test)\nprint(classification_report(model_xgb_pred, y_test))\n\n\nprint('-----')\nprint(f'Best parameters {model_xgb.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: ' + \n    f'{model_xgb.best_score_:.3f}'\n)\ncross_valid_scores['xgboost'] = model_xgb.best_score_\nprint('-----')","076b1f89":"%%time\nparameters = {\n    'n_estimators': [5, 10, 15, 20, 25, 50, 100],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'num_leaves': [7, 15, 31],\n}\n\nmodel_lgbm = lgbm.LGBMClassifier(\n    random_state=42,\n    class_weight='balanced',\n)\n\nmodel_lgbm = GridSearchCV(\n    model_lgbm, \n    parameters, \n    cv=5)\n\nmodel_lgbm.fit(X_train, y_train)\nmodel_lgbm_pred = model_lgbm.predict(X_test)\nprint(classification_report(model_lgbm_pred, y_test))\n\nprint('-----')\nprint(f'Best parameters {model_lgbm.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: ' + \n    f'{model_lgbm.best_score_:.3f}'\n)\ncross_valid_scores['lightgbm'] = model_lgbm.best_score_\nprint('-----')","ae83dcd4":"%%time\nparameters = {\n    \"n_estimators\": [5, 10, 15, 20, 25, 50, 75, 100], \n    \"learning_rate\": [0.001, 0.01, 0.1, 1.],\n}\n\nmodel_adaboost = AdaBoostClassifier(\n    random_state=42,\n)\n\nmodel_adaboost = GridSearchCV(\n    model_adaboost, \n    parameters, \n    cv=5,\n    scoring='accuracy',\n)\n\nmodel_adaboost.fit(X_train, y_train)\nmodel_adaboost_pred = model_adaboost.predict(X_test)\nprint(classification_report(model_adaboost_pred, y_test))\n\nprint('-----')\nprint(f'Best parameters {model_adaboost.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: '+ \\\n    f'{model_adaboost.best_score_:.3f}'\n)\ncross_valid_scores['ada_boost'] = model_adaboost.best_score_\nprint('-----')","dc8e057f":"%%time\nparameters = {\n    \"C\": [0.001, 0.01, 0.1, 1.],\n    \"penalty\": [\"l1\", \"l2\"]\n}\n\nmodel_lr = LogisticRegression(\n    random_state=42,\n    class_weight=\"balanced\",\n    solver=\"liblinear\",\n)\n\nmodel_lr = GridSearchCV(\n    model_lr, \n    parameters, \n    cv=5,\n    scoring='accuracy',\n)\n\nmodel_lr.fit(X_train, y_train)\nmodel_lr_pred = model_lr.predict(X_test)\nprint(classification_report(model_lr_pred, y_test))\n\nprint('-----')\nprint(f'Best parameters {model_lr.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: ' + \n    f'{model_lr.best_score_:.3f}'\n)\ncross_valid_scores['logistic_regression'] = model_lr.best_score_\nprint('-----')","99d98124":"%%time\nparameters = {\n    \"weights\": [\"uniform\", \"distance\"],\n}\n\nmodel_k_neighbors = KNeighborsClassifier(\n)\n\nmodel_k_neighbors = GridSearchCV(\n    model_k_neighbors, \n    parameters, \n    cv=5,\n    scoring='accuracy',\n)\n\nmodel_k_neighbors.fit(X_train, y_train)\nmodel_k_neighbors_pred = model_k_neighbors.predict(X_test)\nprint(classification_report(model_k_neighbors_pred, y_test))\n\nprint('-----')\nprint(f'Best parameters {model_k_neighbors.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: ' + \n    f'{model_k_neighbors.best_score_:.3f}'\n)\ncross_valid_scores['k_neighbors'] = model_k_neighbors.best_score_\nprint('-----')","f7b9c545":"# EDA\n\n## BOXPLOT","8cfe043a":"Oversampling is successful","2975ecfe":"# Preprocessing","6578c998":"## LightGBM","eae5b68d":"## KNN","9e57c6ad":"# Modeling","30747928":"## Random Forest","338309c1":"# Data loading and overview","935aecb7":"## Logistic Regression","7b184649":"# Import Libs","0cdb6c02":"## XGBoost","dbda5fa0":"# Model","070927b9":"## Decision Tree","03f234ee":"# Thank for watching!","ffbd633a":"**UPD: 11.03.21**","4e9e81a8":"Good insights :)\n\nWe have oultiers in avg_glucose_level and bmi.","71da5cb5":"## Logistic Regression","9811d758":"### It's best F1 score","78a5cb82":"### Remove outliers","d45f0c6a":"## Decision Tree","28ed799f":"## KNN","515a7b9f":"## SVC","e0a66cbb":"## Random Forest","174eb02f":"'Unknown' and 'never smoker' has a low percentage of strokes in the sample. Let's combine them into one group: 'never smoker'.","66c83ab8":"## Adaboost","c92c9d49":"# If you liked my work then upvoted or write your opinion","2dc5aea4":"### We use sampling because there is an imbalance of the target feature","faf90f61":"# Grid Search"}}