{"cell_type":{"8fe2b4c4":"code","b6bd5b4d":"code","5c91a0f4":"code","c322f0c4":"code","9909552b":"code","2a1332d6":"code","5047966c":"code","5061e117":"code","f5ba947c":"code","1709bfa6":"code","4797ad3a":"code","afd6ba5c":"code","64ec9124":"code","284f5528":"code","dd2c0abf":"code","34744508":"code","713b9d61":"code","97e7a596":"code","96576011":"code","50e0e07a":"code","e0f14281":"code","e1839c9d":"code","c693b698":"code","14ac249f":"code","f4e032a3":"code","d7c6ee57":"markdown","ae6a5ca5":"markdown","197a94bd":"markdown","7a32485f":"markdown","e022eb13":"markdown","58ba2464":"markdown","c6018114":"markdown","69f81804":"markdown","575cee61":"markdown","edd86bed":"markdown","efbaea74":"markdown","2f05662b":"markdown","6954dfb3":"markdown","623688a9":"markdown","3c8967d6":"markdown","8a5a51cd":"markdown","8bbaa595":"markdown","9f08d210":"markdown","13ab0c86":"markdown","7de28ec2":"markdown","0aa7a887":"markdown","bc44a894":"markdown","da36c8bc":"markdown"},"source":{"8fe2b4c4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport seaborn as sns\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom math import sqrt\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.metrics import make_scorer, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold, cross_val_score\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b6bd5b4d":"train_df = pd.read_csv('\/kaggle\/input\/cap-4611-2021-fall-assignment-3\/train.csv')\neval_df = pd.read_csv('\/kaggle\/input\/cap-4611-2021-fall-assignment-3\/eval.csv')","5c91a0f4":"sns.histplot(train_df[\"Eat\"])","c322f0c4":"train_columns_to_drop = [\"Eat\", \"pubchem_id\", \"id\"]\neval_columns_to_drop = [\"id\", \"pubchem_id\"]\n\ny = train_df[\"Eat\"]\nx = train_df.drop(train_columns_to_drop, axis=1)\nx_eval = eval_df.drop(eval_columns_to_drop, axis=1)","9909552b":"# Split data into train and test set\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=321)","2a1332d6":"x.head(1)","5047966c":"x_eval.head(1)","5061e117":"print(train_df.isna().sum())","f5ba947c":"print(eval_df.isna().sum())","1709bfa6":"x_train = x_train.to_numpy()\ny_train = y_train.to_numpy()\nx_test = x_test.to_numpy()\ny_test = y_test.to_numpy()","4797ad3a":"# Create mean squared error scorer\nmse = make_scorer(mean_squared_error)","afd6ba5c":"# hyperparameters to adjust\nbatch_size = 100\nepochs = 50\nvalidation_split = 0.1\nactivation = \"relu\"\n\ndef create_model_1():\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Dense(300, input_shape=(1275, ), activation=activation))\n    model.add(tf.keras.layers.Dense(1))\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=.003), loss=tf.keras.losses.MeanSquaredError(),\n                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    return model\n\nmodel = KerasRegressor(build_fn=create_model_1, epochs=epochs, batch_size=batch_size, verbose=0)\nscores = cross_val_score(model, x_train, y_train, cv=10, scoring=mse, error_score='raise')","64ec9124":"pd.Series(scores).describe()\nsns.histplot(scores)","284f5528":"history = model.fit(x_train, y_train, batch_size=batch_size,\n                    epochs=epochs, shuffle=True, validation_split=validation_split, verbose=0)\npred = model.predict(x_test)\nrmse = mean_squared_error(y_test, pred, squared=False)\nprint(\"RSME: \" + str(rmse))","dd2c0abf":"pd.DataFrame(history.history).plot()\nplt.grid(True)\nplt.gca().set_ylim(0,1)\nplt.show()","34744508":"# Hyperparameters\nbatch_size = 50\nepochs = 50\nvalidation_split = 0.1\nactivation = \"elu\"\n\ndef create_model_2():\n    earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error', patience=10)\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Dense(100, activation=activation))\n    model.add(tf.keras.layers.Dense(100, activation=activation))\n    model.add(tf.keras.layers.Dense(1))\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=.003), loss=tf.keras.losses.MeanSquaredError(),\n                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    return model\n\nmodel = KerasRegressor(build_fn=create_model_2, epochs=epochs, batch_size=batch_size, verbose=0)\nscores = cross_val_score(model, x_train, y_train, cv=10, scoring=mse, error_score='raise')\n","713b9d61":"pd.Series(scores).describe()\nsns.histplot(scores)","97e7a596":"history = model.fit(x_train, y_train, batch_size=batch_size,\n                    epochs=epochs, shuffle=True, validation_split=validation_split, verbose=0)\npred = model.predict(x_test)\nrmse = mean_squared_error(y_test, pred, squared=False)\nprint(\"RSME: \" + str(rmse))","96576011":"pd.DataFrame(history.history).plot()\nplt.grid(True)\nplt.gca().set_ylim(0,1)\nplt.show()","50e0e07a":"def custom_loss(y_true, y_pred):\n    return mean_squared_error(y_true, y_pred, squared=False)\n\ndef create_model(nl=1, nn=256, activation=\"relu\", learning_rate=.0001):\n    earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error', patience=5)\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_root_mean_squared_error', factor=0.2,\n                              patience=5, min_lr=0.001)\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Dense(300, input_shape=(1275, ), activation=activation))\n    model.add(tf.keras.layers.BatchNormalization(axis=-1, momentum=0.92, epsilon=0.001, center=True, \n                                               scale=True, beta_initializer=\"zeros\", gamma_initializer=\"ones\",\n                                               moving_mean_initializer=\"zeros\", moving_variance_initializer=\"ones\"))\n    for i in range(nl):\n        model.add(tf.keras.layers.Dense(nn, activation=activation))\n    model.add(tf.keras.layers.Dense(1))\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss=tf.keras.losses.MeanSquaredError(),\n              metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    return model\n\nrmse = make_scorer(custom_loss)\nmodel = KerasRegressor(build_fn=create_model)\nparams = dict(batch_size=[100, 300], learning_rate=[.001, .0003, .0001],\n              activation=['relu', 'elu'], nl=[1,4,12], nn=[100, 200, 300])\nrandom_search = RandomizedSearchCV(model, params, cv=KFold(5))\nrandom_search_results = random_search.fit(x_train, y_train)\n\nprint(\"Best: \" + str(abs(random_search_results.best_score_)))\nprint(\"using \" + str(random_search_results.best_params_))","e0f14281":"# Hyperparameters were manually tweaked after RandomSearch to get a smooth learning curve\nbatch_size = 500\nepochs = 150\nvalidation_split = 0.1\n\ndef create_model_3():\n    # Early stopping and reduc\n    earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error', patience=10)\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_root_mean_squared_error', factor=0.2,\n                                                     patience=5, min_lr=0.0001)\n    layer = tf.keras.layers.Normalization()\n    layer.adapt(np.array(x_train.copy()))\n\n    model = tf.keras.models.Sequential()\n    model.add(layer)\n    model.add(tf.keras.layers.Dense(200, input_shape=(1275, ), activation=\"elu\"))\n    model.add(tf.keras.layers.Dense(200, activation=\"elu\"))\n    model.add(tf.keras.layers.Dense(200, activation=\"elu\"))\n    model.add(tf.keras.layers.Dense(200, activation=\"elu\"))\n    model.add(tf.keras.layers.Dense(200, activation=\"elu\"))\n    model.add(tf.keras.layers.Dense(200, activation=\"elu\"))\n    model.add(tf.keras.layers.Dense(200, activation=\"elu\"))\n    model.add(tf.keras.layers.Dense(200, activation=\"elu\"))\n    model.add(tf.keras.layers.Dense(200, activation=\"elu\"))\n    model.add(tf.keras.layers.Dense(200, activation=\"elu\"))\n    model.add(tf.keras.layers.Dense(200, activation=\"elu\"))\n    model.add(tf.keras.layers.Dense(200, activation=\"elu\"))\n    model.add(tf.keras.layers.Dense(200, activation=\"elu\"))\n    model.add(tf.keras.layers.Dense(1))\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=.00009), loss=tf.keras.losses.MeanSquaredError(),\n                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    return model\n    \nmodel = KerasRegressor(build_fn=create_model_3, epochs=epochs, batch_size=batch_size, verbose=0)\nscores = cross_val_score(model, x_train, y_train, cv=10, scoring=mse, error_score='raise')","e1839c9d":"pd.Series(scores).describe()\nsns.histplot(scores)","c693b698":"history = model.fit(x_train, y_train, batch_size=batch_size,\n                    epochs=epochs, shuffle=True, validation_split=validation_split, verbose=0)\npred = model.predict(x_test)\nrmse = mean_squared_error(y_test, pred, squared=False)\nprint(\"RSME: \" + str(rmse))","14ac249f":"pd.DataFrame(history.history).plot()\nplt.grid(True)\nplt.gca().set_ylim(0,1)\nplt.show()","f4e032a3":"index = range(3249)\npredictions = model.predict(x_eval)\noutput = pd.DataFrame({'id': index, 'Eat': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(output.to_string())","d7c6ee57":"### Validation Distributions","ae6a5ca5":"### Calculate test set rmse","197a94bd":"### Validation Distributions","7a32485f":"## Model 3","e022eb13":"### Eat feature has a nice normal distribution, no funky numbers","58ba2464":"# Building the models","c6018114":"#### Check for missing values","69f81804":"### Validation Distributions","575cee61":"### Calculate test set rmse","edd86bed":"### Examine learning curves","efbaea74":"### Examine learning curves","2f05662b":"# EDA, cleaning, and feature engineering","6954dfb3":"### Make sure learning curves are smooth as they converge","623688a9":"### Calculate test set rmse","3c8967d6":"### Performing random search","8a5a51cd":"#### Convert dataframes and series to numpy arrays","8bbaa595":"#### Examine features","9f08d210":"##### Model 3 was by far the best based on rmse score and validation distribution. Model 3 has the most consistent distribution.  All my time went into making that model the best it could be so that is why I am using it","13ab0c86":"## Model 1","7de28ec2":"## Model 2","0aa7a887":"# Load the data","bc44a894":"##### In the directions it said to predict based off the 1275 molecular features so i am dropping \"pubchem_id\" and \"Eat\"","da36c8bc":"# Selecting the best model and making predictions"}}