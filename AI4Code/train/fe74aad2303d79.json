{"cell_type":{"30e86bdb":"code","62b8dca8":"code","751f7d86":"code","59f3061d":"code","64b5383d":"code","a3af94a3":"code","91ed8065":"code","e455e93f":"code","6354f3e1":"code","92992299":"code","29bb90de":"code","33170af3":"code","8bfaac99":"code","e2b4278d":"code","bf70e883":"code","481aa92f":"code","e6acf6e1":"code","3fc52f4d":"code","1b521717":"code","4d2687dd":"code","2e148572":"code","c3178e7c":"code","721c08fe":"code","cf9a0d83":"code","5e8eec94":"code","454f0a81":"code","5ca4da73":"code","ed349621":"code","b8458e32":"code","e773c57a":"code","3a758ca0":"code","250cb24f":"code","9ac4e604":"code","334eb32a":"code","e50a3a4b":"code","cd6a820c":"code","7480cf77":"code","eb86f405":"code","d3e9b333":"code","c7a8502f":"code","7264d700":"code","9d1fd62e":"code","90cf1c06":"code","19c51e1e":"code","c91f4de4":"code","6392cb29":"code","8ad112bf":"code","18b61189":"markdown","e8314e06":"markdown","8c6685ad":"markdown","a702c3ae":"markdown","a2f754a2":"markdown","e6797100":"markdown","37617f55":"markdown","e1e26f40":"markdown","fb635bf0":"markdown","1341075d":"markdown","c4364068":"markdown","9126b197":"markdown","aff68f75":"markdown","8d9c11c6":"markdown","35504b65":"markdown","feb1aa5d":"markdown","1ff48955":"markdown","4aa17c0b":"markdown","f739b9b4":"markdown"},"source":{"30e86bdb":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport pandas\nimport numpy as np\nimport csv\nfrom textblob import TextBlob\nimport sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix","62b8dca8":"messages = [line.rstrip() for line in open('..\/input\/spamdatacollection\/SMSSpamCollection')]\nprint(len(messages))","751f7d86":"messages[5573]","59f3061d":"for message_no, message in enumerate(messages[:10]):\n    print(message_no, message)","64b5383d":"messages = pandas.read_csv('..\/input\/spamdatacollection\/SMSSpamCollection', sep='\\t', quoting=csv.QUOTE_NONE,\n                           names=[\"label\", \"message\"])\nprint(messages)","a3af94a3":"messages.groupby('label').describe()","91ed8065":"messages['length'] = messages['message'].map(lambda text: len(text))\nprint(messages.head())","e455e93f":"messages.length.plot(bins=50, kind='hist')\nplt.show()","6354f3e1":"messages.length.describe()","92992299":"print(list(messages.message[messages.length > 900]))","29bb90de":"messages.hist(column='length', by='label', bins=50)\nplt.show()","33170af3":"def split_into_tokens(message):\n    return TextBlob(message).words","8bfaac99":"messages.message.head()","e2b4278d":"messages.message.head().apply(split_into_tokens)","bf70e883":"TextBlob(\"Hello world, how is it going?\").tags  # list of (word, POS) pairs","481aa92f":"def split_into_lemmas(message):\n    message = message.lower()\n    words = TextBlob(message).words\n    # for each word, take its \"base form\" = lemma \n    return [word.lemma for word in words]\n\nmessages.message.head().apply(split_into_lemmas)","e6acf6e1":"bow_transformer = CountVectorizer(analyzer=split_into_lemmas).fit(messages['message'])\nprint(len(bow_transformer.vocabulary_))","3fc52f4d":"message4 = messages['message'][3]\nprint(message4)","1b521717":"bow4 = bow_transformer.transform([message4])\nprint(bow4)\nprint(bow4.shape)","4d2687dd":"print(bow_transformer.get_feature_names()[6726])\nprint(bow_transformer.get_feature_names()[8002])","2e148572":"messages_bow = bow_transformer.transform(messages['message'])\nprint('sparse matrix shape:', messages_bow.shape)\nprint('number of non-zeros:', messages_bow.nnz)\nprint('sparsity: %.2f%%' % (100.0 * messages_bow.nnz \/ (messages_bow.shape[0] * messages_bow.shape[1])))","c3178e7c":"tfidf_transformer = TfidfTransformer().fit(messages_bow)\ntfidf4 = tfidf_transformer.transform(bow4)\nprint(tfidf4)","721c08fe":"print(tfidf_transformer.idf_[bow_transformer.vocabulary_['u']])\nprint(tfidf_transformer.idf_[bow_transformer.vocabulary_['university']])","cf9a0d83":"messages_tfidf = tfidf_transformer.transform(messages_bow)\nprint(messages_tfidf.shape)","5e8eec94":"%time spam_detector = MultinomialNB().fit(messages_tfidf, messages['label'])","454f0a81":"print('predicted:', spam_detector.predict(tfidf4)[0])\nprint('expected:', messages.label[3])","5ca4da73":"all_predictions = spam_detector.predict(messages_tfidf)\nprint(all_predictions)","ed349621":"print('accuracy', accuracy_score(messages['label'], all_predictions))\nprint('confusion matrix\\n', confusion_matrix(messages['label'], all_predictions))\nprint('(row=expected, col=predicted)')","b8458e32":"plt.matshow(confusion_matrix(messages['label'], all_predictions), cmap=plt.cm.binary, interpolation='nearest')\nplt.title('confusion matrix')\nplt.colorbar()\nplt.ylabel('expected label')\nplt.xlabel('predicted label')\nplt.show()","e773c57a":"print(classification_report(messages['label'], all_predictions))","3a758ca0":"text_feat = messages['message'].copy()","250cb24f":"def text_process(text):\n    \n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n    \n    return \" \".join(text)","9ac4e604":"import string\nfrom nltk.corpus import stopwords\ntext_feat = text_feat.apply(text_process)","334eb32a":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(input=\"english\")","e50a3a4b":"features = vectorizer.fit_transform(text_feat)","cd6a820c":"from sklearn.model_selection import train_test_split","7480cf77":"features_train, features_test, labels_train, labels_test = train_test_split(features, messages['label'], test_size=0.3, random_state=111)","eb86f405":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import accuracy_score","d3e9b333":"svc = SVC(kernel='sigmoid', gamma=1.0)\nknc = KNeighborsClassifier(n_neighbors=49)\nmnb = MultinomialNB(alpha=0.2)\ndtc = DecisionTreeClassifier(min_samples_split=7, random_state=111)\nlrc = LogisticRegression(solver='liblinear', penalty='l1')\nrfc = RandomForestClassifier(n_estimators=31, random_state=111)\nabc = AdaBoostClassifier(n_estimators=62, random_state=111)\nbc = BaggingClassifier(n_estimators=9, random_state=111)\netc = ExtraTreesClassifier(n_estimators=9, random_state=111)","c7a8502f":"clfs = {'SVC' : svc,'KN' : knc, 'NB': mnb, 'DT': dtc, 'LR': lrc, 'RF': rfc, 'AdaBoost': abc, 'BgC': bc, 'ETC': etc}","7264d700":"def train_classifier(clf, feature_train, labels_train):    \n    clf.fit(feature_train, labels_train)","9d1fd62e":"def predict_labels(clf, features):\n    return (clf.predict(features))","90cf1c06":"pred_scores = []\nfor k,v in clfs.items():\n    train_classifier(v, features_train, labels_train)\n    pred = predict_labels(v,features_test)\n    pred_scores.append((k, [accuracy_score(labels_test,pred)]))","19c51e1e":"result =[]\nfor i in pred_scores:\n    result.append([i[0],i[1][0]])","c91f4de4":"df = pandas.DataFrame(result,columns=['Classifiers','Score'])","6392cb29":"df","8ad112bf":"\ndf.plot(kind='bar', ylim=(0.9,1.0), figsize=(11,6), align='center', colormap=\"Accent\")\nplt.xticks(np.arange(9), df['Classifiers'].values)\nplt.ylabel('Accuracy Score')\nplt.xlabel('Classifiers')\nplt.title('Distribution by Classifier')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","18b61189":"Now we'll convert each message, represented as a list of tokens (lemmas) above, into a vector that machine learning models can understand.\n\nDoing that requires essentially three steps, in the bag-of-words model:\n\n1. counting how many times does a word occur in each message (term frequency)\n2. weighting the counts, so that frequent tokens get lower weight (inverse document frequency)\n3. normalizing the vectors to unit length, to abstract from the original text length (L2 norm)","e8314e06":"Each vector has as many dimensions as there are unique words in the SMS corpus:","8c6685ad":"What is the IDF (inverse document frequency) of the word `\"u\"`? Of word `\"university\"`?","a702c3ae":"So, nine unique words in message nr. 4, two of them appear twice, the rest only once. Sanity check: what are these words the appear twice?","a2f754a2":"There are quite a few possible metrics for evaluating model performance. Which one is the most suitable depends on the task. For example, the cost of mispredicting \"spam\" as \"ham\" is probably much lower than mispredicting \"ham\" as \"spam\".","e6797100":"And finally, after the counting, the term weighting and normalization can be done with [TF-IDF](http:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf), using scikit-learn's `TfidfTransformer`:","37617f55":"https:\/\/archive.ics.uci.edu\/ml\/datasets\/SMS+Spam+Collection","e1e26f40":"## Step 2: Data preprocessing","fb635bf0":"With messages represented as vectors, we can finally train our spam\/ham classifier. This part is pretty straightforward, and there are many libraries that realize the training algorithms.","1341075d":"## Step 4: Training a model, detecting spam","c4364068":"From this confusion matrix, we can compute precision and recall, or their combination (harmonic mean) F1:","9126b197":"Let's try classifying our single random message:","aff68f75":"We'll be using scikit-learn here, choosing the [Naive Bayes](http:\/\/en.wikipedia.org\/wiki\/Naive_Bayes_classifier) classifier to start with:","8d9c11c6":"## Step 1: Load data, look around","35504b65":"Here we used `scikit-learn` (`sklearn`), a powerful Python library for teaching machine learning. It contains a multitude of various methods and options.\n\nLet's take one text message and get its bag-of-words counts as a vector, putting to use our new `bow_transformer`:","feb1aa5d":"Data downloaded from:-","1ff48955":"To transform the entire bag-of-words corpus into TF-IDF corpus at once:","4aa17c0b":"## Step 3: Data to vectors","f739b9b4":"The bag-of-words counts for the entire SMS corpus are a large, sparse matrix:"}}