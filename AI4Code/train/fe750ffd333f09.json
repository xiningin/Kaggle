{"cell_type":{"5f59e65c":"code","8611d3fb":"code","b2bc8f9e":"code","83c7e7ab":"code","1667757f":"code","1520d5fd":"code","0118e58c":"code","4e0e328f":"code","7ea696e6":"code","2fa044eb":"code","7d9849fe":"code","f35233fb":"code","30f5d536":"code","940d659e":"code","c9a8d31a":"code","615f8e6f":"code","fe2c68c9":"code","90b42038":"code","346a5253":"code","f7d5c96e":"code","1447dd35":"code","fcd8b81b":"code","f0dd9aa5":"code","c7b4e4b3":"code","ca3fbd57":"code","9fdee00e":"code","93c37e31":"code","480cb512":"code","6eeb2bb7":"code","9e5fd43e":"code","ecf91ad9":"code","8fe6bda3":"code","10606e37":"code","e97a1044":"code","1ffb0e09":"code","f0f35a97":"markdown","59ce7895":"markdown","c865f9fe":"markdown","b02b8df4":"markdown","49110e08":"markdown","1e1a4e7d":"markdown","cce506e5":"markdown","7fd17a1a":"markdown","fd7d5890":"markdown","550bf9dc":"markdown","657a58e9":"markdown","59a1b580":"markdown","3e5ae5f9":"markdown"},"source":{"5f59e65c":"# load library\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sb","8611d3fb":"# load dataset\ncnames = ['symboling', 'normalized-losses', 'make', 'fuel-type', 'aspiration', 'num-of-doors', 'body-style',\n                'drive-wheels', 'engine-location', 'wheel-base', 'length', 'width', 'height', 'curb-weight',\n                'engine-type', 'num-of-cylinders', 'engine-size', 'fuel-system', 'bore', 'stroke', 'compression-ratio',\n                'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']\ndata = pd.read_csv('\/kaggle\/input\/automobile-dataset\/Automobile_data.csv')\ndata = data.replace('?', np.nan)","b2bc8f9e":"data.shape","83c7e7ab":"data.info()","1667757f":"data.head()","1520d5fd":"data.drop(columns= 'symboling', inplace= True)\ndata.drop(columns= 'normalized-losses', inplace= True)\ndata.drop(columns= 'engine-location', inplace= True)\ndata.drop(columns= 'wheel-base', inplace= True)\ndata.drop(columns= 'engine-type', inplace= True)\ndata.drop(columns= 'bore', inplace= True)\ndata.drop(columns= 'stroke', inplace= True)\ndata.drop(columns= 'highway-mpg', inplace= True)\ndata.drop(columns= 'compression-ratio', inplace= True)\ndata.drop(columns= 'width', inplace= True)\ndata.drop(columns= 'height', inplace= True)\n","0118e58c":"# format horsepower\ndata['horsepower'] = pd.to_numeric(data['horsepower'])\n# format engine-size\ndata['engine-size'] = pd.to_numeric(data['engine-size'])\n# format num_of_cylinders\nnum_of_cylinders_code = {'two': 2, 'three': 3, 'four': 4, 'five': 5, 'six': 6, 'eight': 8, 'twelve': 12}\ndata['num-of-cylinders'] = data['num-of-cylinders'].map(num_of_cylinders_code)\n# format num_of_doors\nnum_of_doors_code = {'two': 2, 'four': 4}\ndata['num-of-doors'] = data['num-of-doors'].map(num_of_doors_code)\n# format peak-rpm\ndata['peak-rpm'] = pd.to_numeric(data['peak-rpm'])\n# format price\ndata['price'] = pd.to_numeric(data['price'])","4e0e328f":"# View data type after formatting\ndata.info()","7ea696e6":"import scipy\nimport sys\nfor i in ['engine-size', 'horsepower', 'city-mpg', 'price']:\n    upper_outlayer = data[i] > (np.percentile(data[i], 75) + 1.5* scipy.stats.iqr(data[i]))\n    lower_outlayer = data[i] < (np.percentile(data[i], 25) - 1.5* scipy.stats.iqr(data[i]))\n    outlayer_all = lower_outlayer | upper_outlayer\n    data[i].loc[outlayer_all] = data[i].mean()","2fa044eb":"# Detect variable inclue missing value\ndata.isna().sum(0)","7d9849fe":"# Solve missing values\ndata['num-of-doors'].loc[data['num-of-doors'].isna()] = data['num-of-doors'].mode()[0]\nfor i in ['horsepower', 'horsepower', 'peak-rpm', 'price']:\n    data[i].loc[data[i].isna()] = data[i].mean()","f35233fb":"# recode aspiration\naspiration_code = {'std': 0, 'turbo': 1}\ndata['aspiration'] = data['aspiration'].map(aspiration_code)\n\n# recode fuel-type\nfuel_type_code = {'diesel': 0, 'gas': 1}\ndata['fuel-type'] = data['fuel-type'].map(fuel_type_code)\n\n# Onehot endcoding\ndata = pd.get_dummies(data= data, prefix= ['make', 'body-style', 'drive-wheels', 'fuel-system'])","30f5d536":"data.head()","940d659e":"y = data['price'].values\nx = data.drop(columns = ['price'])\n# MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\nx= MinMaxScaler().fit(x).transform(x)","c9a8d31a":"# Split dataset to train and test datasets\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.2, random_state=4)","615f8e6f":"# Building linear model\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()  \nregressor.fit(x_train, y_train)","fe2c68c9":"# Predict on test set\ny_pred = regressor.predict(x_test)","90b42038":"r = scipy.stats.pearsonr(y_test, y_pred)\npearson_r = str(round(r[0],2))\npearson_p = str(r[1])\n\nplt.figure(figsize= (10,4))\nax1 = plt.subplot(121)\nsb.scatterplot(y_test, y_pred)\nplt.text(5000, 35000, 'pearson R: '+ pearson_r + '\\np: ' + pearson_p, va = 'top')\n\nax2 = plt.subplot(122)\nsb.kdeplot(y_test, shade=True, color=\"r\", legend=True, label = 'Test')\nsb.kdeplot(y_pred, shade=True, color=\"b\", legend=True, label = 'Predict')\nplt.legend()","346a5253":"# Mean squared error\nfrom sklearn.metrics import mean_squared_error\nMSE = mean_squared_error(y_test,y_pred)\nprint('Mean squared error:', MSE)\n\n# Mean absolute error\nfrom sklearn.metrics import mean_absolute_error\nMAE = mean_absolute_error(y_test,y_pred)\nprint('Mean absolute error:', MAE)\n\n# R2 score\nfrom sklearn.metrics import r2_score\nr2 = r2_score(y_test, y_pred)\nprint('R2 score:', r2)","f7d5c96e":"from sklearn.linear_model import Ridge\nridge = Ridge(alpha = 0.05, normalize = True)\nridge.fit(x_train, y_train)","1447dd35":"y_pred = ridge.predict(x_test)","fcd8b81b":"r = scipy.stats.pearsonr(y_test, y_pred)\npearson_r = str(round(r[0],2))\npearson_p = str(r[1])\n\nplt.figure(figsize= (10,4))\nax1 = plt.subplot(121)\nsb.scatterplot(y_test, y_pred)\nplt.text(5000, 35000, 'pearson R: '+ pearson_r + '\\np: ' + pearson_p, va = 'top')\n\nax2 = plt.subplot(122)\nsb.kdeplot(y_test, shade=True, color=\"r\", legend=True, label = 'Test')\nsb.kdeplot(y_pred, shade=True, color=\"b\", legend=True, label = 'Predict')\nplt.legend()","f0dd9aa5":"# Mean squared error\nfrom sklearn.metrics import mean_squared_error\nMSE = mean_squared_error(y_test,y_pred)\nprint('Mean squared error:', MSE)\n\n# Mean absolute error\nfrom sklearn.metrics import mean_absolute_error\nMAE = mean_absolute_error(y_test,y_pred)\nprint('Mean absolute error:', MAE)\n\n# R2 score\nfrom sklearn.metrics import r2_score\nr2 = r2_score(y_test, y_pred)\nprint('R2 score:', r2)","c7b4e4b3":"from sklearn.linear_model import Lasso\nlasso_model = Lasso(alpha = 0.05, normalize = True)\nlasso_model.fit(x_train, y_train)","ca3fbd57":"y_pred = lasso_model.predict(x_test)","9fdee00e":"r = scipy.stats.pearsonr(y_test, y_pred)\npearson_r = str(round(r[0],2))\npearson_p = str(r[1])\n\nplt.figure(figsize= (10,4))\nax1 = plt.subplot(121)\nsb.scatterplot(y_test, y_pred)\nplt.text(5000, 35000, 'pearson R: '+ pearson_r + '\\np: ' + pearson_p, va = 'top')\n\nax2 = plt.subplot(122)\nsb.kdeplot(y_test, shade=True, color=\"r\", legend=True, label = 'Test')\nsb.kdeplot(y_pred, shade=True, color=\"b\", legend=True, label = 'Predict')\nplt.legend()","93c37e31":"# Mean squared error\nfrom sklearn.metrics import mean_squared_error\nMSE = mean_squared_error(y_test,y_pred)\nprint('Mean squared error:', MSE)\n\n# Mean absolute error\nfrom sklearn.metrics import mean_absolute_error\nMAE = mean_absolute_error(y_test,y_pred)\nprint('Mean absolute error:', MAE)\n\n# R2 score\nfrom sklearn.metrics import r2_score\nr2 = r2_score(y_test, y_pred)\nprint('R2 score:', r2)","480cb512":"from sklearn.linear_model import ElasticNet\nenet_model = ElasticNet(alpha=0.01, l1_ratio=0.5, normalize=False)\nenet_model.fit(x_train, y_train)","6eeb2bb7":"y_pred = enet_model.predict(x_test)","9e5fd43e":"r = scipy.stats.pearsonr(y_test, y_pred)\npearson_r = str(round(r[0],2))\npearson_p = str(r[1])\n\nplt.figure(figsize= (10,4))\nax1 = plt.subplot(121)\nsb.scatterplot(y_test, y_pred)\nplt.text(5000, 35000, 'pearson R: '+ pearson_r + '\\np: ' + pearson_p, va = 'top')\n\nax2 = plt.subplot(122)\nsb.kdeplot(y_test, shade=True, color=\"r\", legend=True, label = 'Test')\nsb.kdeplot(y_pred, shade=True, color=\"b\", legend=True, label = 'Predict')\nplt.legend()","ecf91ad9":"# Mean squared error\nfrom sklearn.metrics import mean_squared_error\nMSE = mean_squared_error(y_test,y_pred)\nprint('Mean squared error:', MSE)\n\n# Mean absolute error\nfrom sklearn.metrics import mean_absolute_error\nMAE = mean_absolute_error(y_test,y_pred)\nprint('Mean absolute error:', MAE)\n\n# R2 score\nfrom sklearn.metrics import r2_score\nr2 = r2_score(y_test, y_pred)\nprint('R2 score:', r2)","8fe6bda3":"from sklearn.ensemble import RandomForestRegressor\nRandomForest = RandomForestRegressor()\nRandomForest.fit(x_train, y_train)","10606e37":"y_pred = RandomForest.predict(x_test)","e97a1044":"r = scipy.stats.pearsonr(y_test, y_pred)\npearson_r = str(round(r[0],2))\npearson_p = str(r[1])\n\nplt.figure(figsize= (10,4))\nax1 = plt.subplot(121)\nsb.scatterplot(y_test, y_pred)\nplt.text(5000, 35000, 'pearson R: '+ pearson_r + '\\np: ' + pearson_p, va = 'top')\n\nax2 = plt.subplot(122)\nsb.kdeplot(y_test, shade=True, color=\"r\", legend=True, label = 'Test')\nsb.kdeplot(y_pred, shade=True, color=\"b\", legend=True, label = 'Predict')\nplt.legend()","1ffb0e09":"# Mean squared error\nfrom sklearn.metrics import mean_squared_error\nMSE = mean_squared_error(y_test,y_pred)\nprint('Mean squared error:', MSE)\n\n# Mean absolute error\nfrom sklearn.metrics import mean_absolute_error\nMAE = mean_absolute_error(y_test,y_pred)\nprint('Mean absolute error:', MAE)\n\n# R2 score\nfrom sklearn.metrics import r2_score\nr2 = r2_score(y_test, y_pred)\nprint('R2 score:', r2)","f0f35a97":"# Standardized data type","59ce7895":"# Solve outliers","c865f9fe":"# Random Forest Regressor ","b02b8df4":"# ElasticNet Regression","49110e08":"# Solve missing values","1e1a4e7d":"# Ridge Regression","cce506e5":"# Lasso Regression","7fd17a1a":"# Data Encoding","fd7d5890":"#### Attribute Information:\nAttribute: Attribute Range\n1. symboling: -3, -2, -1, 0, 1, 2, 3.\n2. normalized-losses: continuous from 65 to 256.\n3. make:\nalfa-romero, audi, bmw, chevrolet, dodge, honda,\nisuzu, jaguar, mazda, mercedes-benz, mercury,\nmitsubishi, nissan, peugot, plymouth, porsche,\nrenault, saab, subaru, toyota, volkswagen, volvo\n\n4. fuel-type: diesel, gas.\n5. aspiration: std, turbo.\n6. num-of-doors: four, two.\n7. body-style: hardtop, wagon, sedan, hatchback, convertible.\n8. drive-wheels: 4wd, fwd, rwd.\n9. engine-location: front, rear.\n10. wheel-base: continuous from 86.6 120.9.\n11. length: continuous from 141.1 to 208.1.\n12. width: continuous from 60.3 to 72.3.\n13. height: continuous from 47.8 to 59.8.\n14. curb-weight: continuous from 1488 to 4066.\n15. engine-type: dohc, dohcv, l, ohc, ohcf, ohcv, rotor.\n16. num-of-cylinders: eight, five, four, six, three, twelve, two.\n17. engine-size: continuous from 61 to 326.\n18. fuel-system: 1bbl, 2bbl, 4bbl, idi, mfi, mpfi, spdi, spfi.\n19. bore: continuous from 2.54 to 3.94.\n20. stroke: continuous from 2.07 to 4.17.\n21. compression-ratio: continuous from 7 to 23.\n22. horsepower: continuous from 48 to 288.\n23. peak-rpm: continuous from 4150 to 6600.\n24. city-mpg: continuous from 13 to 49.\n25. highway-mpg: continuous from 16 to 54.\n26. price: continuous from 5118 to 45400.","550bf9dc":"# Linear model","657a58e9":"# Scaling data ","59a1b580":"# Feature selection","3e5ae5f9":"# Data exploration"}}