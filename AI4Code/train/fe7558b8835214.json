{"cell_type":{"5d77883b":"code","a1a04fae":"code","2051cbd5":"code","0c43bcc3":"code","581087d4":"code","90856ebb":"code","d5bff230":"code","6a4389b3":"code","116219ca":"code","2a2850ff":"code","36e70eca":"code","977c233a":"code","4cbb77b6":"code","23486236":"code","dbc8acc8":"code","e972d550":"code","9001cf4e":"code","dd714b68":"code","92067307":"code","07d87c80":"code","79d52113":"code","e4e01556":"code","c902b185":"code","8bc9efee":"code","58977962":"code","005b884b":"markdown","b7a31da5":"markdown"},"source":{"5d77883b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os, random ,cv2, glob\nimport keras\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, Dense, Flatten, Dropout, Activation, MaxPool2D\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.losses import binary_crossentropy\nfrom keras.preprocessing.image import ImageDataGenerator\nimport keras.preprocessing.image\nimport matplotlib.pyplot as plt","a1a04fae":"memes_path = \"\/kaggle\/input\/reddit*\/memes\/memes\/*\"\nmemes = glob.glob(memes_path)\n\nprint(\"Memes: \",len(memes))\n\nphoto_path = \"\/kaggle\/input\/mso*\/*\/*\/*\"\nphotos = glob.glob(photo_path)\n\nprint(\"Photos: \",len(photos))","2051cbd5":"def prep_data(memes, photos):\n    x=[]\n    y=[]\n    for i in memes:\n        image = keras.preprocessing.image.load_img(i, color_mode = \"rgb\", target_size = (rows,cols))\n        image_arr = keras.preprocessing.image.img_to_array(image, data_format = \"channels_last\")\n        x.append(image_arr)\n        y.append(1)\n        \n    for i in photos:\n        image = keras.preprocessing.image.load_img(i, color_mode = \"rgb\", target_size = (rows,cols))\n        image_arr = keras.preprocessing.image.img_to_array(image, data_format = \"channels_last\")\n        x.append(image_arr)\n        y.append(0)\n    return x,y","0c43bcc3":"#define image dimensions \nrows = 150\ncols = 150\nchannels = 3\n\nX, y = prep_data(memes, photos)","581087d4":"#split X,y into a train and validation data sets\nX_train, X_val, y_train, y_val = train_test_split(X,y, test_size=(0.2), random_state=1)","90856ebb":"#import VGG16\nfrom keras.applications import VGG19\n#creating an object of vgg19 model and discarding the top layer\nmodel_vgg19 = VGG19(include_top=False, weights='imagenet', input_shape=(rows,cols,channels))\n#model_vgg16.summary()","d5bff230":"#copy vgg19 layers into our model\nmodel = Sequential()\nfor layer in model_vgg19.layers:\n    model.add(layer)","6a4389b3":"#freezing vgg19 layers (saving its original weights)\nfor i in model.layers:\n    i.trainable = False","116219ca":"#add top layer for fine-tune VGG19\nmodel.add(Flatten())\nmodel.add(Dense(10))\nmodel.add(Activation('relu'))\nmodel.add(Dense(1, activation='sigmoid'))","2a2850ff":"#check layers trainability\nnum_layers = len(model.layers)\nfor x in range(0,num_layers):\n    print(model.layers[x])\n    print(model.layers[x].trainable)","36e70eca":"model.compile(optimizer='Adam', metrics=['accuracy'], loss='binary_crossentropy')","977c233a":"#create a data generator object with some image augmentation specs\ndatagen = ImageDataGenerator(\n    rotation_range = 40,\n    rescale=1.\/ 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True\n)","4cbb77b6":"train_gen = datagen.flow(x=np.array(X_train), y=y_train, batch_size=50)\nvalid_gen = datagen.flow(x=np.array(X_val), y=y_val, batch_size=50)","23486236":"#train\/validate model\nhistory = model.fit(train_gen, steps_per_epoch=100, epochs=22, verbose=1, validation_data=valid_gen, validation_steps=30)","dbc8acc8":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.title('Model Accuracy')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\n","e972d550":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')","9001cf4e":"test_path = glob.glob(\"\/kaggle\/input\/test-images\/imgs\/*.jpg\")\n\ntest = []\nfor i in test_path:\n    try:\n        image = keras.preprocessing.image.load_img(i, color_mode = \"rgb\", target_size = (rows,cols))\n    except:\n        print(\"Couldn't load Image\")\n    image_arr = keras.preprocessing.image.img_to_array(image, data_format = \"channels_last\")\n    test.append(image_arr)","dd714b68":"test_gen = datagen.flow(x = np.array(test), batch_size=50)","92067307":"pred = model.predict(test_gen)","07d87c80":"print(pred)","79d52113":"ids = [x.rstrip(\".jpg\").lstrip(\"\/kaggle\/input\/test-images\/imgs\/\") for x in test_path]\npreds = [x[0] for x in pred]\n\nprint(ids[0])","e4e01556":"df = {'ids': ids, 'predictions': preds}\n\ndataset = pd.DataFrame(df)\n\nprint(dataset.head())","c902b185":"dataset.to_csv(\"predictions.csv\")","8bc9efee":"model.save_weights(\"ImgMemeWeights.h5\")\nmodelJson = model.to_json()","58977962":"import json\n\nwith open(\"config.json\", \"w\") as file:\n    json.dump(modelJson, file)","005b884b":"# Image vs Meme Classifier\n\nMerging two datasets:\n1. Reddit-Memes-Dataset\n2. MSO-Dataset","b7a31da5":"## Gathering Memes"}}