{"cell_type":{"1ffe5114":"code","dffff8cc":"code","bc522604":"code","9b329d12":"code","f9b6f042":"code","26b6ded5":"code","5e9b2ff0":"code","59f927cd":"code","ce89e570":"code","bd921773":"code","2e078c32":"code","e1f82820":"code","d3dd93a7":"code","3ebe9fcf":"code","4cf40576":"code","02a95bee":"code","c386dd77":"code","0b5701c9":"code","accbff95":"code","c57b6548":"code","1f4b57b5":"code","11bbc101":"code","47e6589f":"code","a595c15f":"code","c8b274f4":"code","c84e5d0b":"code","9cfc68c4":"code","acdccc56":"code","dfd3f006":"code","1e7b7b81":"code","79e4ff67":"code","216a7770":"code","1ddb2063":"code","d35facd8":"code","1d1f4302":"code","c66c8df8":"code","ca6dc4da":"code","de136043":"code","a4b4454b":"code","47d2f2d8":"markdown","9a4c3c9e":"markdown","3e1b62aa":"markdown","581c7b41":"markdown","28e6c4ed":"markdown","e3c6c54e":"markdown","ff57f2a9":"markdown","06226243":"markdown","3c52239e":"markdown","abc61e9e":"markdown","5302e4d0":"markdown","d4987079":"markdown","05364643":"markdown","323a8fe4":"markdown","c2b750fa":"markdown","73bac373":"markdown","586403e8":"markdown"},"source":{"1ffe5114":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns \n%matplotlib inline ","dffff8cc":"data = pd.read_csv('..\/input\/creditscoring-data\/data_train.csv')","bc522604":"data.head()","9b329d12":"data.describe()","f9b6f042":"data.shape","26b6ded5":"data.isnull().sum()","5e9b2ff0":"data.info()","59f927cd":"data['Score_point'] # this colums has some objects ","ce89e570":"df = data ","bd921773":"df = df.replace({'-':0}) # replacing the object to 0 score","2e078c32":"df['Score_point']","e1f82820":"# converting object and other data types to interger \n\ncolumns = df.columns\n\nfor c in list(columns):\n    df[c] = df[c].astype('int64')","d3dd93a7":"df.info() # now we have only integer data type ","3ebe9fcf":"df.head()","4cf40576":"# checking for correlation of the columns \n\nplt.subplots(figsize=(10,7))\nsns.heatmap(df.corr())","02a95bee":"df.corr().sum()","c386dd77":"counting_0_1 = df.pivot_table(columns=['label'], aggfunc='size')\nprint(counting_0_1)","0b5701c9":"y = df['label']\nX = df.drop(['label'], axis=1, inplace=False)","accbff95":"from imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler \nfrom collections import Counter ","c57b6548":"# instantiating the random over sampLer \nros = RandomOverSampler()\n # resampLing X, y x_ros, y_\nX_ros, y_ros = ros.fit_resample(X, y) \n# new class distribution\nprint(Counter(y_ros)) ","1f4b57b5":"from sklearn.model_selection import train_test_split","11bbc101":"X_train, X_test, y_train, y_test = train_test_split(X_ros,y_ros, test_size = 0.30, random_state = 101)","47e6589f":"print(X_train.shape, X_test.shape)","a595c15f":"print(y_train.shape, y_test.shape)","c8b274f4":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report,confusion_matrix \n\nmodel = SVC()\nmodel.fit(X_train,y_train)\nsvm_pred = model.predict(X_test)\n\n# Evaluating Model\nprint(accuracy_score(y_test, svm_pred).round(3))\nprint(confusion_matrix(y_test, svm_pred))\nprint(classification_report(y_test, svm_pred))\n","c84e5d0b":"from sklearn.tree import DecisionTreeClassifier\n\nregressor = DecisionTreeClassifier(random_state =0)\nregressor.fit(X_train,y_train)\nDectree_pred = regressor.predict(X_test)\n\n# Evaluating Model\nprint(accuracy_score(y_test, Dectree_pred).round(3))\nprint(confusion_matrix(y_test, Dectree_pred))\nprint(classification_report(y_test, Dectree_pred))\n\n","9cfc68c4":"# for this model we need scale this dataset to increase accuracy \nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit(X_train)\nX_trainKnn = sc.transform(X_train)\nX_testKnn = sc.transform(X_test)","acdccc56":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_trainKnn, y_train)\nknn_pred = knn.predict(X_testKnn)\n\n# Evaluating Model\nprint(accuracy_score(y_test, knn_pred).round(3))\nprint(confusion_matrix(y_test, knn_pred))\nprint(classification_report(y_test, knn_pred))\n","dfd3f006":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier()\ngb.fit(X_train, y_train)\ngb_pred = gb.predict(X_test)\n\nprint(accuracy_score(y_test, gb_pred).round(3))\nprint(confusion_matrix(y_test, gb_pred))\nprint(classification_report(y_test, gb_pred))\n","1e7b7b81":"df2 = pd.read_csv('..\/input\/creditscoring-data\/data_test.csv')","79e4ff67":"df2.head()","216a7770":"df2 = df2.replace({'-':0})","1ddb2063":"df2['Score_point'].head()","d35facd8":"New_Y = df2['label']\nNew_X = df2.drop(['label'], axis=1, inplace=False)","1d1f4302":"New_X.head()","c66c8df8":"svm_new = model.predict(New_X)\n\n# Evaluating Prediction\nprint(accuracy_score(New_Y, svm_new).round(3))\nprint(confusion_matrix(New_Y, svm_new))\nprint(classification_report(New_Y, svm_new))\n","ca6dc4da":"Dectree_new = regressor.predict(New_X)\n\n# Evaluating Prediction\nprint(accuracy_score(New_Y, Dectree_new).round(3))\nprint(confusion_matrix(New_Y, Dectree_new))\nprint(classification_report(New_Y, Dectree_new))","de136043":"sc = StandardScaler()\nsc.fit(New_X)\nX_trainKnn = sc.transform(New_X)\n\n\nknn_pred = knn.predict(X_trainKnn)\n\n# Evaluating Prediction\nprint(accuracy_score(New_Y, knn_pred).round(3))\nprint(confusion_matrix(New_Y, knn_pred))\nprint(classification_report(New_Y, knn_pred))","a4b4454b":"gb_pred = gb.predict(New_X)\n\n#Evaluating Prediction\nprint(accuracy_score(New_Y, gb_pred).round(3))\nprint(confusion_matrix(New_Y, gb_pred))\nprint(classification_report(New_Y, gb_pred))","47d2f2d8":"## SVM Classification","9a4c3c9e":"## K-Nearest Neighbors","3e1b62aa":"# Training Data in diffirent classification Models","581c7b41":"## Decision Tree","28e6c4ed":"# Predicting Customers for Credit validation","e3c6c54e":"### Reading and Cleaning the data","ff57f2a9":"# Predicting new Testing Data ","06226243":"### as we have unbalanced data so we gonna balance it","3c52239e":"## KNearestNeighbor","abc61e9e":"  ##  Note\n   All of our model are working perfect almost near to 100%. But We choose perfect one after testing data to all of them","5302e4d0":"### SVM","d4987079":"# Predicting new dataset with our Models","05364643":"## Splitting data to train and test","323a8fe4":"# Gradient Booster","c2b750fa":"# Decision Tree Classification","73bac373":"# Conclusion\n    Well, two of the models working perfectly! So, We can choose one of the for the implimentation ","586403e8":"## GradientBooster"}}