{"cell_type":{"7091095c":"code","81af537c":"code","f137e61e":"code","95f501b1":"code","cf35e758":"code","2b0a186c":"code","8a67ceb2":"code","e599ef6d":"code","d5c91539":"code","e8c4e10b":"code","7fd3a0aa":"code","6bbf28f0":"code","b7b9b28d":"code","4eb7cb4f":"code","72441c22":"code","ef3190d1":"code","d2c8227d":"code","ede4dce3":"code","a3e2b983":"code","6dc210da":"code","62373e68":"code","ca017c13":"code","6590cbc5":"code","8a9682f2":"code","52e94229":"code","4da91819":"code","bf0403fc":"code","046a0ff6":"markdown","fe0bdc58":"markdown","90007f2a":"markdown","958a7a14":"markdown","8cf8b51c":"markdown","c0e866f0":"markdown","e4f440ac":"markdown","0c10b4a4":"markdown","a07c409a":"markdown","84af6fd4":"markdown","8ab5d291":"markdown","a8e38764":"markdown","8a7ce525":"markdown","09d06607":"markdown","26f0a3c6":"markdown"},"source":{"7091095c":"import os\nimport gc\nimport cv2\nimport math\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Concatenate, Dense, Conv2D, BatchNormalization, Dropout, GlobalAveragePooling2D, GlobalMaxPooling2D\nfrom tensorflow.keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom sklearn.model_selection import train_test_split, GroupShuffleSplit, GroupKFold\nfrom sklearn.metrics import make_scorer, accuracy_score, roc_auc_score, roc_curve\n\nprint(\"Tensorflow version \" + tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE","81af537c":"# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","f137e61e":"MIXED_PRECISION = False\nXLA_ACCELERATE = False\n\nif MIXED_PRECISION:\n    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n    if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n    mixed_precision.set_policy(policy)\n    print('Mixed precision enabled')\n\nif XLA_ACCELERATE:\n    tf.config.optimizer.set_jit(True)\n    print('Accelerated Linear Algebra enabled')","95f501b1":"def read_image(filepath):\n    return cv2.imread(os.path.join(data_dir, filepath)) \n\ndef resize_image(image, image_size):\n    return cv2.resize(image.copy(), image_size, interpolation = cv2.INTER_AREA)\n\ndef get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation \/ 180.\n    shear = math.pi * shear \/ 180.\n    \n    # ROTATION MATRIX\n    c1 = tf.math.cos(rotation)\n    s1 = tf.math.sin(rotation)\n    one = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n        \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n    \n    # ZOOM MATRIX\n    zoom_matrix = tf.reshape( tf.concat([one\/height_zoom,zero,zero, zero,one\/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n    \n    # SHIFT MATRIX\n    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))\n\ndef transform(image,label):\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    DIM = IMAGE_SIZE\n    XDIM = DIM%2 #fix for size 331\n    \n    rot = 15. * tf.random.normal([1],dtype='float32')\n    shr = 5. * tf.random.normal([1],dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')\/10.\n    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')\/10.\n    h_shift = 16. * tf.random.normal([1],dtype='float32') \n    w_shift = 16. * tf.random.normal([1],dtype='float32') \n  \n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x = tf.repeat( tf.range(DIM\/\/2,-DIM\/\/2,-1), DIM )\n    y = tf.tile( tf.range(-DIM\/\/2,DIM\/\/2),[DIM] )\n    z = tf.ones([DIM*DIM],dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n    idx2 = K.cast(idx2,dtype='int32')\n    idx2 = K.clip(idx2,-DIM\/\/2+XDIM+1,DIM\/\/2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack( [DIM\/\/2-idx2[0,], DIM\/\/2-1+idx2[1,]] )\n    d = tf.gather_nd(image,tf.transpose(idx3))\n        \n    return tf.reshape(d,[DIM,DIM,3]),label\n\ndef get_training_dataset(dataset, do_aug = True):\n    if do_aug: \n        dataset = dataset.map(transform, num_parallel_calls = AUTO)\n    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_validation_dataset(dataset):\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset","cf35e758":"disease_types=['COVID', 'non-COVID']\ndata_dir = '..\/input\/sarscov2-ctscan-dataset\/'\ntrain_dir = os.path.join(data_dir)","2b0a186c":"train_data = []\nfor defects_id, sp in enumerate(disease_types):\n    for file in os.listdir(os.path.join(train_dir, sp)):\n        train_data.append(['{}\/{}'.format(sp, file), defects_id, sp])      \ntrain = pd.DataFrame(train_data, columns=['File', 'DiseaseID','Disease Type'])","8a67ceb2":"IMAGE_SIZE = 64\nX = np.zeros((train.shape[0], IMAGE_SIZE, IMAGE_SIZE, 3))\nfor i, file in tqdm(enumerate(train['File'].values), total = len(train)):\n    image = read_image(file)\n    if image is not None:\n        X[i] = resize_image(image, (IMAGE_SIZE, IMAGE_SIZE))\nX \/= 255.\ny = train['DiseaseID'].values\nprint(X.shape)\nprint(y.shape)","e599ef6d":"from sklearn.cluster import KMeans\n\nX0 = X[y == 0].reshape(len(X[y == 0]), -1)\nX1 = X[y == 1].reshape(len(X[y == 1]), -1)\n\nk = 60\nkmeans = KMeans(k)\ncluster0 = kmeans.fit_predict(X0)\ncluster1 = kmeans.fit_predict(X1)\ncluster1 += k\ncluster = np.concatenate([cluster0, cluster1])","d5c91539":"np.random.seed(42)\n\nrows = 5 \ncols = 5\n\nfig, ax = plt.subplots(rows, cols, figsize=(12, 12))\nfor i in range(rows):\n    clt = np.random.randint(0, 2 * k)\n    clt_idx = np.random.choice(np.where(cluster == clt)[0], cols, replace = True)\n    X_clt = X[clt_idx]\n    for j in range(cols):\n        ax[i, j].set_xticks([])\n        ax[i, j].set_yticks([])\n        ax[i, j].set_title([f'Cluster {clt}'])\n        ax[i, j].imshow(X_clt[j])","e8c4e10b":"train_idx, val_idx = next(GroupShuffleSplit(test_size = 0.2, \n                                            n_splits = 2, \n                                            random_state = 42).split(X, groups = cluster))\n\nX_train, X_val, Y_train, Y_val = X[train_idx], X[val_idx], y[train_idx], y[val_idx]","7fd3a0aa":"BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n\ntrain_dataset = get_training_dataset(tf.data.Dataset.from_tensor_slices((X_train, Y_train)))\nval_dataset = get_validation_dataset(tf.data.Dataset.from_tensor_slices((X_val, Y_val)))","6bbf28f0":"# EfficientNet models are not applicable on Tensorflow 2.2.0 for TPU\nwith strategy.scope():\n\n    if not tpu:\n        net = tf.keras.applications.EfficientNetB0(include_top = False,\n                                                   weights = 'imagenet',\n                                                   pooling = None)\n    else:\n        net = tf.keras.applications.DenseNet121(include_top = False,\n                                                weights = 'imagenet',\n                                                pooling = None)\n\n    inp = Input(shape = (IMAGE_SIZE, IMAGE_SIZE, 3))\n    x = Conv2D(3, (3, 3), padding = 'same')(inp)\n    x = net(x)\n    x1 = GlobalAveragePooling2D()(x)\n    x2 = GlobalMaxPooling2D()(x)\n    x = Concatenate()([x1, x2])\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    x = Dense(512, activation = 'relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    out = Dense(1, activation = 'sigmoid')(x)\n\n    model = Model(inputs = inp, outputs = out)\n\n    metric = tf.keras.metrics.AUC(name = 'auc')\n    \n    model.compile(loss = 'binary_crossentropy', optimizer = 'nadam', metrics = [metric])\n\nmodel.summary()","b7b9b28d":"plot_model(model, \n           show_shapes = True, \n           show_layer_names = True, \n           rankdir = 'TB', \n           expand_nested = False, \n           dpi = 60)","4eb7cb4f":"# Cosine Annealing Learning Rate from 'https:\/\/github.com\/4uiiurz1\/keras-cosine-annealing'\nclass CosineAnnealingScheduler(Callback):\n    \"\"\"Cosine annealing scheduler.\n    \"\"\"\n\n    def __init__(self, T_max, eta_max, eta_min=0, verbose=0):\n        super(CosineAnnealingScheduler, self).__init__()\n        self.T_max = T_max\n        self.eta_max = eta_max\n        self.eta_min = eta_min\n        self.verbose = verbose\n\n    def on_epoch_begin(self, epoch, logs=None):\n        if not hasattr(self.model.optimizer, 'lr'):\n            raise ValueError('Optimizer must have a \"lr\" attribute.')\n        lr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch \/ self.T_max)) \/ 2\n        K.set_value(self.model.optimizer.lr, lr)\n        if self.verbose > 0:\n            print('\\nEpoch %05d: CosineAnnealingScheduler setting learning '\n                  'rate to %s.' % (epoch + 1, lr))\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        logs['lr'] = K.get_value(self.model.optimizer.lr)","72441c22":"EPOCHS = 50\n\n# earlystopping = EarlyStopping(monitor = 'val_auc', min_delta = 0.001, patience = 10, verbose = 1, mode = 'max', restore_best_weights = True)\n# annealer = ReduceLROnPlateau(monitor = 'val_auc', factor = 0.2, patience = 5, verbose = 1, min_lr = 1e-4, mode = 'max')\n# checkpoint = ModelCheckpoint('model.h5', monitor = 'val_auc', verbose = 0, mode = 'max', save_best_only = True)\nannealer = CosineAnnealingScheduler(EPOCHS, 1e-3, 1e-5)\n\nhist = model.fit(train_dataset,\n                 steps_per_epoch = X_train.shape[0] \/\/ BATCH_SIZE,\n                 epochs = EPOCHS,\n                 verbose = 1,\n                 callbacks = [annealer],\n                 validation_data = val_dataset)","ef3190d1":"# AUC score is better than accuracy if the classes are imbalanced\nprint('Best Validation AUC:\\t', round(max(hist.history['val_auc']), 2))\n\nauc = hist.history['auc']\nval_auc = hist.history['val_auc']\nloss = hist.history['loss']\nval_loss = hist.history['val_loss']\n\nepochs = range(len(auc))\n\nplt.figure(figsize = (7, 5))\nplt.title('Training and Validation AUC')\nplt.plot(epochs, auc, 'r', label = 'Train')\nplt.plot(epochs, val_auc, 'b', label = 'Val')\nplt.xlabel('Epochs')\nplt.ylabel('AUC')\nplt.legend(loc = 0)\nplt.show()","d2c8227d":"pred_proba = model.predict(X_val)\nauc_score = roc_auc_score(Y_val, pred_proba)\nfpr, tpr, th = roc_curve(Y_val, pred_proba)\nprint('AUC Score:\\t', round(auc_score, 2))","ede4dce3":"plt.figure(figsize = (7, 5))\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, 'r')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False Positive')\nplt.ylabel('True Positive')\nplt.legend(loc = 4)\nplt.show()","a3e2b983":"def threshold_optimisation(y_true, y_pred, thresholds):\n    best_th = thresholds[0]\n    best_acc = accuracy_score(y_true, np.where(y_pred > thresholds[0], 1, 0))\n    for th in thresholds[1:]:\n        acc = accuracy_score(y_true, np.where(y_pred > th, 1, 0))\n        if acc > best_acc:\n            best_th = th\n            best_acc = acc\n    return best_acc, best_th","6dc210da":"best_acc, best_th = threshold_optimisation(Y_val, pred_proba, th)\nprint('Best Accuracy:\\t', round(best_acc, 2))\nprint('Best Threshold:\\t', best_th)","62373e68":"! conda install -c conda-forge gdcm -y","ca017c13":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom glob import glob\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom skimage import exposure\nimport cv2\nimport warnings\nwarnings.filterwarnings('ignore')\nimport shutil \nimport tensorflow as tf\n%matplotlib inline\n\n\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nimport pprint\nimport pydicom as dicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport wandb\n\nimport PIL\nfrom PIL import Image\nfrom colorama import Fore, Back, Style\nviz_counter=0\n\ndef create_dir(dir, v=1):\n    \"\"\"\n    Creates a directory without throwing an error if directory already exists.\n    dir : The directory to be created.\n    v : Verbosity\n    \"\"\"\n    if not os.path.exists(dir):\n        os.makedirs(dir)\n        if v:\n            print(\"Created Directory : \", dir)\n        return 1\n    else:\n        if v:\n            print(\"Directory already existed : \", dir)\n        return 0\n\nvoi_lut=True\nfix_monochrome=True\n\ndef dicom_dataset_to_dict(filename):\n    \"\"\"Credit: https:\/\/github.com\/pydicom\/pydicom\/issues\/319\n               https:\/\/www.kaggle.com\/raddar\/convert-dicom-to-np-array-the-correct-way\n    \"\"\"\n    \n    dicom_header = dicom.dcmread(filename) \n    \n    #====== DICOM FILE DATA ======\n    dicom_dict = {}\n    repr(dicom_header)\n    for dicom_value in dicom_header.values():\n        if dicom_value.tag == (0x7fe0, 0x0010):\n            #discard pixel data\n            continue\n        if type(dicom_value.value) == dicom.dataset.Dataset:\n            dicom_dict[dicom_value.name] = dicom_dataset_to_dict(dicom_value.value)\n        else:\n            v = _convert_value(dicom_value.value)\n            dicom_dict[dicom_value.name] = v\n      \n    del dicom_dict['Pixel Representation']\n    \n    #====== DICOM IMAGE DATA ======\n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom_header.pixel_array, dicom_header)\n    else:\n        data = dicom_header.pixel_array\n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom_header.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    modified_image_data = (data * 255).astype(np.uint8)\n    \n    return dicom_dict, modified_image_data\n\ndef _sanitise_unicode(s):\n    return s.replace(u\"\\u0000\", \"\").strip()\n\ndef _convert_value(v):\n    t = type(v)\n    if t in (list, int, float):\n        cv = v\n    elif t == str:\n        cv = _sanitise_unicode(v)\n    elif t == bytes:\n        s = v.decode('ascii', 'replace')\n        cv = _sanitise_unicode(s)\n    elif t == dicom.valuerep.DSfloat:\n        cv = float(v)\n    elif t == dicom.valuerep.IS:\n        cv = int(v)\n    else:\n        cv = repr(v)\n    return cv\n\n\nimport os, fnmatch\ndef find(pattern, path):\n    \"\"\"Utility to find files wrt a regex search\"\"\"\n    result = []\n    for root, dirs, files in os.walk(path):\n        for name in files:\n            if fnmatch.fnmatch(name, pattern):\n                result.append(os.path.join(root, name))\n    return result\n","6590cbc5":"from keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import *\nfrom keras import backend as keras\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\n\n\ndef dice_coef(y_true, y_pred):\n    y_true_f = keras.flatten(y_true)\n    y_pred_f = keras.flatten(y_pred)\n    intersection = keras.sum(y_true_f * y_pred_f)\n    return (2. * intersection + 1) \/ (keras.sum(y_true_f) + keras.sum(y_pred_f) + 1)\n\ndef dice_coef_loss(y_true, y_pred):\n    return -dice_coef(y_true, y_pred)\n\ndef unet(input_size=(64,64,1)):\n    inputs = Input(input_size)\n    \n    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n\n    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n\n    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n\n    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n\n    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n\n    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n\n    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n\n    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n\n    return Model(inputs=[inputs], outputs=[conv10])","8a9682f2":"model = unet(input_size=(64,64,1))\nmodel.compile(optimizer=Adam(lr=1e-5), loss=dice_coef_loss,\n                  metrics=[dice_coef, 'binary_accuracy'])\nmodel.summary()","52e94229":"model_weights_path = \"\/kaggle\/input\/unet-lung-segmentation-weights-for-chest-x-rays\/cxr_reg_weights.best.hdf5\"\n\nmodel.load_weights(model_weights_path)","4da91819":"image_id = []\ndim0 = []\ndim1 = []\nsplits = []\n\nfor split in ['test', 'train']:\n    # save_dir = f'\/kaggle\/tmp\/{split}\/'\n    save_dir = f'\/kaggle\/working\/segmented_data\/{split}\/'\n    print(split)\n    os.makedirs(save_dir, exist_ok=True)\n    print(save_dir)\n    for dirname, _, filenames in tqdm(os.walk(f'..\/input\/sarscov2-ctscan-dataset')):\n        print(filenames)\n        for file in tqdm(os.walk(f'..\/input\/sarscov2-ctscan-dataset\/{split}')):\n            # set keep_ratio=True to have original aspect ratio\n            fpath = os.path.join(dirname, file)\n#             dicom_dict, modified_image_data = dicom_dataset_to_dict(fpath)\n            # props(resized_image_data)\n            modified_image_data = cv2.imread(fpath,0)\n            resized_image_data = cv2.resize(modified_image_data,(64,64)) # cv2 has this opposite\n            props(resized_image_data)\n\n            prep_unet_input_img_1 = resized_image_data.reshape(1,64,64,1)\n            prep_unet_input_img = (prep_unet_input_img_1-127.0)\/127.0\n            pred_img = model.predict(prep_unet_input_img)\n            pred_img_preprocessed_1 = np.squeeze(pred_img)\n            pred_img_preprocessed = (pred_img_preprocessed_1*255>127).astype(np.int8)\n            # props(pred_img_preprocessed)\n            # print(\"Unique Values :\",np.unique(pred_img_preprocessed))\n            res = cv2.bitwise_and(resized_image_data,resized_image_data,mask = pred_img_preprocessed)\n            save_path = os.path.join(save_dir, file.replace('png', 'png'))\n            cv2.imwrite(save_path,res)\n\n#             image_id.append(file.replace('.dcm', ''))\n            dim0.append(res.shape[0])\n            dim1.append(res.shape[1])\n            splits.append(split)\n\"\"\"\n2475\/?\n12386\/?\n07:34 | 5.38it\/s\n36:51 | 8.13it\/s\n\"\"\"\nprint(\"Generation Complete!\")","bf0403fc":"image_id = []\ndim0 = []\ndim1 = []\nsplits = []\n\nfor i in range(1):\n    # save_dir = f'\/kaggle\/tmp\/{split}\/'\n    save_dir = f'\/kaggle\/working\/segmented_data\/'\n    os.makedirs(save_dir, exist_ok=True)\n    \n    for dirname, _, filenames in tqdm(os.walk(f'..\/input\/sarscov2-ctscan-dataset\/')):\n        for file in filenames:\n            # set keep_ratio=True to have original aspect ratio\n            fpath = os.path.join(dirname, file)\n#             dicom_dict, modified_image_data = dicom_dataset_to_dict(fpath)\n            # props(resized_image_data)\n            modified_image_data = cv2.imread(fpath,0)\n            resized_image_data = cv2.resize(modified_image_data,(64,64)) # cv2 has this opposite\n            props(resized_image_data)\n\n            prep_unet_input_img_1 = resized_image_data.reshape(1,64,64,1)\n            prep_unet_input_img = (prep_unet_input_img_1-127.0)\/127.0\n            pred_img = model.predict(prep_unet_input_img)\n            pred_img_preprocessed_1 = np.squeeze(pred_img)\n            pred_img_preprocessed = (pred_img_preprocessed_1*255>127).astype(np.int8)\n            # props(pred_img_preprocessed)\n            # print(\"Unique Values :\",np.unique(pred_img_preprocessed))\n            res = cv2.bitwise_and(resized_image_data,resized_image_data,mask = pred_img_preprocessed)\n            save_path = os.path.join(save_dir, file.replace('png', 'png'))\n            cv2.imwrite(save_path,res)\n\n#             image_id.append(file.replace('.dcm', ''))\n            dim0.append(res.shape[0])\n            dim1.append(res.shape[1])\n            splits.append(split)\n\"\"\"\n2475\/?\n12386\/?\n07:34 | 5.38it\/s\n36:51 | 8.13it\/s\n\"\"\"\nprint(\"Generation Complete!\")","046a0ff6":"# Plot Images from The Same Cluster\nIt can be seen that the images from the same cluster are generally very similar to each other. They are more likely to be from the same patient. \n\nSome clusters contain multiple types of images. This means they contain images from multiple patients. Since we only want the images from the same patient to be in the same cluster, one cluster containing multiple patients does not matter.","fe0bdc58":"# Covid-19 Diagnosis on TPU with Patients Clustering using KMeans \nThis notebook shows how to use TPU to train model on this Covid-19 dataset. The KMeans algorithm is used to cluster images that may come from the same patients in order to prevent data leakage during splitting training and validation sets. \n\nThe data augmentation methods are proposed by Chris in [Rotation Augmentation GPU\/TPU - [0.96+]][1].\n\n[1]: https:\/\/www.kaggle.com\/cdeotte\/rotation-augmentation-gpu-tpu-0-96","90007f2a":"# Configurations","958a7a14":"# Threshold Optimisation","8cf8b51c":"# Plot AUC Score","c0e866f0":"# Create Training and Validation Sets","e4f440ac":"# Mixed Precision and\/or XLA","0c10b4a4":"# Create Model","a07c409a":"# Train Model","84af6fd4":"# Add Data Augmentation","8ab5d291":"# **Save till here.. deeni taravata I'm testing something..**","a8e38764":"#### ","8a7ce525":"# KMeans Clustering for patients\n\nThe raw data does not contain patient ID for each image though there are multiple images come from the same pateints. Before split the train and validation sets, we need to make sure that images from the same patients do not appear in both sets to prevent data leakage. Here I use KMeans to cluster the images have close patterns for Covid and non-Covid separately. It is worth noting that we do not need to know the exact number of patients since we only need to roughly cluster those images that have close patterns. That is, one cluster can contain multiple patients.","09d06607":"# Don't run this UNET Segmentation yahan se","26f0a3c6":"# Read Dataset"}}