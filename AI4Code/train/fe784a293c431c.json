{"cell_type":{"c12c2715":"code","2c8ff0aa":"code","8dc71205":"code","259f5bb5":"code","d04ea51e":"code","b7c8bed4":"code","f3dbf2df":"code","af71b340":"code","83ef5d5c":"code","0b7dae7b":"code","ee2955f6":"code","dd5573b3":"code","3f5503b2":"code","f9fa29b7":"code","522026e0":"code","18b31e20":"code","aca0c791":"code","b7dae0f6":"code","e1098403":"code","412cc334":"code","8deae7e3":"code","b3ce8826":"code","2cb883e3":"code","7e118a42":"code","fec260ad":"code","e57ab26a":"code","3325c015":"code","beb5d564":"code","edcea6d0":"code","c65cb9fe":"code","d5fb22c3":"code","890127ac":"code","90e3dafc":"code","ed7d8e3e":"code","e65a666f":"code","f2f49b31":"code","6d838fb4":"code","5e8ed866":"code","df711c9b":"code","422b92f6":"code","eb3b9210":"code","b78f80cb":"code","a634b9eb":"code","127fcac7":"code","a761720c":"code","45b18987":"code","0863e313":"code","39db27f8":"code","8cbf8c01":"code","7284983b":"code","ede9ced7":"code","3744a4ee":"code","2d2ee7b2":"code","5595a73d":"code","33581a48":"code","40780c79":"code","8c0bcfe7":"code","64b42e68":"code","93783833":"code","e91bbbed":"code","9854ae18":"code","e37e8bf1":"code","9743414e":"code","3dd4cf91":"code","558573a5":"code","be08228b":"code","3835a3d5":"code","dac3e933":"code","b141c4d6":"code","22ec490a":"code","2077f55b":"code","d303c2a3":"code","32aadd7e":"code","b975ff4f":"code","bd09e1a5":"code","c05b700e":"code","bfb3eb19":"code","773242d4":"code","d129393b":"code","1bfba80d":"code","689c38e3":"code","9c721f64":"code","d61a2ade":"code","fb4fee8a":"code","1714c701":"code","43b82ed5":"code","5838770e":"code","25ea8e11":"code","1d9f8739":"code","6319794b":"code","453cbf84":"code","84c30999":"code","14df4e4c":"code","c3911e36":"code","51c1caea":"code","6390b71e":"code","468fd323":"code","9d5602b6":"code","c1eed0d7":"code","7f597a10":"code","9f6e57c7":"code","42b4df54":"code","606c5b77":"code","501b5017":"code","1ce5c1f7":"code","2866e05d":"code","a71a692f":"code","d6f3a1e2":"code","84784fdd":"code","f90ec8a4":"code","63d474db":"code","81f6c001":"code","dd1f9d4a":"code","4d1c1f3c":"code","b71fd6d2":"code","d0787286":"code","86bdb323":"code","d1fae187":"code","0d26e6e9":"code","40f5c40f":"code","17055682":"code","850b3efb":"code","041ef8b4":"code","25a76e05":"code","5d7fe3a3":"code","549300b3":"code","8db7886d":"code","b6cef7b7":"code","11e2db7d":"code","e8d78884":"code","39fdacf3":"code","27fb01b6":"code","5cb13751":"code","26de1d12":"code","4ef410c3":"code","b3a83389":"code","826ddbe4":"code","015df53d":"code","44aed3b0":"code","9393a9bd":"code","aaeb504c":"code","e3dbaf92":"code","b71791e9":"code","04614d1c":"code","c1c108ff":"code","0d78bcd7":"code","d7e1f59e":"code","17835d3f":"code","db44e660":"code","b3583f08":"markdown","0b745e22":"markdown","6e36cdf8":"markdown","a9c5424a":"markdown","62b63c4e":"markdown","4bcdd733":"markdown","4bab1c9b":"markdown","7752a97c":"markdown","688a908d":"markdown","bd5243ca":"markdown","439e9d33":"markdown","7ad71fa6":"markdown","f2c105cf":"markdown","c5e1e7ff":"markdown","d5040e74":"markdown","f9265a85":"markdown","1cecffd4":"markdown","562c9156":"markdown","80a580c0":"markdown"},"source":{"c12c2715":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2c8ff0aa":"import numpy as np\nimport pandas as pd","8dc71205":"loan_data_inputs_train = pd.read_csv('\/kaggle\/input\/loan-data-prepared-files\/loan_data_inputs_train.csv', index_col = 0)\nloan_data_targets_train = pd.read_csv('\/kaggle\/input\/loan-data-prepared-files\/loan_data_targets_train.csv', index_col = 0)\nloan_data_inputs_test = pd.read_csv('\/kaggle\/input\/loan-data-prepared-files\/loan_data_inputs_test.csv', index_col = 0)\nloan_data_targets_test = pd.read_csv('\/kaggle\/input\/loan-data-prepared-files\/loan_data_targets_test.csv', index_col = 0)","259f5bb5":"loan_data_inputs_train.head()","d04ea51e":"loan_data_targets_train.head()","b7c8bed4":"loan_data_inputs_train.shape","f3dbf2df":"loan_data_targets_train.shape","af71b340":"loan_data_inputs_test.shape","83ef5d5c":"loan_data_targets_test.shape","0b7dae7b":"# Here we select a limited set of input variables in a new dataframe.\ninputs_train_with_ref_cat = loan_data_inputs_train.loc[: , ['grade:A',\n'grade:B',\n'grade:C',\n'grade:D',\n'grade:E',\n'grade:F',\n'grade:G',\n'home_ownership:RENT_OTHER_NONE_ANY',\n'home_ownership:OWN',\n'home_ownership:MORTGAGE',\n'addr_state:ND_NE_IA_NV_FL_HI_AL',\n'addr_state:NM_VA',\n'addr_state:NY',\n'addr_state:OK_TN_MO_LA_MD_NC',\n'addr_state:CA',\n'addr_state:UT_KY_AZ_NJ',\n'addr_state:AR_MI_PA_OH_MN',\n'addr_state:RI_MA_DE_SD_IN',\n'addr_state:GA_WA_OR',\n'addr_state:WI_MT',\n'addr_state:TX',\n'addr_state:IL_CT',\n'addr_state:KS_SC_CO_VT_AK_MS',\n'addr_state:WV_NH_WY_DC_ME_ID',\n'verification_status:Not Verified',\n'verification_status:Source Verified',\n'verification_status:Verified',\n'purpose:educ__sm_b__wedd__ren_en__mov__house',\n'purpose:credit_card',\n'purpose:debt_consolidation',\n'purpose:oth__med__vacation',\n'purpose:major_purch__car__home_impr',\n'initial_list_status:f',\n'initial_list_status:w',\n'term:36',\n'term:60',\n'emp_length:0',\n'emp_length:1',\n'emp_length:2-4',\n'emp_length:5-6',\n'emp_length:7-9',\n'emp_length:10',\n'mths_since_issue_d:<38',\n'mths_since_issue_d:38-39',\n'mths_since_issue_d:40-41',\n'mths_since_issue_d:42-48',\n'mths_since_issue_d:49-52',\n'mths_since_issue_d:53-64',\n'mths_since_issue_d:65-84',\n'mths_since_issue_d:>84',\n'int_rate:<9.548',\n'int_rate:9.548-12.025',\n'int_rate:12.025-15.74',\n'int_rate:15.74-20.281',\n'int_rate:>20.281',\n'mths_since_earliest_cr_line:<140',\n'mths_since_earliest_cr_line:141-164',\n'mths_since_earliest_cr_line:165-247',\n'mths_since_earliest_cr_line:248-270',\n'mths_since_earliest_cr_line:271-352',\n'mths_since_earliest_cr_line:>352',\n'delinq_2yrs:0',\n'delinq_2yrs:1-3',\n'delinq_2yrs:>=4',\n'inq_last_6mths:0',\n'inq_last_6mths:1-2',\n'inq_last_6mths:3-6',\n'inq_last_6mths:>6',\n'open_acc:0',\n'open_acc:1-3',\n'open_acc:4-12',\n'open_acc:13-17',\n'open_acc:18-22',\n'open_acc:23-25',\n'open_acc:26-30',\n'open_acc:>=31',\n'pub_rec:0-2',\n'pub_rec:3-4',\n'pub_rec:>=5',\n'total_acc:<=27',\n'total_acc:28-51',\n'total_acc:>=52',\n'acc_now_delinq:0',\n'acc_now_delinq:>=1',\n'total_rev_hi_lim:<=5K',\n'total_rev_hi_lim:5K-10K',\n'total_rev_hi_lim:10K-20K',\n'total_rev_hi_lim:20K-30K',\n'total_rev_hi_lim:30K-40K',\n'total_rev_hi_lim:40K-55K',\n'total_rev_hi_lim:55K-95K',\n'total_rev_hi_lim:>95K',\n'annual_inc:<20K',\n'annual_inc:20K-30K',\n'annual_inc:30K-40K',\n'annual_inc:40K-50K',\n'annual_inc:50K-60K',\n'annual_inc:60K-70K',\n'annual_inc:70K-80K',\n'annual_inc:80K-90K',\n'annual_inc:90K-100K',\n'annual_inc:100K-120K',\n'annual_inc:120K-140K',\n'annual_inc:>140K',\n'dti:<=1.4',\n'dti:1.4-3.5',\n'dti:3.5-7.7',\n'dti:7.7-10.5',\n'dti:10.5-16.1',\n'dti:16.1-20.3',\n'dti:20.3-21.7',\n'dti:21.7-22.4',\n'dti:22.4-35',\n'dti:>35',\n'mths_since_last_delinq:Missing',\n'mths_since_last_delinq:0-3',\n'mths_since_last_delinq:4-30',\n'mths_since_last_delinq:31-56',\n'mths_since_last_delinq:>=57',\n'mths_since_last_record:Missing',\n'mths_since_last_record:0-2',\n'mths_since_last_record:3-20',\n'mths_since_last_record:21-31',\n'mths_since_last_record:32-80',\n'mths_since_last_record:81-86',\n]]","ee2955f6":"# Here we store the names of the reference category dummy variables in a list.\nref_categories = ['grade:G',\n'home_ownership:RENT_OTHER_NONE_ANY',\n'addr_state:ND_NE_IA_NV_FL_HI_AL',\n'verification_status:Verified',\n'purpose:educ__sm_b__wedd__ren_en__mov__house',\n'initial_list_status:f',\n'term:60',\n'emp_length:0',\n'mths_since_issue_d:>84',\n'int_rate:>20.281',\n'mths_since_earliest_cr_line:<140',\n'delinq_2yrs:>=4',\n'inq_last_6mths:>6',\n'open_acc:0',\n'pub_rec:0-2',\n'total_acc:<=27',\n'acc_now_delinq:0',\n'total_rev_hi_lim:<=5K',\n'annual_inc:<20K',\n'dti:>35',\n'mths_since_last_delinq:0-3',\n'mths_since_last_record:0-2']","dd5573b3":"inputs_train = inputs_train_with_ref_cat.drop(ref_categories, axis = 1)\n# From the dataframe with input variables, we drop the variables with variable names in the list with reference categories. \ninputs_train.head()","3f5503b2":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics","f9fa29b7":"reg = LogisticRegression()\n# We create an instance of an object from the 'LogisticRegression' class.","522026e0":"pd.options.display.max_rows = None\n# Sets the pandas dataframe options to display all columns\/ rows.","18b31e20":"reg.fit(inputs_train, loan_data_targets_train)\n# Estimates the coefficients of the object from the 'LogisticRegression' class\n# with inputs (independent variables) contained in the first dataframe\n# and targets (dependent variables) contained in the second dataframe.","aca0c791":"reg.intercept_\n# Displays the intercept contain in the estimated (\"fitted\") object from the 'LogisticRegression' class.","b7dae0f6":"reg.coef_\n# Displays the coefficients contained in the estimated (\"fitted\") object from the 'LogisticRegression' class.","e1098403":"feature_name = inputs_train.columns.values\n# Stores the names of the columns of a dataframe in a variable.","412cc334":"summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n# Creates a dataframe with a column titled 'Feature name' and row values contained in the 'feature_name' variable.\nsummary_table['Coefficients'] = np.transpose(reg.coef_)\n# Creates a new column in the dataframe, called 'Coefficients',\n# with row values the transposed coefficients from the 'LogisticRegression' object.\nsummary_table.index = summary_table.index + 1\n# Increases the index of every row of the dataframe with 1.\nsummary_table.loc[0] = ['Intercept', reg.intercept_[0]]\n# Assigns values of the row with index 0 of the dataframe.\nsummary_table = summary_table.sort_index()\n# Sorts the dataframe by index.\nsummary_table","8deae7e3":"# P values for sklearn logistic regression.\n\n# Class to display p-values for logistic regression in sklearn.\n\nfrom sklearn import linear_model\nimport scipy.stats as stat\n\nclass LogisticRegression_with_p_values:\n    \n    def __init__(self,*args,**kwargs):#,**kwargs):\n        self.model = linear_model.LogisticRegression(*args,**kwargs)#,**args)\n\n    def fit(self,X,y):\n        self.model.fit(X,y)\n        \n        #### Get p-values for the fitted model ####\n        denom = (2.0 * (1.0 + np.cosh(self.model.decision_function(X))))\n        denom = np.tile(denom,(X.shape[1],1)).T\n        F_ij = np.dot((X \/ denom).T,X) ## Fisher Information Matrix\n        Cramer_Rao = np.linalg.inv(F_ij) ## Inverse Information Matrix\n        sigma_estimates = np.sqrt(np.diagonal(Cramer_Rao))\n        z_scores = self.model.coef_[0] \/ sigma_estimates # z-score for eaach model coefficient\n        p_values = [stat.norm.sf(abs(x)) * 2 for x in z_scores] ### two tailed test for p-values\n        \n        self.coef_ = self.model.coef_\n        self.intercept_ = self.model.intercept_\n        #self.z_scores = z_scores\n        self.p_values = p_values\n        #self.sigma_estimates = sigma_estimates\n        #self.F_ij = F_ij","b3ce8826":"from sklearn import linear_model\nimport scipy.stats as stat\n\nclass LogisticRegression_with_p_values:\n    \n    def __init__(self,*args,**kwargs):\n        self.model = linear_model.LogisticRegression(*args,**kwargs)\n\n    def fit(self,X,y):\n        self.model.fit(X,y)\n        denom = (2.0 * (1.0 + np.cosh(self.model.decision_function(X))))\n        denom = np.tile(denom,(X.shape[1],1)).T\n        F_ij = np.dot((X \/ denom).T,X)\n        Cramer_Rao = np.linalg.inv(F_ij)\n        sigma_estimates = np.sqrt(np.diagonal(Cramer_Rao))\n        z_scores = self.model.coef_[0] \/ sigma_estimates\n        p_values = [stat.norm.sf(abs(x)) * 2 for x in z_scores]\n        self.coef_ = self.model.coef_\n        self.intercept_ = self.model.intercept_\n        self.p_values = p_values","2cb883e3":"reg = LogisticRegression_with_p_values()\n# We create an instance of an object from the newly created 'LogisticRegression_with_p_values()' class.","7e118a42":"reg.fit(inputs_train, loan_data_targets_train)\n# Estimates the coefficients of the object from the 'LogisticRegression' class\n# with inputs (independent variables) contained in the first dataframe\n# and targets (dependent variables) contained in the second dataframe.","fec260ad":"# Same as above.\nsummary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\nsummary_table['Coefficients'] = np.transpose(reg.coef_)\nsummary_table.index = summary_table.index + 1\nsummary_table.loc[0] = ['Intercept', reg.intercept_[0]]\nsummary_table = summary_table.sort_index()\nsummary_table","e57ab26a":"# This is a list.\np_values = reg.p_values\n# We take the result of the newly added method 'p_values' and store it in a variable 'p_values'.","3325c015":"# Add the intercept for completeness.\np_values = np.append(np.nan, np.array(p_values))\n# We add the value 'NaN' in the beginning of the variable with p-values.","beb5d564":"summary_table['p_values'] = p_values\n# In the 'summary_table' dataframe, we add a new column, called 'p_values', containing the values from the 'p_values' variable.","edcea6d0":"summary_table","c65cb9fe":"# We are going to remove some features, the coefficients for all or almost all of the dummy variables for which,\n# are not tatistically significant.\n\n# We do that by specifying another list of dummy variables as reference categories, and a list of variables to remove.\n# Then, we are going to drop the two datasets from the original list of dummy variables.\n\n# Variables\ninputs_train_with_ref_cat = loan_data_inputs_train.loc[: , ['grade:A',\n'grade:B',\n'grade:C',\n'grade:D',\n'grade:E',\n'grade:F',\n'grade:G',\n'home_ownership:RENT_OTHER_NONE_ANY',\n'home_ownership:OWN',\n'home_ownership:MORTGAGE',\n'addr_state:ND_NE_IA_NV_FL_HI_AL',\n'addr_state:NM_VA',\n'addr_state:NY',\n'addr_state:OK_TN_MO_LA_MD_NC',\n'addr_state:CA',\n'addr_state:UT_KY_AZ_NJ',\n'addr_state:AR_MI_PA_OH_MN',\n'addr_state:RI_MA_DE_SD_IN',\n'addr_state:GA_WA_OR',\n'addr_state:WI_MT',\n'addr_state:TX',\n'addr_state:IL_CT',\n'addr_state:KS_SC_CO_VT_AK_MS',\n'addr_state:WV_NH_WY_DC_ME_ID',\n'verification_status:Not Verified',\n'verification_status:Source Verified',\n'verification_status:Verified',\n'purpose:educ__sm_b__wedd__ren_en__mov__house',\n'purpose:credit_card',\n'purpose:debt_consolidation',\n'purpose:oth__med__vacation',\n'purpose:major_purch__car__home_impr',\n'initial_list_status:f',\n'initial_list_status:w',\n'term:36',\n'term:60',\n'emp_length:0',\n'emp_length:1',\n'emp_length:2-4',\n'emp_length:5-6',\n'emp_length:7-9',\n'emp_length:10',\n'mths_since_issue_d:<38',\n'mths_since_issue_d:38-39',\n'mths_since_issue_d:40-41',\n'mths_since_issue_d:42-48',\n'mths_since_issue_d:49-52',\n'mths_since_issue_d:53-64',\n'mths_since_issue_d:65-84',\n'mths_since_issue_d:>84',\n'int_rate:<9.548',\n'int_rate:9.548-12.025',\n'int_rate:12.025-15.74',\n'int_rate:15.74-20.281',\n'int_rate:>20.281',\n'mths_since_earliest_cr_line:<140',\n'mths_since_earliest_cr_line:141-164',\n'mths_since_earliest_cr_line:165-247',\n'mths_since_earliest_cr_line:248-270',\n'mths_since_earliest_cr_line:271-352',\n'mths_since_earliest_cr_line:>352',\n'inq_last_6mths:0',\n'inq_last_6mths:1-2',\n'inq_last_6mths:3-6',\n'inq_last_6mths:>6',\n'acc_now_delinq:0',\n'acc_now_delinq:>=1',\n'annual_inc:<20K',\n'annual_inc:20K-30K',\n'annual_inc:30K-40K',\n'annual_inc:40K-50K',\n'annual_inc:50K-60K',\n'annual_inc:60K-70K',\n'annual_inc:70K-80K',\n'annual_inc:80K-90K',\n'annual_inc:90K-100K',\n'annual_inc:100K-120K',\n'annual_inc:120K-140K',\n'annual_inc:>140K',\n'dti:<=1.4',\n'dti:1.4-3.5',\n'dti:3.5-7.7',\n'dti:7.7-10.5',\n'dti:10.5-16.1',\n'dti:16.1-20.3',\n'dti:20.3-21.7',\n'dti:21.7-22.4',\n'dti:22.4-35',\n'dti:>35',\n'mths_since_last_delinq:Missing',\n'mths_since_last_delinq:0-3',\n'mths_since_last_delinq:4-30',\n'mths_since_last_delinq:31-56',\n'mths_since_last_delinq:>=57',\n'mths_since_last_record:Missing',\n'mths_since_last_record:0-2',\n'mths_since_last_record:3-20',\n'mths_since_last_record:21-31',\n'mths_since_last_record:32-80',\n'mths_since_last_record:81-86',\n]]\n\nref_categories = ['grade:G',\n'home_ownership:RENT_OTHER_NONE_ANY',\n'addr_state:ND_NE_IA_NV_FL_HI_AL',\n'verification_status:Verified',\n'purpose:educ__sm_b__wedd__ren_en__mov__house',\n'initial_list_status:f',\n'term:60',\n'emp_length:0',\n'mths_since_issue_d:>84',\n'int_rate:>20.281',\n'mths_since_earliest_cr_line:<140',\n'inq_last_6mths:>6',\n'acc_now_delinq:0',\n'annual_inc:<20K',\n'dti:>35',\n'mths_since_last_delinq:0-3',\n'mths_since_last_record:0-2']","d5fb22c3":"inputs_train = inputs_train_with_ref_cat.drop(ref_categories, axis = 1)\ninputs_train.head()","890127ac":"# Here we run a new model.\nreg2 = LogisticRegression_with_p_values()\nreg2.fit(inputs_train, loan_data_targets_train)","90e3dafc":"feature_name = inputs_train.columns.values","ed7d8e3e":"# Same as above.\nsummary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\nsummary_table['Coefficients'] = np.transpose(reg2.coef_)\nsummary_table.index = summary_table.index + 1\nsummary_table.loc[0] = ['Intercept', reg2.intercept_[0]]\nsummary_table = summary_table.sort_index()\nsummary_table","e65a666f":"# We add the 'p_values' here, just as we did before.\np_values = reg2.p_values\np_values = np.append(np.nan,np.array(p_values))\nsummary_table['p_values'] = p_values\nsummary_table\n# Here we get the results for our final PD model.","f2f49b31":"import pickle","6d838fb4":"pickle.dump(reg2, open('pd_model.sav', 'wb'))\n# Here we export our model to a 'SAV' file with file name 'pd_model.sav'.","5e8ed866":"# Here, from the dataframe with inputs for testing, we keep the same variables that we used in our final PD model.\ninputs_test_with_ref_cat = loan_data_inputs_test.loc[: , ['grade:A',\n'grade:B',\n'grade:C',\n'grade:D',\n'grade:E',\n'grade:F',\n'grade:G',\n'home_ownership:RENT_OTHER_NONE_ANY',\n'home_ownership:OWN',\n'home_ownership:MORTGAGE',\n'addr_state:ND_NE_IA_NV_FL_HI_AL',\n'addr_state:NM_VA',\n'addr_state:NY',\n'addr_state:OK_TN_MO_LA_MD_NC',\n'addr_state:CA',\n'addr_state:UT_KY_AZ_NJ',\n'addr_state:AR_MI_PA_OH_MN',\n'addr_state:RI_MA_DE_SD_IN',\n'addr_state:GA_WA_OR',\n'addr_state:WI_MT',\n'addr_state:TX',\n'addr_state:IL_CT',\n'addr_state:KS_SC_CO_VT_AK_MS',\n'addr_state:WV_NH_WY_DC_ME_ID',\n'verification_status:Not Verified',\n'verification_status:Source Verified',\n'verification_status:Verified',\n'purpose:educ__sm_b__wedd__ren_en__mov__house',\n'purpose:credit_card',\n'purpose:debt_consolidation',\n'purpose:oth__med__vacation',\n'purpose:major_purch__car__home_impr',\n'initial_list_status:f',\n'initial_list_status:w',\n'term:36',\n'term:60',\n'emp_length:0',\n'emp_length:1',\n'emp_length:2-4',\n'emp_length:5-6',\n'emp_length:7-9',\n'emp_length:10',\n'mths_since_issue_d:<38',\n'mths_since_issue_d:38-39',\n'mths_since_issue_d:40-41',\n'mths_since_issue_d:42-48',\n'mths_since_issue_d:49-52',\n'mths_since_issue_d:53-64',\n'mths_since_issue_d:65-84',\n'mths_since_issue_d:>84',\n'int_rate:<9.548',\n'int_rate:9.548-12.025',\n'int_rate:12.025-15.74',\n'int_rate:15.74-20.281',\n'int_rate:>20.281',\n'mths_since_earliest_cr_line:<140',\n'mths_since_earliest_cr_line:141-164',\n'mths_since_earliest_cr_line:165-247',\n'mths_since_earliest_cr_line:248-270',\n'mths_since_earliest_cr_line:271-352',\n'mths_since_earliest_cr_line:>352',\n'inq_last_6mths:0',\n'inq_last_6mths:1-2',\n'inq_last_6mths:3-6',\n'inq_last_6mths:>6',\n'acc_now_delinq:0',\n'acc_now_delinq:>=1',\n'annual_inc:<20K',\n'annual_inc:20K-30K',\n'annual_inc:30K-40K',\n'annual_inc:40K-50K',\n'annual_inc:50K-60K',\n'annual_inc:60K-70K',\n'annual_inc:70K-80K',\n'annual_inc:80K-90K',\n'annual_inc:90K-100K',\n'annual_inc:100K-120K',\n'annual_inc:120K-140K',\n'annual_inc:>140K',\n'dti:<=1.4',\n'dti:1.4-3.5',\n'dti:3.5-7.7',\n'dti:7.7-10.5',\n'dti:10.5-16.1',\n'dti:16.1-20.3',\n'dti:20.3-21.7',\n'dti:21.7-22.4',\n'dti:22.4-35',\n'dti:>35',\n'mths_since_last_delinq:Missing',\n'mths_since_last_delinq:0-3',\n'mths_since_last_delinq:4-30',\n'mths_since_last_delinq:31-56',\n'mths_since_last_delinq:>=57',\n'mths_since_last_record:Missing',\n'mths_since_last_record:0-2',\n'mths_since_last_record:3-20',\n'mths_since_last_record:21-31',\n'mths_since_last_record:32-80',\n'mths_since_last_record:81-86',\n]]","df711c9b":"# And here, in the list below, we keep the variable names for the reference categories,\n# only for the variables we used in our final PD model.\nref_categories = ['grade:G',\n'home_ownership:RENT_OTHER_NONE_ANY',\n'addr_state:ND_NE_IA_NV_FL_HI_AL',\n'verification_status:Verified',\n'purpose:educ__sm_b__wedd__ren_en__mov__house',\n'initial_list_status:f',\n'term:60',\n'emp_length:0',\n'mths_since_issue_d:>84',\n'int_rate:>20.281',\n'mths_since_earliest_cr_line:<140',\n'inq_last_6mths:>6',\n'acc_now_delinq:0',\n'annual_inc:<20K',\n'dti:>35',\n'mths_since_last_delinq:0-3',\n'mths_since_last_record:0-2']","422b92f6":"inputs_test = inputs_test_with_ref_cat.drop(ref_categories, axis = 1)\ninputs_test.head()","eb3b9210":"y_hat_test = reg2.model.predict(inputs_test)\n# Calculates the predicted values for the dependent variable (targets)\n# based on the values of the independent variables (inputs) supplied as an argument.","b78f80cb":"y_hat_test\n# This is an array of predicted discrete classess (in this case, 0s and 1s).","a634b9eb":"y_hat_test_proba = reg2.model.predict_proba(inputs_test)\n# Calculates the predicted probability values for the dependent variable (targets)\n# based on the values of the independent variables (inputs) supplied as an argument.","127fcac7":"y_hat_test_proba\n# This is an array of arrays of predicted class probabilities for all classes.\n# In this case, the first value of every sub-array is the probability for the observation to belong to the first class, i.e. 0,\n# and the second value is the probability for the observation to belong to the first class, i.e. 1.","a761720c":"y_hat_test_proba[:][:,1]\n# Here we take all the arrays in the array, and from each array, we take all rows, and only the element with index 1,\n# that is, the second element.\n# In other words, we take only the probabilities for being 1.","45b18987":"y_hat_test_proba = y_hat_test_proba[: ][: , 1]\n# We store these probabilities in a variable.","0863e313":"y_hat_test_proba\n# This variable contains an array of probabilities of being 1.","39db27f8":"loan_data_targets_test_temp = loan_data_targets_test","8cbf8c01":"loan_data_targets_test_temp.reset_index(drop = True, inplace = True)\n# We reset the index of a dataframe.","7284983b":"df_actual_predicted_probs = pd.concat([loan_data_targets_test_temp, pd.DataFrame(y_hat_test_proba)], axis = 1)\n# Concatenates two dataframes.","ede9ced7":"df_actual_predicted_probs.shape","3744a4ee":"df_actual_predicted_probs.columns = ['loan_data_targets_test', 'y_hat_test_proba']","2d2ee7b2":"df_actual_predicted_probs.index = loan_data_inputs_test.index\n# Makes the index of one dataframe equal to the index of another dataframe.","5595a73d":"df_actual_predicted_probs.head()","33581a48":"tr = 0.9\n# We create a new column with an indicator,\n# where every observation that has predicted probability greater than the threshold has a value of 1,\n# and every observation that has predicted probability lower than the threshold has a value of 0.\ndf_actual_predicted_probs['y_hat_test'] = np.where(df_actual_predicted_probs['y_hat_test_proba'] > tr, 1, 0)","40780c79":"pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted'])\n# Creates a cross-table where the actual values are displayed by rows and the predicted values by columns.\n# This table is known as a Confusion Matrix.","8c0bcfe7":"pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) \/ df_actual_predicted_probs.shape[0]\n# Here we divide each value of the table by the total number of observations,\n# thus getting percentages, or, rates.","64b42e68":"(pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) \/ df_actual_predicted_probs.shape[0]).iloc[0, 0] + (pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) \/ df_actual_predicted_probs.shape[0]).iloc[1, 1]\n# Here we calculate Accuracy of the model, which is the sum of the diagonal rates.","93783833":"from sklearn.metrics import roc_curve, roc_auc_score","e91bbbed":"roc_curve(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])\n# Returns the Receiver Operating Characteristic (ROC) Curve from a set of actual values and their predicted probabilities.\n# As a result, we get three arrays: the false positive rates, the true positive rates, and the thresholds.","9854ae18":"fpr, tpr, thresholds = roc_curve(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])\n# Here we store each of the three arrays in a separate variable. ","e37e8bf1":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","9743414e":"plt.plot(fpr, tpr)\n# We plot the false positive rate along the x-axis and the true positive rate along the y-axis,\n# thus plotting the ROC curve.\nplt.plot(fpr, fpr, linestyle = '--', color = 'k')\n# We plot a seconary diagonal line, with dashed line style and black color.\nplt.xlabel('False positive rate')\n# We name the x-axis \"False positive rate\".\nplt.ylabel('True positive rate')\n# We name the x-axis \"True positive rate\".\nplt.title('ROC curve')\n# We name the graph \"ROC curve\".","3dd4cf91":"AUROC = roc_auc_score(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])\n# Calculates the Area Under the Receiver Operating Characteristic Curve (AUROC)\n# from a set of actual values and their predicted probabilities.\nAUROC","558573a5":"df_actual_predicted_probs = df_actual_predicted_probs.sort_values('y_hat_test_proba')\n# Sorts a dataframe by the values of a specific column.","be08228b":"df_actual_predicted_probs.head()","3835a3d5":"df_actual_predicted_probs.tail()","dac3e933":"df_actual_predicted_probs = df_actual_predicted_probs.reset_index()\n# We reset the index of a dataframe and overwrite it.","b141c4d6":"df_actual_predicted_probs.head()","22ec490a":"df_actual_predicted_probs['Cumulative N Population'] = df_actual_predicted_probs.index + 1\n# We calculate the cumulative number of all observations.\n# We use the new index for that. Since indexing in ython starts from 0, we add 1 to each index.\ndf_actual_predicted_probs['Cumulative N Good'] = df_actual_predicted_probs['loan_data_targets_test'].cumsum()\n# We calculate cumulative number of 'good', which is the cumulative sum of the column with actual observations.\ndf_actual_predicted_probs['Cumulative N Bad'] = df_actual_predicted_probs['Cumulative N Population'] - df_actual_predicted_probs['loan_data_targets_test'].cumsum()\n# We calculate cumulative number of 'bad', which is\n# the difference between the cumulative number of all observations and cumulative number of 'good' for each row.","2077f55b":"df_actual_predicted_probs.head()","d303c2a3":"df_actual_predicted_probs['Cumulative Perc Population'] = df_actual_predicted_probs['Cumulative N Population'] \/ (df_actual_predicted_probs.shape[0])\n# We calculate the cumulative percentage of all observations.\ndf_actual_predicted_probs['Cumulative Perc Good'] = df_actual_predicted_probs['Cumulative N Good'] \/ df_actual_predicted_probs['loan_data_targets_test'].sum()\n# We calculate cumulative percentage of 'good'.\ndf_actual_predicted_probs['Cumulative Perc Bad'] = df_actual_predicted_probs['Cumulative N Bad'] \/ (df_actual_predicted_probs.shape[0] - df_actual_predicted_probs['loan_data_targets_test'].sum())\n# We calculate the cumulative percentage of 'bad'.","32aadd7e":"df_actual_predicted_probs.head()","b975ff4f":"df_actual_predicted_probs.tail()","bd09e1a5":"# Plot Gini\nplt.plot(df_actual_predicted_probs['Cumulative Perc Population'], df_actual_predicted_probs['Cumulative Perc Bad'])\n# We plot the cumulative percentage of all along the x-axis and the cumulative percentage 'good' along the y-axis,\n# thus plotting the Gini curve.\nplt.plot(df_actual_predicted_probs['Cumulative Perc Population'], df_actual_predicted_probs['Cumulative Perc Population'], linestyle = '--', color = 'k')\n# We plot a seconary diagonal line, with dashed line style and black color.\nplt.xlabel('Cumulative % Population')\n# We name the x-axis \"Cumulative % Population\".\nplt.ylabel('Cumulative % Bad')\n# We name the y-axis \"Cumulative % Bad\".\nplt.title('Gini')\n# We name the graph \"Gini\".","c05b700e":"Gini = AUROC * 2 - 1\n# Here we calculate Gini from AUROC.\nGini","bfb3eb19":"# Plot KS\nplt.plot(df_actual_predicted_probs['y_hat_test_proba'], df_actual_predicted_probs['Cumulative Perc Bad'], color = 'r')\n# We plot the predicted (estimated) probabilities along the x-axis and the cumulative percentage 'bad' along the y-axis,\n# colored in red.\nplt.plot(df_actual_predicted_probs['y_hat_test_proba'], df_actual_predicted_probs['Cumulative Perc Good'], color = 'b')\n# We plot the predicted (estimated) probabilities along the x-axis and the cumulative percentage 'good' along the y-axis,\n# colored in red.\nplt.xlabel('Estimated Probability for being Good')\n# We name the x-axis \"Estimated Probability for being Good\".\nplt.ylabel('Cumulative %')\n# We name the y-axis \"Cumulative %\".\nplt.title('Kolmogorov-Smirnov')\n# We name the graph \"Kolmogorov-Smirnov\".","773242d4":"KS = max(df_actual_predicted_probs['Cumulative Perc Bad'] - df_actual_predicted_probs['Cumulative Perc Good'])\n# We calculate KS from the data. It is the maximum of the difference between the cumulative percentage of 'bad'\n# and the cumulative percentage of 'good'.\nKS","d129393b":"pd.options.display.max_columns = None\n# Sets the pandas dataframe options to display all columns\/ rows.","1bfba80d":"inputs_test_with_ref_cat.head()","689c38e3":"summary_table","9c721f64":"y_hat_test_proba","d61a2ade":"summary_table","fb4fee8a":"ref_categories","1714c701":"df_ref_categories = pd.DataFrame(ref_categories, columns = ['Feature name'])\n# We create a new dataframe with one column. Its values are the values from the 'reference_categories' list.\n# We name it 'Feature name'.\ndf_ref_categories['Coefficients'] = 0\n# We create a second column, called 'Coefficients', which contains only 0 values.\ndf_ref_categories['p_values'] = np.nan\n# We create a third column, called 'p_values', with contains only NaN values.\ndf_ref_categories","43b82ed5":"df_scorecard = pd.concat([summary_table, df_ref_categories])\n# Concatenates two dataframes.\ndf_scorecard = df_scorecard.reset_index()\n# We reset the index of a dataframe.\ndf_scorecard","5838770e":"df_scorecard['Original feature name'] = df_scorecard['Feature name'].str.split(':').str[0]\n# We create a new column, called 'Original feature name', which contains the value of the 'Feature name' column,\n# up to the column symbol.\ndf_scorecard","25ea8e11":"min_score = 300\nmax_score = 850","1d9f8739":"df_scorecard.groupby('Original feature name')['Coefficients'].min()\n# Groups the data by the values of the 'Original feature name' column.\n# Aggregates the data in the 'Coefficients' column, calculating their minimum.","6319794b":"min_sum_coef = df_scorecard.groupby('Original feature name')['Coefficients'].min().sum()\n# Up to the 'min()' method everything is the same as in te line above.\n# Then, we aggregate further and sum all the minimum values.\nmin_sum_coef","453cbf84":"\ndf_scorecard.groupby('Original feature name')['Coefficients'].max()\n# Groups the data by the values of the 'Original feature name' column.\n# Aggregates the data in the 'Coefficients' column, calculating their maximum.","84c30999":"max_sum_coef = df_scorecard.groupby('Original feature name')['Coefficients'].max().sum()\n# Up to the 'min()' method everything is the same as in te line above.\n# Then, we aggregate further and sum all the maximum values.\nmax_sum_coef","14df4e4c":"df_scorecard['Score - Calculation'] = df_scorecard['Coefficients'] * (max_score - min_score) \/ (max_sum_coef - min_sum_coef)\n# We multiply the value of the 'Coefficients' column by the ration of the differences between\n# maximum score and minimum score and maximum sum of coefficients and minimum sum of cefficients.\ndf_scorecard","c3911e36":"df_scorecard['Score - Calculation'][0] = ((df_scorecard['Coefficients'][0] - min_sum_coef) \/ (max_sum_coef - min_sum_coef)) * (max_score - min_score) + min_score\n# We divide the difference of the value of the 'Coefficients' column and the minimum sum of coefficients by\n# the difference of the maximum sum of coefficients and the minimum sum of coefficients.\n# Then, we multiply that by the difference between the maximum score and the minimum score.\n# Then, we add minimum score. \ndf_scorecard","51c1caea":"df_scorecard['Score - Preliminary'] = df_scorecard['Score - Calculation'].round()\n# We round the values of the 'Score - Calculation' column.\ndf_scorecard","6390b71e":"min_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Preliminary'].min().sum()\n# Groups the data by the values of the 'Original feature name' column.\n# Aggregates the data in the 'Coefficients' column, calculating their minimum.\n# Sums all minimum values.\nmin_sum_score_prel","468fd323":"max_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Preliminary'].max().sum()\n# Groups the data by the values of the 'Original feature name' column.\n# Aggregates the data in the 'Coefficients' column, calculating their maximum.\n# Sums all maximum values.\nmax_sum_score_prel","9d5602b6":"# One has to be subtracted from the maximum score for one original variable. Which one? We'll evaluate based on differences.","c1eed0d7":"df_scorecard['Difference'] = df_scorecard['Score - Preliminary'] - df_scorecard['Score - Calculation']\ndf_scorecard","7f597a10":"df_scorecard['Score - Final'] = df_scorecard['Score - Preliminary']\ndf_scorecard['Score - Final'][77] = 16\ndf_scorecard","9f6e57c7":"min_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Final'].min().sum()\n# Groups the data by the values of the 'Original feature name' column.\n# Aggregates the data in the 'Coefficients' column, calculating their minimum.\n# Sums all minimum values.\nmin_sum_score_prel","42b4df54":"max_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Final'].max().sum()\n# Groups the data by the values of the 'Original feature name' column.\n# Aggregates the data in the 'Coefficients' column, calculating their maximum.\n# Sums all maximum values.\nmax_sum_score_prel","606c5b77":"inputs_test_with_ref_cat.head()","501b5017":"df_scorecard","1ce5c1f7":"inputs_test_with_ref_cat_w_intercept = inputs_test_with_ref_cat","2866e05d":"inputs_test_with_ref_cat_w_intercept.insert(0, 'Intercept', 1)\n# We insert a column in the dataframe, with an index of 0, that is, in the beginning of the dataframe.\n# The name of that column is 'Intercept', and its values are 1s.","a71a692f":"inputs_test_with_ref_cat_w_intercept.head()","d6f3a1e2":"inputs_test_with_ref_cat_w_intercept = inputs_test_with_ref_cat_w_intercept[df_scorecard['Feature name'].values]\n# Here, from the 'inputs_test_with_ref_cat_w_intercept' dataframe, we keep only the columns with column names,\n# exactly equal to the row values of the 'Feature name' column from the 'df_scorecard' dataframe.","84784fdd":"inputs_test_with_ref_cat_w_intercept.head()","f90ec8a4":"scorecard_scores = df_scorecard['Score - Final']","63d474db":"inputs_test_with_ref_cat_w_intercept.shape","81f6c001":"scorecard_scores.shape","dd1f9d4a":"scorecard_scores = scorecard_scores.values.reshape(101, 1)","4d1c1f3c":"scorecard_scores.shape","b71fd6d2":"y_scores = inputs_test_with_ref_cat_w_intercept.dot(scorecard_scores)\n# Here we multiply the values of each row of the dataframe by the values of each column of the variable,\n# which is an argument of the 'dot' method, and sum them. It's essentially the sum of the products.","d0787286":"y_scores.head()","86bdb323":"y_scores.tail()","d1fae187":"sum_coef_from_score = ((y_scores - min_score) \/ (max_score - min_score)) * (max_sum_coef - min_sum_coef) + min_sum_coef\n# We divide the difference between the scores and the minimum score by\n# the difference between the maximum score and the minimum score.\n# Then, we multiply that by the difference between the maximum sum of coefficients and the minimum sum of coefficients.\n# Then, we add the minimum sum of coefficients.","0d26e6e9":"y_hat_proba_from_score = np.exp(sum_coef_from_score) \/ (np.exp(sum_coef_from_score) + 1)\n# Here we divide an exponent raised to sum of coefficients from score by\n# an exponent raised to sum of coefficients from score plus one.\ny_hat_proba_from_score.head()","40f5c40f":"y_hat_test_proba[0: 5]","17055682":"df_actual_predicted_probs['y_hat_test_proba'].head()","850b3efb":"# We need the confusion matrix again.\n#np.where(np.squeeze(np.array(loan_data_targets_test)) == np.where(y_hat_test_proba >= tr, 1, 0), 1, 0).sum() \/ loan_data_targets_test.shape[0]\ntr = 0.9\ndf_actual_predicted_probs['y_hat_test'] = np.where(df_actual_predicted_probs['y_hat_test_proba'] > tr, 1, 0)\n#df_actual_predicted_probs['loan_data_targets_test'] == np.where(df_actual_predicted_probs['y_hat_test_proba'] >= tr, 1, 0)","041ef8b4":"pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted'])","25a76e05":"pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) \/ df_actual_predicted_probs.shape[0]","5d7fe3a3":"(pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) \/ df_actual_predicted_probs.shape[0]).iloc[0, 0] + (pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) \/ df_actual_predicted_probs.shape[0]).iloc[1, 1]","549300b3":"from sklearn.metrics import roc_curve, roc_auc_score","8db7886d":"roc_curve(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])","b6cef7b7":"fpr, tpr, thresholds = roc_curve(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])","11e2db7d":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","e8d78884":"plt.plot(fpr, tpr)\nplt.plot(fpr, fpr, linestyle = '--', color = 'k')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')","39fdacf3":"thresholds","27fb01b6":"thresholds.shape","5cb13751":"df_cutoffs = pd.concat([pd.DataFrame(thresholds), pd.DataFrame(fpr), pd.DataFrame(tpr)], axis = 1)\n# We concatenate 3 dataframes along the columns.","26de1d12":"df_cutoffs.columns = ['thresholds', 'fpr', 'tpr']\n# We name the columns of the dataframe 'thresholds', 'fpr', and 'tpr'.","4ef410c3":"df_cutoffs.head()","b3a83389":"df_cutoffs['thresholds'][0] = 1 - 1 \/ np.power(10, 16)\n# Let the first threshold (the value of the thresholds column with index 0) be equal to a number, very close to 1\n# but smaller than 1, say 1 - 1 \/ 10 ^ 16.","826ddbe4":"df_cutoffs['Score'] = ((np.log(df_cutoffs['thresholds'] \/ (1 - df_cutoffs['thresholds'])) - min_sum_coef) * ((max_score - min_score) \/ (max_sum_coef - min_sum_coef)) + min_score).round()\n# The score corresponsing to each threshold equals:\n# The the difference between the natural logarithm of the ratio of the threshold and 1 minus the threshold and\n# the minimum sum of coefficients\n# multiplied by\n# the sum of the minimum score and the ratio of the difference between the maximum score and minimum score and \n# the difference between the maximum sum of coefficients and the minimum sum of coefficients.","015df53d":"df_cutoffs.head()","44aed3b0":"df_cutoffs['Score'][0] = max_score","9393a9bd":"df_cutoffs.head()","aaeb504c":"df_cutoffs.tail()","e3dbaf92":"# We define a function called 'n_approved' which assigns a value of 1 if a predicted probability\n# is greater than the parameter p, which is a threshold, and a value of 0, if it is not.\n# Then it sums the column.\n# Thus, if given any percentage values, the function will return\n# the number of rows wih estimated probabilites greater than the threshold. \ndef n_approved(p):\n    return np.where(df_actual_predicted_probs['y_hat_test_proba'] >= p, 1, 0).sum()","b71791e9":"df_cutoffs['N Approved'] = df_cutoffs['thresholds'].apply(n_approved)\n# Assuming that all credit applications above a given probability of being 'good' will be approved,\n# when we apply the 'n_approved' function to a threshold, it will return the number of approved applications.\n# Thus, here we calculate the number of approved appliations for al thresholds.\ndf_cutoffs['N Rejected'] = df_actual_predicted_probs['y_hat_test_proba'].shape[0] - df_cutoffs['N Approved']\n# Then, we calculate the number of rejected applications for each threshold.\n# It is the difference between the total number of applications and the approved applications for that threshold.\ndf_cutoffs['Approval Rate'] = df_cutoffs['N Approved'] \/ df_actual_predicted_probs['y_hat_test_proba'].shape[0]\n# Approval rate equalts the ratio of the approved applications and all applications.\ndf_cutoffs['Rejection Rate'] = 1 - df_cutoffs['Approval Rate']\n# Rejection rate equals one minus approval rate.","04614d1c":"df_cutoffs.head()","c1c108ff":"df_cutoffs.tail()","0d78bcd7":"df_cutoffs.iloc[5000: 6200, ]\n# Here we display the dataframe with cutoffs form line with index 5000 to line with index 6200.","d7e1f59e":"df_cutoffs.iloc[1000: 2000, ]\n# Here we display the dataframe with cutoffs form line with index 1000 to line with index 2000.","17835d3f":"inputs_train_with_ref_cat.to_csv('inputs_train_with_ref_cat.csv')","db44e660":"df_scorecard.to_csv('df_scorecard.csv')","b3583f08":"### Creating a Scorecard","0b745e22":"## Build a Logistic Regression Model with P-Values","6e36cdf8":"### Caclulating Credit Score","a9c5424a":"### Out-of-sample validation (test)","62b63c4e":"### From Credit Score to PD","4bcdd733":"# PD Model Estimation","4bab1c9b":"### Explore Data","7752a97c":"### Selecting the Features","688a908d":"# PD Model Validation (Test)","bd5243ca":"# Applying the PD Model","439e9d33":"### Accuracy and Area under the Curve","7ad71fa6":"# Import Libraries","f2c105cf":"## Logistic Regression","c5e1e7ff":"### Setting Cut-offs","d5040e74":"### Gini and Kolmogorov-Smirnov","f9265a85":"### Calculating PD of individual accounts","1cecffd4":"This is the part - 2 of Credit Risk modelling on the consumer loans dataset which contains data from 800,000 consumers issued from 2007 to 2015 by Lending Club: a large US peer-to-peer lending company. \n\nIn the previous versions we have prepared the data from the raw data to be fed into the Logistic Regression model.\n\nHence the input to this notebook is the output from the notebook from part-I.\n\nFor now I have manually uploaded the files received as output from part-I. So any changes in part-I will not be reflected here.","562c9156":"### Import Data","80a580c0":"# Loading the Data and Selecting the Features"}}