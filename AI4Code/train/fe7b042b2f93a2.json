{"cell_type":{"9e8db0cf":"code","08279fd9":"code","cbaee64a":"code","c71ac50a":"code","54a8ac97":"code","15501473":"code","e43b1e8b":"code","a8bb14f6":"code","e8d2c4bd":"code","31a7c5f1":"code","c3e2a347":"markdown","a390aec9":"markdown","60594511":"markdown","6f7a3b0d":"markdown","319e5805":"markdown","c0d180a3":"markdown","9dbaef5e":"markdown","68f439ad":"markdown","e5755ade":"markdown","8cf081e0":"markdown"},"source":{"9e8db0cf":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader as DL\nfrom torch.nn.utils import weight_norm as WN\nimport torch.nn.functional as F\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nimport os\nimport gc\nfrom time import time\n\nseed = 42\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","08279fd9":"def breaker():\n    print(\"\\n\" + 50*\"-\" + \"\\n\")\n    \ndef head(x=None, no_of_ele=5):\n    print(x[:no_of_ele])\n    \ndef getCol(x=None):\n    return [col for col in x.columns]\n\ndef getObjCol(x=None):\n    s = (x.dtypes == \"object\")\n    return list(s[s].index)\n\nsc = StandardScaler()\nle = LabelEncoder()","cbaee64a":"tr_set = pd.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/train.csv\")\nts_set = pd.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/test.csv\")\n\nbreaker()\nprint(\"Train Set Shape : {}\".format(tr_set.shape))\nbreaker()\nprint(\"Test Set Shape  : {}\".format(ts_set.shape))\nbreaker()\n\nlabels = tr_set[\"target\"].copy().values\n\n# Dropping categorical feature 10 due to KeyError when transforming test set\ntr_set = tr_set.drop(labels=[\"id\", \"target\", \"cat10\"], axis=1)\nts_set = ts_set.drop(labels=[\"id\", \"cat10\"], axis=1)\n\ntr_set = tr_set.copy().values\nts_set = ts_set.copy().values","c71ac50a":"for i in range(18):\n    tr_set[:, i] = le.fit_transform(tr_set[:, i])\n    ts_set[:, i] = le.transform(ts_set[:, i])\n    \ntr_set = sc.fit_transform(tr_set)\nX_test = sc.transform(ts_set)\n\nX_train, X_valid, y_train, y_valid = train_test_split(tr_set, labels, test_size=0.2,\n                                                      shuffle=True, random_state=seed)\n\ndel tr_set, ts_set\n\nbreaker()\nprint(\"Garbage Collected : {}\".format(gc.collect()))\nbreaker()","54a8ac97":"class DS(Dataset):\n    def __init__(this, X=None, y=None, mode=\"train\"):\n        this.mode = mode\n        this.X = X\n        if mode == \"train\" or mode == \"valid\":\n            this.y = y\n            \n    def __len__(this):\n        return this.X.shape[0]\n\n    def __getitem__(this, idx):\n        if this.mode == \"train\" or this.mode == \"valid\":\n            return torch.FloatTensor(this.X[idx]), torch.FloatTensor(this.y[idx])\n        else:\n            return torch.FloatTensor(this.X[idx])","15501473":"class CFG():\n    tr_batch_size = 256\n    va_batch_size = 256\n    ts_batch_size = 256\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    OL = 1\n    \n    def __init__(this, IL=None, HL=None, DP=None, epochs=None, n_folds=None):\n        this.IL = IL\n        this.HL = HL\n        this.DP = DP\n        this.epochs = epochs\n        this.n_folds = n_folds","e43b1e8b":"class ANN(nn.Module):\n    def __init__(this, IL=None, HL=None, OL=None, use_DP=True, DP=0.5):\n\n        super(ANN, this).__init__()\n\n        this.use_DP = use_DP\n        this.HL = HL\n        if use_DP:\n            this.DP_ = nn.Dropout(p=DP)\n        \n        if len(HL) == 1:\n            this.BN1 = nn.BatchNorm1d(num_features=IL, eps=1e-5)\n            this.FC1 = nn.Linear(in_features=IL, out_features=HL[0])\n\n            this.BN2 = nn.BatchNorm1d(num_features=HL[0], eps=1e-5)\n            this.FC2 = nn.Linear(in_features=HL[0], out_features=OL)\n        \n        elif len(HL) == 2:\n            this.BN1 = nn.BatchNorm1d(num_features=IL, eps=1e-5)\n            this.FC1 = nn.Linear(in_features=IL, out_features=HL[0])\n\n            this.BN2 = nn.BatchNorm1d(num_features=HL[0], eps=1e-5)\n            this.FC2 = nn.Linear(in_features=HL[0], out_features=HL[1])\n\n            this.BN3 = nn.BatchNorm1d(num_features=HL[1], eps=1e-5)\n            this.FC3 = nn.Linear(in_features=HL[1], out_features=OL)\n        \n        elif len(HL) == 3:\n            this.BN1 = nn.BatchNorm1d(num_features=IL, eps=1e-5)\n            this.FC1 = nn.Linear(in_features=IL, out_features=HL[0])\n\n            this.BN2 = nn.BatchNorm1d(num_features=HL[0], eps=1e-5)\n            this.FC2 = nn.Linear(in_features=HL[0], out_features=HL[1])\n\n            this.BN3 = nn.BatchNorm1d(num_features=HL[1], eps=1e-5)\n            this.FC3 = nn.Linear(in_features=HL[1], out_features=HL[2])\n\n            this.BN4 = nn.BatchNorm1d(num_features=HL[2], eps=1e-5)\n            this.FC4 = nn.Linear(in_features=HL[2], out_features=OL)\n\n    def getOptimizer(this, A_S=True, lr=1e-3, wd=0):\n        if A_S:\n            return optim.Adam(this.parameters(), lr=lr, weight_decay=wd)\n        else:\n            return optim.SGD(this.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n    \n    def getPlateauLR(this, optimizer=None, patience=None, eps=None):\n        return optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, patience=patience, eps=eps, verbose=True)\n    \n    def forward(this, x):\n        if not this.use_DP:\n            if len(this.HL) == 1:\n                x = F.relu(this.FC1(this.BN1(x)))\n                x = this.FC2(this.BN2(x))\n                return x\n\n            elif len(this.HL) == 2:\n                x = F.relu(this.FC1(this.BN1(x)))\n                x = F.relu(this.FC2(this.BN2(x)))\n                x = this.FC3(this.BN3(x))\n                return x\n\n            elif len(this.HL) == 3:\n                x = F.relu(this.FC1(this.BN1(x)))\n                x = F.relu(this.FC2(this.BN2(x)))\n                x = F.relu(this.FC3(this.BN3(x)))\n                x = this.FC4(this.BN4(x))\n                return x\n\n        else:\n            if len(this.HL) == 1:\n                x = F.relu(this.DP_(this.FC1(this.BN1(x))))\n                x = this.FC2(this.BN2(x))\n                return x\n\n            elif len(this.HL) == 1:\n                x = F.relu(this.DP_(this.FC1(this.BN1(x))))\n                x = F.relu(this.DP_(this.FC2(this.BN2(x))))\n                x = this.FC3(this.BN3(x))\n                return x\n\n            elif len(this.HL) == 3:\n                x = F.relu(this.DP_(this.FC1(this.BN1(x))))\n                x = F.relu(this.DP_(this.FC2(this.BN2(x))))\n                x = F.relu(this.DP_(this.FC3(this.BN3(x))))\n                x = this.FC4(this.BN4(x))\n                return x","a8bb14f6":"def fit_(model=None, optimizer=None, scheduler=None, epochs=None, \n         trainloader=None, validloader=None, \n         criterion=None, device=None, verbose=True, path=None):\n    \n    breaker()\n    print(\"Training ...\")\n    breaker()\n\n    model.to(device)\n\n    Losses = []\n    AUCs = []\n\n    bestLoss = {\"train\" : np.inf, \"valid\" : np.inf}\n    bestAUCs = {\"train\" : 0.0, \"valid\" : 0.0}\n    DLS = {\"train\" : trainloader, \"valid\" : validloader}\n\n    start_time = time()\n    for e in range(epochs):\n        e_st = time()\n\n        epochLoss = {\"train\" : 0.0, \"valid\" : 0.0}\n        epochAUCs = {\"train\" : 0.0, \"valid\" : 0.0}\n\n        for phase in [\"train\", \"valid\"]:\n            if phase == \"train\":\n                model.train()\n            else:\n                model.eval()\n            \n            lossPerPass = []\n            aucsPerPass = []\n\n            for X, y in DLS[phase]:\n                X, y = X.to(device), y.to(device)\n\n                optimizer.zero_grad()\n                with torch.set_grad_enabled(phase == \"train\"):\n                    output = model(X)\n                    loss = criterion(output, y)\n                    if phase == \"train\":\n                        loss.backward()\n                        optimizer.step()\n                lossPerPass.append(loss.item())\n                aucsPerPass.append(getAUC(y, output))\n            epochLoss[phase] = np.mean(np.array(lossPerPass))\n            epochAUCs[phase] = np.mean(np.array(aucsPerPass))\n        Losses.append(epochLoss)\n        AUCs.append(epochAUCs)\n\n        torch.save({\"model_state_dict\" : model.state_dict(),\n                    \"optim_state_dict\" : optimizer.state_dict()},\n                   \".\/Epoch_{}.pt\".format(e+1))\n        \n        if epochLoss[\"valid\"] < bestLoss[\"valid\"]:\n            bestLoss = epochLoss\n            bestLossEpoch = e+1\n        \n        if epochAUCs[\"valid\"] > bestAUCs[\"valid\"]:\n            bestAUCs = epochAUCs\n            bestAUCsEpoch = e+1\n        \n        if scheduler:\n            scheduler.step(epochLoss[\"valid\"])\n        \n        if verbose:\n            print(\"Epoch : {} | Train Loss : {:.5f} | Valid Loss : {:.5f} \\\n| Train AUC : {:.5f} | Valid AUC : {:.5f} | Time : {:.2f} seconds\".format(e + 1, epochLoss[\"train\"], epochLoss[\"valid\"],\n                                                                                 epochAUCs[\"train\"], epochAUCs[\"valid\"],\n                                                                                 time() - e_st))\n\n    breaker()\n    print(\"Best Validation Loss at Epoch {}\".format(bestLossEpoch))   \n    breaker()\n    print(\"Best Validation AUCs at Epoch {}\".format(bestAUCsEpoch))  \n    breaker()\n    print(\"Time Taken [{} Epochs] : {:.2f} seconds\".format(epochs, (time() - start_time) \/ 60))\n    breaker()\n    print(\"Training Complete\")\n    breaker()\n\n    return Losses, AUCs, bestLossEpoch, bestAUCsEpoch\n\ndef predict_(model=None, dataloader=None, device=None, mode=\"valid\", path=None):\n    if path:\n        model.load_state_dict(torch.load(path)[\"model_state_dict\"])\n    \n    model.to(device)\n    model.eval()\n\n    y_pred = torch.zeros(1, 1).to(device)\n\n    if mode == \"valid\":\n        for X, y in dataloader:\n            X = X.to(device)\n            with torch.no_grad():\n                output = torch.sigmoid(model(X))\n            y_pred = torch.cat((y_pred, output.view(-1, 1)), dim=0)\n    elif mode == \"test\":\n        for X in dataloader:\n            X = X.to(device)\n            with torch.no_grad():\n                output = torch.sigmoid(model(X))\n            y_pred = torch.cat((y_pred, output.view(-1, 1)), dim=0)\n    else:\n        print(\"Invalid Mode\")\n    \n    return y_pred[1:].detach().cpu().numpy()\n\ndef getAUC(y_true=None, y_pred=None):\n    y_true = y_true.detach().cpu().numpy()\n    y_pred = torch.sigmoid(y_pred).detach().cpu().numpy()\n\n    return roc_auc_score(y_true, y_pred)","e8d2c4bd":"cfg = CFG(IL=X_train.shape[1], HL=[16], epochs=50)\n\ntr_data_setup = DS(X=X_train, y=y_train.reshape(-1, 1), mode=\"train\")\nva_data_setup = DS(X=X_valid, y=y_valid.reshape(-1, 1), mode=\"valid\")\nts_data_setup = DS(X=X_test, y=None, mode=\"test\")\n\ntr_data = DL(tr_data_setup, batch_size=cfg.tr_batch_size, shuffle=True, generator=torch.manual_seed(seed), drop_last=True)\nva_data = DL(va_data_setup, batch_size=cfg.va_batch_size, shuffle=False)\nts_data = DL(ts_data_setup, batch_size=cfg.ts_batch_size, shuffle=False)\n\ntorch.manual_seed(seed)\nmodel = ANN(IL=cfg.IL, HL=cfg.HL, OL=cfg.OL, use_DP=False)\noptimizer = model.getOptimizer(A_S=True, lr=1e-3, wd=1e-5)\n# scheduler = model.getPlateauLR(optimizer=optimizer, patience=5, eps=1e-8)\n\nLosses, AUCs, bestLossEpoch, bestAUCsEpoch = fit_(model=model, optimizer=optimizer, scheduler=None, epochs=cfg.epochs,\n                                                  trainloader=tr_data, validloader=va_data,\n                                                  criterion=nn.BCEWithLogitsLoss(), device=cfg.device)\n\nTL = []\nVL = []\nTA = []\nVA = []\n\nfor i in range(len(Losses)):\n    TL.append(Losses[i][\"train\"])\n    VL.append(Losses[i][\"valid\"])\n    TA.append(AUCs[i][\"train\"])\n    VA.append(AUCs[i][\"valid\"])\n\nplt.figure()\nplt.plot([i+1 for i in range(len(TL))], TL, \"r\", label=\"Train Loss\")\nplt.plot([i+1 for i in range(len(VL))], VL, \"b--\", label=\"Valid Loss\")\nplt.xlabel(\"Epochs -->\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.grid()\nplt.show()\n\nplt.figure()\nplt.plot([i+1 for i in range(len(TA))], TA, \"r\", label=\"Train AUC\")\nplt.plot([i+1 for i in range(len(VA))], VA, \"b--\", label=\"Valid AUC\")\nplt.xlabel(\"Epochs -->\")\nplt.ylabel(\"AUC\")\nplt.legend()\nplt.grid()\nplt.show()","31a7c5f1":"ss = pd.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/sample_submission.csv\")\n\ny_pred_bl = predict_(model=model, dataloader=ts_data, device=cfg.device, mode=\"test\", path=\".\/Epoch_{}.pt\".format(bestLossEpoch))\n# y_pred_ba = predict_(model=model, dataloader=ts_data, device=cfg.device, mode=\"test\", path=\".\/Epoch_{}.pt\".format(bestAUCsEpoch))\n\nss[\"target\"] = y_pred_bl\nss.to_csv(\".\/submission.csv\", index=False)\nss.head(5)","c3e2a347":"**Config**","a390aec9":"# Helper Functions","60594511":"**ANN Helpers**","6f7a3b0d":"**Training**","319e5805":"# Library Imports","c0d180a3":"Label Encoding, Scaling, Splitting","9dbaef5e":"**Setup**","68f439ad":"# Data Handling","e5755ade":"# ANN Configuration and Setup","8cf081e0":"**Dataset Template**"}}