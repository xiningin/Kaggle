{"cell_type":{"c42366dd":"code","2e68c3ce":"code","f66fe6f9":"code","07bc9ce9":"code","20d2dfad":"code","d121be3b":"code","c73dc97b":"code","6067a055":"code","92b739de":"code","9ef3732f":"markdown","8047b88b":"markdown","90252562":"markdown","1b330286":"markdown","d239276d":"markdown","465f0c1e":"markdown","8eb139d4":"markdown","d4dd2450":"markdown","c4767050":"markdown"},"source":{"c42366dd":"import pandas as pd\n\nmelbourne_file_path = '..\/input\/melbourne-housing-snapshot\/melb_data.csv'\nmelbourne_data = pd.read_csv(melbourne_file_path) \nmelbourne_data.columns","2e68c3ce":"# The Melbourne data has some missing values (some houses for which some variables weren't recorded.)\n# We'll learn to handle missing values in a later tutorial.  \n# Your Iowa data doesn't have missing values in the columns you use. \n# So we will take the simplest option for now, and drop houses from our data. \n# Don't worry about this much for now, though the code is:\n\n# dropna drops missing values (think of na as \"not available\")\nmelbourne_data = melbourne_data.dropna(axis=0)","f66fe6f9":"y = melbourne_data.Price","07bc9ce9":"melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude']","20d2dfad":"X = melbourne_data[melbourne_features]","d121be3b":"X.describe()","c73dc97b":"X.head()","6067a055":"from sklearn.tree import DecisionTreeRegressor\n\n# Define model. Specify a number for random_state to ensure same results each run\nmelbourne_model = DecisionTreeRegressor(random_state=1)\n\n# Fit model\nmelbourne_model.fit(X, y)","92b739de":"print(\"Making predictions for the following 5 houses:\")\nprint(X.head())\nprint(\"The predictions are\")\nprint(melbourne_model.predict(X.head()))","9ef3732f":"# Choosing \"Features\"\nThe columns that are inputted into our model (and later used to make predictions) are called \"features.\" In our case, those would be the columns used to determine the home price. Sometimes, you will use all columns except the target as features. Other times you'll be better off with fewer features. \n\nFor now, we'll build a model with only a few features. Later on you'll see how to iterate and compare models built with different features.\n\nWe select multiple features by providing a list of column names inside brackets. Each item in that list should be a string (with quotes).\n\nHere is an example:","8047b88b":"Many machine learning models allow some randomness in model training. Specifying a number for `random_state` ensures you get the same results in each run. This is considered a good practice. You use any number, and model quality won't depend meaningfully on exactly what value you choose.\n\nWe now have a fitted model that we can use to make predictions.\n\nIn practice, you'll want to make predictions for new houses coming on the market rather than the houses we already have prices for. But we'll make predictions for the first few rows of the training data to see how the predict function works.\n","90252562":"There are many ways to select a subset of your data. The [Pandas Micro-Course](https:\/\/www.kaggle.com\/learn\/pandas) covers these in more depth, but we will focus on two approaches for now.\n\n1. Dot notation, which we use to select the \"prediction target\"\n2. Selecting with a column list, which we use to select the \"features\" \n\n## Selecting The Prediction Target \nYou can pull out a variable with **dot-notation**.  This single column is stored in a **Series**, which is broadly like a DataFrame with only a single column of data.  \n\nWe'll use the dot notation to select the column we want to predict, which is called the **prediction target**. By convention, the prediction target is called **y**. So the code we need to save the house prices in the Melbourne data is","1b330286":"# Selecting Data for Modeling\nYour dataset had  too many variables to wrap your head around, or even to print out nicely.  How can you pare down this overwhelming amount of data to something you can understand?\n\nWe'll start by picking a few variables using our intuition. Later courses will show you statistical techniques to automatically prioritize variables.\n\nTo choose variables\/columns, we'll need to see a list of all columns in the dataset. That is done with the **columns** property of the DataFrame (the bottom line of code below).\n","d239276d":"# Your Turn\nTry it out yourself in the **[Model Building Exercise](https:\/\/www.kaggle.com\/kernels\/fork\/2247864)**","465f0c1e":"By convention, this data is called **X**.","8eb139d4":"---\n# Building Your Model\n\nYou will use the **scikit-learn** library to create your models.  When coding, this library is written as **sklearn**, as you will see in the sample code. Scikit-learn is easily the most popular library for modeling the types of data typically stored in DataFrames. \n\nThe steps to building and using a model are:\n* **Define:** What type of model will it be?  A decision tree?  Some other type of model? Some other parameters of the model type are specified too.\n* **Fit:** Capture patterns from provided data. This is the heart of modeling.\n* **Predict:** Just what it sounds like\n* **Evaluate**: Determine how accurate the model's predictions are.\n\nHere is an example of defining a decision tree model with scikit-learn and fitting it with the features and target variable.","d4dd2450":"Let's quickly review the data we'll be using to predict house prices using the `describe` method and the `head` method, which shows the top few rows.","c4767050":"Visually checking your data with these commands is an important part of a data scientist's job.  You'll frequently find surprises in the dataset that deserve further inspection."}}