{"cell_type":{"165eb9a6":"code","f63172cc":"code","6541b0cb":"code","9d81363c":"code","eb81ad0e":"code","22382a00":"code","a8ae57c1":"code","a54a3184":"code","699d5210":"code","23c2a843":"code","4389a66b":"code","e514239f":"code","9d0c34d8":"code","9b85d9fd":"code","f6f73528":"code","b3daf84e":"code","bd6b7940":"code","1e72c39b":"code","34f3b5b7":"code","f21fdb93":"code","7ca1a7f3":"code","f79276a0":"code","03884de7":"code","9804d20f":"code","e57ebb54":"code","c3aa3ce3":"code","f91a05a7":"code","a8ea71cf":"code","95f8ee2d":"code","4a8c457a":"code","cef404ca":"code","4d143170":"code","e8cae6f8":"code","8642e88f":"code","ef2c6a24":"code","4a4a5274":"code","ac70848e":"code","d1797f7c":"code","1feb2439":"code","f6fcd4ac":"code","2449d387":"code","220f3f92":"code","dad70ce5":"code","3ad9ce93":"code","d118a283":"code","37bb1eef":"code","0162eca5":"code","8381dd77":"code","b12d56ab":"code","0f09ee94":"code","1327e764":"code","88d3e859":"code","be0fd7e1":"code","e36c8bac":"code","f8c50982":"code","67e37544":"code","49a491a2":"code","df651bc7":"code","41a9c6ce":"code","08d8c692":"code","745b96e9":"code","3c74d5a1":"code","f45c23a7":"code","b208e9e5":"code","8370c3c8":"code","51a2076e":"code","2ffd0a63":"code","1a979a6e":"code","95efea1a":"code","9861ce0a":"code","24b902d4":"code","38c673e2":"code","b9892e29":"code","6cc0bc0a":"code","9ffa8667":"code","85be5efc":"code","a7658ab5":"code","f1752e47":"code","bfed98c9":"code","8d21cade":"code","606b0ba2":"code","5ee6cb3f":"code","af882c1d":"code","a8389563":"code","7df4a2a3":"code","dfab1de0":"code","2d9ff5a2":"code","e84fedce":"code","ae37d208":"code","f8749cc6":"code","26084f9e":"code","a3f54a74":"code","ebd0ec26":"code","29cda0f5":"code","b4617b70":"code","fd1b0fab":"code","197603d7":"code","c733dcb0":"code","939b69ad":"code","137594dd":"code","c0d2e583":"code","b4e1168e":"code","3913804b":"code","59d8a798":"code","5bd2fe7c":"code","88d97fb6":"code","eafe2be2":"code","ff7999e1":"code","dec5d54a":"code","2c444ebc":"code","c42d6f1d":"code","6954e1e2":"code","d991abd1":"code","f6b7e47f":"code","f02e391c":"code","fbe7c8b3":"code","553980d5":"code","c88b9e57":"markdown","e817c657":"markdown","0675ebc5":"markdown","88db23ad":"markdown","8aaecbef":"markdown","3cc5cc64":"markdown","1bc9b6e9":"markdown","7eab253a":"markdown","b33a05ad":"markdown","351090f3":"markdown","87e4c9e9":"markdown","c4edaeed":"markdown","07c506fc":"markdown"},"source":{"165eb9a6":"import os, gc, pickle, copy, datetime, warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nimport pandas_profiling\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler\nfrom sklearn import metrics\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 500)\nwarnings.filterwarnings('ignore')","f63172cc":"df_train = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/train.csv\")\nprint(df_train.shape)\nprint(df_train.Date.min(), df_train.Date.max())\ndf_train.head()","6541b0cb":"train_min_date, train_max_date = df_train.Date.min(), df_train.Date.max()\ntrain_min_dayofyear, train_max_dayofyear = (pd.to_datetime(train_min_date)).dayofyear, (pd.to_datetime(train_max_date)).dayofyear\nprint(train_min_dayofyear, train_max_dayofyear)","9d81363c":"train_valid_cutoff_dayofyear = train_min_dayofyear + ( train_max_dayofyear - train_min_dayofyear ) \/\/ 3 * 2\ntrain_valid_cutoff_dayofyear","eb81ad0e":"df_test = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/test.csv\")\nprint(df_test.shape)\ntest_min_date, test_max_date = df_test.Date.min(), df_test.Date.max()\nprint(test_min_date, test_max_date)\ndf_test.head()","22382a00":"# concat train and test\ndf_traintest = pd.concat([df_train, df_test])\nprint(df_train.shape, df_test.shape, df_traintest.shape)\ndf_traintest.head()","a8ae57c1":"# concat Country\/Region and Province\/State\ndef concat_country_province(x):\n    try:\n        x_new = x['Country_Region'] + \"\/\" + x['Province_State']\n    except:\n        x_new = x['Country_Region']\n    return x_new\n        \ndf_traintest['place_id'] = df_traintest.apply(lambda x: concat_country_province(x), axis=1)\ntmp = np.sort(df_traintest['place_id'].unique())\nprint(\"num unique places: {}\".format(len(tmp)))\nprint(tmp[:10])","a54a3184":"# process date\n# df_traintest['Date'] = pd.to_datetime(df_traintest['Date'])\n# df_traintest['day'] = df_traintest['Date'].apply(lambda x: x.dayofyear).astype(np.int16)\n# df_traintest['dayofmonth'] = df_traintest['Date'].apply(lambda x: x.day).astype(np.int16)\n# df_traintest['dayofweek'] = df_traintest['Date'].apply(lambda x: x.dayofweek).astype(np.int16)\n# df_traintest.head()\n\n#     # time features\ndf_traintest['Date'] = pd.to_datetime(df_traintest['Date'])\ntime_cols = [\n#     \"year\", \"quarter\", \n    \"month\", \n    \"week\", \n    \"day\", \n    \"dayofyear\", \n    \"dayofweek\", \n#     \"is_year_end\", \"is_year_start\", \"is_quarter_end\", \"is_quarter_start\", \n#     \"is_month_end\",\"is_month_start\",\n]\n\nfor attr in time_cols:\n    dtype = np.int if attr == \"year\" else np.int8\n#     df_traintest[attr] = getattr(df_traintest['Date'].dt, attr).astype(dtype)\n    df_traintest[attr] = getattr(df_traintest['Date'].dt, attr)\n# df_traintest[\"is_weekend\"] = df_traintest[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n# time_cols += [\"is_weekend\"]\nprint(time_cols)\ndf_traintest.head(10)","699d5210":"day_before_valid = train_valid_cutoff_dayofyear\nday_before_public = 92 #2020-04-01\nday_before_private = df_traintest['dayofyear'][pd.isna(df_traintest['ForecastId'])].max() # last day of train\nprint(df_traintest['Date'][df_traintest['dayofyear']==day_before_valid].values[0])\nprint(df_traintest['Date'][df_traintest['dayofyear']==day_before_public].values[0])\nprint(df_traintest['Date'][df_traintest['dayofyear']==day_before_private].values[0])","23c2a843":"( df_traintest[pd.isna(df_traintest['ForecastId'])].groupby('place_id')['Date'].max() ).min()","4389a66b":"# calc cases and fatalities per day\ndf_traintest['cases\/day'] = 0\ndf_traintest['fatal\/day'] = 0\nplaces = np.sort(df_traintest['place_id'].unique())\nfor place in places:\n    tmp = df_traintest['ConfirmedCases'][df_traintest['place_id']==place].values\n    tmp[1:] -= tmp[:-1]\n    df_traintest['cases\/day'][df_traintest['place_id']==place] = tmp\n    tmp = df_traintest['Fatalities'][df_traintest['place_id']==place].values\n    tmp[1:] -= tmp[:-1]\n    df_traintest['fatal\/day'][df_traintest['place_id']==place] = tmp\n    \ndf_traintest[df_traintest['place_id']=='China\/Hubei']","e514239f":"# aggregate cases and fatalities\ndef do_aggregation(df, col, mean_range, method='mean', val_cols=[]):\n    df_new = copy.deepcopy(df)\n    col_new = '{}_{}_({}-{})'.format(col, method, mean_range[0], mean_range[1])\n    val_cols.append(col_new)\n    df_new[col_new] = 0\n    if method=='mean':\n        tmp = df_new[col].rolling(mean_range[1]-mean_range[0]+1).mean()\n    elif method=='std':\n        tmp = df_new[col].rolling(mean_range[1]-mean_range[0]+1).std()\n    df_new[col_new][mean_range[0]:] = tmp[:-(mean_range[0])]\n    df_new[col_new][pd.isna(df_new[col_new])] = 0\n    return df_new[[col_new]].reset_index(drop=True)\n\n# def do_aggregations(df):\n#     for method in ['mean']:\n#         df = pd.concat([df, do_aggregation(df, 'cases\/day', [1,1], method).reset_index(drop=True)], axis=1)\n#         df = pd.concat([df, do_aggregation(df, 'cases\/day', [1,7], method).reset_index(drop=True)], axis=1)\n#         df = pd.concat([df, do_aggregation(df, 'cases\/day', [8,14], method).reset_index(drop=True)], axis=1)\n#         df = pd.concat([df, do_aggregation(df, 'fatal\/day', [1,1], method).reset_index(drop=True)], axis=1)\n#         df = pd.concat([df, do_aggregation(df, 'fatal\/day', [1,7], method).reset_index(drop=True)], axis=1)\n#         df = pd.concat([df, do_aggregation(df, 'fatal\/day', [8,14], method).reset_index(drop=True)], axis=1)\n#     return df\n\ndef do_aggregations(df, roll_ranges=[[1,1], [1,7], [8,14]], val_cols=[]):\n    for method in ['mean']:\n        for roll_range in roll_ranges:\n            df = pd.concat([df, do_aggregation(df, 'cases\/day', roll_range, method, val_cols).reset_index(drop=True)], axis=1)\n            df = pd.concat([df, do_aggregation(df, 'fatal\/day', roll_range, method, val_cols).reset_index(drop=True)], axis=1)\n            \n    for threshold in [1, 10, 100]:\n        days_under_threshold = (df['ConfirmedCases']<threshold).sum()\n        tmp = df['dayofyear'].values - 22 - days_under_threshold\n        tmp[tmp<=0] = 0\n        df['days_since_{}cases'.format(threshold)] = tmp\n        val_cols.append('days_since_{}cases'.format(threshold))\n            \n    for threshold in [1, 10, 100]:\n        days_under_threshold = (df['Fatalities']<threshold).sum()\n        tmp = df['dayofyear'].values - 22 - days_under_threshold\n        tmp[tmp<=0] = 0\n        df['days_since_{}fatal'.format(threshold)] = tmp\n        val_cols.append('days_since_{}fatal'.format(threshold))\n    \n    # process China\/Hubei\n    if df['place_id'][0]=='China\/Hubei':\n        df['days_since_1cases'] += 35 # 2019\/12\/8\n        df['days_since_10cases'] += 35-13 # 2019\/12\/8-2020\/1\/2 assume 2019\/12\/8+13\n        df['days_since_100cases'] += 4 # 2020\/1\/18\n        df['days_since_1fatal'] += 13 # 2020\/1\/9\n    return df","9d0c34d8":"df_traintest[df_traintest['dayofyear']<0]","9b85d9fd":"df_traintest2 = []\nval_cols = []\nroll_ranges = [[i,i] for i in range(1,15)]\nroll_ranges += [[1,7], [8,14], [15,21]]\n\nfor place in places[:]:\n    df_tmp = df_traintest[df_traintest['place_id']==place].reset_index(drop=True)\n    df_tmp = do_aggregations(df_tmp, roll_ranges=roll_ranges, val_cols=val_cols)\n    df_traintest2.append(df_tmp)\ndf_traintest2 = pd.concat(df_traintest2).reset_index(drop=True)\n\nval_cols = list(set(val_cols))\nprint(val_cols)\ndf_traintest2[df_traintest2['place_id']=='China\/Hubei'].head(20)","f6f73528":"roll_ranges","b3daf84e":"# add Smoking rate per country\n# data of smoking rate is obtained from https:\/\/ourworldindata.org\/smoking\ndf_smoking = pd.read_csv(\"..\/input\/shareofadultswhosmoke\/adults-smoking-2000-2016.csv\")\nprint(np.sort(df_smoking['Entity'].unique())[:10])\ndf_smoking.head()","bd6b7940":"# extract newest data\ndf_smoking_recent = df_smoking.sort_values('Year', ascending=False).reset_index(drop=True)\ndf_smoking_recent = df_smoking_recent[df_smoking_recent['Entity'].duplicated()==False]\ndf_smoking_recent['Country\/Region'] = df_smoking_recent['Entity']\ndf_smoking_recent['SmokingRate'] = df_smoking_recent['Share of adults who smoke (%)']\n\ndf_smoking_recent[\"Country\/Region\"] = df_smoking_recent[\"Country\/Region\"].str.replace(\"South Korea\", \"Korea, South\")\ndf_smoking_recent[\"Country\/Region\"] = df_smoking_recent[\"Country\/Region\"].str.replace(\"United States\", \"US\")\n\ndf_smoking_recent.head()","1e72c39b":"# merge\ndf_traintest3 = pd.merge(df_traintest2, df_smoking_recent[['Country\/Region', 'SmokingRate']], left_on='Country_Region', right_on='Country\/Region', how='left')\ndf_traintest3.drop('Country\/Region', axis=1, inplace=True)\ndf_traintest3.head()","34f3b5b7":"## fill na with world smoking rate\nSmokingRate = df_smoking_recent['SmokingRate'][df_smoking_recent['Entity']=='World'].values[0]\nprint(\"Smoking rate of the world: {:.6f}\".format(SmokingRate))\ndf_traintest3['SmokingRate'][pd.isna(df_traintest3['SmokingRate'])] = SmokingRate\ndf_traintest3.head()","f21fdb93":"world_happiness_index = pd.read_csv(\"..\/input\/world-bank-datasets\/World_Happiness_Index.csv\")\nworld_happiness_grouped = world_happiness_index.groupby('Country name').nth(-1)\nworld_happiness_grouped.drop(\"Year\", axis=1, inplace=True)\n\nworld_happiness_grouped.dropna(axis=1, how='all', inplace=True)\n\nprint(world_happiness_grouped.shape)\n\nworld_happiness_grouped.index = world_happiness_grouped.index.str.replace(\"Taiwan Province of China\", \"Taiwan*\")\nworld_happiness_grouped.index = world_happiness_grouped.index.str.replace(\"United States\", \"US\")\nworld_happiness_grouped.index = world_happiness_grouped.index.str.replace(\"South Korea\", \"Korea, South\")\nworld_happiness_grouped.index = world_happiness_grouped.index.str.replace(\"Ivory Coast\", \"Cote d'Ivoire\")\n\ndf_traintest3 = pd.merge(left=df_traintest3, right=world_happiness_grouped, how='left', left_on='Country_Region', right_on='Country name')","7ca1a7f3":"wh_cols = world_happiness_grouped.columns.to_list()\nprint(wh_cols)","f79276a0":"for wh_col in wh_cols:\n    df_traintest3[wh_col][pd.isna(df_traintest3[wh_col])] = world_happiness_grouped[wh_col].mean()","03884de7":"malaria_world_health = pd.read_csv(\"..\/input\/world-bank-datasets\/Malaria_World_Health_Organization.csv\")\n\ndf_traintest3 = pd.merge(left=df_traintest3, right=malaria_world_health, how='left', left_on='Country_Region', right_on='Country')\ndf_traintest3.drop(\"Country\", axis=1, inplace=True)\n\nmwh_cols = [ col for col in malaria_world_health.columns.to_list() if col != \"Country\" ]\nprint(mwh_cols)","9804d20f":"df_traintest3[['Country_Region','Estimated number of malaria cases']][pd.isna(df_traintest3['Estimated number of malaria cases'])]","e57ebb54":"df_traintest3[['Estimated number of malaria cases']].isnull().sum()","c3aa3ce3":"human_development_index = pd.read_csv(\"..\/input\/world-bank-datasets\/Human_Development_Index.csv\")\nhuman_development_index.drop([\"Gross national income (GNI) per capita 2018\"], axis=1, inplace=True)\n\nhuman_development_index['Country'] = human_development_index['Country'].str.replace(\"South Korea\", \"Korea, South\")\nhuman_development_index['Country'] = human_development_index['Country'].str.replace(\"United States\", \"US\")\n\ndf_traintest3 = pd.merge(left=df_traintest3, right=human_development_index, how='left', left_on='Country_Region', right_on='Country')\ndf_traintest3.drop(\"Country\", axis=1, inplace=True)\n\nhdi_cols = [ col for col in human_development_index.columns.to_list() if col != \"Country\" ]\nprint(hdi_cols)\n\nfor hdi_col in hdi_cols:\n    df_traintest3[hdi_col][pd.isna(df_traintest3[hdi_col])] = human_development_index[hdi_col].mean()","f91a05a7":"# add additional info from countryinfo dataset\ndf_country = pd.read_csv(\"..\/input\/countryinfo\/covid19countryinfo.csv\", thousands=',')\ndf_country = df_country[df_country['country'].duplicated()==False]\nprint(df_country.shape)\ndf_country.head()","a8ea71cf":"country_info_cols = ['density', 'pop', 'fertility']\nprint(country_info_cols)","95f8ee2d":"df_country[country_info_cols].isnull().sum()","4a8c457a":"df_traintest3 = pd.merge(left=df_traintest3, \n                         right=df_country[['country']+country_info_cols], \n                         left_on=['Country_Region'], right_on=['country'], how='left')\ndf_traintest3.drop('country', axis=1, inplace=True)\n\ndf_traintest3['density'][df_traintest3['place_id']==\"South Sudan\"] = 18\ndf_traintest3['density'][df_traintest3['place_id']==\"Angola\"] = 14.8\ndf_traintest3['density'][df_traintest3['place_id']==\"Botswana\"] = 3\ndf_traintest3['density'][df_traintest3['place_id']==\"Burma\"] = 83\ndf_traintest3['density'][df_traintest3['place_id']==\"Burundi\"] = 463\ndf_traintest3['density'][df_traintest3['place_id']==\"Malawi\"] = 129\ndf_traintest3['density'][df_traintest3['place_id']==\"Papua New Guinea\"] = 17.8\ndf_traintest3['density'][df_traintest3['place_id']==\"Sao Tome and Principe\"] = 228\ndf_traintest3['density'][df_traintest3['place_id']==\"Sierra Leone\"] = 105\ndf_traintest3['density'][df_traintest3['place_id']==\"West Bank and Gaza\"] = 758.98\ndf_traintest3['density'][df_traintest3['place_id']==\"Western Sahara\"] = 2\ndf_traintest3['density'][df_traintest3['place_id']==\"MS Zaandam\"] = 1432.0+615.0\n\ndf_traintest3['pop'][df_traintest3['place_id']==\"South Sudan\"] = 10.98e6\ndf_traintest3['pop'][df_traintest3['place_id']==\"Angola\"] = 30.81e6\ndf_traintest3['pop'][df_traintest3['place_id']==\"Botswana\"] = 2.254e6\ndf_traintest3['pop'][df_traintest3['place_id']==\"Burma\"] = 53.71e6\ndf_traintest3['pop'][df_traintest3['place_id']==\"Burundi\"] = 11.18e6\ndf_traintest3['pop'][df_traintest3['place_id']==\"Malawi\"] = 18.14e6\ndf_traintest3['pop'][df_traintest3['place_id']==\"Papua New Guinea\"] = 8.606e6\ndf_traintest3['pop'][df_traintest3['place_id']==\"Sao Tome and Principe\"] = 211028.0\ndf_traintest3['pop'][df_traintest3['place_id']==\"Sierra Leone\"] = 7.65e6\ndf_traintest3['pop'][df_traintest3['place_id']==\"West Bank and Gaza\"] = 4.569e6\ndf_traintest3['pop'][df_traintest3['place_id']==\"Western Sahara\"] = 567402.0\ndf_traintest3['pop'][df_traintest3['place_id']==\"MS Zaandam\"] = 1432.0+615.0\n\ndf_traintest3['fertility'][df_traintest3['place_id']==\"South Sudan\"] = 4.78\ndf_traintest3['fertility'][df_traintest3['place_id']==\"Angola\"] = 5.60\ndf_traintest3['fertility'][df_traintest3['place_id']==\"Botswana\"] = 2.91\ndf_traintest3['fertility'][df_traintest3['place_id']==\"Burma\"] = 2.17\ndf_traintest3['fertility'][df_traintest3['place_id']==\"Burundi\"] = 5.50\ndf_traintest3['fertility'][df_traintest3['place_id']==\"Malawi\"] = 4.30\ndf_traintest3['fertility'][df_traintest3['place_id']==\"Papua New Guinea\"] = 3.61\ndf_traintest3['fertility'][df_traintest3['place_id']==\"Sao Tome and Principe\"] = 4.37\ndf_traintest3['fertility'][df_traintest3['place_id']==\"Sierra Leone\"] = 4.36\ndf_traintest3['fertility'][df_traintest3['place_id']==\"West Bank and Gaza\"] = 3.74\ndf_traintest3['fertility'][df_traintest3['place_id']==\"Western Sahara\"] = 3.79\n# df_traintest3['fertility'][df_traintest3['place_id']==\"MS Zaandam\"] = 0.0","cef404ca":"df_traintest3['place_id'][pd.isna(df_traintest3['density'])].unique()","4d143170":"df_traintest3['place_id'][pd.isna(df_traintest3['pop'])].unique()","e8cae6f8":"df_traintest3['place_id'][pd.isna(df_traintest3['fertility'])].unique()","8642e88f":"# df_lat_long = pd.concat( [ pd.read_csv(\"..\/input\/covid19-global-forecasting-week-1\/train.csv\"), pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/test.csv\") ] )\n# df_lat_long = df_lat_long[['Country\/Region', 'Province\/State', 'Lat', 'Long']].drop_duplicates()\n# df_lat_long = df_lat_long.rename(columns={'Country\/Region': 'Country_Region', 'Province\/State': 'Province_State'})\n# df_lat_long['place_id'] = df_lat_long.apply(lambda x: concat_country_province(x), axis=1)\n# df_lat_long.drop([\"Country_Region\", 'Province_State'], axis=1, inplace=True)\n\n# df_traintest3 = pd.merge(left=df_traintest3, right=df_lat_long, how='left', on='place_id')","ef2c6a24":"# df_lat_long = pd.concat( [ pd.read_csv(\"..\/input\/covid19-global-forecasting-week-1\/train.csv\"), pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/test.csv\") ] )\n# df_lat_long = df_lat_long[['Country\/Region', 'Province\/State', 'Lat', 'Long']].drop_duplicates()\n# df_lat_long = df_lat_long.rename(columns={'Country\/Region': 'Country_Region', 'Province\/State': 'Province_State'})\n# df_lat_long.to_csv(\"lat_long.csv\", index=None)","4a4a5274":"# df_lat_long = pd.read_csv(\"..\/input\/lat-long\/lat_long.csv\")\n# df_lat_long['place_id'] = df_lat_long.apply(lambda x: concat_country_province(x), axis=1)\n# df_lat_long.drop([\"Country_Region\", 'Province_State'], axis=1, inplace=True)\n\n# df_traintest3 = pd.merge(left=df_traintest3, right=df_lat_long, how='left', on='place_id')","ac70848e":"# df_lat_long.head()","d1797f7c":"# tmp = df_lat_long['place_id'].unique()\n# print(\"num unique places: {}\".format(len(tmp)))","1feb2439":"df_lat_long2 = pd.read_csv(\"..\/input\/coronavirus-2019ncov\/covid-19-all.csv\")\ndf_lat_long2 = df_lat_long2[[\"Country\/Region\", \"Province\/State\", \"Latitude\", \"Longitude\"]].drop_duplicates()\ndf_lat_long2 = df_lat_long2.rename(columns = {\"Country\/Region\":\"Country_Region\", \n                                              \"Province\/State\":\"Province_State\", \n                                              \"Latitude\":\"Lat\", \n                                              \"Longitude\":\"Long\"})\n\ndf_lat_long2['place_id'] = df_lat_long2.apply(lambda x: concat_country_province(x), axis=1)\ndf_lat_long2.drop([\"Country_Region\", 'Province_State'], axis=1, inplace=True)\n\ndf_lat_long2[\"place_id\"] = df_lat_long2[\"place_id\"].str.replace(\"Ivory Coast\", \"Cote d'Ivoire\")\ndf_lat_long2[\"place_id\"] = df_lat_long2[\"place_id\"].str.replace(\"South Korea\", \"Korea, South\")\ndf_lat_long2[\"place_id\"] = df_lat_long2[\"place_id\"].str.replace(\"Taiwan\", \"Taiwan*\")\ndf_lat_long2[\"place_id\"] = df_lat_long2[\"place_id\"].str.replace(\"Vatican City\", \"Holy See\")\n\ndf_lat_long2 = pd.concat([df_lat_long2,\n          pd.DataFrame({\"place_id\": ['Czechia', 'Dominica', 'Niger'],\n             \"Lat\": [49.8175, 15.4150, 17.6078],\n             \"Long\": [15.4730, 61.3710, 8.0817]})])\n\ndf_lat_long2 = df_lat_long2[(pd.isna(df_lat_long2[\"Lat\"])==False) & (pd.isna(df_lat_long2[\"Long\"])==False)]\n# df_lat_long2 = df_lat_long2.groupby([\"Country_Region\", \"Province_State\"]).mean().reset_index()\ndf_lat_long2 = df_lat_long2.groupby([\"place_id\"]).mean().reset_index()\nprint(df_lat_long2.shape)\ndf_lat_long2.head()","f6fcd4ac":"df_traintest3 = pd.merge(left=df_traintest3, right=df_lat_long2, how='left', on=[\"place_id\"])","2449d387":"assert(len(df_traintest3[(pd.isna(df_traintest3[\"Long\"])) | (pd.isna(df_traintest3[\"Lat\"]))])==0)","220f3f92":"def encode_label(df, col, freq_limit=0):\n    df[col][pd.isna(df[col])] = 'nan'\n    tmp = df[col].value_counts()\n    cols = tmp.index.values\n    freq = tmp.values\n    num_cols = (freq>=freq_limit).sum()\n    print(\"col: {}, num_cat: {}, num_reduced: {}\".format(col, len(cols), num_cols))\n\n    col_new = '{}_le'.format(col)\n    df_new = pd.DataFrame(np.ones(len(df), np.int16)*(num_cols-1), columns=[col_new])\n    for i, item in enumerate(cols[:num_cols]):\n        df_new[col_new][df[col]==item] = i\n\n    return df_new\n\ndef get_df_le(df, col_index, col_cat):\n    df_new = df[[col_index]]\n    for col in col_cat:\n        df_tmp = encode_label(df, col)\n        df_new = pd.concat([df_new, df_tmp], axis=1)\n    return df_new\n\ndf_traintest3['id_le'] = np.arange(len(df_traintest3))\ndf_le = get_df_le(df_traintest3, 'id_le', ['Country_Region', 'Province_State'])\ndf_traintest3 = pd.merge(df_traintest3, df_le, on='id_le', how='left')","dad70ce5":"le_cols = [\"Country_Region_le\", \"Province_State_le\"]\nprint(le_cols)","3ad9ce93":"# df_tmp = pd.get_dummies(df_traintest3['Province_State'], prefix='ps')\n# ps_cols = df_tmp.columns.to_list()\n# print(ps_cols)\n# df_traintest3 = pd.concat([df_traintest3,df_tmp],axis=1)","d118a283":"# df_tmp = pd.get_dummies(df_traintest3['Country_Region'], prefix='cr')\n# cr_cols = df_tmp.columns.to_list()\n# print(cr_cols)\n# df_traintest3 = pd.concat([df_traintest3,df_tmp],axis=1)","37bb1eef":"df_wk3 = pd.read_csv(\"..\/input\/covid19-country-data-wk3-release\/Data Join - RELEASE.csv\")\ndf_wk3['place_id'] = df_wk3.apply(lambda x: concat_country_province(x), axis=1)\ndf_wk3['Personality_uai'][df_wk3['Personality_uai']=='#NULL!'] = np.nan\ndf_wk3['Personality_uai'] = df_wk3['Personality_uai'].astype(np.float64)\nprint(df_wk3.shape)\ndf_wk3.head()","0162eca5":"df_wk3[['Personality_uai']].info()","8381dd77":"df_wk3['Personality_uai'].unique()","b12d56ab":"wk3_cols = ['Personality_uai']\nprint(wk3_cols)","0f09ee94":"df_wk3[wk3_cols].isnull().sum()","1327e764":"df_traintest3 = pd.merge(left=df_traintest3, right=df_wk3[['place_id']+wk3_cols], how='left', on=[\"place_id\"])","88d3e859":"df_traintest3[\"place_id\"][pd.isna(df_traintest3['Personality_uai'])].unique()","be0fd7e1":"df_traintest3[['Personality_uai']].info()","e36c8bac":"df_traintest3[df_traintest3['place_id']=='China\/Hubei']","f8c50982":"def calc_score(y_true, y_pred):\n    y_true[y_true<0] = 0\n    score = metrics.mean_squared_error(np.log(y_true.clip(0, 1e10)+1), np.log(y_pred[:]+1))**0.5\n    return score","67e37544":"# params\nSEED = 42\nparams = {'num_leaves': 8,\n          'min_data_in_leaf': 5,  # 42,\n          'objective': 'regression',\n          'max_depth': 8,\n          'learning_rate': 0.02,\n          'boosting': 'gbdt',\n          'bagging_freq': 5,  # 5\n          'bagging_fraction': 0.8,  # 0.5,\n          'feature_fraction': 0.8201,\n          'bagging_seed': SEED,\n          'reg_alpha': 1,  # 1.728910519108444,\n          'reg_lambda': 4.9847051755586085,\n          'random_state': SEED,\n          'metric': 'mse',\n          'verbosity': 100,\n          'min_gain_to_split': 0.02,  # 0.01077313523861969,\n          'min_child_weight': 5,  # 19.428902804238373,\n          'num_threads': 6,\n          }","49a491a2":"df_traintest3.info()","df651bc7":"# train model to predict fatalities\/day\ncol_target = 'fatal\/day'\ncol_var = [\n    'Lat', 'Long',\n#    'cases\/day_mean_(1-1)', 'cases\/day_mean_(1-7)', 'cases\/day_mean_(8-14)', \n#      'fatal\/day_mean_(1-1)', 'fatal\/day_mean_(1-7)', 'fatal\/day_mean_(8-14)',\n#    'cases\/day_std_(1-1)', 'cases\/day_std_(1-7)', 'cases\/day_std_(8-14)', \n#      'fatal\/day_std_(1-1)', 'fatal\/day_std_(1-7)', 'fatal\/day_std_(8-14)',\n    'SmokingRate',\n#     'dayofyear',\n#     'day',\n#     'dayofweek',\n]\ncol_var += val_cols\ncol_var += time_cols\n# extra_cols = wh_cols + mwh_cols + hdi_cols + ps_cols + cr_cols\nextra_cols = wh_cols + mwh_cols + hdi_cols\ncol_var += extra_cols\ncol_var += country_info_cols\ncol_var += le_cols\ncol_var += wk3_cols\n\n# df_train = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (df_traintest3['dayofyear']<train_valid_cutoff_dayofyear)]\n# df_valid = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (df_traintest3['dayofyear']>=train_valid_cutoff_dayofyear)]\ndf_train = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (df_traintest3['dayofyear']<=day_before_valid)]\ndf_valid = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (day_before_valid<df_traintest3['dayofyear']) & (df_traintest3['dayofyear']<=day_before_public)]\n\ndf_test = df_traintest3[pd.isna(df_traintest3['ForecastId'])==False]\nX_train = df_train[col_var].values\nX_valid = df_valid[col_var].values\nprint(len(X_train), len(X_valid))\n\n# scaler = MinMaxScaler()\n# scaler = RobustScaler()\n# X_train = scaler.fit_transform(X_train)\n# X_valid = scaler.transform(X_valid)\n\n# y_train = df_train[col_target].values\n# y_valid = df_valid[col_target].values\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\n\ntrain_data = lgb.Dataset(X_train, label=y_train)\nvalid_data = lgb.Dataset(X_valid, label=y_valid)\nnum_round = 15000\nmodel = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)","41a9c6ce":"y_true = df_valid['fatal\/day'].values\ny_pred = np.exp(model.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score))","08d8c692":"df_features = pd.merge( left=(pd.DataFrame(model.feature_importance(), index=col_var, columns=[\"importance\"])).sort_values('importance', ascending=False),\n                      right=df_train[col_var].isnull().sum().to_frame(name='count_null'),\n                      how='left', left_index=True, right_index=True)\ndf_features","745b96e9":"important_features = df_features.index[df_features['importance']>=10].to_list()\nprint(len(important_features))\nimportant_features","3c74d5a1":"# df_train_profile = df_train[col_var].profile_report(title='Pandas Profile Report:Train Data')","f45c23a7":"# df_train_profile","b208e9e5":"# rejected_var = df_train_profile.get_rejected_variables()\n# rejected_var","8370c3c8":"col_var = important_features\n\nX_train = df_train[col_var].values\nX_valid = df_valid[col_var].values\n\n# scaler = MinMaxScaler()\n# scaler = RobustScaler()\n# X_train = scaler.fit_transform(X_train)\n# X_valid = scaler.transform(X_valid)\n\n# y_train = df_train[col_target].values\n# y_valid = df_valid[col_target].values\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\n\ntrain_data = lgb.Dataset(X_train, label=y_train)\nvalid_data = lgb.Dataset(X_valid, label=y_valid)\nnum_round = 15000\nmodel = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\nbest_itr = model.best_iteration","51a2076e":"y_true = df_valid['fatal\/day'].values\ny_pred = np.exp(model.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score))","2ffd0a63":"# train with all data before public\ndf_train = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (df_traintest3['dayofyear']<=day_before_public)]\ndf_valid = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (df_traintest3['dayofyear']<=day_before_public)]\ndf_test = df_traintest3[pd.isna(df_traintest3['ForecastId'])==False]\nX_train = df_train[col_var].values\nX_valid = df_valid[col_var].values\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train)\nvalid_data = lgb.Dataset(X_valid, label=y_valid)\nmodel_pub = lgb.train(params, train_data, best_itr, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)","1a979a6e":"# from lightgbm import LGBMRegressor\n# lgb_reg = LGBMRegressor(random_state=17)","95efea1a":"# %%time\n# lgb_reg.fit(X_train, y_train)","9861ce0a":"# from sklearn.metrics import mean_squared_error\n\n# mean_squared_error(y_valid, lgb_reg.predict(X_valid))","24b902d4":"# param_grid = {'num_leaves': [7, 15, 31, 63], \n#               'max_depth': [3, 4, 5, 6, -1]}","38c673e2":"# from sklearn.model_selection import train_test_split, GridSearchCV\n\n# grid_searcher = GridSearchCV(estimator=lgb_reg, param_grid=param_grid, \n#                              cv=5, verbose=1, n_jobs=4)","b9892e29":"# grid_searcher.fit(X_train, y_train)","6cc0bc0a":"# grid_searcher.best_params_, grid_searcher.best_score_","9ffa8667":"# mean_squared_error(y_valid, grid_searcher.predict(X_valid))","85be5efc":"# num_iterations = 500\n# lgb_reg2 = LGBMRegressor(random_state=17, max_depth=3, \n#                           num_leaves=7, n_estimators=num_iterations,\n#                           n_jobs=1)\n\n# param_grid2 = {'learning_rate': np.logspace(-3, 0, 10)}\n# grid_searcher2 = GridSearchCV(estimator=lgb_reg2, param_grid=param_grid2,\n#                                cv=5, verbose=1, n_jobs=4)\n# grid_searcher2.fit(X_train, y_train)\n# print(grid_searcher2.best_params_, grid_searcher2.best_score_)\n# print(mean_squared_error(y_valid, grid_searcher2.predict(X_valid)))","a7658ab5":"# model = xgb.XGBRegressor(n_estimators=1000)\n# eval_set = [(df_valid[col_var], df_valid[col_target])]\n# model.fit(df_train[col_var], df_train[col_target], eval_metric=\"rmse\", eval_set=eval_set, verbose=True)","f1752e47":"# 19.30146**2","bfed98c9":"# plot = plot_importance(model, height=0.9, max_num_features=20)","8d21cade":"# train model to predict cases\/day\ncol_target2 = 'cases\/day'\ncol_var2 = [\n    'Lat', 'Long',\n#    'cases\/day_mean_(1-1)', 'cases\/day_mean_(1-7)', 'cases\/day_mean_(8-14)', \n#      'fatal\/day_mean_(1-1)', 'fatal\/day_mean_(1-7)', 'fatal\/day_mean_(8-14)',\n#    'cases\/day_std_(1-1)', 'cases\/day_std_(1-7)', 'cases\/day_std_(8-14)', \n#      'fatal\/day_std_(1-1)', 'fatal\/day_std_(1-7)', 'fatal\/day_std_(8-14)',\n    'SmokingRate',\n#     'day',\n#     'dayofmonth',\n#     'dayofweek'\n]\ncol_var2 += val_cols\ncol_var2 += time_cols\n# col_var2 += ps_cols\n# col_var2 += cr_cols\ncol_var2 += extra_cols\ncol_var2 += country_info_cols\ncol_var2 += le_cols\ncol_var2 += wk3_cols\n\ndf_train = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (df_traintest3['dayofyear']<=day_before_valid)]\ndf_valid = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (day_before_valid<df_traintest3['dayofyear']) & (df_traintest3['dayofyear']<=day_before_public)]\n\nX_train = df_train[col_var2].values\nX_valid = df_valid[col_var2].values\nprint(len(X_train), len(X_valid))\n\n# scaler = MinMaxScaler()\n# scaler = RobustScaler()\n# X_train = scaler.fit_transform(X_train)\n# X_valid = scaler.transform(X_valid)\n\n# y_train = df_train[col_target2].values\n# y_valid = df_valid[col_target2].values\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\n\ntrain_data = lgb.Dataset(X_train, label=y_train)\nvalid_data = lgb.Dataset(X_valid, label=y_valid)\nnum_round = 15000\nmodel2 = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)","606b0ba2":"y_true = df_valid['cases\/day'].values\ny_pred = np.exp(model2.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score))","5ee6cb3f":"df_features2 = pd.merge( left=(pd.DataFrame(model2.feature_importance(), index=col_var2, columns=[\"importance\"])).sort_values('importance', ascending=False),\n                      right=df_train[col_var2].isnull().sum().to_frame(name='count_null'),\n                      how='left', left_index=True, right_index=True)\ndf_features2","af882c1d":"important_features2 = df_features2.index[df_features2['importance']>=18].to_list()\nprint(len(important_features2))\nimportant_features2","a8389563":"col_var2 = important_features2\n\nX_train = df_train[col_var2].values\nX_valid = df_valid[col_var2].values\n\n# scaler = MinMaxScaler()\n# scaler = RobustScaler()\n# X_train = scaler.fit_transform(X_train)\n# X_valid = scaler.transform(X_valid)\n\n# y_train = df_train[col_target2].values\n# y_valid = df_valid[col_target2].values\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\n\ntrain_data = lgb.Dataset(X_train, label=y_train)\nvalid_data = lgb.Dataset(X_valid, label=y_valid)\nnum_round = 15000\nmodel2 = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\nbest_itr2 = model2.best_iteration","7df4a2a3":"y_true = df_valid['cases\/day'].values\ny_pred = np.exp(model2.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score))","dfab1de0":"df_train = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (df_traintest3['dayofyear']<=day_before_public)]\ndf_valid = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (df_traintest3['dayofyear']<=day_before_public)]\nX_train = df_train[col_var2].values\nX_valid = df_valid[col_var2].values\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train)\nvalid_data = lgb.Dataset(X_valid, label=y_valid)\nmodel2_pub = lgb.train(params, train_data, best_itr2, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)","2d9ff5a2":"# train model to predict fatalities\/day\ndf_train = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (df_traintest3['dayofyear']<=day_before_public)]\ndf_valid = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (day_before_public<df_traintest3['dayofyear'])]\ndf_test = df_traintest3[pd.isna(df_traintest3['ForecastId'])==False]\nX_train = df_train[col_var].values\nX_valid = df_valid[col_var].values\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train)\nvalid_data = lgb.Dataset(X_valid, label=y_valid)\nnum_round = 15000\nmodel = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\n\nbest_itr = model.best_iteration","e84fedce":"# train with all data\ndf_train = df_traintest3[(pd.isna(df_traintest3['ForecastId']))]\ndf_valid = df_traintest3[(pd.isna(df_traintest3['ForecastId']))]\nX_train = df_train[col_var].values\nX_valid = df_valid[col_var].values\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train)\nvalid_data = lgb.Dataset(X_valid, label=y_valid)\nmodel_pri = lgb.train(params, train_data, best_itr, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)","ae37d208":"# train model to predict cases\/day\ndf_train = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (df_traintest3['dayofyear']<=day_before_public)]\ndf_valid = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (day_before_public<df_traintest3['dayofyear'])]\nX_train = df_train[col_var2].values\nX_valid = df_valid[col_var2].values\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train)\nvalid_data = lgb.Dataset(X_valid, label=y_valid)\nmodel2 = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\nbest_itr2 = model2.best_iteration","f8749cc6":"# train with all data\ndf_train = df_traintest3[(pd.isna(df_traintest3['ForecastId']))]\ndf_valid = df_traintest3[(pd.isna(df_traintest3['ForecastId']))]\nX_train = df_train[col_var2].values\nX_valid = df_valid[col_var2].values\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train)\nvalid_data = lgb.Dataset(X_valid, label=y_valid)\nmodel2_pri = lgb.train(params, train_data, best_itr2, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)","26084f9e":"# model2 = xgb.XGBRegressor(n_estimators=1000)\n# eval_set = [(df_valid[col_var2], df_valid[col_target2])]\n# model.fit(df_train[col_var2], df_train[col_target2], eval_metric=\"rmse\", eval_set=eval_set, verbose=True)","a3f54a74":"print(df_traintest['Date'][df_traintest['dayofyear']==day_before_valid].values[0])\nprint(df_traintest['Date'][df_traintest['dayofyear']==day_before_public].values[0])\nprint(df_traintest['Date'][df_traintest['dayofyear']==day_before_private].values[0])","ebd0ec26":"# remove overlap for public LB prediction\ndf_tmp = df_traintest3[\n    ((df_traintest3['dayofyear']<=day_before_public)  & (pd.isna(df_traintest3['ForecastId'])))\n    | ((day_before_public<df_traintest3['dayofyear']) & (pd.isna(df_traintest3['ForecastId'])==False))].reset_index(drop=True)\n# df_tmp = df_tmp.drop([\n#     'cases\/day_(1-1)', 'cases\/day_(1-7)', 'cases\/day_(8-14)', 'cases\/day_(15-21)', \n#     'fatal\/day_(1-1)', 'fatal\/day_(1-7)', 'fatal\/day_(8-14)', 'fatal\/day_(15-21)',\n#     'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n#     'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n#                                ],  axis=1)\ndf_tmp = df_tmp.drop(val_cols, axis=1)\ndf_traintest9 = []\nfor i, place in enumerate(places[:]):\n    df_tmp2 = df_tmp[df_tmp['place_id']==place].reset_index(drop=True)\n    df_tmp2 = do_aggregations(df_tmp2, roll_ranges=roll_ranges)\n    df_traintest9.append(df_tmp2)\ndf_traintest9 = pd.concat(df_traintest9).reset_index(drop=True)\ndf_traintest9[df_traintest9['dayofyear']>day_before_public-2].head()","29cda0f5":"# remove overlap for private LB prediction\ndf_tmp = df_traintest3[\n    ((df_traintest3['dayofyear']<=day_before_private)  & (pd.isna(df_traintest3['ForecastId'])))\n    | ((day_before_private<df_traintest3['dayofyear']) & (pd.isna(df_traintest3['ForecastId'])==False))].reset_index(drop=True)\n# df_tmp = df_tmp.drop([\n#     'cases\/day_(1-1)', 'cases\/day_(1-7)', 'cases\/day_(8-14)', 'cases\/day_(15-21)', \n#     'fatal\/day_(1-1)', 'fatal\/day_(1-7)', 'fatal\/day_(8-14)', 'fatal\/day_(15-21)',\n#     'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n#     'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n#                                ],  axis=1)\ndf_tmp = df_tmp.drop(val_cols, axis=1)\ndf_traintest10 = []\nfor i, place in enumerate(places[:]):\n    df_tmp2 = df_tmp[df_tmp['place_id']==place].reset_index(drop=True)\n    df_tmp2 = do_aggregations(df_tmp2, roll_ranges=roll_ranges)\n    df_traintest10.append(df_tmp2)\ndf_traintest10 = pd.concat(df_traintest10).reset_index(drop=True)\ndf_traintest10[df_traintest10['dayofyear']>day_before_private-2].head()","b4617b70":"# predict test data in public\n# predict the cases and fatatilites one day at a time and use the predicts as next day's feature recursively.\ndf_preds = []\nfor i, place in enumerate(places[:]):\n    df_interest = copy.deepcopy(df_traintest9[df_traintest9['place_id']==place].reset_index(drop=True))\n    df_interest['cases\/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    df_interest['fatal\/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    len_known = (df_interest['dayofyear']<=day_before_public).sum()\n    len_unknown = (day_before_public<df_interest['dayofyear']).sum()\n    for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n        X_valid = df_interest[col_var].iloc[j+len_known]\n        X_valid2 = df_interest[col_var2].iloc[j+len_known]\n        pred_f = model_pub.predict(X_valid)\n        pred_c = model2_pub.predict(X_valid2)\n        pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n        pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n        df_interest['fatal\/day'][j+len_known] = pred_f\n        df_interest['cases\/day'][j+len_known] = pred_c\n        df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n        df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n#         print(df_interest['ConfirmedCases'][j+len_known-1], df_interest['ConfirmedCases'][j+len_known], pred_c)\n#         df_interest = df_interest.drop([\n#             'cases\/day_(1-1)', 'cases\/day_(1-7)', 'cases\/day_(8-14)', 'cases\/day_(15-21)', \n#             'fatal\/day_(1-1)', 'fatal\/day_(1-7)', 'fatal\/day_(8-14)', 'fatal\/day_(15-21)',\n#             'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n#             'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n\n#                                        ],  axis=1)\n        df_interest = df_interest.drop(val_cols,  axis=1)\n        df_interest = do_aggregations(df_interest, roll_ranges=roll_ranges)\n    if (i+1)%10==0:\n        print(\"{:3d}\/{}  {}, len known: {}, len unknown: {}\".format(i+1, len(places), place, len_known, len_unknown), df_interest.shape)\n    df_interest['fatal_pred'] = np.cumsum(df_interest['fatal\/day'].values)\n    df_interest['cases_pred'] = np.cumsum(df_interest['cases\/day'].values)\n    df_preds.append(df_interest)\ndf_preds = pd.concat(df_preds)\ndf_preds.to_csv(\"df_preds.csv\", index=None)","fd1b0fab":"# predict test data in private\ndf_preds_pri = []\nfor i, place in enumerate(places[:]):\n    df_interest = copy.deepcopy(df_traintest10[df_traintest10['place_id']==place].reset_index(drop=True))\n    df_interest['cases\/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    df_interest['fatal\/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    len_known = (df_interest['dayofyear']<=day_before_private).sum()\n    len_unknown = (day_before_private<df_interest['dayofyear']).sum()\n    for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n        X_valid = df_interest[col_var].iloc[j+len_known]\n        X_valid2 = df_interest[col_var2].iloc[j+len_known]\n        pred_f = model_pri.predict(X_valid)\n        pred_c = model2_pri.predict(X_valid2)\n        pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n        pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n        df_interest['fatal\/day'][j+len_known] = pred_f\n        df_interest['cases\/day'][j+len_known] = pred_c\n        df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n        df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n#         print(df_interest['ConfirmedCases'][j+len_known-1], df_interest['ConfirmedCases'][j+len_known], pred_c)\n#         df_interest = df_interest.drop([\n#             'cases\/day_(1-1)', 'cases\/day_(1-7)', 'cases\/day_(8-14)', 'cases\/day_(15-21)', \n#             'fatal\/day_(1-1)', 'fatal\/day_(1-7)', 'fatal\/day_(8-14)', 'fatal\/day_(15-21)',\n#             'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n#             'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n\n#                                        ],  axis=1)\n        df_interest = df_interest.drop(val_cols,  axis=1)\n        df_interest = do_aggregations(df_interest, roll_ranges=roll_ranges)\n    if (i+1)%10==0:\n        print(\"{:3d}\/{}  {}, len known: {}, len unknown: {}\".format(i+1, len(places), place, len_known, len_unknown), df_interest.shape)\n    df_interest['fatal_pred'] = np.cumsum(df_interest['fatal\/day'].values)\n    df_interest['cases_pred'] = np.cumsum(df_interest['cases\/day'].values)\n    df_preds_pri.append(df_interest)\ndf_preds_pri = pd.concat(df_preds_pri)\ndf_preds_pri.to_csv(\"df_preds_pri.csv\", index=None)","197603d7":"places_sort = df_traintest10[['place_id', 'ConfirmedCases']][df_traintest10['dayofyear']==day_before_private]\nplaces_sort = places_sort.sort_values('ConfirmedCases', ascending=False).reset_index(drop=True)['place_id'].values\nprint(len(places_sort))\nplaces_sort[:5]","c733dcb0":"print(\"Fatalities \/ Public\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds[df_preds['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['fatal\/day'].values\n    tmp = np.cumsum(tmp)\n#     print(len(tmp), places_sort[idx])\n    sns.lineplot(x=df_interest['dayofyear'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['dayofyear']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['dayofyear'].values, y=df_interest2['Fatalities'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()","939b69ad":"print(\"Confirmed Cases \/ Public\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds[df_preds['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['cases\/day'].values\n    tmp = np.cumsum(tmp)\n    sns.lineplot(x=df_interest['dayofyear'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['dayofyear']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['dayofyear'].values, y=df_interest2['ConfirmedCases'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()","137594dd":"print(\"Fatalities \/ Private\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds_pri[df_preds_pri['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['fatal\/day'].values\n    tmp = np.cumsum(tmp)\n    sns.lineplot(x=df_interest['dayofyear'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['dayofyear']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['dayofyear'].values, y=df_interest2['Fatalities'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()","c0d2e583":"print(\"ConfirmedCases \/ Private\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds_pri[df_preds_pri['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['cases\/day'].values\n    tmp = np.cumsum(tmp)\n    sns.lineplot(x=df_interest['dayofyear'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['dayofyear']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['dayofyear'].values, y=df_interest2['ConfirmedCases'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()","b4e1168e":"# # remove overlaps between train and test\n# df_traintest4 = copy.deepcopy(df_traintest3)\n# df_traintest4['unique'] = df_traintest4.apply(lambda x: x['place_id'] + str(x['dayofyear']), axis=1)\n# print(len(df_traintest4))\n# df_traintest4 = df_traintest4[df_traintest4['unique'].duplicated()==False]\n# print(len(df_traintest4))\n# df_traintest4[(df_traintest4['place_id']=='China\/Hubei') & (df_traintest4['dayofyear']>75)].head() #2020-03-15","3913804b":"# # count the fatalities per place until Feb.\n# df_tmp = df_traintest[pd.isna(df_traintest['Fatalities'])==False]\n# df_tmp = df_tmp[df_tmp['dayofyear']<61]\n# df_agg = df_tmp.groupby('place_id')['Fatalities'].agg('max').reset_index()\n# df_agg = df_agg.sort_values('Fatalities', ascending=False)\n# df_agg.head()","59d8a798":"print(len(col_var), len(col_var2))\ncol_var, col_var2","5bd2fe7c":"# # Check the predictions of some hot areas.\n# place = 'China\/Hubei'\n# # place = 'Iran'\n# df_interest_base = df_traintest4[df_traintest4['place_id']==place].reset_index(drop=True)\n# df_interest = copy.deepcopy(df_interest_base)\n# df_interest['cases\/day'] = df_interest['cases\/day'].astype(np.float)\n# df_interest['fatal\/day'] = df_interest['fatal\/day'].astype(np.float)\n# df_interest['cases\/day'][df_interest['dayofyear']>=train_valid_cutoff_dayofyear] = -1\n# df_interest['fatal\/day'][df_interest['dayofyear']>=train_valid_cutoff_dayofyear] = -1\n# len_known = (df_interest['cases\/day']!=-1).sum()\n# len_unknown = (df_interest['cases\/day']==-1).sum()\n# print(\"len train: {}, len prediction: {}\".format(len_known, len_unknown))\n# for i in range(len_unknown): # use predicted cases and fatal for next days' prediction\n# #     print(i)\n#     X_valid = df_interest[col_var].iloc[i+len_known]\n#     X_valid2 = df_interest[col_var2].iloc[i+len_known]\n# #     print(X_valid.shape)\n#     pred_f = model.predict(X_valid)\n#     pred_c = model2.predict(X_valid2)\n#     df_interest['fatal\/day'][i+len_known] = pred_f\n#     df_interest['cases\/day'][i+len_known] = pred_c\n#     df_interest = df_interest[['cases\/day', 'fatal\/day', 'Long', 'Lat', 'SmokingRate']+time_cols+extra_cols+country_info_cols+le_cols+wk3_cols]\n#     df_interest = do_aggregations(df_interest, roll_ranges=roll_ranges)\n\n# # visualize\n# tmp = df_interest_base['fatal\/day'].values\n# tmp = np.cumsum(tmp)\n# sns.lineplot(x=df_interest_base['dayofyear'][pd.isna(df_interest_base['Fatalities'])==False],\n#              y=tmp[pd.isna(df_interest_base['Fatalities'])==False], label='true')\n# tmp = df_interest['fatal\/day'].values\n# tmp = np.cumsum(tmp)\n# sns.lineplot(x=df_interest_base['dayofyear'], y=tmp, label='pred')\n# plt.show()","88d97fb6":"# place = 'Iran'\n# df_interest_base = df_traintest4[df_traintest4['place_id']==place].reset_index(drop=True)\n# df_interest = copy.deepcopy(df_interest_base)\n# df_interest['cases\/day'] = df_interest['cases\/day'].astype(np.float)\n# df_interest['fatal\/day'] = df_interest['fatal\/day'].astype(np.float)\n# df_interest['cases\/day'][df_interest['dayofyear']>=train_valid_cutoff_dayofyear] = -1\n# df_interest['fatal\/day'][df_interest['dayofyear']>=train_valid_cutoff_dayofyear] = -1\n# len_known = (df_interest['cases\/day']!=-1).sum()\n# len_unknown = (df_interest['cases\/day']==-1).sum()\n# print(\"len train: {}, len prediction: {}\".format(len_known, len_unknown))\n# for i in range(len_unknown): # use predicted cases and fatal for next days' prediction\n#     X_valid = df_interest[col_var].iloc[i+len_known]\n#     X_valid2 = df_interest[col_var2].iloc[i+len_known]\n# #     print(X_valid.shape)\n#     pred_f = model.predict(X_valid)\n#     pred_c = model2.predict(X_valid2)\n#     df_interest['fatal\/day'][i+len_known] = pred_f\n#     df_interest['cases\/day'][i+len_known] = pred_c\n#     df_interest = df_interest[['cases\/day', 'fatal\/day', 'Long', 'Lat', 'SmokingRate']+time_cols+extra_cols+country_info_cols+le_cols+wk3_cols]\n#     df_interest = do_aggregations(df_interest, roll_ranges=roll_ranges)\n\n# # visualize\n# tmp = df_interest_base['fatal\/day'].values\n# tmp = np.cumsum(tmp)\n# sns.lineplot(x=df_interest_base['dayofyear'][pd.isna(df_interest_base['Fatalities'])==False],\n#              y=tmp[pd.isna(df_interest_base['Fatalities'])==False], label='true')\n# tmp = df_interest['fatal\/day'].values\n# tmp = np.cumsum(tmp)\n# sns.lineplot(x=df_interest_base['dayofyear'], y=tmp, label='pred')\n# plt.show()","eafe2be2":"# place = 'Italy'\n# df_interest_base = df_traintest4[df_traintest4['place_id']==place].reset_index(drop=True)\n# df_interest = copy.deepcopy(df_interest_base)\n# df_interest['cases\/day'] = df_interest['cases\/day'].astype(np.float)\n# df_interest['fatal\/day'] = df_interest['fatal\/day'].astype(np.float)\n# df_interest['cases\/day'][df_interest['dayofyear']>=train_valid_cutoff_dayofyear] = -1\n# df_interest['fatal\/day'][df_interest['dayofyear']>=train_valid_cutoff_dayofyear] = -1\n# len_known = (df_interest['cases\/day']!=-1).sum()\n# len_unknown = (df_interest['cases\/day']==-1).sum()\n# print(\"len train: {}, len prediction: {}\".format(len_known, len_unknown))\n# for i in range(len_unknown): # use predicted cases and fatal for next days' prediction\n#     X_valid = df_interest[col_var].iloc[i+len_known]\n#     X_valid2 = df_interest[col_var2].iloc[i+len_known]\n# #     print(X_valid.shape)\n#     pred_f = model.predict(X_valid)\n#     pred_c = model2.predict(X_valid2)\n#     df_interest['fatal\/day'][i+len_known] = pred_f\n#     df_interest['cases\/day'][i+len_known] = pred_c\n#     df_interest = df_interest[['cases\/day', 'fatal\/day', 'Long', 'Lat', 'SmokingRate']+time_cols+extra_cols+country_info_cols+le_cols+wk3_cols]\n#     df_interest = do_aggregations(df_interest, roll_ranges=roll_ranges)\n\n# # visualize\n# tmp = df_interest_base['fatal\/day'].values\n# tmp = np.cumsum(tmp)\n# sns.lineplot(x=df_interest_base['dayofyear'][pd.isna(df_interest_base['Fatalities'])==False],\n#              y=tmp[pd.isna(df_interest_base['Fatalities'])==False], label='true')\n# tmp = df_interest['fatal\/day'].values\n# tmp = np.cumsum(tmp)\n# sns.lineplot(x=df_interest_base['dayofyear'], y=tmp, label='pred')\n# plt.show()","ff7999e1":"# # train model to predict fatalities\/day\n# # col_target = 'fatal\/day'\n# # col_var = [\n# #     'Lat', 'Long',\n# # #     'cases\/day_(1-1)', 'cases\/day_(1-7)', 'cases\/day_(8-14)', \n# #     'fatal\/day_(1-1)', 'fatal\/day_(1-7)', 'fatal\/day_(8-14)',\n# #     'SmokingRate',\n# # #     'day'\n# # ]\n# df_train = df_traintest3[(pd.isna(df_traintest3['ForecastId']))]\n# X_train = df_train[col_var].values\n# X_valid = df_train[col_var].values\n# y_train = df_train[col_target].values\n# y_valid = df_train[col_target].values\n# train_data = lgb.Dataset(X_train, label=y_train)\n# valid_data = lgb.Dataset(X_valid, label=y_valid)\n# # num_round = 575\n# num_round = 15000\n# model = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data], verbose_eval=100)","dec5d54a":"# # train model to predict cases\/day\n# df_train = df_traintest3[(pd.isna(df_traintest3['ForecastId']))]\n# X_train = df_train[col_var2].values\n# X_valid = df_train[col_var2].values\n# y_train = df_train[col_target2].values\n# y_valid = df_train[col_target2].values\n# train_data = lgb.Dataset(X_train, label=y_train)\n# valid_data = lgb.Dataset(X_valid, label=y_valid)\n# # num_round = 225\n# num_round = 15000\n# model2 = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data], verbose_eval=100,)","2c444ebc":"# # predict test data\n# df_preds = []\n# for i, place in enumerate(places[:]):\n#     df_interest = copy.deepcopy(df_traintest4[df_traintest4['place_id']==place].reset_index(drop=True))\n#     df_interest['cases\/day'] = df_interest['cases\/day'].astype(np.float)\n#     df_interest['fatal\/day'] = df_interest['fatal\/day'].astype(np.float)\n#     df_interest['cases\/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n#     df_interest['fatal\/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n#     len_known = (df_interest['cases\/day']!=-1).sum()\n#     len_unknown = (df_interest['cases\/day']==-1).sum()\n#     if (i+1)%10==0:\n#         print(\"{:3d}\/{}  {}, len known: {}, len unknown: {}\".format(i+1, len(places), place, len_known, len_unknown), df_interest.shape)\n#     for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n#         X_valid = df_interest[col_var].iloc[j+len_known]\n#         X_valid2 = df_interest[col_var2].iloc[j+len_known]\n# #         print(X_valid.shape)\n#         pred_f = model.predict(X_valid)\n#         pred_c = model2.predict(X_valid2)\n# #         print(pred_f, pred_c)\n#         df_interest['fatal\/day'][j+len_known] = pred_f\n#         df_interest['cases\/day'][j+len_known] = pred_c\n#         df_interest = df_interest[['cases\/day', 'fatal\/day', 'Long', 'Lat', 'SmokingRate', 'ForecastId', 'place_id']+time_cols+extra_cols+country_info_cols+le_cols+wk3_cols]\n#         df_interest = do_aggregations(df_interest, roll_ranges=roll_ranges)\n#     df_interest['fatal_pred'] = np.cumsum(df_interest['fatal\/day'].values)\n#     df_interest['cases_pred'] = np.cumsum(df_interest['cases\/day'].values)\n#     df_preds.append(df_interest)","c42d6f1d":"len(df_preds), len(df_preds_pri)","6954e1e2":"# merge 2 preds\ndf_preds[df_preds['dayofyear']>day_before_private] = df_preds_pri[df_preds['dayofyear']>day_before_private]","d991abd1":"df_preds.to_csv(\"df_preds2.csv\", index=None)","f6b7e47f":"# # concat prediction\n# df_preds= pd.concat(df_preds)\n# df_preds = df_preds.sort_values('dayofyear')\n# col_tmp = ['place_id', 'ForecastId', 'dayofyear', 'cases\/day', 'cases_pred', 'fatal\/day', 'fatal_pred',]\n# df_preds[col_tmp][(df_preds['place_id']=='Afghanistan') & (df_preds['dayofyear']>75)].head(10)","f02e391c":"# load sample submission\ndf_sub = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/submission.csv\")\nprint(len(df_sub))\ndf_sub.head()","fbe7c8b3":"# merge prediction with sub\ndf_sub = pd.merge(df_sub, df_traintest3[['ForecastId', 'place_id', 'dayofyear']])\ndf_sub = pd.merge(df_sub, df_preds[['place_id', 'dayofyear', 'cases_pred', 'fatal_pred']], on=['place_id', 'dayofyear',], how='left')\ndf_sub.head(10)","553980d5":"# save\ndf_sub['ConfirmedCases'] = df_sub['cases_pred']\ndf_sub['Fatalities'] = df_sub['fatal_pred']\ndf_sub = df_sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\ndf_sub.to_csv(\"submission.csv\", index=None)\ndf_sub.head(10)","c88b9e57":"## Add World Bank Dataset","e817c657":"## Visualize prediction","0675ebc5":"Reference: https:\/\/www.kaggle.com\/osciiart\/covid-19-lightgbm-no-leak\/notebook","88db23ad":"## Preprocessing","8aaecbef":"**2 stage of hyper-param tuning: convergence**","3cc5cc64":"## Data Loading","1bc9b6e9":"## Adding Smoking Rate Data","7eab253a":"## Add Country Info Data","b33a05ad":"## Make Submission","351090f3":"## Prediction","87e4c9e9":"**Train a model for private LB**","c4edaeed":"## Model Training","07c506fc":"**1 stage of hyper-param tuning: tuning model complexity**"}}