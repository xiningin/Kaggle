{"cell_type":{"99be4fb1":"code","892738f3":"code","f32483d8":"code","b9f5ba2c":"code","e88b45a8":"code","54009635":"code","25c4151e":"code","3e0bb34c":"code","e9466c66":"code","4d18e372":"code","4174e327":"code","38547425":"code","6e50f737":"code","8b4e36a2":"code","7de39608":"code","ba9ec6cc":"code","5793a227":"code","936d8565":"code","0f6de305":"code","fac3495a":"code","0ccbed91":"code","55f09e08":"code","99c15d53":"code","91f12417":"code","44ec0269":"code","a432ed56":"code","4d26aae6":"code","115633aa":"code","c8b2100d":"code","8964f4d9":"code","95326270":"code","4335cc86":"code","cf4b40f0":"code","f1f0d2aa":"code","157f3ec0":"code","fa11718b":"code","5fd3bc66":"code","2453db7a":"code","1c056050":"code","6e288ddf":"code","1c4db9f2":"code","1f641db4":"code","58a6d050":"code","fd2b0e98":"code","26dcd165":"code","4c4e315c":"code","727d533a":"code","b083fe26":"code","6fd8de46":"code","5d82e43d":"code","63c4d519":"code","822434b4":"code","af24af79":"code","9a83f932":"code","212e7036":"code","5bff721c":"code","883efdd0":"code","9e63824f":"code","dc210c3d":"code","7bac77ed":"code","6738bf57":"code","4419ea4a":"code","f6ec30f2":"code","d038f57d":"code","80ba674a":"code","4b691bad":"code","194e7ad6":"code","9bb28331":"code","5300d875":"code","f75ff9a9":"code","9a8bbfa3":"code","35450ca5":"code","880d8476":"code","0dfdfb3b":"code","eccb61d4":"code","b2c9c93e":"code","a58d78c8":"code","c51c1516":"code","ba832f63":"code","5a470b8c":"code","38df6c44":"code","8a3e23d3":"code","b5982dfa":"code","cdfe0272":"code","42e67ff9":"code","84a309f1":"code","c1f95e51":"code","87891c9b":"code","1cc15bd7":"code","0f8061db":"code","85a8c198":"code","400b3a39":"code","ef4140f4":"code","3c3b28da":"code","e6c0924f":"code","69fb4311":"code","2ec4653e":"code","8d3275c7":"code","ef21e5c8":"code","4a5658e8":"code","cfbb20e8":"markdown","d96aed25":"markdown","94db7499":"markdown","e1e93389":"markdown","b445a109":"markdown","b8e1af05":"markdown","1e602990":"markdown","d96dfe6f":"markdown","d3fb7759":"markdown","d13a2473":"markdown","f6efd6cb":"markdown","0c08cfcf":"markdown","2caa3ca1":"markdown","399b9fc4":"markdown","e0ea6eaf":"markdown","0e3e2cdd":"markdown","c69e540e":"markdown","1654b52e":"markdown","30c91443":"markdown","d2f2872d":"markdown","f567471e":"markdown","e42bb4aa":"markdown","fde3e296":"markdown","79232908":"markdown","8a10b514":"markdown","de30a1ff":"markdown","516dba05":"markdown","225ee215":"markdown","871b8f44":"markdown","74e1bb9e":"markdown","90f91a40":"markdown","c5ed5af2":"markdown","57793de8":"markdown","fee172bb":"markdown","eedb93d4":"markdown","257f140f":"markdown","8c2e3d73":"markdown","6a5ff1a8":"markdown","eb36403c":"markdown","9cb04077":"markdown","afa5ea85":"markdown","aa5ce1e3":"markdown","3dae2f6a":"markdown","ebdc3000":"markdown","6d8eee47":"markdown","3ac00d8a":"markdown","609b4bc4":"markdown","1627b584":"markdown"},"source":{"99be4fb1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","892738f3":"#REQUIRED INSTALLATION\n#pip install pandas-profiling\n#pip install plotly","f32483d8":"#to avoid warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#os\nimport os\n\n#linear algebra libraries\nimport numpy as np, pandas as pd\nimport pandas_profiling as pp\n\n#libraries for plotting graphs\nimport matplotlib.pyplot as plt, seaborn as sns, matplotlib\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\n#for model building\nfrom sklearn.preprocessing import scale, StandardScaler\nfrom sklearn import linear_model\nfrom sklearn import metrics\nimport statsmodels.api as sm\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, mean_squared_error as MSE","b9f5ba2c":"#heart analysis dataset\nheart_df = pd.read_csv(\"\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")\nheart_df.head(4)","e88b45a8":"#o2 saturation level dataset\nsaturation = pd.read_csv(\"\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/o2Saturation.csv\")\nsaturation.head(4)","54009635":"#checking profile summary of heart attack dataset\npp.ProfileReport(heart_df)","25c4151e":"#getting all types of info from dataset\nheart_df.info()","3e0bb34c":"#checking for null values\nheart_df.isnull().sum()","e9466c66":"#decribing columns\nheart_df.describe()","4d18e372":"#separating categorical columns\ncat_heart = heart_df[['sex', 'cp', 'fbs', 'restecg', 'exng', 'slp', 'caa', 'thall']]\n\n#separating continuous columns\nconti_heart = heart_df[['age', 'trtbps', 'chol', 'thalachh', 'oldpeak']]","4174e327":"#decribing continuous columns\nconti_heart.describe()","38547425":"#value counts for the categorical columns\n\nsex_count = heart_df['sex'].value_counts()\ncp_count = heart_df['cp'].value_counts()\nfbs_count = heart_df['fbs'].value_counts()\nrestecg_count = heart_df['restecg'].value_counts()\nslp_count = heart_df['slp'].value_counts()\nexng_count = heart_df['exng'].value_counts()\ncaa_count = heart_df['caa'].value_counts()\nthall_count = heart_df['thall'].value_counts()\n\n#printing all the values\nprint(\"sex_count :\\n\", sex_count)\nprint(\"cp_count :\\n\", cp_count)\nprint(\"fbs_count :\\n\", fbs_count)\nprint(\"restecg_count :\\n\", restecg_count)\nprint(\"slp_count :\\n\", slp_count)\nprint(\"exng_count :\\n\", exng_count)\nprint(\"caa_count :\\n\", caa_count)\nprint(\"thall_count :\\n\", thall_count)","6e50f737":"def distplot_check(column):\n    plt.title('Checking Outliers with distplot()')\n    sns.distplot(column, bins=10)\n    plt.show()","8b4e36a2":"#resting blood pressure (in mm Hg)\ndistplot_check(heart_df['trtbps'])","7de39608":"#cholestoral in mg\/dl fetched via BMI sensor\ndistplot_check(heart_df['chol'])","ba9ec6cc":"#maximum heart rate achieved\ndistplot_check(heart_df['thalachh'])","5793a227":"#ST depression induced by exercise relative to rest\ndistplot_check(heart_df['oldpeak'])","936d8565":"#analysis of the output variable : how much chance of heart attack\nax = sns.countplot(data = heart_df,  x = 'output', palette = ['#85bfdc','#f64c72'])\nax.set(xticklabels = ['less chance of heart attack', 'more chance of heart attack'], title = \"Target Distribution\")\nax.tick_params(bottom = False)","0f6de305":"#analysis of the age variable : which age has the chance of heart attack\nfig = px.histogram(heart_df, x=\"age\", color=\"output\", marginal=\"box\", \n                   hover_data = heart_df.columns, color_discrete_sequence=['#f64c72','#85bfdc'])\n\n#layout\nfig.update_layout(\n    title = \"Heart attack chance corresponding to age\"\n)\n\n#plot\nfig.show()\n\n#index\nprint(\"*1 : high chance of heart attack\\n*0 : low chance of heart attack\")","fac3495a":"#analysis of the sex variable : which sex has the chance of heart attack\nax = sns.countplot(data = heart_df, x = 'sex', hue = 'output')\nax.set(xticklabels = ['female', 'male'], title = \"Heart attack chance corresponding to Gender\")\nax.tick_params(bottom = False)\n\n#index\nprint(\"Output if --\\n*1 : high chance of heart attack\\n*0 : low chance of heart attack\")","0ccbed91":"def bivariate(column):\n    \n    #if 1 : high chance of heart attack\n    high = heart_df[heart_df['output']==1][column]\n    #else 0 : low chance of heart attack\n    low = heart_df[heart_df['output']==0][column]\n    \n    #plotting\n    fig = ff.create_distplot([high, low],\n                             ['more chance of heart attack', 'less chance of heart attack'], \n                             show_hist=False, colors=['#f64c72', '#85bfdc'])\n    \n    #getting layout\n    fig.update_layout(\n        title = \"Heart Attack chance corresponding to \" + column,\n        xaxis_title = 'Feature Variable : ' + column,\n    )\n\n    #show plot\n    fig.show()","55f09e08":"print(\"Analysis of trtbps variable : how blood pressure is related to the chance of heart attack\")\nbivariate('trtbps')","99c15d53":"print(\"Analysis of chol variable : how cholestoral is related to the chance of heart attack\")\nbivariate('chol')","91f12417":"print(\"Analysis of thalachh variable : how maximum heart rate achieved is related to the chance of heart attack\")\nbivariate('thalachh')","44ec0269":"print(\"Analysis of oldepeak variable : how ST depression induced by exercise relative to rest is related to the chance of heart attack\")\nbivariate('oldpeak')","a432ed56":"fig, ax = plt.subplots(1, 1, figsize=(6,6))\ndf_cor = conti_heart.corr()\n\nhalf = np.triu(np.ones_like(df_cor, dtype=np.bool))\n\nmy_colors = ['#85bfdc','#f64c72']\ncmap = matplotlib.colors.LinearSegmentedColormap.from_list('Custom', my_colors)\n\nheatmap = sns.heatmap(df_cor, \n            square=True, \n            mask=half,\n            linewidth=2.5, \n            vmax=0.4, vmin=0, \n            cmap=cmap, \n            cbar=False, \n            ax=ax,annot=True)\n\nheatmap.set(title=\"Heatmap of continous variables\")\nheatmap.set_yticklabels(heatmap.get_xticklabels(), rotation = 0)\nheatmap.spines['top'].set_visible(True)\n\nplt.tight_layout()","4d26aae6":"fig, ax = plt.subplots(1, 1, figsize=(6,6))\ndf_cor = cat_heart.corr()\n\nhalf = np.triu(np.ones_like(df_cor, dtype=np.bool))\n\nmy_colors = ['#85bfdc','#f64c72']\ncmap = matplotlib.colors.LinearSegmentedColormap.from_list('Custom', my_colors)\n\nheatmap = sns.heatmap(df_cor, \n            square=True, \n            mask=half,\n            linewidth=2.5, \n            vmax=0.4, vmin=0, \n            cmap=cmap, \n            cbar=False, \n            ax=ax,annot=True)\n\nheatmap.set(title=\"Heatmap of categorical variables\")\nheatmap.set_yticklabels(heatmap.get_xticklabels(), rotation = 0)\nheatmap.spines['top'].set_visible(True)\n\nplt.tight_layout()","115633aa":"fig, ax = plt.subplots(1, 1, figsize=(15,10))\ndf_cor = heart_df.corr()\n\nhalf = np.triu(np.ones_like(df_cor, dtype=np.bool))\n\nmy_colors = ['#85bfdc','#f64c72']\ncmap = matplotlib.colors.LinearSegmentedColormap.from_list('Custom', my_colors)\n\nheatmap = sns.heatmap(df_cor, \n            square=True, \n            mask=half,\n            linewidth=2.5, \n            vmax=0.4, vmin=0, \n            cmap=cmap, \n            cbar=False, \n            ax=ax,annot=True)\n\nheatmap.set(title=\"Heatmap of all variables in the heart dataset\")\nheatmap.set_yticklabels(heatmap.get_xticklabels(), rotation = 0)\nheatmap.spines['top'].set_visible(True)\n\nplt.tight_layout()","c8b2100d":"#correlation matrix more clearly among the variables\ncluster_map = sns.clustermap(df_cor.corr(), annot=True)\n\nplt.title(\"CLUSTERMAP of all variables in the HEART ANALYSIS\\n\\n\")\nplt.tight_layout()","8964f4d9":"#thalachh vs chol vs cp vs sex WITH RESPECT TO output\nfig = px.scatter(heart_df,\n    x='thalachh',\n    y= 'chol',\n    color='output',\n    facet_col='cp', \n    facet_row='sex',\n    color_discrete_sequence=['#f64c72','#85bfdc'], \n)\n\nfig.show()","95326270":"#thalachh vs chol vs restecg vs sex WITH RESPECT TO output\nfig = px.scatter(heart_df,\n    x='thalachh',\n    y= 'chol',\n    color='output',\n    facet_col='restecg', \n    facet_row='sex',\n    color_discrete_sequence=['#f64c72','#85bfdc'], \n)\n\nfig.show()","4335cc86":"#thalachh vs chol vs ca vs sex WITH RESPECT TO output\nfig = px.scatter(heart_df,\n    x='thalachh',\n    y= 'chol',\n    color='output',\n    facet_col='caa', \n    facet_row='sex',\n    color_discrete_sequence=['#f64c72','#85bfdc'], \n)\n\nfig.show()","cf4b40f0":"#thalachh vs chol vs exng vs sex WITH RESPECT TO output\nfig = px.scatter(heart_df,\n    x='thalachh',\n    y= 'chol',\n    color='output',\n    facet_col='exng', \n    facet_row='sex',\n    color_discrete_sequence=['#f64c72','#85bfdc'], \n)\n\nfig.show()","f1f0d2aa":"#thalachh vs chol vs fbs vs sex WITH RESPECT TO output\nfig = px.scatter(heart_df,\n    x='thalachh',\n    y= 'chol',\n    color='output',\n    facet_col='fbs', \n    facet_row='sex',\n    color_discrete_sequence=['#f64c72','#85bfdc'], \n)\n\nfig.show()","157f3ec0":"#mapping the categorical columns\nheart_df['cp'] = heart_df['cp'].map({0:'asymptomatic', 1:'atypical angina', 2:'non-anginal pain' , 3:'typical angina'})\nheart_df['restecg'] = heart_df['restecg'].map({0:'left ventricular hypertrophy', 1:'normal', 2:'ST-T wave abnormality'})\nheart_df['thall'] = heart_df['thall'].map({1:'fixed defect', 2:'normal', 3:'reversable defect', 0:'nothing'})","fa11718b":"heart_df.head(3)","5fd3bc66":"#getting dummy variables\nheart_data = pd.get_dummies(heart_df, drop_first=False)\nheart_data.columns","2453db7a":"#store the required column\ntemp_df = heart_data['thall_fixed defect']\n\n#getting dummy variables\nheart_data = pd.get_dummies(heart_df, drop_first=True)\nheart_data.head(4)","1c056050":"#checking the thall_nothing value_counts\nheart_data['thall_nothing'].value_counts()","6e288ddf":"#merge the two dataframe\nmerge_df = [heart_data, temp_df]\n\n#concatenate the data\nheart_final = pd.concat(merge_df, axis=1)\n\n#drop the null column\nheart_final.drop('thall_nothing', axis=1, inplace=True)\n\n#dataframe\nheart_final.head()","1c4db9f2":"#checking columns\nheart_final.columns","1f641db4":"#shape of the dataset\nheart_final.shape","58a6d050":"X = heart_final.drop('output', axis = 1)\ny = heart_final['output']","fd2b0e98":"X.columns","26dcd165":"print(X.shape)\nprint(y.shape)","4c4e315c":"#80-20 split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","727d533a":"#normalize the X variable data\nX_train = (X_train - np.min(X_train)) \/ (np.max(X_train) - np.min(X_train)).values\nX_test = (X_test - np.min(X_test)) \/ (np.max(X_test) - np.min(X_test)).values","b083fe26":"#shape of train sets\nprint(X_train.shape)\nprint(y_train.shape)\n\n#shape of test sets\nprint(X_test.shape)\nprint(y_test.shape)","6fd8de46":"#y_train\nprint(\"y_train :\")\nprint(\"counts of label 1\")\ntrain_label1 = sum(y_train==1)\nprint(train_label1)\n\nprint(\"counts of label 0\")\ntrain_label0 = sum(y_train==0)\nprint(train_label0)","5d82e43d":"#y_test\nprint(\"y_test :\")\nprint(\"counts of label 1\")\ntest_label1 = sum(y_test==1)\nprint(test_label1)\n\nprint(\"counts of label 0\")\ntest_label0 = sum(y_test==0)\nprint(test_label0)","63c4d519":"#create pipeline\npca = Pipeline([('scaler', StandardScaler()), ('pca', PCA())])\n\n#fitting pca\npca.fit(X_train)\nheart_pca = pca.fit_transform(X_train)\n\n#pca model extraction\npca = pca.named_steps['pca']\n\n#explainded variance of PCA components\nprint(pd.Series(np.round(pca.explained_variance_ratio_.cumsum(), 4)*100))","822434b4":"#First Training Model\n\n#Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","af24af79":"#Feature Selection Using RFE\nlogreg = LogisticRegression()\n\n#running RFE with 12 variables\nrfe = RFE(logreg, 12)\n\n#fit rfe\nrfe = rfe.fit(X_train, y_train)\n\n#rfe support variables\nrfe.support_","9a83f932":"#listing support an ranking together\nlist(zip(X_train.columns, rfe.support_, rfe.ranking_))","212e7036":"col = X_train.columns[rfe.support_]\ncol","5bff721c":"#columns not supported by rfe\nX_train.columns[~rfe.support_]","883efdd0":"#model with StatsModels\nX_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","9e63824f":"#predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","dc210c3d":"#getting predicted values\ny_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","7bac77ed":"#figure size\nplt.figure(figsize = (20,10))\n\n#heatmap for train data\nsns.heatmap(X_train[col].corr(),annot = True)","6738bf57":"#figure size\nplt.figure(figsize = (20,10))\n\n#heatmap for test data\nsns.heatmap(X_test[col].corr(),annot = True)","4419ea4a":"#creating PCA class\npca = PCA(svd_solver='randomized', random_state=42)\n\n#pca fitting on train data\npca.fit(X_train)","f6ec30f2":"#getting pca components\npc = pca.components_\n\n#listing all the columns of X_train together\ncol_names = list(X_train.columns)\n\n#finding top 10 pca components\npca_features = pd.DataFrame({'PC1':pc[0],'PC2':pc[1],'PC3':pc[2],'PC4':pc[3],'PC5':pc[4],\n                             'PC6':pc[5],'PC7':pc[6],'PC8':pc[7],'PC9':pc[8],'PC10':pc[9], \n                            'Features':col_names})\n\npca_features.head(10)","d038f57d":"%matplotlib inline\nfig = plt.figure(figsize=(10,5))\n\ncum_sum = np.cumsum(pca.explained_variance_ratio_)\nplt.plot(cum_sum)\n\n#labels\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\n\nplt.show()","80ba674a":"#creating pipeline\nPCA_VARS = 18\nsteps = [('scaler', StandardScaler()),\n         (\"pca\", PCA(n_components=PCA_VARS)),\n         (\"logistic\", LogisticRegression(class_weight='balanced'))\n        ]\npipeline = Pipeline(steps)","4b691bad":"#train data\n\n#fit pipeline model\npipeline.fit(X_train, y_train)\n\n#checking score on train data\npipeline.score(X_train, y_train)","194e7ad6":"#churn prediction on test data\ny_pred = pipeline.predict(X_test)\n\n#confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\nTP = cm[1,1] # true positive \nTN = cm[0,0] # true negatives\nFP = cm[0,1] # false positives\nFN = cm[1,0] # false negatives\n\n#Let's see the sensitivity\nprint('\\nSensitivity: ', TP \/ float(TP+FN))\n\n#Let us calculate specificity\nprint('Specificity: ',TN \/ float(TN+FP))\n\n#area under curve (AUC)\ny_pred_prob = pipeline.predict_proba(X_test)[:, 1]\nprint(\"\\n AUC: \", round(roc_auc_score(y_test, y_pred_prob),2))","9bb28331":"#identifying class imbalance\ny_train.value_counts()\/y_train.shape","5300d875":"#pca = PCA()\n\n#logistic Regression with class_weight parameter\nlogistic = LogisticRegression(class_weight='balanced')\n\n#creating pipeline\nsteps = [(\"scaler\", StandardScaler()), (\"pca\", pca),(\"logistic\", logistic)]\n\n#pipeline\npca_logistic = Pipeline(steps)\n\n#hyperparameter\nparams = {'pca__n_components': [14, 4], 'logistic__C': [0.1, 0.5, 1, 2, 3, 4, 5, 10], 'logistic__penalty': ['l1', 'l2']}\n\n#create 5 folds\nfolds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)\n\n#gridsearch object\nmodel = GridSearchCV(estimator=pca_logistic, cv=folds, param_grid=params, scoring='roc_auc', n_jobs=-1, verbose=1)","f75ff9a9":"#fit model\nmodel.fit(X_train, y_train)","9a8bbfa3":"#print best hyperparameters\nprint(\"Best AUC: \", model.best_score_)\nprint(\"Best hyperparameters: \", model.best_params_)","35450ca5":"# cross validation results\npd.DataFrame(model.cv_results_)","880d8476":"# predict churn on test data\ny_pred = model.predict(X_test)\n\n# create onfusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\nTP = cm[1,1] # true positive \nTN = cm[0,0] # true negatives\nFP = cm[0,1] # false positives\nFN = cm[1,0] # false negatives\n\n#Let's see the sensitivity\nprint('\\nSensitivity: ', TP \/ float(TP+FN))\n\n#Let us calculate specificity\nprint('Specificity: ',TN \/ float(TN+FP))\n\n# check area under curve\ny_pred_prob = model.predict_proba(X_test)[:, 1]\nprint(\"\\nAUC: \", round(roc_auc_score(y_test, y_pred_prob),2))","0dfdfb3b":"# random forest classifier\nforest = RandomForestClassifier(class_weight='balanced', n_jobs = -1)\n\n#hyperparameter\nparams = {\"criterion\": ['gini', 'entropy'], \"max_features\": ['auto', 0.4]}\n\n#create 5 folds\nfolds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)\n\n#gridsearch object\nmodel = GridSearchCV(estimator=forest, cv=folds, param_grid=params, scoring='roc_auc', n_jobs=-1, verbose=1)\n\n#fit model\nmodel.fit(X_train, y_train)","eccb61d4":"#best hyperparameters\nprint(\"Best AUC: \", model.best_score_)\nprint(\"Best hyperparameters: \", model.best_params_)","b2c9c93e":"#churn prediction on test data\ny_pred = model.predict(X_test)\n\n#confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\nTP = cm[1,1] # true positive \nTN = cm[0,0] # true negatives\nFP = cm[0,1] # false positives\nFN = cm[1,0] # false negatives\n\n#sensitivity\nprint('\\nSensitivity: ', TP \/ float(TP+FN))\n\n#specificity\nprint('Specificity: ',TN \/ float(TN+FP))\n\n#area under curve (AUC)\ny_pred_prob = model.predict_proba(X_test)[:, 1]\nprint(\"\\nAUC: \", round(roc_auc_score(y_test, y_pred_prob),2))","a58d78c8":"#specify number of folds for k-fold CV\nn_folds = 5\n\n#parameters\nparameters = {'max_depth': range(2, 20, 5)}\n\n#random forest classifier\nrf_hyper = RandomForestClassifier(class_weight='balanced')\n\n\n# fit tree on training data\nrf_hyper = GridSearchCV(rf_hyper, parameters, \n                    cv=n_folds, return_train_score=True,\n                   scoring=\"accuracy\", n_jobs=-1)\n\n#fit rf model\nrf_hyper.fit(X_train, y_train)","c51c1516":"#GridSearch CV scores\nscores = rf_hyper.cv_results_\npd.DataFrame(scores).head()","ba832f63":"#plot accuracies with min_samples_split\nplt.figure()\n\n#training accuracy\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\n\n#test accuracy\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\n\n#labels\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"Accuracy\")\n\n\nplt.legend()\nplt.show()","5a470b8c":"#specify number of folds for k-fold CV\nn_folds = 5\n\n#min_samples_leaf parameter\nparameters = {'min_samples_leaf': range(100, 400, 50)}\n\n#random forest classifier\nrf_hyper = RandomForestClassifier(class_weight='balanced')\n\n\n#fit tree on training data\nrf_hyper = GridSearchCV(rf_hyper, parameters, \n                    cv=n_folds, return_train_score=True,\n                   scoring=\"accuracy\", n_jobs=-1)\n\nrf_hyper.fit(X_train, y_train)","38df6c44":"#GridSearch CV scores\nscores = rf_hyper.cv_results_\npd.DataFrame(scores).head()","8a3e23d3":"#plot accuracies with min_samples_split\nplt.figure()\n\n#training accuracy\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\n\n#test accuracy\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\n\n#labels\nplt.xlabel(\"min_samples_leaf\")\nplt.ylabel(\"Accuracy\")\n\n\nplt.legend()\nplt.show()","b5982dfa":"print(rf_hyper.best_score_)\nprint(rf_hyper.best_params_)","cdfe0272":"#hyperparameter model\nrf_final = RandomForestClassifier(bootstrap=True, class_weight='balanced',\n                                              criterion='gini', max_depth=10,\n                                              min_samples_leaf=5,\n                                              min_samples_split=5,\n                                              n_estimators=100)\n\n#fit\nrf_final.fit(X_train,y_train)","42e67ff9":"#prediction\ny_pred_default = rf_final.predict(X_test)\n\n#classification report\nprint(classification_report(y_test,y_pred_default))\n\n#confusion matrix\nprint(confusion_matrix(y_test,y_pred_default))\n\n#accuracy\nprint('accuracy_score: ',accuracy_score(y_test,y_pred_default))","84a309f1":"#Confusion Matrix\nconfusion_rf_hyper=confusion_matrix(y_test,y_pred_default)\nconfusion_rf_hyper","c1f95e51":"TN = confusion_rf_hyper[0,0] # true positive \nTP = confusion_rf_hyper[1,1] # true negatives\nFP = confusion_rf_hyper[0,1] # false positives\nFN = confusion_rf_hyper[1,0] # false negatives\n\nprint('Accuracy Score: ',accuracy_score(y_test,y_pred_default))\n\n#sensitivity\nprint('Sensitivity: ', TP \/ float(TP+FN))\n\n#specificity\nprint('Specificity: ',TN \/ float(TN+FP))","87891c9b":"#number of features consider to split each node\nmax_features = int(round(np.sqrt(X_train.shape[1])))\nprint(max_features)","1cc15bd7":"#creating PCA class\npca = PCA(svd_solver='randomized', random_state=42)\n\n#pca fitting on train data\npca.fit(X_train)","0f8061db":"#getting pca components\npc = pca.components_\n\n#listing all the columns of X_train together\ncol_names = list(X_train.columns)\n\n#finding top 10 pca components\npca_features = pd.DataFrame({'PC1':pc[0],'PC2':pc[1],'PC3':pc[2],'PC4':pc[3],'PC5':pc[4],\n                             'PC6':pc[5],'PC7':pc[6],'PC8':pc[7],'PC9':pc[8],'PC10':pc[9], \n                            'Features':col_names})\n\n#creating pipeline\nPCA_VARS = 18\nsteps = [('scaler', StandardScaler()),\n         (\"pca\", PCA(n_components=PCA_VARS)),\n         (\"logistic\", LogisticRegression(class_weight='balanced'))\n        ]\npipeline = Pipeline(steps)\n\n\n\n#train data\n\n#fit pipeline model\npipeline.fit(X_train, y_train)\n\n#checking score on train data\npipeline.score(X_train, y_train)","85a8c198":"#churn prediction on test data\ny_pred = pipeline.predict(X_test)\n\n#confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\nTP = cm[1,1] # true positive \nTN = cm[0,0] # true negatives\nFP = cm[0,1] # false positives\nFN = cm[1,0] # false negatives\n\n#Let's see the sensitivity\nprint('\\nSensitivity: ', TP \/ float(TP+FN))\n\n#Let us calculate specificity\nprint('Specificity: ',TN \/ float(TN+FP))\n\n#area under curve (AUC)\ny_pred_prob = pipeline.predict_proba(X_test)[:, 1]\nprint(\"\\n AUC: \", round(roc_auc_score(y_test, y_pred_prob),2))","400b3a39":"#identifying class imbalance\ny_train.value_counts()\/y_train.shape","ef4140f4":"#pca = PCA()\n\n#logistic Regression with class_weight parameter\nlogistic = LogisticRegression(class_weight='balanced')\n\n#creating pipeline\nsteps = [(\"scaler\", StandardScaler()), (\"pca\", pca),(\"logistic\", logistic)]\n\n#pipeline\npca_logistic = Pipeline(steps)\n\n#hyperparameter\nparams = {'pca__n_components': [14, 4], 'logistic__C': [0.1, 0.5, 1, 2, 3, 4, 5, 10], 'logistic__penalty': ['l1', 'l2']}\n\n#create 5 folds\nfolds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)\n\n#gridsearch object\nmodel = GridSearchCV(estimator=pca_logistic, cv=folds, param_grid=params, scoring='roc_auc', n_jobs=-1, verbose=1)","3c3b28da":"#fit model\nmodel.fit(X_train, y_train)","e6c0924f":"#print best hyperparameters\nprint(\"Best AUC: \", model.best_score_)\nprint(\"Best hyperparameters: \", model.best_params_)","69fb4311":"# predict on test data\ny_pred = model.predict(X_test)\n\n# create onfusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\nTP = cm[1,1] # true positive \nTN = cm[0,0] # true negatives\nFP = cm[0,1] # false positives\nFN = cm[1,0] # false negatives\n\n#Let's see the sensitivity\nprint('\\nSensitivity: ', TP \/ float(TP+FN))\n\n#Let us calculate specificity\nprint('Specificity: ',TN \/ float(TN+FP))\n\n# check area under curve\ny_pred_prob = model.predict_proba(X_test)[:, 1]\nprint(\"\\nAUC: \", round(roc_auc_score(y_test, y_pred_prob),2))","2ec4653e":"fpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr,tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve for Heart disease classifier')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\nplt.grid(True)","8d3275c7":"#heatmap of prediction\nsns.heatmap(confusion_matrix(y_test,y_pred),annot=True)","ef21e5c8":"#library to save model\nimport pickle\n  \n#Save the trained model as a pickle string.\nsaved_model = pickle.dumps(model)","4a5658e8":"# Load the pickled model\nlogis_from_pickle = pickle.loads(saved_model)\n  \n# Use the loaded pickled model to make predictions\nlogis_from_pickle.predict(X_test)","cfbb20e8":"**MODEL EVALUATION**","d96aed25":"**Logistic Regression with PCA**","94db7499":"Let's go for other algorithm, like Random Forest.","e1e93389":"**Choosing Best Model -- LOGISTIC REGRESSION**","b445a109":"**Logistic Regression with RFE**","b8e1af05":"**--------------- by Sakshi Maharana -----------------------**","1e602990":"Since one hot encoding dropped \"thall_fixed defect\" column which was a useful column compared to 'thall_nothing' which is a null column, we dropped 'thall_nothing' and concatinated 'thall_fixed defect'.","d96dfe6f":"**DATA SOURCING AND UNDERSTANDING**","d3fb7759":"Here, we can say that,\n\n1. 4 : 99% variance\n2. 14 : 95% variance","d13a2473":"**PS. Reviews, Comments, Discussion and Feedbacks are welcomed. This code was to focus on the EDA and PCA model building. Hope you liked it!! Upvotes for my first kaggle kernel code.**\n\n**THANK YOU!!**","f6efd6cb":"*HEART ANALYSIS VIA HEATMAP*","0c08cfcf":"**Univariate Analysis**","2caa3ca1":"*OUTPUT VS OLDPEAK*","399b9fc4":"**CONCLUSION**","e0ea6eaf":"These features are normally distributed!!","0e3e2cdd":"**Bivariate Analysis**","c69e540e":"**RANDOM FOREST with PCA**","1654b52e":"**Cummulative Explained Variance**","30c91443":"For model building, we can use logistic regression for predicting the unseen data, as this model produces 92% accuracy, that is best among the other model predicted.","d2f2872d":"*CONTINUOUS VARIABLE*","f567471e":"We can conclude our code from the following insights ---","e42bb4aa":"*Hyperparameter tuning doesn't helped us.The accuracy remained as it is.*","fde3e296":"**MODEL BUILDING**","79232908":"**Principal Component Analysis (PCA)**","8a10b514":"**Feature Engineering**","de30a1ff":"**DATA PREPARATION**","516dba05":"**Test Data Evaluation**","225ee215":"SUMMARY OF TWO MODELS :\n\n1. Logistic Regression --\n\n    Sensitivity:  0.7941176470588235\n    \n    Specificity:  0.8888888888888888\n    \n    AUC:  0.92\n\n2. Random Forest -- \n\n    Accuracy Score:  0.819672131147541\n    \n    Sensitivity:  0.7941176470588235\n    \n    Specificity:  0.8518518518518519","871b8f44":"*OUTPUT VS THALACHH*","74e1bb9e":"*Correlation Matrix*","90f91a40":"*OUTPUT VS TRTBPS*","c5ed5af2":"*CATEGORICAL VARIABLES*","57793de8":"**Hyperparameter Tuning**","fee172bb":"**Hyperparameter Tuning**","eedb93d4":"**Heatmap**","257f140f":"**Train-Test Split**","8c2e3d73":"**Test data evaluation**","6a5ff1a8":"This shows that it is almost null values!!! Getting the useful column thall_fixed defect.","eb36403c":"**Hyperparameter tuning**","9cb04077":"**Objective**\n\nWith the dataset provided for heart analysis, we have to analyse the possibilities of heart attack under various factors\/features, and then a prediction from the classification will tell us that if a person is prone to heart attack or not. Th detailed analysis can be proceed with the exploratory data analysis (EDA). The classification for predication can be done using various machine learning model algorithms, choose the best suited model for heart attack analysis and finally save the model in the pickle (.pkl) file.\n\n\n**About the dataset**\n\nColumns in the dataset are defined as :-\n\n1. Age : Age of the patient\n\n2. Sex : Sex of the patient (1 = male; 0 = female)\n\n3. exng : exercise induced angina (1 = yes; 0 = no)\n\n4. caa : number of major vessels (0-3) colored by flourosopy.\n\n5. cp : Chest Pain type chest pain type\n\n     Value 1: typical angina\n\n     Value 2: atypical angina\n\n     Value 3: non-anginal pain\n\n     Value 4: asymptomatic\n     \n6. trtbps : resting blood pressure (in mm Hg)\n\n7. chol : cholestoral in mg\/dl fetched via BMI sensor\n\n8. fbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n\n9. restecg : resting electrocardiographic results\n\n     Value 0: normal\n\n     Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n\n     Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n\n10. oldpeak : ST depression induced by exercise relative to rest.\n\n11. slp : the slope of the peak exercise ST segment (1 = upsloping; 2 = flat; 3 = downsloping)\n\n12. thal : 3 = normal; 6 = fixed defect; 7 = reversable defect.\n\n13. thalachh : maximum heart rate achieved\n\n14. output :\n\n    0= less chance of heart attack \n\n    1= more chance of heart attack\n    \n \n**Project Data**\n\nThe data was taken the kaggle dataset. We have been provided with the two dataset, that are -\n\n    1. heart.csv : Stored the details of the various parameters required heart analysis\n    2. o2Saturation.csv : Stored the details of the oxygen (o2) saturation level\n\n\n**THE STEPS TO BE FOLLOWED --**\n\n0. Any required installations\n1. Data Sourcing and Understanding\n2. Data Cleaning\n3. Data Visualisation(EDA)\n4. Data Preparation\n5. Model Building\n6. Model Evaluation\n7. Recommendations\/Conclusion","afa5ea85":"**ROC Curve**","aa5ce1e3":"**Heatmap for predictions**","3dae2f6a":"*OUTPUT VS CHOL*","ebdc3000":"**RECOMMENDATIONS**","6d8eee47":"**Multivariate Analysis**","3ac00d8a":"**Saving the model**","609b4bc4":"1. Numeric Variables - No outliers were found!\n\n2. In the count of target showed up that we have more chance of heart attack details.\n\n3. Age from 40-60 years have the high chance of heart attack.\n\n4. Male gender has more chance of heart attack compared to female ones.\n\n5. High Blood Pressure, High Cholestrol and High Heart Rate leads to high chance of heart attack.\n\n6. Highly Correlated factors in this dataset are :\n\n        ** Age and trtbps (blood pressure rate)\n\n        ** Age and chol (cholestrol level)","1627b584":"**DATA VISUALISATION (EDA)**"}}