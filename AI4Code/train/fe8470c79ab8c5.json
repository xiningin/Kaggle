{"cell_type":{"4ae734bf":"code","2c713bea":"code","556f77c4":"code","8033ad7a":"code","f3cf1637":"code","45c948a3":"code","e6256506":"code","e79fe306":"code","8c1d03a0":"code","89f05072":"code","16537d30":"code","72009f9b":"code","ba295943":"code","a79ea0ce":"markdown","f57455b5":"markdown","388019ca":"markdown","790313d1":"markdown","63ae6f31":"markdown","c33d1273":"markdown","7f702b24":"markdown","ca192811":"markdown","2778970a":"markdown","518dc27b":"markdown"},"source":{"4ae734bf":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, TensorBoard\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\nfrom functools import partial\nimport numpy as np\nimport pandas as pd\nimport json\nimport matplotlib.pyplot as plt\n%matplotlib inline  ","2c713bea":"FILENAMES = tf.io.gfile.glob(\"\/kaggle\/input\/data-hubmap-tif-2-jpg-tfrecords-128-256-512-1024\/Cortex-512-*.tfrecord\")\nsplit_ind = int(0.75 * len(FILENAMES))\nTRAINING_FILENAMES, VALID_FILENAMES = FILENAMES[:split_ind], FILENAMES[split_ind:]\n\nprint(\"Train TFRecord Files:\", len(TRAINING_FILENAMES))\nprint(\"Validation TFRecord Files:\", len(VALID_FILENAMES))","556f77c4":"IMG_SIZE = 512\nIMAGE_SIZE = [IMG_SIZE, IMG_SIZE]\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 16\n\n# hyperparameters saved for later use during inference\nhparams = {\n    \"IMG_SIZE\": IMG_SIZE,\n    \"SCALE_FACTOR\": 2,\n    \"BATCH_SIZE\": BATCH_SIZE}\nwith open(\"hparams.json\", \"w\") as json_file:\n    json_file.write(json.dumps(hparams, indent = 4))\n\n# decode image or mask\ndef decode_image(image, isjpeg=True):\n    if isjpeg:\n        ch = 3\n        image = tf.image.decode_jpeg(image, channels=ch)\n    else:\n        ch = 1\n        image = tf.image.decode_png(image, channels=ch)\n        image = tf.expand_dims(image, -1)\n    image = tf.cast(image, tf.float32)\n    image = image \/255.\n    image = tf.reshape(image, [*IMAGE_SIZE, ch])\n    return image\n\n# read a single record \ndef read_tfrecord(example):\n    tfrecord_format = ( # only extract features we are interested in\n        {\n            \"image\/encoded\": tf.io.FixedLenFeature([], tf.string),\n            \"mask\/encoded\": tf.io.FixedLenFeature([], tf.string),\n        }\n    )\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example[\"image\/encoded\"], True) # jpeg format\n    mask = decode_image(example[\"mask\/encoded\"], False) # png format\n    return image, mask\n\n# read a single record and do augmentation\ndef read_tfrecord_tr(example):\n    image, mask = read_tfrecord(example)\n    # basic augmentation  (expand as desired)\n    if tf.random.uniform(()) > 0.5:\n        image = tf.image.flip_left_right(image)\n        mask = tf.image.flip_left_right(mask)\n    if tf.random.uniform(()) > 0.5:\n        image = tf.image.flip_up_down(image)\n        mask = tf.image.flip_up_down(mask)\n    if tf.random.uniform(()) > 0.75:\n        image = tf.image.rot90(image, k=1)\n        mask = tf.image.rot90(mask, k=1)\n    if tf.random.uniform(()) > 0.75: # random contrast\/brightness\n        if tf.random.uniform(()) > 0.5:\n            a = tf.random.uniform((), 0.7, 1.3)\n            image = tf.image.adjust_contrast(image, a)\n        else:\n            a = tf.random.uniform((), 0., 0.5)\n            image = tf.image.adjust_brightness(image, a)\n    if tf.random.uniform(()) > 0.8: # add noise\n        gnoise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=0.1, dtype=tf.float32)\n        image = tf.add(image, gnoise)\n    if tf.random.uniform(()) > 0.8: # change hue\n        a = tf.random.uniform((), -0.2, 0.2)\n        image = tf.image.adjust_hue(image, a)  \n        \n    return image, mask\n\ndef load_dataset(filenames, IsTrain=True):\n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = False  # disable order, increase speed\n    dataset = tf.data.TFRecordDataset(\n        filenames\n    )  # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(\n        ignore_order\n    )  # uses data as soon as it streams in, rather than in its original order\n    if IsTrain: # augmentation\n        dataset = dataset.map(\n            partial(read_tfrecord_tr), num_parallel_calls=AUTOTUNE\n        )\n    else: # no augmentation\n        dataset = dataset.map(\n            partial(read_tfrecord), num_parallel_calls=AUTOTUNE\n        )\n    # returns a dataset of (image, mask) pairs \n    return dataset\n\ndef get_dataset(filenames, IsTrain=True):\n    dataset = load_dataset(filenames, IsTrain)\n    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.repeat()\n    return dataset","8033ad7a":"train_dataset = get_dataset(TRAINING_FILENAMES, True)\nvalid_dataset = get_dataset(VALID_FILENAMES, False)\n\nimage_batch, mask_batch = next(iter(train_dataset))\n\ndef show_batch(image_batch, mask_batch):\n    plt.figure(figsize=(16, 16))\n    for n in range(min(BATCH_SIZE,16)):\n        ax = plt.subplot(4, 4, n + 1)\n        plt.imshow(image_batch[n])\n        plt.imshow(np.squeeze(mask_batch[n]), alpha=0.25)#, cmap='binary')\n        plt.axis(\"off\")\n\nshow_batch(image_batch.numpy(), mask_batch.numpy())","f3cf1637":"# calculate number of images in train\/val sets\ndf = pd.read_pickle('\/kaggle\/input\/data-hubmap-tif-2-jpg-tfrecords-128-256-512-1024\/record_stats.pkl')\ntcnt, vcnt = 0, 0\nfor i in VALID_FILENAMES:\n    fname = i.split('\/')[-1]\n    vcnt += df[df.File == fname].ImgCount.iloc[0]\nfor i in TRAINING_FILENAMES:\n    fname = i.split('\/')[-1]\n    tcnt += df[df.File == fname].ImgCount.iloc[0]\n\nprint('Train images: {}, Validation images: {}'.format(tcnt, vcnt))","45c948a3":"def conv_block(x, num_filters):\n    x = Conv2D(num_filters, (3, 3), padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n\n    x = Conv2D(num_filters, (3, 3), padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n\n    return x","e6256506":"def build_model():\n    size = IMG_SIZE\n    num_filters = [12, 24, 48, 96] # adjust filter quantity and sizes as desired\n    inputs = Input((size, size, 3))\n\n    skip_x = []\n    x = inputs\n\n    ## Encoder\n    for f in num_filters:\n        x = conv_block(x, f)\n        skip_x.append(x)\n        x = MaxPool2D((2, 2))(x)\n\n    ## Bridge\n    x = conv_block(x, num_filters[-1])\n\n    num_filters.reverse()\n    skip_x.reverse()\n\n    ## Decoder\n    for i, f in enumerate(num_filters):\n        x = UpSampling2D((2, 2))(x)\n        xs = skip_x[i]\n        x = Concatenate()([x, xs])\n        x = conv_block(x, f)\n\n    ## Output\n    x = Conv2D(1, (1, 1), padding=\"same\")(x)\n    x = Activation(\"sigmoid\")(x)\n\n    return Model(inputs, x)","e79fe306":"# metrics and loss functions\nsmooth = 1.\n\ndef dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1.-dice_coef(y_true, y_pred)\n\n\ndef iou(y_true, y_pred):\n    def f(y_true, y_pred):\n        intersection = (y_true * y_pred).sum()\n        union = y_true.sum() + y_pred.sum() - intersection\n        x = (intersection + 1e-15) \/ (union + 1e-15)\n        x = x.astype(np.float32)\n        return x\n    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n\ndef tversky(y_true, y_pred, smooth=1, alpha=0.7):\n    y_true_pos = K.flatten(y_true)\n    y_pred_pos = K.flatten(y_pred)\n    true_pos = K.sum(y_true_pos * y_pred_pos)\n    false_neg = K.sum(y_true_pos * (1 - y_pred_pos))\n    false_pos = K.sum((1 - y_true_pos) * y_pred_pos)\n    return (true_pos + smooth) \/ (true_pos + alpha * false_neg + (1 - alpha) * false_pos + smooth)\n\ndef tversky_loss(y_true, y_pred):\n    return 1 - tversky(y_true, y_pred)\n\ndef focal_tversky_loss(y_true, y_pred, gamma=0.75):\n    tv = tversky(y_true, y_pred)\n    return K.pow((1 - tv), gamma)","8c1d03a0":"lr = 5e-4\n\nmodel = build_model()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model.to_json())\n\nopt = tf.keras.optimizers.Adam(lr)\nmetrics = [\"acc\", iou, dice_coef, tversky]\nmodel.compile(loss=focal_tversky_loss, optimizer=opt, metrics=metrics)\nmodel.summary()","89f05072":"callbacks = [\n          ModelCheckpoint(\"model.h5\", save_best_only=True),\n          ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=8, min_lr=0.00001),\n          CSVLogger(\"data.csv\"),\n          TensorBoard(),\n          EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=False)\n     ]","16537d30":"%%capture\nhistory = model.fit(train_dataset,\n            validation_data=valid_dataset,\n            epochs=75,\n            steps_per_epoch=tcnt\/\/BATCH_SIZE,\n            validation_steps=vcnt\/\/BATCH_SIZE,\n            callbacks=callbacks)","72009f9b":"model.save('.')","ba295943":"plt.figure(figsize=(16, 8))\nx = np.arange(1,len(history.history['loss'])+1)\nplt.plot(x, history.history['loss'], label='Train loss')\nplt.plot(x, history.history['val_loss'], label='Validation loss')\nplt.plot(x, history.history['dice_coef'], label='Train Dice coef.')\nplt.plot(x, history.history['val_dice_coef'], label='Validation Dice coef.')\nplt.xlabel('Epoch')\nplt.suptitle('Learning curves')\nplt.legend();","a79ea0ce":"## Loss Functions\nThere are several loss functions to choose from, a few ones are defined below. The Focal Tversky loss is known to perform well on many segmentation tasks. We could also use the Dice coefficient loss or the Tversky loss (experiment to find the best one). ","f57455b5":"## Dataset creation\nWe will use TFRecords with a resolution of 512x512 (downscaled from 1024x1024).","388019ca":"## Train model","790313d1":"Looks pretty good! We are now ready for the final step - making predictions with the saved model. Coming up soon!","63ae6f31":"## Inspect learning curves\nBelow we plot the loss for both training and validation, along with the Dice coefficients. It is important to keep an eye on these curves to verify that our model and training are setup correctly.","c33d1273":"There is a Pandas pickle file accompanying the TFRecords files containing the number of images per TFRecord.","7f702b24":"Plot a few images from the dataset to check that everything is OK (including augmentation):","ca192811":"# Build U-Net model\nThe U-Net model is really simple to build, and easy to modify as well.","2778970a":"## Keras U-Net training with TFRecords input\nIn this notebook we will train a U-Net architecture implemented in Keras\/TensorFlow. We already created TFRecords of the dataset in this notebook:\n  * [HuBMAP TIF 2 JPG+TFRecords](https:\/\/www.kaggle.com\/mistag\/data-hubmap-tif-2-jpg-tfrecords-128-256-512-1024\/edit\/run\/47859494)  \n  \nU-Net code snippets have been reused from [Polyp Segmentation using UNET in TensorFlow 2.0](https:\/\/idiotdeveloper.com\/polyp-segmentation-using-unet-in-tensorflow-2\/) by Nikhil Tomar.","518dc27b":"## Compile model\nNote that we save the built model as a .json file for use during inference."}}