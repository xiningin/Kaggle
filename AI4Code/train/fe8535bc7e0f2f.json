{"cell_type":{"75f22872":"code","79300212":"code","8e239d44":"code","27f20fa5":"code","3fe7e0be":"code","a0db8d1f":"code","181b8648":"code","e77e2070":"code","db46654f":"code","cba26556":"code","ac4cafcf":"code","67ef8ad0":"code","1163c6c3":"code","8ac582ba":"code","01a11d3f":"code","ceb6d477":"code","d7ee50fc":"code","b24d0d59":"code","7a667cd3":"code","ff1a0729":"code","9dd1749d":"code","6a58fe75":"code","49f7a6f7":"code","fd9c46d1":"code","fa1d0802":"code","0c909a8c":"code","c3da2356":"code","be8228fd":"code","10fa56db":"code","d79474a4":"code","f89d12f7":"code","c5ec776b":"code","fb2d3e55":"code","12d58920":"code","29f1f884":"code","41aa4c97":"code","b4962892":"code","ce8b2a29":"code","af28b280":"code","23a7a5d9":"markdown","8f97aa12":"markdown","909e41d1":"markdown","0f9582a8":"markdown","1a2b74a4":"markdown"},"source":{"75f22872":"import pandas as pd\nfrom pathlib import Path\n\n\nimport matplotlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n\ndata_dir = Path('..\/input\/tabular-playground-series-sep-2021\/')\n\ndf_train = pd.read_csv(\n    data_dir \/ \"train.csv\",\n    index_col='id',\n    #nrows=25000,  # comment this row to use the full dataset\n)\n\ndf_train.head(20)","79300212":"df_train.describe()","8e239d44":"df_train.shape","27f20fa5":"df_train.info()","3fe7e0be":"# Heatmap to View Missing Values by Variable\nplt.figure(figsize = (14,6))\np = sns.heatmap(df_train.isnull(), yticklabels = False, cbar = False, cmap = 'viridis')\np.axes.set_title(\"Valores Ausentes\", fontsize = 20)","a0db8d1f":"# Check the nan values\n{df_train[col].isna().sum():col for col in df_train.columns if df_train[col].isna().sum() > 0}","181b8648":"# Check the ZERO values\n{(df_train[col] == 0).sum():col for col in df_train.columns if (df_train[col] == 0).sum() > 0}","e77e2070":"# creating a feature with a count of null columns per row\ndf_train[\"null_count\"] = df_train.isnull().sum(axis=1)","db46654f":"df_train","cba26556":"#df_train_slice = df_train[((df_train.null_count > 3) &  (df_train.null_count < 7))] \n#df_train_slice[\"variance\"] = df_train_slice.var(axis=1)","ac4cafcf":"#df_train_slice.groupby(['null_count','claim'])['null_count'].count()","67ef8ad0":"# Features and target\nFEATURES = df_train.drop('claim', axis = 1)\nTARGET = df_train['claim'].astype(int).astype(str)","1163c6c3":"sns.set(style=\"whitegrid\")\n\n# Using a bar chart to show the distribution of classes\nbp = sns.countplot(x=df_train['claim'])\nplt.title(\"Distribui\u00e7\u00e3o de classe do conjunto de dados\")\nbp.set_xticklabels([\"0\",\"1\"])\nplt.show()","8ac582ba":"from numpy import mean\nfrom numpy import std\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import mean_absolute_error, classification_report\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer,QuantileTransformer","01a11d3f":"# sliced impucation of missing values\nimp = SimpleImputer(missing_values=np.nan, strategy='median') # feel free to use others strategy\nFEATURES[(FEATURES.null_count < 5)] = imp.fit_transform(FEATURES[(FEATURES.null_count < 5)])","ceb6d477":"# sliced impucation of missing values\n#imp = KNNImputer(n_neighbors=5) # feel free to use others strategy\nimp = SimpleImputer(missing_values=np.nan, strategy='mean') # feel free to use others strategy\nFEATURES[((FEATURES.null_count > 4) &  (FEATURES.null_count < 7))] = imp.fit_transform(FEATURES[((FEATURES.null_count > 4) &  (FEATURES.null_count < 7))])","d7ee50fc":"# sliced impucation of missing values\nimp = SimpleImputer(missing_values=np.nan, strategy='most_frequent') # feel free to use others strategy\n#imp = KNNImputer(n_neighbors=15) # feel free to use others strategy\nFEATURES[((FEATURES.null_count > 6) &  (FEATURES.null_count < 13))] = imp.fit_transform(FEATURES[((FEATURES.null_count > 6) &  (FEATURES.null_count < 13))])","b24d0d59":"# sliced impucation of missing values\nimp = SimpleImputer(strategy='mean') # feel free to use others strategy\nFEATURES[(FEATURES.null_count > 12)] = imp.fit_transform(FEATURES[(FEATURES.null_count > 12)])","7a667cd3":"# sliced impucation of missing values\n#imp = KNNImputer(n_neighbors=5) # feel free to use others strategy\n#FEATURES[(FEATURES.null_count > 12)] = imp.fit_transform(FEATURES[(FEATURES.null_count > 12)])","ff1a0729":"from sklearn.model_selection import train_test_split","9dd1749d":"# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(FEATURES, TARGET, \n                                                      train_size=0.8, test_size=0.2, random_state=42, shuffle=True)","6a58fe75":"# machine learning model configuration\nXGB = XGBClassifier(\n        learning_rate= 0.00312345,\n        reg_alpha = 0.1,\n        max_depth=5,\n        subsample=0.8,\n        colsample_bytree=0.4,\n        objective='multi:softprob',\n        n_estimators=27000,\n        eval_metric='auc',\n        num_class=2,\n        n_jobs=-1,\n        tree_method='gpu_hist',\n        # Uncomment if you want to use GPU. Recommended for whole training set.\n        #tree_method='gpu_hist',\n        random_state=42,\n        )\n\n#steps = [('imputer', SimpleImputer(strategy='most_frequent')),\nsteps = [('scle', MinMaxScaler()),\n         ('m', XGB)]\nmodel = Pipeline(steps=steps)","49f7a6f7":"X = X_train\ny = y_train.values","fd9c46d1":"# Fit the model\nmodel.fit(X, y)","fa1d0802":"# get predictions\ny_pred = model.predict_proba(X_valid)","0c909a8c":"from sklearn.metrics import *","c3da2356":"# retrieve just the probabilities for the positive class\npos_probs = y_pred[:, 1]\n# plot no skill roc curve\nplt.plot([0, 1], [0, 1], linestyle='--', label='No Skill')\n# calculate roc curve for model\nfpr, tpr, _ = roc_curve(y_valid.astype(str).astype(int), pos_probs)\n# plot model roc curve\nplt.plot(fpr, tpr, marker='.', label='Logistic')\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","be8228fd":"precisions, recalls, thresholds = precision_recall_curve(y_valid.astype(str).astype(int), y_pred[:,1])\n\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.xlabel(\"Threshold\")\n    plt.legend(loc=\"upper left\")\n    plt.ylim([0, 1])\n    \nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.show()","10fa56db":"import scikitplot as skplt\nskplt.metrics.plot_roc(y_valid.astype(str).astype(int), y_pred, figsize=(10, 8))","d79474a4":"# reading test data\nX_test = pd.read_csv(data_dir \/ \"test.csv\", index_col='id')\nX_test[\"null_count\"] = X_test.isnull().sum(axis=1)","f89d12f7":"# sliced impucation of missing values\nimp = SimpleImputer(missing_values=np.nan, strategy='median') # feel free to use others strategy\nX_test[(X_test.null_count < 5)] = imp.fit_transform(X_test[(X_test.null_count < 5)])","c5ec776b":"# sliced impucation of missing values\nimp = SimpleImputer(missing_values=np.nan, strategy='mean') # feel free to use others strategy\nX_test[((X_test.null_count > 4) &  (X_test.null_count < 7))] = imp.fit_transform(X_test[((X_test.null_count > 4) &  (X_test.null_count < 7))])","fb2d3e55":"# sliced impucation of missing values\nimp = SimpleImputer(missing_values=np.nan, strategy='most_frequent') # feel free to use others strategy\nX_test[((X_test.null_count > 6) &  (X_test.null_count < 13))] = imp.fit_transform(X_test[((X_test.null_count > 6) &  (X_test.null_count < 13))])","12d58920":"# sliced impucation of missing values\nimp = SimpleImputer(strategy='mean') # feel free to use others strategy\nX_test[(X_test.null_count > 12)] = imp.fit_transform(X_test[(X_test.null_count > 12)])","29f1f884":"# get predictions\ny_pred = model.predict_proba(X_test)","41aa4c97":"X_test.groupby(['null_count'])['null_count'].count()","b4962892":"y_pred[:, 1]","ce8b2a29":"y_pred_test = pd.Series(\n    y_pred[:, 1],\n    index=X_test.index,\n    name='claim',\n)","af28b280":"# Create submission file\ny_pred_test.to_csv(\"submission.csv\")","23a7a5d9":"A \"neutral\" AUC is 0.5, so anything better than that means our model learned something useful.","8f97aa12":"# Make Submission #\n\nOur predictions are binary 0 and 1, but you're allowed to submit probabilities instead. In scikit-learn, you would use the `predict_proba` method instead of `predict`.","909e41d1":"\n# Evaluation #\n\nThe evaluation metric is AUC, which stands for \"area under curve\".  Run the next code cell to evaluate the model.","0f9582a8":"# Model #\n\nLet's try out a simple XGBoost model. This algorithm can handle missing values, but you could try imputing them instead.  We use `XGBClassifier` (instead of `XGBRegressor`, for instance), since this is a classification problem.","1a2b74a4":"# Welcome to the September 2021 Tabular Playground Competition! #\n\nIn this competition, we predict whether a customer will make an insurance claim.\n\n# Data #\n\nThe full dataset has almost one million rows. We'll use just a sample so we can explore the data more quickly."}}