{"cell_type":{"44966679":"code","607a94c6":"code","0825d840":"code","5bcf611c":"code","4ed6576f":"code","e33a7489":"code","d2c167ef":"code","92ce8fe7":"code","63527c0d":"code","6d5e8903":"code","a5e90221":"code","dd72df13":"code","3899112b":"code","dda3851b":"code","0dc58c87":"code","acae9876":"code","04f26341":"code","ea153c48":"code","9f1d1ffb":"code","9b87e928":"code","9962ed6b":"code","b66646d1":"code","862d69b0":"code","8c0afb2e":"code","a97e5f83":"code","a77da56f":"code","70bffb49":"code","cdca24a5":"code","cb2937ff":"code","607bac0c":"code","4b588942":"code","fc7f65ab":"code","ad780bd3":"code","db45c5ff":"code","95539888":"code","33606529":"code","b1bc16b9":"code","9e07833f":"code","30b5e70b":"code","9eb80d83":"code","08bedea7":"code","04cf011d":"code","277286e2":"code","8bb5e094":"code","7a7dcfdf":"code","33923a5b":"code","cf3412dc":"code","8e048e87":"code","d2b877c2":"code","0321ce8f":"code","21c5812e":"code","0df97b2e":"code","81807f8b":"code","463734d5":"code","3add7837":"code","91d1c539":"code","f80a4ae4":"code","3042ad1d":"code","be24437c":"code","c77eaf66":"code","67d53df2":"code","59e97723":"code","4fe28499":"code","dc906f27":"code","4e8b774a":"code","ac8ef6fb":"code","21900d0d":"code","87c36b09":"code","7b5553ef":"code","1bab873e":"code","64aa47cc":"code","a387cd9f":"code","1e34bc53":"code","f8fefbd2":"code","937eef0d":"code","3a3ccad1":"code","fdd65648":"code","0c6b394c":"code","7d87b252":"code","e2dbbd83":"code","2fe27d4c":"code","1bc5a794":"code","5857d9a1":"code","3b895531":"code","f69347ea":"code","e3237f8b":"code","a8d9caac":"code","afb25d47":"code","a6d7139e":"markdown","44832ef3":"markdown","0cecc5ca":"markdown","3a07e0c4":"markdown","781506e5":"markdown","8ceecb21":"markdown","3a36a64b":"markdown","e9669f7a":"markdown","36488f16":"markdown","dd5cc556":"markdown","34ecf4b5":"markdown","64652b0d":"markdown","c2483611":"markdown","5737414b":"markdown","52b379ff":"markdown","7ed0a467":"markdown","27c418e8":"markdown","c8a01558":"markdown","dd1217ff":"markdown","eda184a7":"markdown","482de326":"markdown","3eeec74a":"markdown","fa0c951e":"markdown","62ecdd49":"markdown","ec1219e0":"markdown","b3b3ec4d":"markdown","ab0222b2":"markdown","a99721b9":"markdown","0c3ff511":"markdown","f03be449":"markdown","2bb59178":"markdown","7dc53da6":"markdown","cf58be4b":"markdown","f5a96f54":"markdown","8de68d5b":"markdown","f8603ae8":"markdown","381494ba":"markdown","87b62b33":"markdown","8d788cb3":"markdown","6376bc9e":"markdown","16c1b075":"markdown","1c99fed0":"markdown","f1ee6a08":"markdown","18efea84":"markdown","90c4bfb1":"markdown","9751f3aa":"markdown","8df55c43":"markdown","98bbd569":"markdown","d22a0c7c":"markdown","e184af9b":"markdown","4584a732":"markdown","e4a7d509":"markdown","85d5f845":"markdown","fbb9ae74":"markdown","d0fa500d":"markdown","f0b7dd89":"markdown","189eef1b":"markdown","8d2b6a81":"markdown","a2414474":"markdown","6bc06473":"markdown","31a282bf":"markdown","018f8c2a":"markdown","524c4e15":"markdown","b6ab4bfa":"markdown","360e62d7":"markdown","77133a1e":"markdown","6135a76e":"markdown","006f237a":"markdown","17dba71d":"markdown","da9c38c3":"markdown","cadf8d48":"markdown","54997a9a":"markdown","076598c8":"markdown","5b6fbc9f":"markdown","54a74e42":"markdown","1bad9dfd":"markdown","a06490f4":"markdown","54195fda":"markdown","e5dda160":"markdown","1cc59b6d":"markdown","c46fd8e3":"markdown","6e540b71":"markdown","b35f1d0a":"markdown","b29f9da7":"markdown","e43aa599":"markdown","2a3a708f":"markdown","01c47bf0":"markdown","566bd5ba":"markdown","a5a63566":"markdown","7b8b5c35":"markdown","a6032757":"markdown","46b48bfe":"markdown","15d76689":"markdown","663a6708":"markdown","99a09690":"markdown","255d97d3":"markdown","5276ffe0":"markdown","5694633c":"markdown","135417bb":"markdown","b8d91bda":"markdown","60bd5db6":"markdown","305c0b5e":"markdown"},"source":{"44966679":"random_state_split = 42\nDropout_num = 0\nlearning_rate = 5.95e-6\nvalid = 0.15\nepochs_num = 3\nbatch_size_num = 16\ntarget_corrected = False\ntarget_big_corrected = False","607a94c6":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\n\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\n\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nfrom collections import defaultdict\nfrom collections import Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\n\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\n\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\n\nimport torch\n\nimport warnings\nwarnings.simplefilter('ignore')","0825d840":"tweet= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsubmission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","5bcf611c":"# # From https:\/\/www.kaggle.com\/wrrosa\/keras-bert-using-tfhub-modified-train-data - \n# # author of this kernel read tweets in training data and figure out that some of them have errors:\n# ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n# tweet.loc[tweet['id'].isin(ids_with_target_error),'target'] = 0\n# tweet[tweet['id'].isin(ids_with_target_error)]","4ed6576f":"print('There are {} rows and {} columns in train'.format(tweet.shape[0],tweet.shape[1]))\nprint('There are {} rows and {} columns in test'.format(test.shape[0],test.shape[1]))","e33a7489":"tweet.head(10)","d2c167ef":"# extracting the number of examples of each class\nReal_len = tweet[tweet['target'] == 1].shape[0]\nNot_len = tweet[tweet['target'] == 0].shape[0]","92ce8fe7":"# bar plot of the 3 classes\nplt.rcParams['figure.figsize'] = (7, 5)\nplt.bar(10,Real_len,3, label=\"Real\", color='blue')\nplt.bar(15,Not_len,3, label=\"Not\", color='red')\nplt.legend()\nplt.ylabel('Number of examples')\nplt.title('Propertion of examples')\nplt.show()","63527c0d":"def length(text):    \n    '''a function which returns the length of text'''\n    return len(text)","6d5e8903":"tweet['length'] = tweet['text'].apply(length)","a5e90221":"plt.rcParams['figure.figsize'] = (18.0, 6.0)\nbins = 150\nplt.hist(tweet[tweet['target'] == 0]['length'], alpha = 0.6, bins=bins, label='Not')\nplt.hist(tweet[tweet['target'] == 1]['length'], alpha = 0.8, bins=bins, label='Real')\nplt.xlabel('length')\nplt.ylabel('numbers')\nplt.legend(loc='upper right')\nplt.xlim(0,150)\nplt.grid()\nplt.show()","dd72df13":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=tweet[tweet['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='blue')\nax1.set_title('disaster tweets')\ntweet_len=tweet[tweet['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='red')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","3899112b":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=tweet[tweet['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='blue')\nax1.set_title('disaster tweets')\ntweet_len=tweet[tweet['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='red')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweet')\nplt.show()\n","dda3851b":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=tweet[tweet['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='blue')\nax1.set_title('disaster')\nword=tweet[tweet['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='red')\nax2.set_title('Not disaster')\nfig.suptitle('Average word length in each tweet')","0dc58c87":"def create_corpus(target):\n    corpus=[]\n    \n    for x in tweet[tweet['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","acae9876":"def create_corpus_df(tweet, target):\n    corpus=[]\n    \n    for x in tweet[tweet['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","04f26341":"corpus=create_corpus(0)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]","ea153c48":"# displaying the stopwords\nnp.array(stop)","9f1d1ffb":"plt.rcParams['figure.figsize'] = (18.0, 6.0)\nx,y=zip(*top)\nplt.bar(x,y)","9b87e928":"corpus=create_corpus(1)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    \n\nplt.rcParams['figure.figsize'] = (18.0, 6.0)\nx,y=zip(*top)\nplt.bar(x,y)","9962ed6b":"plt.figure(figsize=(16,5))\ncorpus=create_corpus(1)\n\ndic=defaultdict(int)\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y)","b66646d1":"plt.figure(figsize=(16,5))\ncorpus=create_corpus(0)\ndic=defaultdict(int)\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y,color='green')","862d69b0":"plt.figure(figsize=(16,5))\ncounter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)","8c0afb2e":"sns.barplot(x=y,y=x)","a97e5f83":"def get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","a77da56f":"plt.figure(figsize=(16,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(tweet['text'])[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","70bffb49":"df=pd.concat([tweet,test])\ndf.shape","cdca24a5":"example=\"New competition launched :https:\/\/www.kaggle.com\/c\/nlp-getting-started\"","cb2937ff":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\nremove_URL(example)","607bac0c":"df['text']=df['text'].apply(lambda x : remove_URL(x))","4b588942":"example = \"\"\"<div>\n<h1>Real or Fake<\/h1>\n<p>Kaggle <\/p>\n<a href=\"https:\/\/www.kaggle.com\/c\/nlp-getting-started\">getting started<\/a>\n<\/div>\"\"\"","fc7f65ab":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\nprint(remove_html(example))","ad780bd3":"df['text']=df['text'].apply(lambda x : remove_html(x))","db45c5ff":"# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another Earthquake \ud83d\ude14\ud83d\ude14\")","95539888":"df['text']=df['text'].apply(lambda x: remove_emoji(x))","33606529":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nexample=\"I am a #king\"\nprint(remove_punct(example))","b1bc16b9":"df['text']=df['text'].apply(lambda x : remove_punct(x))","9e07833f":"corpus_new1=create_corpus_df(df,1)\nlen(corpus_new1)","30b5e70b":"corpus_new1[:10]","9eb80d83":"# Generating the wordcloud with the values under the category dataframe\nplt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n                          background_color='black',\n                          max_font_size = 80\n                         ).generate(\" \".join(corpus_new1[:50]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","08bedea7":"corpus_new0=create_corpus_df(df,0)\nlen(corpus_new0)","04cf011d":"corpus_new0[:10]","277286e2":"# Generating the wordcloud with the values under the category dataframe\nplt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n                          background_color='black',\n                          max_font_size = 80\n                         ).generate(\" \".join(corpus_new0[:50]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","8bb5e094":"df.head(10)","7a7dcfdf":"def cv(data):\n    count_vectorizer = CountVectorizer()\n\n    emb = count_vectorizer.fit_transform(data)\n\n    return emb, count_vectorizer\n\nlist_corpus = df[\"text\"].tolist()\nlist_labels = df[\"target\"].tolist()\n\nX_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, \n                                                                                random_state=random_state_split)\n\nX_train_counts, count_vectorizer = cv(X_train)\nX_test_counts = count_vectorizer.transform(X_test)","33923a5b":"def plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n        lsa = TruncatedSVD(n_components=2)\n        lsa.fit(test_data)\n        lsa_scores = lsa.transform(test_data)\n        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n        color_column = [color_mapper[label] for label in test_labels]\n        colors = ['orange','blue']\n        if plot:\n            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n            orange_patch = mpatches.Patch(color='orange', label='Not')\n            blue_patch = mpatches.Patch(color='blue', label='Real')\n            plt.legend(handles=[orange_patch, blue_patch], prop={'size': 30})\n\nfig = plt.figure(figsize=(16, 16))          \nplot_LSA(X_train_counts, y_train)\nplt.show()","cf3412dc":"def tfidf(data):\n    tfidf_vectorizer = TfidfVectorizer()\n\n    train = tfidf_vectorizer.fit_transform(data)\n\n    return train, tfidf_vectorizer\n\nX_train_tfidf, tfidf_vectorizer = tfidf(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)","8e048e87":"fig = plt.figure(figsize=(16, 16))          \nplot_LSA(X_train_tfidf, y_train)\nplt.show()","d2b877c2":"def create_corpus_new(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet)]\n        corpus.append(words)\n    return corpus   ","0321ce8f":"corpus=create_corpus_new(df)","21c5812e":"embedding_dict={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word = values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","0df97b2e":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","81807f8b":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","463734d5":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i < num_words:\n        emb_vec=embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i]=emb_vec           ","3add7837":"tweet_pad[0][0:]","91d1c539":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=3e-4)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","f80a4ae4":"model.summary()","3042ad1d":"train=tweet_pad[:tweet.shape[0]]\ntest=tweet_pad[tweet.shape[0]:]","be24437c":"X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.2)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","c77eaf66":"fig = plt.figure(figsize=(16, 16))          \nplot_LSA(train,tweet['target'])\nplt.show()","67d53df2":"# Recomended 10-20 epochs\nhistory=model.fit(X_train,y_train,batch_size=4,epochs=10,validation_data=(X_test,y_test),verbose=2)","59e97723":"train_pred_GloVe = model.predict(train)\ntrain_pred_GloVe_int = train_pred_GloVe.round().astype('int')","4fe28499":"# We will use the official tokenization script created by the Google team\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","dc906f27":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nimport tokenization","4e8b774a":"# Thanks to https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","ac8ef6fb":"# Thanks to https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub\ndef build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    \n    if Dropout_num == 0:\n        # Without Dropout\n        out = Dense(1, activation='sigmoid')(clf_output)\n    else:\n        # With Dropout(Dropout_num), Dropout_num > 0\n        x = Dropout(Dropout_num)(clf_output)\n        out = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","21900d0d":"# Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\ndef clean_tweets(tweet):\n    \"\"\"Removes links and non-ASCII characters\"\"\"\n    \n    tweet = ''.join([x for x in tweet if x in string.printable])\n    \n    # Removing URLs\n    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n    \n    return tweet","87c36b09":"# Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","7b5553ef":"# Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\ndef remove_punctuations(text):\n    punctuations = '@#!?+&*[]-%.:\/();$=><|{}^' + \"'`\"\n    \n    for p in punctuations:\n        text = text.replace(p, f' {p} ')\n\n    text = text.replace('...', ' ... ')\n    \n    if '...' not in text:\n        text = text.replace('..', ' ... ')\n    \n    return text","1bab873e":"# Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\nabbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}","64aa47cc":"# Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\ndef convert_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word","a387cd9f":"# Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\ndef convert_abbrev_in_text(text):\n    tokens = word_tokenize(text)\n    tokens = [convert_abbrev(word) for word in tokens]\n    text = ' '.join(tokens)\n    return text","1e34bc53":"# Load BERT from the Tensorflow Hub\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","f8fefbd2":"# Load CSV files containing training data\ntrain = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","937eef0d":"# Thanks to https:\/\/www.kaggle.com\/wrrosa\/keras-bert-using-tfhub-modified-train-data - \n# author of this kernel read tweets in training data and figure out that some of them have errors:\nif target_corrected:\n    ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n    train.loc[train['id'].isin(ids_with_target_error),'target'] = 0\n    train[train['id'].isin(ids_with_target_error)]","3a3ccad1":"# Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\nif target_big_corrected:\n    train[\"text\"] = train[\"text\"].apply(lambda x: clean_tweets(x))\n    test[\"text\"] = test[\"text\"].apply(lambda x: clean_tweets(x))\n    \n    train[\"text\"] = train[\"text\"].apply(lambda x: remove_emoji(x))\n    test[\"text\"] = test[\"text\"].apply(lambda x: remove_emoji(x))\n    \n    train[\"text\"] = train[\"text\"].apply(lambda x: remove_punctuations(x))\n    test[\"text\"] = test[\"text\"].apply(lambda x: remove_punctuations(x))\n    \n    train[\"text\"] = train[\"text\"].apply(lambda x: convert_abbrev_in_text(x))\n    test[\"text\"] = test[\"text\"].apply(lambda x: convert_abbrev_in_text(x))","fdd65648":"# Thanks to https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub\n# Load tokenizer from the bert layer\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","0c6b394c":"# Thanks to https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub\n# Encode the text into tokens, masks, and segment flags\ntrain_input = bert_encode(train.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = train.target.values","7d87b252":"# Thanks to https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub\n# Build BERT model with my tuning\nmodel_BERT = build_model(bert_layer, max_len=160)\nmodel_BERT.summary()","e2dbbd83":"# Thanks to https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub\n# Train BERT model with my tuning\ncheckpoint = ModelCheckpoint('model_BERT.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model_BERT.fit(\n    train_input, train_labels,\n    validation_split = valid,\n    epochs = epochs_num, # recomended 3-5 epochs\n    callbacks=[checkpoint],\n    batch_size = batch_size_num\n)","2fe27d4c":"# Thanks to https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub\n# Prediction by BERT model with my tuning\nmodel_BERT.load_weights('model_BERT.h5')\ntest_pred_BERT = model_BERT.predict(test_input)\ntest_pred_BERT_int = test_pred_BERT.round().astype('int')","1bc5a794":"# Prediction by BERT model with my tuning for the training data - for the Confusion Matrix\ntrain_pred_BERT = model_BERT.predict(train_input)\ntrain_pred_BERT_int = train_pred_BERT.round().astype('int')","5857d9a1":"pred = pd.DataFrame(test_pred_BERT, columns=['preds'])\npred.plot.hist()","3b895531":"submission['target'] = test_pred_BERT_int\nsubmission.head(10)","f69347ea":"submission.to_csv(\"submission_BERT.csv\", index=False, header=True)","e3237f8b":"# Showing Confusion Matrix\ndef plot_cm(y_true, y_pred, title, figsize=(5,5)):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","a8d9caac":"# Showing Confusion Matrix for GloVe model\nplot_cm(train_pred_GloVe_int, train['target'].values, 'Confusion matrix for GloVe model', figsize=(7,7))","afb25d47":"# Showing Confusion Matrix for BERT model\nplot_cm(train_pred_BERT_int, train['target'].values, 'Confusion matrix for BERT model', figsize=(7,7))","a6d7139e":"### Commit 37\n#### From https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub (commit 2)\n* without Dropout\n* Adam(lr=2e-5)\n* validation_split = 0.2,\n* epochs = 3,\n* batch_size = 32\n\n**LB = 0.81390** - This is strange, since this original model gave LB = 0.84355","44832ef3":"## 1.6. Previous commits: with training tweets correction <a class=\"anchor\" id=\"1.6\"><\/a>\n\n[Back to Table of Contents](#0.1)","0cecc5ca":"### Removing Emojis","3a07e0c4":"### Commit 29\n* Dropout(0.2)\n* Adam(lr=1e-5)\n* validation_split = 0.2,\n* epochs = 4,\n* batch_size = 32\n\n**LB = 0.83742**","781506e5":"## 11. Showing Confusion Matrices<a class=\"anchor\" id=\"11\"><\/a>\n\n[Back to Table of Contents](#0.1)","8ceecb21":"## 7. Bag of Words Counts <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)","3a36a64b":"### Commit 43\n* without Dropout\n* Adam(lr=5e-6)\n* validation_split = 0.2,\n* epochs = 4,\n* batch_size = 32\n\n**LB = 0.83844**","e9669f7a":"### 10.1. Submission<a class=\"anchor\" id=\"10.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","36488f16":"## 3. Download data <a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","dd5cc556":"[Go to Top](#0)","34ecf4b5":"I hope you find this notebook useful and enjoyable.","64652b0d":"## 4. EDA <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","c2483611":"### Commit 32\n\n* Dropout(0.2)\n* Adam(lr=2e-5)\n* validation_split = 0.2,\n* epochs = 4,\n* batch_size = 32\n\n**LB = 0.82004**","5737414b":"## 8. TF IDF <a class=\"anchor\" id=\"8\"><\/a>\n\n[Back to Table of Contents](#0.1)","52b379ff":"### Commit 28\n* Dropout(0.2)\n* Adam(lr=5e-6)\n* validation_split = 0.2,\n* epochs = 4,\n* batch_size = 32\n\n**LB = 0.82924**","7ed0a467":"### Number of characters in tweets","27c418e8":"### Commit 41\n* without Dropout\n* Adam(lr=1e-5)\n* validation_split = 0.2,\n* epochs = 5,\n* batch_size = 32\n\n**LB = 0.83231**","c8a01558":"Here we will use GloVe pretrained corpus model to represent our words. It is available in 3 varieties : 50D, 100D and 200 Dimentional. We will try 100D here.","dd1217ff":"### Removing HTML tags","eda184a7":"# Acknowledgements\n\nThis kernel uses such good notebooks: \n* [Basic EDA,Cleaning and GloVe](https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove)\n* [Spooky NLP and Topic Modelling tutorial](https:\/\/www.kaggle.com\/arthurtok\/spooky-nlp-and-topic-modelling-tutorial)\n* [NLP Tutorial using Python](https:\/\/www.kaggle.com\/itratrahman\/nlp-tutorial-using-python)\n* [Basic NLP with TensorFlow and WordCloud](https:\/\/www.kaggle.com\/marcovasquez\/basic-nlp-with-tensorflow-and-wordcloud)\n* [[QUEST] Bert-base TF2.0](https:\/\/www.kaggle.com\/akensert\/bert-base-tf2-0-minimalistic)\n* [Bert-base TF2.0 (minimalistic) III](https:\/\/www.kaggle.com\/khoongweihao\/bert-base-tf2-0-minimalistic-iii)\n* [Disaster NLP: Keras BERT using TFHub & tuning, PCA](https:\/\/www.kaggle.com\/vbmokin\/disaster-nlp-keras-bert-using-tfhub-tuning)\n* [Bert starter (inference)](https:\/\/www.kaggle.com\/user123454321\/bert-starter-inference)\n* [Disaster NLP: Keras BERT using TFHub](https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub)\n* [Keras BERT using TFHub (modified train data)](https:\/\/www.kaggle.com\/wrrosa\/keras-bert-using-tfhub-modified-train-data)\n* [Text only - BERT - Keras](https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert)\n* [Supershort NLP classification notebook](https:\/\/www.kaggle.com\/vbmokin\/supershort-nlp-classification-notebook)\n\nmy dataset [NLP with Disaster Tweets - cleaning data - source of data](https:\/\/www.kaggle.com\/vbmokin\/nlp-with-disaster-tweets-cleaning-data)\n\nand other resources:\n* https:\/\/github.com\/hundredblocks\/concrete_NLP_tutorial\/blob\/master\/NLP_notebook.ipynb\n* https:\/\/tfhub.dev\/s?q=bert","482de326":"Your comments and feedback are most welcome.","3eeec74a":"Thanks to https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove","fa0c951e":"## 1.3. Previous commits: epochs = 3 <a class=\"anchor\" id=\"1.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","62ecdd49":"### Commit 20\n* without Dropout\n* Adam(lr=1e-5)\n* validation_split = 0.2,\n* epochs = 3,\n* batch_size = 16\n\n**LB = 0.82413**","ec1219e0":"### Commit 26\n* Dropout(0.2)\n* Adam(lr=4e-5)\n* validation_split = 0.2,\n* epochs = 4,\n* batch_size = 32\n\n**LB = 0.82617**","b3b3ec4d":"## 1.1. Commit now <a class=\"anchor\" id=\"1.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","ab0222b2":"### Commit 24\n* Dropout(0.3)\n* Adam(lr=2e-5)\n* validation_split = 0.2,\n* epochs = 4,\n* batch_size = 32\n\n**LB = 0.80470**","a99721b9":"Now,we will move on to class 0.","0c3ff511":"### N-gram analysis","f03be449":"Thanks to https:\/\/github.com\/hundredblocks\/concrete_NLP_tutorial\/blob\/master\/NLP_notebook.ipynb","2bb59178":"Thanks to very good kernel https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub","7dc53da6":"### Common words","cf58be4b":"### Commit 51\n* without Dropout\n* Adam(lr=3e-6)\n* validation_split = 0.2,\n* epochs = 3,\n* batch_size = 16\n* target_corrected = True\n\n**LB = 0.83231**","f5a96f54":"## 6. WordCloud <a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","8de68d5b":"###  Average word length in a tweet","f8603ae8":"### Commit 14\n* Dropout(0.2)\n* Adam(lr=2e-5)\n* validation_split = 0.2,\n* epochs = 3,\n* batch_size = 32\n\n**LB = 0.83128**","381494ba":"### Commit 25\n* Dropout(0.2)\n* Adam(lr=3e-5)\n* validation_split = 0.2,\n* epochs = 4,\n* batch_size = 32\n\n**LB = 0.83537**","87b62b33":"we will do a bigram (n=2) analysis over the tweets. Let's check the most common bigrams in tweets.","8d788cb3":"### Commit 45\n* without Dropout\n* Adam(lr=2e-6)\n* validation_split = 0.2,\n* epochs = 4,\n* batch_size = 32\n\n**LB = 0.82719**","6376bc9e":"## Baseline Model with GloVe results","16c1b075":"Before we begin with anything else, let's check the class distribution.","1c99fed0":"### Removing punctuations","f1ee6a08":"Thanks to https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove","18efea84":"### Analyzing punctuations","90c4bfb1":"## 1.7. Previous commits: parameters and LB scores<a class=\"anchor\" id=\"1.7\"><\/a>\n\n[Back to Table of Contents](#0.1)","9751f3aa":"### Commit 60 (The best!)\n* without Dropout\n* Adam(lr=6e-6)\n* validation_split = 0.2\n* epochs = 3\n* batch_size = 16\n* target_corrected = False\n\n**LB = 0.85378**","8df55c43":"## Build and train BERT model","98bbd569":"### Commit 30\n* Dropout(0.1)\n* Adam(lr=1e-5)\n* validation_split = 0.2,\n* epochs = 4,\n* batch_size = 32\n\n**LB = 0.83333**","d22a0c7c":"### Commit 33\n* Dropout(0.2)\n* Adam(lr=2e-5)\n* validation_split = 0.3,\n* epochs = 4,\n* batch_size = 32\n\n**LB = 0.83128**","e184af9b":"### Real Disaster","4584a732":"## 1.2. Previous commits: Dropout = 0.1 or 0.3 <a class=\"anchor\" id=\"1.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","e4a7d509":"### Commit 78\n* random_state_split = 21\n* without Dropout\n* Adam(lr=6e-6)\n* validation_split = 0.2\n* epochs = 3\n* batch_size = 16\n* target_corrected = False\n\n**LB = 0.84355**","85d5f845":"Thanks to https:\/\/www.kaggle.com\/marcovasquez\/basic-nlp-with-tensorflow-and-wordcloud","fbb9ae74":"### Commit 49\n* without Dropout\n* Adam(lr=5e-6)\n* validation_split = 0.2,\n* epochs = 3,\n* batch_size = 16\n\n**LB = 0.83537**","d0fa500d":"### Commit 15\n* Dropout(0.2)\n* Adam(lr=2e-5)\n* validation_split = 0.2,\n* epochs = 4,\n* batch_size = 32\n\n**LB = 0.83742**","f0b7dd89":"## 9. GloVe <a class=\"anchor\" id=\"9\"><\/a>\n\n[Back to Table of Contents](#0.1)","189eef1b":"* random_state_split = 42\n* without Dropout\n* Adam(lr=5.95e-6)\n* validation_split = 0.15\n* epochs = 3\n* batch_size = 16\n* target_corrected = False\n* target_big_corrected = False","8d2b6a81":"### Commit 46\n* without Dropout\n* Adam(lr=5e-6)\n* validation_split = 0.2,\n* epochs = 4,\n* batch_size = 32\n* target_corrected = True\n* target_big_corrected = True\n\n**LB = 0.83333**","a2414474":"First let's check tweets indicating real disaster.","6bc06473":"<a class=\"anchor\" id=\"0\"><\/a>\n# [Real or Not? NLP with Disaster Tweets](https:\/\/www.kaggle.com\/c\/nlp-getting-started)\n\n# NLP:\n* EDA (with WordCloud) \n* Bag of Words \n* TF IDF\n* GloVe\n* BERT with TFHub and with Submission\n* PCA visualization for the main models\n* Showing Confusion Matrices for BERT, Simpletransformers with DistilBERT and GloVe","31a282bf":"### Commit 22\n* Dropout(0.2)\n* Adam(lr=1.1e-5)\n* validation_split = 0.2,\n* epochs = 3,\n* batch_size = 16\n\n**LB (for BERT) = 0.80879**","018f8c2a":"### Commit 62\n* without Dropout\n* Adam(lr=5.9e-6)\n* validation_split = 0.2\n* epochs = 3\n* batch_size = 16\n* target_corrected = False\n\n**LB = 0.83537**","524c4e15":"### Commit 39\n* without Dropout\n* Adam(lr=2e-5)\n* validation_split = 0.2,\n* epochs = 4,\n* batch_size = 24\n\n**LB = 0.83435**","b6ab4bfa":"### Commit 18\n* Dropout(0.2)\n* Adam(lr=2e-5)\n* validation_split = 0.2,\n* epochs = 3,\n* batch_size = 16\n\n\n\n**LB = 0.83537**","360e62d7":"In both of them,\"the\" dominates which is followed by \"a\" in class 0 and \"in\" in class 1.","77133a1e":"### Commit 27\n* Dropout(0.2)\n* Adam(lr=5e-4)\n* validation_split = 0.2,\n* epochs = 4,\n* batch_size = 32\n\n**LB = 0.57055**","6135a76e":"## 1.5. Previous commits: epochs = 5 <a class=\"anchor\" id=\"1.5\"><\/a>\n\n[Back to Table of Contents](#0.1)","006f237a":"These embeddings don't look very cleanly separated. Let's see if we can still fit a useful model on them.","17dba71d":"### Class distribution","da9c38c3":"**target_corrected = True :**\n\nFrom https:\/\/www.kaggle.com\/wrrosa\/keras-bert-using-tfhub-modified-train-data - author of this kernel read tweets in training data and figure out that some of them have errors:\n\n    ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n    train.loc[train['id'].isin(ids_with_target_error),'target'] = 0","cadf8d48":"### Commit 40\n* without Dropout\n* Adam(lr=2e-5)\n* validation_split = 0.2,\n* epochs = 5,\n* batch_size = 32\n\n**LB = 0.81186**","54997a9a":"### Commit 23\n* Dropout(0.2)\n* Adam(lr=2e-5)\n* validation_split = 0.2,\n* epochs = 5,\n* batch_size = 32\n\n**LB = 0.83435**","076598c8":"### Commit 73\n* without Dropout\n* Adam(lr=6e-6)\n* validation_split = 0.25\n* epochs = 3\n* batch_size = 16\n* target_corrected = False\n\n**LB = 0.83640**","5b6fbc9f":"### Commit 80\n* random_state_split = 32\n* without Dropout\n* Adam(lr=6e-6)\n* validation_split = 0.2\n* epochs = 3\n* batch_size = 16\n* target_corrected = False\n\n**LB = 0.83665**","54a74e42":"### Number of words in a tweet","1bad9dfd":"### Only successful commits :","a06490f4":"### Commit 55\n* without Dropout\n* Adam(lr=5e-6)\n* validation_split = 0.3\n* epochs = 3\n* batch_size = 16\n* target_corrected = False\n\n**LB = 0.82924**","54195fda":"## 5. Data Cleaning <a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","e5dda160":"Thanks to https:\/\/www.kaggle.com\/arthurtok\/spooky-nlp-and-topic-modelling-tutorial","1cc59b6d":"## 2. Import libraries <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","c46fd8e3":"### Not Disaster","6e540b71":"Now,we will analyze tweets with class 1.","b35f1d0a":"### Common stopwords in tweets","b29f9da7":"### Commit 48\n* without Dropout\n* Adam(lr=5e-6)\n* validation_split = 0.2,\n* epochs = 4,\n* batch_size = 16\n* target_corrected = False\n* target_big_corrected = False\n\n**LB = 0.83231**","e43aa599":"The distribution of both seems to be almost same.120 t0 140 characters in a tweet are the most common among both.","2a3a708f":"## Prediction","01c47bf0":"### Removing urls","566bd5ba":"### Commit 74\n* without Dropout\n* Adam(lr=6e-6)\n* validation_split = 0.15\n* epochs = 3\n* batch_size = 16\n* target_corrected = False\n\n**LB = 0.83946**","a5a63566":"## Target correction","7b8b5c35":"First we  will analyze tweets with class 0.","a6032757":"### Commit 35\n* Dropout(0.15)\n* Adam(lr=2e-5)\n* validation_split = 0.2,\n* epochs = 4,\n* batch_size = 32\n\n**LB = 0.81390**","46b48bfe":"## Download data","15d76689":"Thanks to:\n* https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\n* https:\/\/www.kaggle.com\/arthurtok\/spooky-nlp-and-topic-modelling-tutorial\n* https:\/\/www.kaggle.com\/itratrahman\/nlp-tutorial-using-python","663a6708":"Lot of cleaning needed !","99a09690":"### Visualizing the embeddings","255d97d3":"## Big target correction","5276ffe0":"## 1.4. Previous commits: epochs = 4 <a class=\"anchor\" id=\"1.4\"><\/a>\n\n[Back to Table of Contents](#0.1)","5694633c":"* Commit 60\n* without Dropout\/lr=6e-6\/val=0.2\/epochs=3\/batch=16\/target_corrected=False\n* LB = **0.85378**\n* Commit 43\n* without Dropout\/lr=5e-6\/val=0.2\/epochs=4\/batch=32\/target_corrected=False\n* LB = 0.83844\n* 15\n* Dropout=0.2\/lr=2e-5\/val=0.2\/epochs=4\/batch=32\/target_corrected=False\n* LB = 0.83742\n* 29\n* Dropout=0.2\/lr=1e-5\/val=0.2\/epochs=4\/batch=32\/target_corrected=False\n* LB = 0.83742\n* 62\n* without Dropout\/lr=5.9e-6\/val=0.2\/epochs=3\/batch=16\/target_corrected=False\n* LB = 0.83537\n* 49\n* without Dropout\/lr=5e-6\/val=0.2\/epochs=3\/batch=16\/target_corrected=False\n* LB = 0.83537\n* 18\n* Dropout=0.2\/lr=2e-5\/val=0.2\/epochs=3\/batch=16\/target_corrected=True\n* LB = 0.83537\n* 25\n* Dropout=0.2\/lr=3e-5\/val=0.2\/epochs=4\/batch=32\/target_corrected=False\n* LB = 0.83537\n* 37\n* without Dropout\/lr=2e-5\/val=0.2\/epochs=4\/batch=24\/target_corrected=False\n* LB = 0.83435\n* 23\n* Dropout=0.2\/lr=2e-5\/val=0.2\/epochs=5\/batch=32\/target_corrected=False\n* LB = 0.83435\n* 46\n* without Dropout\/lr=5e-6\/val=0.2\/epochs=4\/batch=32\/target_corrected=True\/target_big_corrected = True\n* LB = 0.83333\n* 30\n* Dropout=0.1\/lr=1e-5\/val=0.2\/epochs=4\/batch=32\/target_corrected=False\n* LB = 0.83333\n* 51\n* without Dropout\/lr=3e-6\/val=0.2\/epochs=3\/batch=16\/target_corrected=True\n* LB = 0.83231\n* 46\n* without Dropout\/lr=5e-6\/val=0.2\/epochs=4\/batch=16\/target_corrected=False\n* LB = 0.83231\n* 33\n* Dropout=0.2\/lr=2e-5\/val=0.3\/epochs=4\/batch=32\/target_corrected=False\n* LB = 0.83128\n* 14\n* Dropout=0.2\/lr=2e-5\/val=0.2\/epochs=3\/batch=32\/target_corrected=True\n* LB = 0.83128\n* 55\n* without Dropout\/lr=5e-6\/val=0.3\/epochs=3\/batch=16\/target_corrected=False\n* LB = 0.82924\n* 28\n* Dropout=0.2\/lr=5e-6\/val=0.2\/epochs=4\/batch=32\/target_corrected=False\n* LB = 0.82924\n* 26\n* Dropout=0.2\/lr=4e-5\/val=0.2\/epochs=4\/batch=32\/target_corrected=False\n* LB = 0.82617\n* 45\n* without Dropout\/lr=2e-6\/val=0.2\/epochs=4\/batch=32\/target_corrected=False\n* LB = 0.82719\n* 20\n* without Dropout\/lr=1e-5\/val=0.2\/epochs=3\/batch=16\/target_corrected=True\n* LB = 0.82413\n* 42\n* without Dropout\/lr=1e-5\/val=0.2\/epochs=5\/batch=32\/target_corrected=False\n* LB = 0.83231\n* 32\n* Dropout=0.2\/lr=2e-5\/val=0.2\/epochs=4\/batch=32\/target_corrected=True\n* LB = 0.82004\n* 37\n* without Dropout\/lr=2e-5\/val=0.2\/epochs=3\/batch=32\/target_corrected=False\n* LB = 0.81390\n* 35\n* Dropout=0.15\/lr=2e-5\/val=0.2\/epochs=4\/batch=32\/target_corrected=False\n* LB = 0.81390\n* 40\n* without Dropout\/lr=2e-5\/val=0.2\/epochs=5\/batch=32\/target_corrected=False\n* LB = 0.81186\n* 22\n* Dropout=0.2\/lr=1.1e-5\/val=0.2\/epochs=3\/batch=16\/target_corrected=True\n* LB = 0.80879\n* 24\n* Dropout=0.3\/lr=2e-5\/val=0.2\/epochs=4\/batch=32\/target_corrected=False\n* LB = 0.80470\n* 31\n* Dropout=0.2\/lr=1e-5\/val=0.15\/epochs=4\/batch=32\/target_corrected=False\n* LB = 0.79856\n* 27\n* Dropout=0.2\/lr=5e-4\/val=0.2\/epochs=4\/batch=32\/target_corrected=False\n* LB = 0.57055","135417bb":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n\n1. [My upgrade BERT model](#1)\n    -  [Commit now](#1.1)\n    -  [Previous commits: Dropout = 0.1 or 0.3](#1.2)\n    -  [Previous commits: epochs = 3](#1.3)\n    -  [Previous commits: epochs = 4](#1.4)\n    -  [Previous commits: epochs = 5](#1.5)\n    -  [Previous commits: with training tweets correction](#1.6)\n    -  [Previous commits: parameters and LB scores](#1.7)    \n1. [Import libraries](#2)\n1. [Download data](#3)\n1. [EDA](#4)\n1. [Data Cleaning](#5)\n1. [WordCloud](#6)\n1. [Bag of Words Counts](#7)\n1. [TF IDF](#8)\n1. [GloVe](#9)\n1. [BERT using TFHub](#10)\n   - [Submission](#10.1)\n1. [Showing Confusion Matrices](#12)","b8d91bda":"### Commit 31\n* Dropout(0.2)\n* Adam(lr=1e-5)\n* validation_split = 0.15,\n* epochs = 4,\n* batch_size = 32\n\n**LB = 0.79856**","60bd5db6":"## 10. BERT using TFHub <a class=\"anchor\" id=\"10\"><\/a>\n\n[Back to Table of Contents](#0.1)","305c0b5e":"## 1. My upgrade BERT model <a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)"}}