{"cell_type":{"bf93c5e2":"code","0e108ba6":"code","ec855cfb":"code","d1bf7a7b":"code","1f26e73b":"code","8ec20873":"code","78c19188":"code","e584aab5":"code","a9bd0998":"code","de15361a":"code","adeea687":"code","ebae4c87":"code","057102f6":"code","21d8bcb5":"code","4085cc60":"code","f4231dab":"code","81f28faf":"code","7e3af8dd":"code","cff6296d":"code","c244caa4":"markdown","7c7505dd":"markdown","1794cf18":"markdown","c8e2fa17":"markdown","a855348f":"markdown"},"source":{"bf93c5e2":"# Import all dependencies\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nimport warnings\nimport datetime\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nimport os\nimport gc\n\nwarnings.filterwarnings(\"ignore\")\nprint(os.listdir(\"..\/input\"))","0e108ba6":"train = pd.read_csv(\"..\/input\/train.csv\", parse_dates=[\"first_active_month\"])\ntest = pd.read_csv(\"..\/input\/test.csv\", parse_dates=[\"first_active_month\"])\nhist_trans = pd.read_csv(\"..\/input\/historical_transactions.csv\")\nnew_trans = pd.read_csv(\"..\/input\/new_merchant_transactions.csv\")","ec855cfb":"train[\"month\"] = train[\"first_active_month\"].dt.month\ntest[\"month\"] = test[\"first_active_month\"].dt.month\ntrain[\"year\"] = train[\"first_active_month\"].dt.year\ntest[\"year\"] = test[\"first_active_month\"].dt.year\ntrain['elapsed_time'] = (datetime.date(2018, 2, 1) - train['first_active_month'].dt.date).dt.days\ntest['elapsed_time'] = (datetime.date(2018, 2, 1) - test['first_active_month'].dt.date).dt.days\ntrain = pd.get_dummies(train, columns=['feature_1', 'feature_2'])\ntest = pd.get_dummies(test, columns=['feature_1', 'feature_2'])\n#train.head()","d1bf7a7b":"hist_trans = pd.get_dummies(hist_trans, columns=['category_2', 'category_3'])\nhist_trans['authorized_flag'] = hist_trans['authorized_flag'].map({'Y': 1, 'N': 0})\nhist_trans['category_1'] = hist_trans['category_1'].map({'Y': 1, 'N': 0})\n#hist_trans.head()","1f26e73b":"def aggregate_transactions(trans, prefix):  \n    trans.loc[:, 'purchase_date'] = pd.DatetimeIndex(trans['purchase_date']).\\\n                                      astype(np.int64) * 1e-9\n    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'category_1': ['mean'],\n        'category_2_1.0': ['mean'],\n        'category_2_2.0': ['mean'],\n        'category_2_3.0': ['mean'],\n        'category_2_4.0': ['mean'],\n        'category_2_5.0': ['mean'],\n        'category_3_A': ['mean'],\n        'category_3_B': ['mean'],\n        'category_3_C': ['mean'],\n        'merchant_id': ['nunique'],\n        'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n        'installments': ['sum', 'mean', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp],\n        'month_lag': ['min', 'max']\n    }\n    agg_trans = trans.groupby(['card_id']).agg(agg_func)\n    agg_trans.columns = [prefix + '_'.join(col).strip() \n                           for col in agg_trans.columns.values]\n    agg_trans.reset_index(inplace=True)\n    \n    df = (trans.groupby('card_id')\n          .size()\n          .reset_index(name='{}transactions_count'.format(prefix)))\n    \n    agg_trans = pd.merge(df, agg_trans, on='card_id', how='left')\n    \n    return agg_trans","8ec20873":"merch_hist = aggregate_transactions(hist_trans, prefix='hist_')\ndel hist_trans\ngc.collect()\ntrain = pd.merge(train, merch_hist, on='card_id',how='left')\ntest = pd.merge(test, merch_hist, on='card_id',how='left')\ndel merch_hist\ngc.collect()\n\nnew_trans = pd.get_dummies(new_trans, columns=['category_2', 'category_3'])\nnew_trans['authorized_flag'] = new_trans['authorized_flag'].map({'Y': 1, 'N': 0})\nnew_trans['category_1'] = new_trans['category_1'].map({'Y': 1, 'N': 0})\nmerch_new = aggregate_transactions(new_trans, prefix='new_')\ndel new_trans\ngc.collect()\n\ntrain = pd.merge(train, merch_new, on='card_id',how='left')\ntest = pd.merge(test, merch_new, on='card_id',how='left')\ndel merch_new\ngc.collect()","78c19188":"IDS = test['card_id']\ntarget = train['target']\ndrops = ['card_id', 'first_active_month', 'target']\nuse_cols = [c for c in train.columns if c not in drops]\nfeatures = list(train[use_cols].columns)\ntrain[features].head()","e584aab5":"print(train[features].shape)\nprint(test[features].shape)","a9bd0998":"train = train[features+['target']]\ntest = test[features]\nprint('Final train data shape is:',train.shape)\nprint('Final test data shape is:', test.shape)","de15361a":"import h2o\nfrom h2o.automl import H2OAutoML","adeea687":"# intializing H2o\nh2o.init()","ebae4c87":"# Convert pandas datarame into h2o dataframe\nhtrain = h2o.H2OFrame(train)\nhtest = h2o.H2OFrame(test)\ndel train, test\ngc.collect()\nprint(htrain.shape, htest.shape)\nprint(htrain.head)","057102f6":"# Assign x as Independent and y as Dependent\nx = htrain.columns\ny = \"target\"\nx.remove(y)","21d8bcb5":"# Uncomment and run locally to understand the different Parameter\n?? H2OAutoML\n\"\"\"\nH2OAutoML(nfolds=5, balance_classes=False, class_sampling_factors=None, \nmax_after_balance_size=5.0, max_runtime_secs=3600, max_models=None, stopping_metric='AUTO', \nstopping_tolerance=None, stopping_rounds=3, seed=None, project_name=None, \nexclude_algos=None, keep_cross_validation_predictions=False, \nkeep_cross_validation_models=False, keep_cross_validation_fold_assignment=False, \nsort_metric='AUTO')\n\"\"\"","4085cc60":"# Since our goal is AutoML, I am specifying the least parameters\nautoelo = H2OAutoML(max_models=20, seed=42)\nautoelo.train(x=x, y=y, training_frame=htrain)","f4231dab":"# Stack up and display the results of the top models, the metrics displayed will be relevant to regression problems\nlb = autoelo.leaderboard\nlb.head(rows=lb.nrows)","81f28faf":"# Display the properties of the leader\nautoelo.leader","7e3af8dd":"# Let us get the predictions against the test data\npreds = autoelo.leader.predict(htest)\npreds = preds.as_data_frame()","cff6296d":"# Let us create a submission\nsub_df = pd.DataFrame({\"card_id\":IDS.values, \"target\": preds.predict.values})\nprint(sub_df.shape)\nsub_df.head()\n# Submit the prediction\nsub_df.to_csv('AML_sub.csv', index=False)","c244caa4":"# All File imports","7c7505dd":"I did not spend time in creating features but I borrowed it from several fantastic starter notebooks below, the top 2  notables ones are:\n[Notebook 1](https:\/\/www.kaggle.com\/youhanlee\/hello-elo-ensemble-will-help-you)\n[Notebook 2](https:\/\/www.kaggle.com\/fabiendaniel\/elo-world)","1794cf18":"# Auto Elo  \n\nIn this kernel I try to the power of [AutoML](http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/automl.html from H2O) I stongly believe that these can serve as good starting points.  \n","c8e2fa17":"# Let us bring in H2O now","a855348f":"# Feature Engineering"}}