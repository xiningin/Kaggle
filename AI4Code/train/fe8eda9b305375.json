{"cell_type":{"fa071bd6":"code","4630a900":"code","a9c31d62":"code","97c08586":"code","b64b636f":"code","e32b83de":"code","353de0cf":"code","a544e9cf":"code","84587023":"code","0a5b8e3d":"code","bb613558":"code","ff4cd2d8":"code","c034d400":"code","6126600a":"code","d7f13572":"code","a342bae5":"code","72a23c22":"code","9cca4b15":"code","cded1e37":"code","39053ccf":"code","daf3f990":"code","7f47cf89":"code","f603a029":"code","c43146f2":"code","aa33aded":"code","b1e6fe81":"code","565cdf47":"markdown","c89bce66":"markdown","3bee666a":"markdown","ac629ebb":"markdown","27ecfd29":"markdown","7bedfbc6":"markdown","b7b30679":"markdown","413b6693":"markdown","48fe4ce0":"markdown","4e30b0c7":"markdown","37c86283":"markdown","35df2f95":"markdown","a5bc914e":"markdown","cbdc7355":"markdown","37774459":"markdown","e455a797":"markdown","90ffadf2":"markdown","02d22f30":"markdown","e2dfaed9":"markdown","ee36115a":"markdown","30214e97":"markdown","e18f90d6":"markdown"},"source":{"fa071bd6":"import pandas as pd\n\ntrain = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n\nprint('Train dataframe shape:', train.shape)\nprint('Test dataframe shape:', test.shape)","4630a900":"train.head()","a9c31d62":"test.head()","97c08586":"CONTRACTION_MAP = {\n\"ain't\": \"is not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"I'd\": \"I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I will\",\n\"I'll've\": \"I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}","b64b636f":"import re \n\ndef expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n                                      flags=re.IGNORECASE|re.DOTALL)\n    def expand_match(contraction):\n        match = contraction.group(0)\n        first_char = match[0]\n        expanded_contraction = contraction_mapping.get(match)\\\n            if contraction_mapping.get(match) \\\n            else contraction_mapping.get(match.lower())\n        expanded_contraction = first_char+expanded_contraction[1:]\n        return expanded_contraction\n    \n    expanded_text = contractions_pattern.sub(expand_match, text)\n    expanded_text = re.sub(\"'\", \"\", expanded_text)\n    return expanded_text\n\nexpand_contractions(\"y'all I've don't I'd we're\")","e32b83de":"from bs4 import BeautifulSoup\nimport nltk\nimport numpy as np\n\nwpt = nltk.WordPunctTokenizer()\n\nstopwords = set(nltk.corpus.stopwords.words('english'))\n\ndef clean_text(text, expand=True):\n    # strip html tags\n    text = BeautifulSoup(text, 'html.parser').get_text()\n    \n    # remove URL\n    text = re.sub(r'http:\/\/\\S+|https?:\/\/\\S+|www\\.\\S+', '', text)\n    \n    if expand:\n        text = expand_contractions(text)\n\n    # lower case, remove special characters, numbers and strip leading and trailing whitespaces\n    text = re.sub(r'[^a-zA-Z\\s]', ' ', text, re.I|re.A)\n    text = text.lower()\n    text = text.strip()\n    \n    # remove emojis\n    emoji_pattern = re.compile(\"[\"\n                       u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                       u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                       u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                       u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                       u\"\\U00002702-\\U000027B0\"\n                       u\"\\U000024C2-\\U0001F251\"\n                       \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    # tokenize\n    tokens = wpt.tokenize(text)\n    \n    # filter out stopwords\n    filtered_tokens = [word for word in tokens if word not in stopwords]\n    \n    # Join tokens\n    text = ' '.join(filtered_tokens)\n    return text\n\nclean_corpus = np.vectorize(clean_text)","353de0cf":"%%time\ntrain['clean_text'] = clean_corpus(train['text'])","a544e9cf":"train.head()","84587023":"import matplotlib.pyplot as plt\n\ntrain['target'].value_counts().plot(kind='bar', figsize=(10, 6), rot=0)\nplt.xlabel('Target label', labelpad=14)\nplt.ylabel('Counts of Target', labelpad=14)\nplt.title('Counts of Each Labeled Target', y=1)","0a5b8e3d":"train['target'].value_counts()\/len(train)","bb613558":"train_0 = train[train['target']==0]\ntrain_1 = train[train['target']==1]","ff4cd2d8":"from wordcloud import WordCloud\n\ndef plot_wordcloud(text, ax, clean_text=False, mask=None, max_words=200, max_font_size=100, \n                   title=None, title_size=40):\n\n    if clean_text:\n        text = clean_text(text)\n    \n    wordcloud = WordCloud(background_color='black',\n                         stopwords = stopwords,\n                         max_words = max_words,\n                         max_font_size = max_font_size,\n                         random_state = 0,\n                         width = 800, \n                         height = 800, \n                         mask = mask)\n    wordcloud.generate(text)\n  \n    ax.imshow(wordcloud)\n    ax.axis('off')\n    ax.set_title(title)","c034d400":"fig, (axis1, axis2) = plt.subplots(nrows=1, ncols=2, figsize=(16, 10))\nplot_wordcloud(' '.join(train_0['clean_text']), ax=axis1, title='Frequent words in non-disaster tweets')\nplot_wordcloud(' '.join(train_1['clean_text']), ax=axis2, title='Frequent words in disaster tweets')","6126600a":"def generate_ngrams(text, n=1):\n    tokens = [token for token in text.lower().split(' ') if token !='']\n    ngrams = zip(*([tokens[i:] for i in range(n)]))\n    return [' '.join(ngram) for ngram in ngrams]\n\n# test ride the function\ntrain_0['clean_text'][:10].apply(lambda x: generate_ngrams(x, n=2))","d7f13572":"from collections import Counter\n\ntrain_0_unigrams = generate_ngrams(' '.join(train_0['clean_text']), n=1)\ntrain_1_unigrams = generate_ngrams(' '.join(train_1['clean_text']), n=1)\n\ntrain_0_counter = pd.DataFrame(Counter(train_0_unigrams).items(), columns = ['token', 'frequency'])\ntrain_0_counter = train_0_counter.sort_values('frequency', ascending=False)\ntrain_1_counter = pd.DataFrame(Counter(train_1_unigrams).items(), columns = ['token', 'frequency'])\ntrain_1_counter = train_1_counter.sort_values('frequency', ascending=False)","a342bae5":"fig, (axis1, axis2) = plt.subplots(nrows=1, ncols=2, figsize=(16, 10))\ntrain_0_counter[:50].plot.barh(x='token', y='frequency', ax=axis1, title='Non disaster tweets token frequency')\ntrain_1_counter[:50].plot.barh(x='token', y='frequency', ax=axis2, title='Disaster tweets token frequency')\nfig.tight_layout()","72a23c22":"train_0_bigrams = generate_ngrams(' '.join(train_0['clean_text']), n=2)\ntrain_1_bigrams = generate_ngrams(' '.join(train_1['clean_text']), n=2)\n\ntrain_0_counter = pd.DataFrame(Counter(train_0_bigrams).items(), columns = ['token', 'frequency'])\ntrain_0_counter = train_0_counter.sort_values('frequency', ascending=False)\ntrain_1_counter = pd.DataFrame(Counter(train_1_bigrams).items(), columns = ['token', 'frequency'])\ntrain_1_counter = train_1_counter.sort_values('frequency', ascending=False)","9cca4b15":"fig, (axis1, axis2) = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\ntrain_0_counter[:50].plot.barh(x='token', y='frequency', ax=axis1, title='Non disaster tweets token frequency')\ntrain_1_counter[:50].plot.barh(x='token', y='frequency', ax=axis2, title='Disaster tweets token frequency')\nfig.tight_layout()","cded1e37":"train_0_trigrams = generate_ngrams(' '.join(train_0['clean_text']), n=3)\ntrain_1_trigrams = generate_ngrams(' '.join(train_1['clean_text']), n=3)\n\ntrain_0_counter = pd.DataFrame(Counter(train_0_trigrams).items(), columns = ['token', 'frequency'])\ntrain_0_counter = train_0_counter.sort_values('frequency', ascending=False)\ntrain_1_counter = pd.DataFrame(Counter(train_1_trigrams).items(), columns = ['token', 'frequency'])\ntrain_1_counter = train_1_counter.sort_values('frequency', ascending=False)","39053ccf":"fig, (axis1, axis2) = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\ntrain_0_counter[:50].plot.barh(x='token', y='frequency', ax=axis1, title='Non disaster tweets token frequency')\ntrain_1_counter[:50].plot.barh(x='token', y='frequency', ax=axis2, title='Disaster tweets token frequency')\nfig.tight_layout()","daf3f990":"train['num_words'] = train['text'].apply(lambda x: len(x.split(' ')))\ntrain['num_characters'] = train['text'].apply(lambda x: len(x))","7f47cf89":"train.head()","f603a029":"fig, (axis1, axis2) = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\n\ntrain.boxplot(column=['num_words'], by=['target'], ax=axis1)\ntrain.boxplot(column=['num_characters'], by=['target'], ax=axis2)","c43146f2":"train.to_csv('train_cleaned.csv', index=False)","aa33aded":"test['clean_text'] = clean_corpus(test['text'])\ntest.head()","b1e6fe81":"test.to_csv('test_cleaned.csv', index=False)","565cdf47":"Tokenize the text and get each word's frequency","c89bce66":"<h3>Word distribution<\/h3>\n\nFrequency of words across the tweets. We'll look at word clouds as an informal visualization and then try to get actual frequencies by unigram, bigrams and trigrams. ","3bee666a":"<h1>Baseline Classification models<\/h1>\n\nIn this notebook, I'll do all the text preprocessing to our data and then I'll start with some baseline classifier models like BOW to classify the disaster tweets.  In the subsequent notebooks, I'll use more advanced methodologies to try to improve on the predictions from this model. ","ac629ebb":"<h4>Word Cloud<\/h4>\nLet's use one of my favorite visuals -- word clouds -- to see the frequency of words in tweets per target, i.e., 0 or 1","27ecfd29":"Let's see some of the most frequent words in the di","7bedfbc6":"<h4>Trigram frequencies<\/h4>","b7b30679":"<h2>Load the data<\/h2>\n\nLet's first start with loading the data","413b6693":"On average, there are more characters in a disaster tweet, but not significant. ","48fe4ce0":"So, disaster tweets are labeled as 1. ","4e30b0c7":"Seems that non-disaster related tweets have a higher frequency of non-influential words, whereas disaster tweets have a higher frequency of relevant tokens like 'fire', 'disaster', 'suicide', 'storm', etc. ","37c86283":"train.to_csv('\/kaggle\/data\/output\/train_cleaned.csv', index=False)","35df2f95":"Trigrams definitely seem extraneous. ","a5bc914e":"Wordclouds immediately show some overall differences in the frequency of words. As expected, there is a higher frequency of words like 'death', 'fire', 'suicide', 'flood' etc. in disaster tweets relative to non-disaster tweets.  But, this is obviously very informal.  Let's try to formalize this a bit and get some real numbers!","cbdc7355":"<h4>Meta features<\/h4>\n\nI'll look at the following meta features since they might be useful features to consider in the final model:\n\n1. Number of words in the text\n2. Number of characters in the text","37774459":"<h2>EDA<\/h2>\n\nLet's visualize the data and look at some distributions to get a feel for the data and identify imbalances in our features.","e455a797":"<h4>Unigram Frequencies<\/h4>","90ffadf2":"<h2>Text Preprocessing<\/h2>\n\nClean up the text using some basic NLP methods","02d22f30":"Roughly, 43% are disaster tweets and 57% are non-disaster tweets.  ","e2dfaed9":"<h4>Bigram Frequencies<\/h4>\n\nNow, let's look at bigram frequencies.  These may be more relevant to us","ee36115a":"<h4>Save the cleaned data<\/h4>","30214e97":"<h3>Target Distribution<\/h3>\n\nLet's look at the labeled train data to see what the distribution of the target feature looks like. ","e18f90d6":"Cool! The bigrams are definitely more informative for us humans. Not sure if bigrams would actually help when training the models since the key disaster-related words like 'suicide', 'wildfire', 'bomber', etc. are still fairly independent and predictive. Either way, let's look at trigrams, just for fun. "}}