{"cell_type":{"73670f59":"code","96ee093c":"code","1f4ca8ed":"code","d49ef60b":"code","3fc3a3a2":"code","8df9b538":"code","f6895692":"code","b840ad3d":"code","2037f7c1":"code","cf15c1e1":"code","a0b8bc24":"code","0c66db38":"code","14d09a21":"code","173500f8":"code","e6fe469d":"code","f3c1372e":"code","f3fd3c1c":"code","4b80a6b4":"code","ab11f601":"code","cdd6955b":"code","2bb93972":"code","182086e5":"code","24dcd6f7":"code","9d8f34e6":"code","00d3d1f0":"code","a03c4844":"code","ce6767c1":"code","a195d8d7":"code","d7c49e11":"code","c0bf30cd":"code","e69af6b2":"code","83a13af1":"code","1e0825ce":"code","4438f913":"code","1af11a8d":"code","63df4998":"code","16355c31":"code","8ae2c157":"code","01504861":"code","dff31ce7":"code","2af11485":"code","3ac6e233":"code","547d7417":"code","a36a0688":"code","8e616165":"code","9747edcb":"code","d08dadb8":"code","1356873d":"code","7bc7acb1":"code","f9a7b50a":"code","a4f321ac":"code","1b9d186b":"code","491cfec5":"code","309974ff":"code","05b21e4b":"code","c99f3b4a":"code","66fe3502":"code","22597bda":"code","1b1ba19d":"code","fd65b974":"markdown","fe2c08fd":"markdown","8d344b9e":"markdown","7e38dee8":"markdown","0368ad71":"markdown","fec7affa":"markdown","f9ec01c4":"markdown","b099cce3":"markdown","dd929fb8":"markdown","0b9d2322":"markdown","149bc052":"markdown","409d5d0e":"markdown","716b24de":"markdown","50231477":"markdown","7567629c":"markdown","0ef74bc3":"markdown","f0fb9ef9":"markdown","c12f6f7a":"markdown","47470890":"markdown","d574d6fd":"markdown","fbba8717":"markdown","a321a77f":"markdown","d1221aa9":"markdown","b8dc0951":"markdown","1eab56c9":"markdown","10369e3e":"markdown","652287b6":"markdown","077a3066":"markdown","dc7d2480":"markdown","c6cb2b1b":"markdown","3b663d5f":"markdown","93f04aaa":"markdown","09da9fd8":"markdown","dd8b57d8":"markdown","6feb6940":"markdown","4277e100":"markdown","3c793447":"markdown","ee1396af":"markdown","aa77f89b":"markdown","95dd15f7":"markdown"},"source":{"73670f59":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport math, time, datetime\n# from math import sqrt\nimport numpy as np \nimport pandas as pd\nfrom scipy import stats\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib import colors\nimport missingno as msno\n%matplotlib inline\n\nfrom sklearn.model_selection import (train_test_split, GridSearchCV, cross_val_score)\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder, scale\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\nfrom sklearn.metrics import (confusion_matrix, mean_squared_error, r2_score, classification_report,\n                            roc_auc_score, roc_curve, precision_recall_curve, auc, log_loss)\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import mutual_info_classif","96ee093c":"df_first = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_test_first = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ndf_gender_sub = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\ndf_first.head()","1f4ca8ed":"fig, ax = plt.subplots(figsize = (12, 2))\nax.barh(df_first['Survived'].unique(), df_first['Survived'].value_counts(), align='center', color=['red', 'green'])\nax.text(530, 0, df_first['Survived'].value_counts()[0], ha='center', va='center', color='w', size=20)\nax.text(320, 1, df_first['Survived'].value_counts()[1], ha='center', va='center', color='w', size=20)\nax.set_yticks(df_first['Survived'].unique())\nax.set_yticklabels(df_first['Survived'].unique())\nax.invert_yaxis()\nax.set_ylabel('Survived')\nax.set_title('How many people survived?')\n\nplt.show()","d49ef60b":"df_first.info()\nprint(\"----------------------------\")\ndf_test_first.info()","3fc3a3a2":"## plot graphic of missing values\nmsno.matrix(df_first, figsize=(12, 6))","8df9b538":"# plot graphic of missing values\nmsno.matrix(df_test_first, figsize=(12, 6))","f6895692":"# drop unnecessary columns, which won't be used in analysis and prediction\ndf = df_first.drop(['PassengerId','Name','Ticket', 'Cabin'], axis=1)\ndf_test = df_test_first.drop(['PassengerId','Name','Ticket', 'Cabin'], axis=1)","b840ad3d":"# Checking for missing values in train data\ndf.isnull().sum()","2037f7c1":"# Checking for missing values in test data\ndf_test.isnull().sum()","cf15c1e1":"# getting the Pclass of the Fare missing value\nPclass_fare_mis = df_test[df_test['Fare'].isnull()]['Pclass'].values[0]\nprint(Pclass_fare_mis)\nprint(\"----------------------------\")\n# getting the mean of Fare for the Pclass 3\ndf_Pclass_mean = df_test.groupby(['Pclass']).mean().loc[Pclass_fare_mis,'Fare']\n\n# fill NaN values in Fare column with mean\ndf_test.loc[152, 'Fare'] = df_Pclass_mean\n\ndf_test.isnull().sum()","a0b8bc24":"# getting the Pclass of the Embarked missing value\nPclass_embarked_mis = df[df['Embarked'].isnull()]['Pclass'].values\nprint(Pclass_embarked_mis)\nprint(\"----------------------------\")\n# distribution in Embarked by their Pclass\nprint(df[df['Pclass'] == 1]['Embarked'].value_counts())\n\n# for Pclass = 1, the most occurring values are S and C. \n# Therefore, one is filled as 'S', and the other as 'C'. \ndf.loc[61, 'Embarked'] = 'S'\ndf.loc[829, 'Embarked'] = 'C'\n\ndf.isnull().sum()","0c66db38":"mis_val_female = df[df['Sex'] == 'female']['Age'].isna().sum()\nmis_val_male = df[df['Sex'] == 'male']['Age'].isna().sum()\n\nprint(f'Missing values in Age for female: {mis_val_female}')\nprint(f'Missing values in Age for male: {mis_val_male}')","14d09a21":"def generate_random_numbers(df):\n    nan_count = df.isna().sum()\n    \n    ax = df.hist(bins=10, density=True, stacked=True, color='teal', alpha=0.6)\n    ax1 = df.plot(kind='kde', color='teal')\n    \n    # mean age\n    df_mean = df.mean(skipna=True)\n    # median age\n    df_median = df.median(skipna=True)\n    # std age\n    df_std = df.std(skipna=True)\n        \n    # getting density peak values\n    density = stats.gaussian_kde(df.dropna())\n    xs = np.linspace(df.min(),df.max(),200)\n    ys = density(xs)\n    index = np.argmax(ys)\n    max_y = ys[index]\n    max_x = xs[index]\n    \n    # density peak plot\n    ax.axvline(max_x, 0, 1, color='blue', label='peak(x): {:.2f}'.format(max_x))\n    \n    # once the median is closer than the mean to the x value of the density peak, median will be used.\n    # otherwise, the mean will be used to fill in the missing values by generated random numbers.\n    if abs(max_x - df_mean) <=  abs(max_x - df_median):\n        rand_numbers = np.random.randint(df_mean - df_std, df_mean + df_std, \n                                   size = nan_count)\n        ax.axvline(df_mean, 0, 1, color='red', label='mean: {:.2f}'.format(df_mean))\n        # we will generate random numbers using the mean {between (mean - std) & (mean + std)}\n        ax.axvspan(df_mean - df_std, df_mean + df_std, 0, 1, \n                   color='red', alpha=0.2, label='random numbers \\ninterval')\n    else: \n        rand_numbers = np.random.randint(df_median - df_std, df_median + df_std, \n                                   size = nan_count)\n        ax.axvline(df_median, 0, 1, color='red', label='median: {:.2f}'.format(df_median))\n        \n        # we will generate random numbers using the median {between (median - std) & (median + std)}\n        ax.axvspan(df_median - df_std, df_median + df_std, 0, 1, \n                   color='red', alpha=0.2, label='random numbers \\ninterval')\n\n    ax.set(xlabel='Age')\n    ax.legend(title='Female')\n    plt.show()\n\n    return (rand_numbers)","173500f8":"df_female_age = df[df['Sex'] == 'female']['Age']\ndf_female_rand_numbers = generate_random_numbers(df_female_age)\n\ndf_female_null = (df[df['Age'].isnull()]['Sex'] == 'female')\ndf_female_ind_null = df_female_null[df_female_null].index\ndf.loc[df_female_ind_null, 'Age'] = df_female_rand_numbers\n\nprint(df.isnull().sum())","e6fe469d":"df_male_age = df[df['Sex'] == 'male']['Age']\ndf_male_rand_numbers = generate_random_numbers(df_male_age)\n\ndf_male_null = (df[df['Age'].isnull()]['Sex'] == 'male')\ndf_male_ind_null = df_male_null[df_male_null].index\ndf.loc[df_male_ind_null, 'Age'] = df_male_rand_numbers\n\nprint(df.isnull().sum())","f3c1372e":"df_test_female_age = df_test[df_test['Sex'] == 'female']['Age']\ndf_test_female_rand_numbers = generate_random_numbers(df_test_female_age)\n\ndf_test_female_null = (df_test[df_test['Age'].isnull()]['Sex'] == 'female')\ndf_test_female_ind_null = df_test_female_null[df_test_female_null].index\ndf_test.loc[df_test_female_ind_null, 'Age'] = df_test_female_rand_numbers\n\nprint(df_test.isnull().sum())","f3fd3c1c":"df_test_male_age = df_test[df_test['Sex'] == 'male']['Age']\ndf_test_male_rand_numbers = generate_random_numbers(df_test_male_age)\n\ndf_test_male_null = (df_test[df_test['Age'].isnull()]['Sex'] == 'male')\ndf_test_male_ind_null = df_test_male_null[df_test_male_null].index\ndf_test.loc[df_test_male_ind_null, 'Age'] = df_test_male_rand_numbers\n\nprint(df_test.isnull().sum())","4b80a6b4":"## Create categorical variable for traveling alone\ndf['TravelAlone']=np.where((df[\"SibSp\"]+df[\"Parch\"])>0, 0, 1)\ndf.drop('SibSp', axis=1, inplace=True)\ndf.drop('Parch', axis=1, inplace=True)\ndf.head()","ab11f601":"## Create categorical variable for traveling alone\ndf_test['TravelAlone']=np.where((df_test[\"SibSp\"]+df_test[\"Parch\"])>0, 0, 1)\ndf_test.drop('SibSp', axis=1, inplace=True)\ndf_test.drop('Parch', axis=1, inplace=True)\ndf_test.head()","cdd6955b":"col_order = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'TravelAlone', 'Survived']\ndf=df[col_order]\ndf.head()","2bb93972":"for col in df:\n    unique_vals = np.unique(df[col])\n    nr_values = len(unique_vals)\n    if nr_values < 10:\n        print(f'The number of values for feature {col} :{nr_values} -- {unique_vals}')\n    else:\n        print(f'The number of values for feature {col} :{nr_values}')","182086e5":"axes = pd.plotting.scatter_matrix(df, alpha=0.7, figsize=(14, 8), diagonal='kde')\nfor ax in axes.flatten():\n    ax.xaxis.label.set_rotation(0)\n    ax.yaxis.label.set_rotation(90)\n    ax.yaxis.label.set_ha('right')\n    ax.set_xticks(())\n    ax.set_yticks(())\n\nplt.gcf().subplots_adjust(wspace=0, hspace=0)\nplt.show()","24dcd6f7":"def subplot_graph(df, features):\n    f_count = len(features)\n    cols = f_count\n    if f_count < 4:\n        rows = 1\n    else:\n        rows = math.ceil(f_count\/3)\n        cols = 3\n                \n    # Set up the matplotlib figure\n    fig, axes = plt.subplots(rows, cols, figsize=(16, 4*rows))\n        \n    sub_rows = 0\n    sub_cols = 0    \n    for f in features:\n        if sub_cols > cols-1:\n            sub_cols = 0\n            sub_rows += 1\n            \n        unique_vals = len(np.unique(df[f]))\n        if unique_vals < 10:\n            df_sub = df.groupby(df.columns[-1])[f].value_counts().sort_index(level=0)\n            x_no = list(df_sub.xs(0, level=0).index)\n            y_no = list(df_sub.xs(0, level=0).values)\n            x_yes = list(df_sub.xs(1, level=0).index)\n            y_yes = list(df_sub.xs(1, level=0).values)\n            list_no = list(zip(x_no, y_no))\n            list_yes = list(zip(x_yes, y_yes))\n            df_no = pd.DataFrame(list_no)\n            df_yes = pd.DataFrame(list_yes)\n\n            df_plot = pd.merge(df_no, df_yes, on=df_no.columns[0], how=\"outer\")\n            df_plot = df_plot.fillna(0)\n            try:\n                axes[sub_cols].bar(df_plot.iloc[:,0], df_plot.iloc[:,1], label='0', color='firebrick')\n                axes[sub_cols].bar(df_plot.iloc[:,0], df_plot.iloc[:,2], bottom=df_plot.iloc[:,1], \n                                   label='1', color='steelblue')\n                axes[sub_cols].legend(title=df.columns[-1])\n                axes[sub_cols].set_title(f)\n                axes[sub_cols].set_xticks(df[f].unique().tolist(), minor=False)\n            except:\n                axes[sub_rows, sub_cols].bar(df_plot.iloc[:,0], df_plot.iloc[:,1], label='0', color='firebrick')\n                axes[sub_rows, sub_cols].bar(df_plot.iloc[:,0], df_plot.iloc[:,2], bottom=df_plot.iloc[:,1], \n                                             label='1', color='steelblue')  \n                axes[sub_rows, sub_cols].legend(title=df.columns[-1])\n                axes[sub_rows, sub_cols].set_title(f)\n                axes[sub_rows, sub_cols].set_xticks(df[f].unique().tolist(), minor=False)\n        else:    \n            df_3 = df[df[df.columns[-1]] == 0][f]\n            df_4 = df[df[df.columns[-1]] == 1][f]\n            try:\n                df_3.plot(ax=axes[sub_cols], kind='kde', color='red', label='0')\n                df_4.plot(ax=axes[sub_cols], kind='kde', color='steelblue', label='1')\n                axes[sub_cols].hist(df_3, bins=20, density=True, stacked=True, color='lightgreen')\n                axes[sub_cols].set_title(f)\n                axes[sub_cols].legend(title=df.columns[-1])\n                axes[sub_cols].set_xlim(xmin=0)\n            except:\n                df_3.plot(ax=axes[sub_rows, sub_cols], kind='kde', color='red', label='0')\n                df_4.plot(ax=axes[sub_rows, sub_cols], kind='kde', color='steelblue', label='1')\n                axes[sub_rows, sub_cols].hist(df_3, bins=20, density=True, stacked=True, color='lightgreen')\n                axes[sub_rows, sub_cols].set_title(f)\n                axes[sub_rows, sub_cols].legend(title=df.columns[-1])\n                axes[sub_rows, sub_cols].set_xlim(xmin=-5, xmax=100)\n            \n        sub_cols += 1\n        \n    fig.tight_layout()\n    fig.show()\n\nfeatures = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'TravelAlone']\nsubplot_graph(df, features)","9d8f34e6":"f_count = len(features)\ncols = f_count\nif f_count < 4:\n    rows = 1\nelse:\n    rows = math.ceil(f_count\/3)\n    cols = 3\n    \n# Set up the matplotlib figure\nfig, axes = plt.subplots(rows, cols, figsize=(16, 4*rows))\n\n\nsub_rows = 0\nsub_cols = 0    \nfor f in features:\n    if sub_cols > cols-1:\n        sub_cols = 0\n        sub_rows += 1\n        \n    \n    unique_vals = len(np.unique(df[f]))\n    if unique_vals < 10:\n        df_yy = pd.DataFrame()\n        df1 = df.groupby([f])[df.columns[-1]].value_counts().sort_index(level=0)\n        df_uniq = df1.index.levels[0]\n        f_uniqs = []\n        sur_rates = []\n        for uniq in df_uniq:\n            try:\n                df2 = df1.xs(uniq, level=0)\n                df_sum = df2.sum()\n                df_lev_1_each = df2[1]\n                sur_rate = round((df_lev_1_each \/ df_sum)*100, 1)\n                sur_rates.append(sur_rate)\n                f_uniqs.append(uniq)\n            except:\n                pass\n\n        df_xx = pd.DataFrame(sur_rates, index=f_uniqs, columns=[f])\n        df_yy = pd.concat([df_yy, df_xx], axis=1)\n        \n        df_sub = df_yy[f].dropna()\n#         print(df_sub)\n\n        try:\n            axes[sub_cols].bar(df_sub.index, df_sub.values, color='steelblue')\n            axes[sub_cols].set(ylabel=\"Survived (%)\")\n            axes[sub_cols].set_title(f)\n            axes[sub_cols].set_xticks(df[f].unique().tolist(), minor=False)\n        except:\n            axes[sub_rows, sub_cols].bar(df_sub.index, df_sub.values, color='steelblue')\n            axes[sub_rows, sub_cols].set(ylabel=\"Survived (%)\")\n            axes[sub_rows, sub_cols].set_title(f)\n            axes[sub_rows, sub_cols].set_xticks(df[f].unique().tolist(), minor=False)\n    else:    \n        df_3 = df[df[df.columns[-1]] == 0][f]\n        df_4 = df[df[df.columns[-1]] == 1][f]\n        try:\n            df_3.plot(ax=axes[sub_cols], kind='kde', color='red', label='0')\n            df_4.plot(ax=axes[sub_cols], kind='kde', color='steelblue', label='1')\n            axes[sub_cols].hist(df_3, bins=20, density=True, stacked=True, color='lightgreen')\n            axes[sub_cols].set_title(f)\n            axes[sub_cols].legend(title=df.columns[-1])\n            axes[sub_cols].set_xlim(xmin=0)\n        except:\n            df_3.plot(ax=axes[sub_rows, sub_cols], kind='kde', color='red', label='0')\n            df_4.plot(ax=axes[sub_rows, sub_cols], kind='kde', color='steelblue', label='1')\n            axes[sub_rows, sub_cols].hist(df_3, bins=20, density=True, stacked=True, color='lightgreen')\n            axes[sub_rows, sub_cols].set_title(f)\n            axes[sub_rows, sub_cols].legend(title=df.columns[-1])\n            axes[sub_rows, sub_cols].set_xlim(xmin=-5, xmax=100)\n        \n    sub_cols += 1\n    \nfig.tight_layout()\nfig.show()","00d3d1f0":"def create_heatmap(hm, figsize=(7, 6)):\n    fig, ax = plt.subplots(figsize=figsize)\n\n    im = ax.imshow(hm, \n    #                vmin=0, vmax=10, \n                   cmap='viridis', aspect='auto')\n\n    # Create colorbar\n    cbar = ax.figure.colorbar(im, ax=ax)\n\n    # We want to show all ticks...\n    ax.set_xticks(np.arange(len(hm.columns)))\n    ax.set_yticks(np.arange(len(hm.columns)))\n    # ... and label them with the respective list entries\n    ax.set_xticklabels(hm.columns)\n    ax.set_yticklabels(hm.columns)\n\n    # Turn spines off and create white grid.\n    ax.spines[:].set_visible(False)\n    ax.set_xticks(np.arange(hm.shape[1]+1)-.5, minor=True)\n    ax.set_yticks(np.arange(hm.shape[0]+1)-.5, minor=True)\n    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n    ax.tick_params(which=\"minor\", bottom=False, left=False)\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    for i in range(len(hm.columns)):\n        for j in range(len(hm.columns)):\n            hm_val = round(hm.values[i, j], 2)\n            if hm_val > 0.85:\n                text = ax.text(j, i, hm_val,\n                               ha=\"center\", va=\"center\", color=\"black\", size=16)\n            else:\n                text = ax.text(j, i, hm_val,\n                               ha=\"center\", va=\"center\", color=\"w\", size=16)\n\n    fig.tight_layout()\n    plt.show()","a03c4844":"hm = df.corr()\ncreate_heatmap(hm)","ce6767c1":"X = df[df.columns[:-1]]\ny = df[df.columns[-1]]\nX_kaggle = df_test.copy()\nprint(X.shape)\nprint(X_kaggle.shape)","a195d8d7":"# There is no categorical columns in the dataframe\ncategorical_cols = list(set(X.columns) - set(X._get_numeric_data().columns))\ncategorical_cols","d7c49e11":"numerical_cols = list(X._get_numeric_data().columns)\nnumerical_cols","c0bf30cd":"categorical_cols + ['Pclass', 'TravelAlone']","e69af6b2":"from sklearn.compose import make_column_transformer\n\ncolumn_trans = make_column_transformer((OneHotEncoder(), categorical_cols + ['Pclass', 'TravelAlone']),\n                                      remainder='passthrough')\nX_trans = column_trans.fit_transform(X)\nX_kaggle_trans = column_trans.fit_transform(X_kaggle)\n\ncols = []\nfor i in column_trans.transformers_[0][2]:\n    cols_enc = sorted(X[i].unique())\n    for col_enc in cols_enc:\n        col_name = str(i) + '_' + str(col_enc)\n        cols.append(col_name)\n        \nfor i in column_trans.transformers_[1][2]:\n    col_name = X.columns[i]\n    cols.append(col_name)\n    \nX_encoded = pd.DataFrame(X_trans, columns=cols)\nX_kaggle_encoded = pd.DataFrame(X_kaggle_trans, columns=cols)\nprint(X_encoded.shape)\nprint(X_kaggle_encoded.shape)\nX_encoded.head()","83a13af1":"X_kaggle_encoded.head()","1e0825ce":"### It will zero variance features\nfrom sklearn.feature_selection import VarianceThreshold\nvar_thres=VarianceThreshold(threshold=0)\nvar_thres.fit(X_encoded)\nX_encoded.columns[var_thres.get_support()]\n\nconstant_columns = [column for column in X_encoded.columns\n                    if column not in X_encoded.columns[var_thres.get_support()]]\n\nprint(len(constant_columns))","4438f913":"X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, train_size = 0.8, test_size=0.2, random_state=42)\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","1af11a8d":"hm_X_train = X_train.corr()\ncreate_heatmap(hm_X_train, figsize=(10, 6))","63df4998":"#  to select highly correlated features\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold:\n                \n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","16355c31":"corr_features = correlation(X_train, 0.85)\ncorr_features","8ae2c157":"X_train = X_train.drop(corr_features, axis=1)\nX_test = X_test.drop(corr_features, axis=1)\nX_kaggle_encoded = X_kaggle_encoded.drop(corr_features, axis=1)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(X_kaggle_encoded.shape)","01504861":"# determine the mutual information\nmutual_info = mutual_info_classif(X_train.values, y_train)\nmutual_info = pd.Series(mutual_info)\nmutual_info.index = X_train.columns\nmutual_info.sort_values(ascending=False)","dff31ce7":"X_train = X_train.drop(['Embarked_C', 'Embarked_Q'], axis=1)\nX_test = X_test.drop(['Embarked_C', 'Embarked_Q'], axis=1)\nX_kaggle_encoded = X_kaggle_encoded.drop(['Embarked_C', 'Embarked_Q'], axis=1)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(X_kaggle_encoded.shape)","2af11485":"def model_evaluation(cols):\n    rf = RandomForestClassifier(n_estimators=100, random_state=0, criterion = 'entropy')  \n    rf.fit(X_train[cols], y_train)\n    \n    y_pred_train = rf.predict(X_train[cols])\n    y_pred_test = rf.predict(X_test[cols])\n\n#     print(\"Test Accuracy: {:.1%}\".format(rf.score(X_test[cols],y_test)))\n#     print(\"mean_squared_error: {:.3}\".format(mean_squared_error(y_test, y_pred_test)))\n#     print(\"r2_score: {:.3}\".format(r2_score(y_test, y_pred_test)))\n#     print('='*10)\n\n    rf_acc = rf.score(X_test[cols],y_test)\n    rf_mse = mean_squared_error(y_test, y_pred_test)\n    rf_r2 = r2_score(y_test, y_pred_test)\n    \n    return rf_acc, rf_mse, rf_r2","3ac6e233":"import itertools\n\nstuff = X_train.columns\n\nbest_cols = []\nbest_acc = []\nbest_mse = []\nbest_r2 = []\nfor L in range(0, len(X_train)+1):\n    for subset in itertools.combinations(stuff, L):\n        sub_list = list(subset)\n        if len(sub_list) > 4:\n            rf_acc, rf_mse, rf_r2 = model_evaluation(sub_list)\n            best_cols.append(sub_list)\n            best_acc.append(rf_acc)\n            best_mse.append(rf_mse)\n            best_r2.append(rf_r2)","547d7417":"best_zip = zip(best_cols, best_acc, best_mse, best_r2)\ndf_best_acc = pd.DataFrame(best_zip, columns=['columns', 'accuracy', 'mse', 'r2'])\ndf_best_acc = df_best_acc.sort_values('accuracy', ascending = False).reset_index(drop=True)\npd.set_option('max_colwidth', -1)\ndf_best_acc.head(10)","a36a0688":"# cols_final = ['Sex_female', 'Embarked_S', 'Pclass_2', 'Pclass_3', 'TravelAlone_0', 'Fare']\ncols_final = ['Embarked_S', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'TravelAlone_0', 'Age', 'Fare']\nX_train_final = X_train[cols_final]\nX_test_final = X_test[cols_final]\nX_kaggle_final = X_kaggle_encoded[cols_final]\n\nprint(X_train_final.shape)\nprint(X_test_final.shape)\nprint(X_kaggle_final.shape)","8e616165":"rf = RandomForestClassifier(n_estimators=100, random_state=0, criterion = 'entropy')    \nrf.fit(X_train_final, y_train)\ny_pred_train = rf.predict(X_train_final)\npred_proba_train = rf.predict_proba(X_train_final)\n\nprint(\"Train Accuracy: {:.1%}\".format(rf.score(X_train_final,y_train)))\nprint(\"mean_squared_error: {:.3}\".format(mean_squared_error(y_train, y_pred_train)))\nprint(\"r2_score: {:.3}\".format(r2_score(y_train, y_pred_train)))\nprint('='*10)\nprint('confusion_matrix')\nprint(confusion_matrix(y_train, y_pred_train))\nprint('='*10)\nprint(classification_report(y_train, y_pred_train))","9747edcb":"y_pred_test = rf.predict(X_test_final)\npred_proba_test = rf.predict_proba(X_test_final)\n\nprint(\"Test Accuracy: {:.1%}\".format(rf.score(X_test_final,y_test)))\nprint(\"mean_squared_error: {:.3}\".format(mean_squared_error(y_test, y_pred_test)))\nprint(\"r2_score: {:.3}\".format(r2_score(y_test, y_pred_test)))\nprint('='*10)\nprint('confusion_matrix')\nprint(confusion_matrix(y_test, y_pred_test))\nprint('='*10)\nprint(classification_report(y_test, y_pred_test))","d08dadb8":"def confusion_matrix_func(cm, cm_title):\n    fig, ax = plt.subplots(figsize=(4, 4))\n\n    # Plot the heatmap\n    im = ax.imshow(cm, interpolation='nearest', cmap='Reds', aspect='auto')\n\n    # We want to show all ticks...\n    ax.set_xticks(np.arange(len(cm.tolist())))\n    ax.set_yticks(np.arange(len(cm.tolist())))\n\n\n    thresh = cm.max() \/ 1.5\n    # Loop over data dimensions and create text annotations.\n    for i in range(len(cm.tolist())):\n        for j in range(len(cm.tolist())):\n            text = ax.text(j, i, cm.tolist()[i][j],\n                           ha=\"center\", va=\"center\", size=16,\n                           color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    # Let the horizontal axes labeling appear on top.\n    ax.xaxis.set_ticks_position('top')\n    ax.xaxis.set_label_position('top')\n\n    plt.xlabel('Actual value', size=16)\n    plt.ylabel('Predicted value', size=16)\n    plt.title(cm_title, size=20, x=0.2, y=1.2)\n    plt.show()\n# https:\/\/matplotlib.org\/stable\/gallery\/images_contours_and_fields\/image_annotated_heatmap.html","1356873d":"def Confusion_matrix_metrics(TP, FP, FN, TN):\n    # Sensitivity, hit rate, recall, or true positive rate\n    TPR = TP \/ (TP + FN)\n    print('The True Positive Rate is: {:.2%}'.format(TPR))\n    # Specificity, selectivity or true negative rate (TNR)\n    TNR = TN \/ (TN + FP)\n    print('The True Negative Rate is: {:.2%}'.format(TNR))\n    print('='*10)\n\n    # accuracy (ACC)\n    ACC = (TP + TN) \/ (TP + TN + FP + FN)\n    print('The Accuracy is: {:.2%}'.format(ACC))\n    # balanced accuracy (BA)\n    BA = (TPR + TNR) \/ 2\n    print('The Balanced Accuracy is: {:.2%}'.format(BA))\n    print('='*10)\n\n    # Precision or positive predictive value\n    PPV = TP \/ (TP + FP)\n    print('The Precision is: {:.2%}'.format(PPV))\n    # negative predictive value (NPV)\n    NPV = TN \/ (TN + FN)\n    print('The Negative Predictive Value is: {:.2%}'.format(NPV))\n    # false discovery rate (FDR)\n    FDR = 1 - PPV\n    print('The False Discovery Rate is: {:.2%}'.format(FDR))\n    # false omission rate (FOR)\n    FOR = 1 - NPV\n    print('The False Omission Rate is: {:.2%}'.format(FOR))\n    print('='*10)\n\n    # prevalence threshold (PT)\n    PT = (math.sqrt(TPR*(1 - TNR)) + TNR - 1)\/(TPR + TNR - 1)\n    print('The Prevalence Threshold is: {:.2}'.format(PT))\n    # F1 score\n    F1 = 2*TP \/ (2*TP + FP + FN)\n    print('The F1 Score is: {:.2}'.format(F1))\n    # Matthews correlation coefficient (MCC) or phi coefficient\n    MCC = ((TP*TN) - (FP*FN)) \/ math.sqrt((TP + FP)*(TP + FN)*(TN + FP)*(TN + FN))\n    print('The Matthews Correlation Coefficient is: {:.2}'.format(MCC))\n    print('='*10)\n\n    # False positive rate or False alarm rate\n    FPR = FP \/ (FP + TN)\n    print('The False positive rate is: {:.2}'.format(FPR))\n    # False negative rate or Miss Rate\n    FNR = FN \/ (FN + TP)\n    print('The False Negative Rate is: {:.2%}'.format(FNR))","7bc7acb1":"cm = confusion_matrix(y_train, y_pred_train).T\nconfusion_matrix_func(cm, cm_title=\"Confusion Matrix_Train\")","f9a7b50a":"# Calculating False Positives (FP), False Negatives (FN), True Positives (TP) & True Negatives (TN)\nTP, FP, FN, TN = cm.ravel()\nConfusion_matrix_metrics(TP, FP, FN, TN)","a4f321ac":"cm_test = confusion_matrix(y_test, y_pred_test).T\nconfusion_matrix_func(cm_test, cm_title=\"Confusion Matrix_Test\")","1b9d186b":"# Calculating False Positives (FP), False Negatives (FN), True Positives (TP) & True Negatives (TN)\nTP, FP, FN, TN = cm_test.ravel()\nConfusion_matrix_metrics(TP, FP, FN, TN)","491cfec5":"# TRAIN\n# calculate scores\nlr_auc = roc_auc_score(y_train, pred_proba_train[:, 1])\n# summarize scores\n# print('Logistic: ROC AUC=%.3f' % (lr_auc))\n# calculate roc curves\nlr_fpr, lr_tpr, thresholds = roc_curve(y_train, pred_proba_train[:, 1])\n\n# Evaluating model performance at various thresholds\ndf_roc = pd.DataFrame({\n    'False Positive Rate': lr_fpr,\n    'True Positive Rate': lr_tpr\n}, index=thresholds)\ndf_roc.index.name = \"Thresholds\"\ndf_roc.columns.name = \"Rate\"\n\n\n# TEST\n# calculate scores\nlr_auc_test = roc_auc_score(y_test, pred_proba_test[:, 1])\n# summarize scores\n# print('Logistic: ROC AUC=%.3f' % (lr_auc_test))\n# calculate roc curves\nlr_fpr_test, lr_tpr_test, thresholds_test = roc_curve(y_test, pred_proba_test[:, 1])\n\n# Evaluating model performance at various thresholds\ndf_roc_test = pd.DataFrame({\n    'False Positive Rate': lr_fpr_test,\n    'True Positive Rate': lr_tpr_test\n}, index=thresholds_test)\ndf_roc_test.index.name = \"Thresholds\"\ndf_roc_test.columns.name = \"Rate\"\n\n\n# GRAPH\n# Set up the matplotlib figure\nfig, axes = plt.subplots(2, 2, figsize=(14, 8))\n\n# row=0, col=0\naxes[0, 0].plot(df_roc.iloc[:,0], df_roc.iloc[:,1], color='red', linewidth=2, \n                label=f'AUC={lr_auc:.2f}')\naxes[0, 0].fill_between(df_roc.iloc[:,0], df_roc.iloc[:,1], 0, color='red', alpha=0.3)\naxes[0, 0].plot(df_roc_test.iloc[:,0], df_roc_test.iloc[:,1], color='black', linewidth=2, \n                label=f'AUC_test={lr_auc_test:.2f}')\naxes[0, 0].plot([0, 1], [0, 1], color='green', linestyle='--', linewidth=1,\n                label='No Skill')\n\n# index of the first threshold for which the sensibility > 0.90\nidx = np.min(np.where(lr_tpr > 0.90))\naxes[0, 0].plot([0,lr_fpr[idx]], [lr_tpr[idx],lr_tpr[idx]], 'k--', color='blue')\naxes[0, 0].plot([lr_fpr[idx],lr_fpr[idx]], [0,lr_tpr[idx]], 'k--', color='blue')\n# Annotation\naxes[0, 0].annotate('(%.2f, %.2f)'%(lr_fpr[idx], lr_tpr[idx]),\n            (lr_fpr[idx], lr_tpr[idx]), \n            xytext =(-2 * 50, -30),\n            textcoords ='offset points',\n            bbox = dict(boxstyle =\"round\", fc =\"0.8\"), \n            arrowprops = dict(arrowstyle = \"->\"))\n\naxes[0, 0].set_xlabel('False Positive Rate', size=12)\naxes[0, 0].set_ylabel('True Positive Rate (recall)', size=12)\naxes[0, 0].legend(title='kNN')\naxes[0, 0].set_title('ROC curve', color='red', size=14)\n\n# row=0, col=1\naxes[0, 1].plot(df_roc.index[1:], df_roc[\"True Positive Rate\"][1:], color='blue', linewidth=2, \n                label='TPR')\naxes[0, 1].plot(df_roc_test.index[1:], df_roc_test[\"True Positive Rate\"][1:], color='black', linewidth=2, \n                label='TPR_test')\naxes[0, 1].plot(df_roc.index[1:], df_roc[\"False Positive Rate\"][1:], color='orange', linewidth=2, \n                label='FPR')\naxes[0, 1].plot(df_roc_test.index[1:], df_roc_test[\"False Positive Rate\"][1:], color='black', linewidth=2, \n                label='FPR_test')\n\naxes[0, 1].set_xlabel('Threshold', size=12)\naxes[0, 1].legend()\naxes[0, 1].set_title('TPR and FPR at every threshold', color='red', size=14)\n\n# row=1, col=0\nprecision, recall, thresholds = precision_recall_curve(y_test, pred_proba_test[:, 1])\n\naxes[1, 0].plot(recall, precision, color='green', linewidth=2, \n                label=f'PR_Curve (AUC={auc(lr_fpr, lr_tpr):.2f})')\naxes[1, 0].fill_between(recall, precision, 0, color='green', alpha=0.3)\n\naxes[1, 0].set_xlabel('Recall', size=12)\naxes[1, 0].set_ylabel('Precision', size=12)\naxes[1, 0].legend()\naxes[1, 0].set_title('Precision-Recall Curve', color='red', size=14)\n\nfig.tight_layout()\nfig.show()","309974ff":"# Running Log loss on training\nprint('The Log Loss on Training is: {:.2}'.format(log_loss(y_train, pred_proba_train[:, 1])))\n\n# Running Log loss on testing\nprint('The Log Loss on Testing Dataset is: {:.2}'.format(log_loss(y_test, pred_proba_test[:, 1])))","05b21e4b":"df_gender_sub.head()","c99f3b4a":"submission = df_gender_sub.drop('Survived', axis=1)\ny_pred_kaggle = rf.predict(X_kaggle_final)\nsubmission['Survived'] = y_pred_kaggle\nsubmission.head()","66fe3502":"# Are our test and submission dataframes the same length?\nif len(submission) == len(df_gender_sub):\n    print(\"Submission dataframe is the same length as test ({} rows).\".format(len(submission)))\nelse:\n    print(\"Dataframes mismatched, won't be able to submit to Kaggle.\")","22597bda":"# Convert submisison dataframe to csv for submission to csv for Kaggle submisison\nsubmission.to_csv('titanic_submission_rf.csv', index=False)\nprint('Submission CSV is ready!')","1b1ba19d":"# Check the submission csv to make sure it's in the right format\nsubmissions_check = pd.read_csv(\"titanic_submission_rf.csv\")\nsubmissions_check.head()","fd65b974":"<a id='35'><\/a>\n### 3_5 Exploratory Data Analysis","fe2c08fd":"<a id='31'><\/a>\n### 3_1 Missing Values","8d344b9e":"Mutual information (MI) measures the dependency between the variables. Higher values mean higher dependency.","7e38dee8":"<a id='462'><\/a>\n#### 4_6_2 Feature_Selection - Mutual information","0368ad71":"<a id='41'><\/a>\n### 4_1 Separate the dataset","fec7affa":"pclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\n\nSibling = brother, sister, stepbrother, stepsister\n\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\n\nParent = mother, father\n\nChild = daughter, son, stepdaughter, stepson\n\nSome children travelled only with a nanny, therefore parch=0 for them.","f9ec01c4":"##### 3_1_3_2 Missing values in test data","b099cce3":"<a id='461'><\/a>\n#### 4_6_1 Feature Selection - Drop Features Using Pearson Correlation","dd929fb8":"<a id='44'><\/a>\n### 4_4 Check zero variance features","0b9d2322":"<a id='4'><\/a>\n## 4 Regressions and Results","149bc052":"<a id='464'><\/a>\n#### 4_6_4 Feature_Selection - Final","409d5d0e":"{'Sex_male'} {'Sex_female'} and {'TravelAlone_0'} {'TravelAlone_1'} are highly correlated features in the dataset.","716b24de":"<a id='33'><\/a>\n### 3_3 Move the dependent column to the end","50231477":"<a id='410'><\/a>\n### 4.10. Logarithmic loss","7567629c":"<a id='312'><\/a>\n#### 3_1_2 Embarked - Missing Values","0ef74bc3":"<a id='46'><\/a>\n### 4_6 Feature Selection","f0fb9ef9":"<a id='321'><\/a>\n#### 3_1_1 Fare - Missing Values","c12f6f7a":"<a id='49'><\/a>\n### 4.9. roc curve and auc","47470890":"##### 3_1_1_1 Missing values in test data","d574d6fd":"## Index\n\n[1 Importing packages](#1)<br>\n[2 Read CSV train\/test files into DataFrame](#2)<br>\n[3 Data Preprocessing](#3)<br>\n    <ul>\n        <li>[3_1 Missing Values](#31)<\/li>\n            <ul><li>[3_1_1 Fare - Missing Values](#311)<\/li>\n            <li>[3_1_2 Embarked - Missing Values](#12)<\/li>\n            <li>[3_1_3 Age - Missing Values](#313)<\/li><\/ul>\n        <li>[3_2 Combine SibSp and Parch for Simplicity](#32)<\/li>\n        <li>[3_3 Move the dependent column to the end](#33)<\/li>\n        <li>[3_4 Investigate all the elements within each Feature](#34)<\/li>\n        <li>[3_5 Exploratory Data Analysis](#35)<\/li>\n    <\/ul>\n[4 Regressions and Results](#4)<br>\n    <ul>\n        <li>[4_1 Separate the dataset](#41)<\/li>\n        <li>[4_2 Check categorical columns](#42)<\/li>\n        <li>[4_3 One_Hot_Encoding](#43)<\/li>\n        <li>[4_4 Check zero variance features](#44)<\/li>\n        <li>[4_5 Separate the dataset into train and test](#45)<\/li>\n        <li>[4_6 Feature Selection](#46)<\/li>\n            <ul><li>[4_6_1 Feature Selection - Drop Features Using Pearson Correlation](#461)<\/li>\n            <li>[4_6_2 Feature_Selection - Mutual information](#462)<\/li>\n            <li>[4_6_3 Feature_Selection - Feature_Selection - Evaluation of all column combinations](#463)<\/li>\n            <li>[4_6_4 Feature_Selection - Final](#464)<\/li><\/ul>\n        <li>[4_7 Evaluating The Decision Trees Model](#47)<\/li>\n        <li>[4_8 Confusion matrix](#48)<\/li>\n        <li>[4_9 roc curve and auc](#49)<\/li>\n        <li>[4_10 Logarithmic loss](#410)<\/li>\n[5. Submission](#5)<br>","fbba8717":"<a id='2'><\/a>\n## 2 Read CSV train\/test files into DataFrame","a321a77f":"<a id='47'><\/a>\n### 4.7. Evaluating The Decision Trees Model","d1221aa9":"<a id='313'><\/a>\n#### 3_1_3 Age - Missing Values","b8dc0951":"<a id='463'><\/a>\n#### 4_6_3 Feature_Selection - Evaluation of all column combinations","1eab56c9":"<a id='34'><\/a>\n### 3_4 Investigate all the elements within each Feature ","10369e3e":"<a id='5'><\/a>\n## 5. Submission","652287b6":"#### 3_2_2 Test data","077a3066":"PLife -> Life is unfortunately not fair. If you're a 3rd class person in this world, it's a chance for you to survive.\n\nSex -> Positive discrimination has been made against women here as well.\n\nAge -> The age distribution for survivors and deceased is very similar except for children. The travelers strived for the survival of the children.\n\nFare -> Passengers who pay lower fare seem less likely to survive.\n\nEmbarked -> Although the number of boarders in Southhampton is higher than those in Cherbourg, the survival rate of Cherbourg is higher. This is probably related to the socioeconomic situation.","dc7d2480":"##### 3_1_2_1 Missing values in train data","c6cb2b1b":"<a id='42'><\/a>\n### 4_2 Check categorical columns","3b663d5f":"<a id='32'><\/a>\n### 3_2 Combine SibSp and Parch for Simplicity","93f04aaa":"**The goal** is to predict the target variable(Survived) using logistic regression.","09da9fd8":"#### 3_2_1 Train data","dd8b57d8":"<a id='43'><\/a>\n### 4_3 One-Hot Encoding","6feb6940":"<a id='48'><\/a>\n### 4.8. Confusion matrix","4277e100":"<a id='45'><\/a>\n### 4_5 Separate the dataset into train and test","3c793447":"![titanic_data_dict.png](attachment:titanic_data_dict.png)","ee1396af":"<a id='3'><\/a>\n## 3 Data Preprocessing","aa77f89b":"##### 3_1_3_1 Missing values in train data","95dd15f7":"<a id='1'><\/a>\n## 1 Importing packages"}}