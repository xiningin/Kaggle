{"cell_type":{"3b58f4bd":"code","9fa1a22b":"code","8a55f827":"code","d7ac7ecc":"code","4e712cff":"code","3b2284bd":"code","7b1f687d":"code","6a8dd110":"code","7719382c":"code","e40202c7":"code","1064a4b7":"code","ddff0333":"code","bbd3160b":"markdown","bb373c3e":"markdown","8bab21f6":"markdown","decb564f":"markdown","a46e2f45":"markdown","e095f7d7":"markdown","95d81e19":"markdown","f05d1569":"markdown"},"source":{"3b58f4bd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nplt.style.use('seaborn-talk')","9fa1a22b":"dataset = pd.read_csv('\/kaggle\/input\/system-identification-of-an-electric-motor\/Dataset_Electric_Motor.csv')\ndataset.shape","8a55f827":"dataset.head()","d7ac7ecc":"fig, axes = plt.subplots(1,2,sharex=True, sharey=True)\nfor c, ax in zip(['n_k', 'n_1k'], axes.flatten()):\n    sns.countplot(x=c, data=dataset, palette=\"ch:.25\", ax=ax)\nunique_elem_vecs = dataset['n_k'].nunique()","4e712cff":"pairs = dataset.assign(pairs=lambda r: r.n_k.astype(str)+'->'+r.n_1k.astype(str))['pairs']\npairs.head()","3b2284bd":"print('Transition between elementary vectors count')\npairs.value_counts()","7b1f687d":"reduced_data = dataset.iloc[::1000, :]\nanalyzed_cols = [c for c in dataset if c != 'n_k']\nfig, axes = plt.subplots(nrows=unique_elem_vecs, ncols=len(analyzed_cols), sharex='col', figsize=(20, 20))\n\nfor k, df in reduced_data.groupby('n_k'):\n    for i, c in enumerate(analyzed_cols):\n        sns.distplot(df[c], ax=axes[k-1, i])\n        if i == 0:\n            axes[k-1, i].set_ylabel(f'n_k = {k}')\nplt.tight_layout()","6a8dd110":"reduced_data['epsilon_k'].describe()","7719382c":"dataset = dataset.assign(sin_eps_k=lambda df: np.sin(df.epsilon_k), \n                         cos_eps_k=lambda df: np.cos(df.epsilon_k),\n                         i_norm=lambda df: np.sqrt(df.id_k**2 + df.iq_k**2)).drop('epsilon_k', axis=1)\ndataset.head()","e40202c7":"corr = reduced_data.corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(250, 15, s=75, l=40,n=9, center=\"dark\", as_cmap=True)\n\nplt.figure(figsize=(14,14))\n_ = sns.heatmap(corr, mask=mask, cmap=cmap, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","1064a4b7":"df = dataset.iloc[::100, :]\\\n            .assign(**{**{f'n_k_{i}': lambda x: (x.n_k == i).astype(int) for i in range(1, 8)},\n                       **{f'n_1k_{i}': lambda x: (x.n_1k == i).astype(int) for i in range(1, 8)}})\\\n            .drop(['n_k', 'n_1k'], axis=1)\n\ntarget_cols = ['id_k1', 'iq_k1']\ninput_cols = [c for c in df if c not in target_cols]\ncv = KFold(shuffle=True, random_state=2020)\n\nss_y = StandardScaler().fit(df[target_cols])\ndf = pd.DataFrame(StandardScaler().fit_transform(df),\n                     columns=df.columns)  # actually methodically unsound, but data is large enough","ddff0333":"\nX, y = df[input_cols].values, df[target_cols].values\n\nscores = []\nfor train_idx, test_idx in cv.split(X, y):\n    ols = LinearRegression().fit(X[train_idx], y[train_idx])\n    pred = ols.predict(X[test_idx])\n    pred = ss_y.inverse_transform(pred)\n    gtruth = ss_y.inverse_transform(y[test_idx])\n    scores.append(mean_squared_error(pred, gtruth))\nscores = np.asarray(scores)\nprint('MSE:')\nprint(f'Scores Mean: {scores.mean():.4f} A\u00b2 +- {2*scores.std():.4f} A\u00b2\\nScores Min: {scores.min():.4f} A\u00b2, Scores Max: {scores.max():.4f} A\u00b2')","bbd3160b":"# Correlation Matrix","bb373c3e":"# Distribution\n\nAccording to [this introductory paper](https:\/\/arxiv.org\/pdf\/2003.07273.pdf), *id_k1* and *iq_k1* are to be treated as target features.\n\nAt the same time, they depend on elementary vectors label-encoded into integers.\n\nLet's analyze how they are distributed.","8bab21f6":"It becomes evident that certain transitions in the elementary vectors are more common than others.\n\nMoreover, depending on the current elementary vector, distribution of currents and rotor angle *epsilon_k* is either unimodal or bimodal distributed.\n\nMore subtle, we recognize a semi-sphere shape of the 2d histogram between the currents (remember, *d* and *q* currents are to be plotted perpendicular to each other).\n\nIt might be auspicious, to add another feature denoting the current vector norm *id^2 + iq^2*.\n\nOn another note, epsilon is the rotor angle, which by design has a value discontinuity in the extreme points over time.","decb564f":"Element vector with k = 1 appears significantly more often than the remaining element vector types.","a46e2f45":"We observe strong linear correlation between consecutive current measurements in *d\/q* coordinates each.\n\nAll other pair-wise comparisons are relatively uncorrelated.","e095f7d7":"Obviously, the value range is clipped to *[-$pi$, $pi$]*.\n\nAs many ML methods do not respond well to discontinuities in the input space with no corresponding effect on the target space, we replace epsilon by its sine and cosine.\n\n# Feature Engineering\n\nWe add sine and cosine of the rotor angle and the current vector norm.","95d81e19":"This is a rather weak estimation.\nCan you beat this score?","f05d1569":"# Linear Regression\n\nWe kick off regression with a linear model, as the correlation matrix suggests expedient estimation performance just from actual currents.\nSince elementary vectors are to be treated as categorical, we one-hot encode them before training.\n\nMoreover, in order to fit in RAM, we subsample the data."}}