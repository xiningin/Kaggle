{"cell_type":{"da930a1c":"code","ebd93660":"code","16709596":"code","37dfe46d":"code","824af1e9":"code","267151a6":"code","14e75b4a":"code","e5746f16":"code","c942a95b":"code","fb70de6a":"code","6d6e669f":"code","465b2a91":"code","4001e4a6":"code","3c1bb661":"code","4036554c":"code","53184e7d":"code","66510258":"code","9ddff1d0":"code","e20e0b13":"code","22808502":"code","e0bba811":"code","e322ec5f":"markdown","4d13cae6":"markdown","d9db5546":"markdown","570645de":"markdown","3d8eb10b":"markdown","eeddff4a":"markdown","e2cff038":"markdown","17de835a":"markdown","22bd6133":"markdown","78417839":"markdown","1d1626c6":"markdown","72350d57":"markdown","ccbe90ff":"markdown"},"source":{"da930a1c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","ebd93660":"input_dir = \"\/kaggle\/input\/tabular-playground-series-dec-2021\/\"\ntrain = pd.read_csv(input_dir+\"train.csv\", index_col='Id')\ntest = pd.read_csv(input_dir+\"test.csv\", index_col='Id')\nsub = pd.read_csv(input_dir+\"sample_submission.csv\")","16709596":"cont_cols = [\"Elevation\",\"Aspect\",\"Slope\",\"Horizontal_Distance_To_Hydrology\", \\\n                   \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Roadways\",\\\n                   \"Horizontal_Distance_To_Fire_Points\",\\\n                  \"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\"]\n\nbinary_cols = [f\"Wilderness_Area{i}\" for i in range(1,5)]+[f\"Soli_Type{i}\" for i in range(1,41)]\ntarget_col = \"Cover_Type\"","37dfe46d":"train[target_col].value_counts()","824af1e9":"row_5 = train[train[target_col]==5] \nfor i in range(20):\n    train = train.append( row_5, ignore_index=True)","267151a6":"train[target_col].value_counts()","14e75b4a":"train[target_col] = train[target_col]-1","e5746f16":"from sklearn.model_selection import train_test_split\ntrain, val, _, _ = train_test_split(train, train[target_col], test_size=0.1, stratify = train[target_col])","c942a95b":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ntrain[cont_cols] = scaler.fit_transform(train[cont_cols])\nval[cont_cols] = scaler.transform(val[cont_cols])\ntest[cont_cols] = scaler.transform(test[cont_cols])","fb70de6a":"all_cols = cont_cols+binary_cols\nn_classes = len(train[target_col].unique())","6d6e669f":"import time\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import optim\n\nclass ForestDataset(Dataset):\n    def __init__(self, csv):\n        if target_col in csv.columns:\n            self.X = csv.drop(columns=[target_col]).values\n            self.y = csv[target_col].values\n        else:\n            self.X = csv.values\n            csv[target_col] = 0\n            self.y = csv[target_col].values\n    def __len__(self):\n        return len(self.y)\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n    \ntrain_dataset = ForestDataset(train)\nval_dataset = ForestDataset(val)\n#test_dataset = ForestDataset(test)","465b2a91":"class MultiLayerPerceptron(nn.Module):\n    def __init__(self, len_fc1, len_fc2):\n        super().__init__()\n        self.fc1 = nn.Linear(len(all_cols), len_fc1)\n        self.act1 = nn.Tanh()\n        self.fc2 = nn.Linear(len_fc1, len_fc2)\n        self.act2 = nn.Tanh()\n        self.fc3 = nn.Linear(len_fc2, n_classes)\n        \n    def forward(self, x):\n        x = self.act1( self.fc1(x) )\n        x = self.act2( self.fc2(x) )\n        return self.fc3(x)","4001e4a6":"mlp_model = MultiLayerPerceptron(3*len(test.columns), 3*len(test.columns)).to('cuda')\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(mlp_model.parameters(), lr=1e-4)","3c1bb661":"def train_epoch(model,criterion,optimizer,dataset,epoch):\n    train_dataset=dataset\n    data_loader=DataLoader(dataset,batch_size=32,shuffle=True,num_workers=4)\n    dataset_size=len(dataset)\n    print(f\"Epoch#{epoch}. Train\")\n    start_time=time.time()\n    model.train()\n    running_loss=0.0 #\u043d\u0430\u043a\u043e\u043f\u043b\u0435\u043d\u0438\u0435 \u043b\u043e\u0441\u0441\u0430\n    running_acc=0.0\n    epoch_loss=0.0\n    \n    for inputs,labels in tqdm( data_loader):\n        inputs=inputs.to('cuda').type(torch.float)\n        labels=labels.to('cuda')#.type(torch.float) #\u043f\u0435\u0440\u0435\u0434\u0430\u0435\u043c \u0431\u0430\u0442\u0447 \u043d\u0430 GPU(cuda)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss=criterion(outputs,labels)\n        loss.backward() # \u043e\u0431\u0440\u0430\u0442\u043d\u043e\u0435 \u0440\u0430\u0441\u043f\u043e\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u0430\n        optimizer.step() # \u0448\u0430\u0433 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440\u0430\n        running_loss+=loss.item()*inputs.size(0)\n        \n        _,preds=torch.max(outputs,dim=1)\n        running_acc+= (torch.sum(preds == labels.data))\n    epoch_loss = running_loss \/ dataset_size\n    epoch_acc = running_acc \/ dataset_size\n    print(f'Loss (cross-entropy): { epoch_loss }')\n    print(f\"Accuracy (multiclass): { epoch_acc }\")\n    print(f\"Epoch#{epoch} (Train) completed. {round(time.time()-start_time,3)}s \")\n    return model, epoch_loss, epoch_acc","4036554c":"def valid_epoch(model,criterion,optimizer,dataset,epoch):\n    val_dataset=dataset\n    data_loader=DataLoader(dataset,batch_size=32,shuffle=True,num_workers=4)\n    dataset_size=len(val_dataset)\n    print(f\"Epoch#{epoch}. Validation\")\n    start_time=time.time()\n    model.eval()\n    running_loss=0.0 # \u043d\u0430\u043a\u043e\u043f\u043b\u0435\u043d\u0438\u0435 \u043b\u043e\u0441c\n    running_acc=0.0\n    epoch_loss=0.0\n    with torch.no_grad():\n        for inputs,labels in tqdm( data_loader):\n            inputs=inputs.to('cuda').type(torch.float)\n            labels=labels.to('cuda')#.type(torch.float) #\u043f\u0435\u0440\u0435\u0434\u0430\u0435\u043c \u0431\u0430\u0442\u0447 \u043d\u0430 GPU(cuda)\n            outputs = model(inputs)\n            loss=criterion(outputs,labels)\n            running_loss+=loss.item()*inputs.size(0)\n            _,preds=torch.max(outputs,dim=1)\n            running_acc+= (torch.sum(preds == labels.data))\n            \n    epoch_loss = running_loss \/ dataset_size\n    epoch_acc = running_acc \/ dataset_size\n    print(f'Loss (cross-entropy): { epoch_loss } ')\n    print(f\"Accuracy (multiclass): { epoch_acc }\")\n    print(f\"Epoch#{epoch} (Validation) completed. {round(time.time()-start_time,3)}s \")\n    return model, epoch_loss, epoch_acc","53184e7d":"best_model = mlp_model\nbest_epoch = 1\nbest_loss = 1000000\nbest_acc = 0\n#num_epochs=len(keys)*2\nnum_epochs = 10\n\ntrain_loss_history = []\nval_loss_history = []\n\ntrain_acc_history = []\nval_acc_history = []\n\nfor epoch in range(1,num_epochs+1):\n    #\u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0430\n    mlp_model, train_loss, train_acc = train_epoch(mlp_model,criterion,optimizer,train_dataset,epoch)\n    train_loss_history.append(train_loss)\n    train_acc_history.append(train_acc)\n    \n    mlp_model, val_loss, val_acc = valid_epoch(mlp_model,criterion,optimizer,val_dataset,epoch)\n    val_loss_history.append(val_loss)\n    val_acc_history.append(val_acc)\n    \n    #if(val_loss<best_loss):\n    if(val_acc>best_acc):\n        best_model = mlp_model\n        best_epoch = epoch","66510258":"#saving\noutput_model_file = 'best_model.bin'\ntorch.save(best_model, output_model_file)","9ddff1d0":"test_dataset = ForestDataset(test)\ntest_dataset","e20e0b13":"data_loader=DataLoader(test_dataset,batch_size=32,shuffle=False,num_workers=4)\ndataset_size=len(test_dataset)\nbest_model.eval()\n\npreds_list = []\nwith torch.no_grad():\n    for inputs,labels in tqdm( data_loader):\n        inputs=inputs.to('cuda').type(torch.float)\n        labels=labels.to('cuda')\n        outputs = best_model(inputs)\n        _,preds=torch.max(outputs,dim=1)\n        preds_list.append(preds)\ntorch.cat(preds_list)","22808502":"sub[\"Cover_Type\"] = torch.cat(preds_list).cpu().detach().numpy()\nsub[\"Cover_Type\"] = sub[\"Cover_Type\"]+1 #\u0432\u0435\u0440\u043d\u0443\u043b\u0438 \u043e\u0431\u0440\u0430\u0442\u043d\u043e \u043d\u043e\u043c\u0435\u0440\u0430 \u043a\u043b\u0430\u0441\u0441\u043e\u0432, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u0432 \u043d\u0430\u0447\u0430\u043b\u0435 \u0434\u0435\u043b\u0430\u043b\u0438 -1","e0bba811":"sub.to_csv(\"submission_mlp.csv\", index=False)","e322ec5f":"# \u041c\u043e\u0434\u0435\u043b\u044c, \u043b\u043e\u0441\u0441, \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440","4d13cae6":"# \u041a\u043b\u0430\u0441\u0441 \u043c\u043e\u0434\u0435\u043b\u0438 (\u043f\u043e\u043b\u043d\u043e\u0441\u0432\u044f\u0437\u043d\u0430\u044f \u0441\u0435\u0442\u044c)","d9db5546":"# Inference - \u0434\u043b\u044f kaggle \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u044f, \u0432\u044b\u0432\u043e\u0434 \u043d\u0430 test-\u0432\u044b\u0431\u043e\u0440\u043a\u0435","570645de":"# \u0444\u0443\u043d\u043a\u0446\u0438\u044f, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0441\u0447\u0438\u0442\u0430\u0435\u0442 1 \u044d\u043f\u043e\u0445\u0443 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438","3d8eb10b":"# \u0418\u0441\u043a\u0443\u0441\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u0434\u043e\u0431\u0430\u0432\u0438\u043b\u0438 20 \u0441\u0442\u0440\u043e\u0447\u0435\u043a \u043a\u043b\u0430\u0441\u0441\u0430 5","eeddff4a":"# train-loop \u043c\u043e\u0434\u0435\u043b\u0438 (\u0432\u044b\u0431\u043e\u0440 \u043b\u0443\u0447\u0448\u0435\u0439 \u0437\u0430 n \u044d\u043f\u043e\u0445 - \u043f\u043e accuracy \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438)","e2cff038":"# \u041d\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \u043d\u0435\u043f\u0440\u0435\u0440\u044b\u0432\u043d\u044b\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438","17de835a":"# \u0412\u044b\u0434\u0435\u043b\u0438\u043b\u0438 \u043d\u0435\u043f\u0440\u0435\u0440\u044b\u0432\u043d\u044b\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438, \u0431\u0438\u043d\u0430\u0440\u043d\u044b\u0435 \u0438 \u0446\u0435\u043b\u0435\u0432\u0443\u044e","22bd6133":"# \u041f\u043e\u0434\u0435\u043b\u0438\u043b\u0438 train\/val 0.9\/0.1","78417839":"# \u0421\u043e\u0445\u0440\u0430\u043d\u0438\u043b\u0438 \u043c\u043e\u0434\u0435\u043b\u044c \u0432 \u0444\u0430\u0439\u043b","1d1626c6":"# \u0444\u0443\u043d\u043a\u0446\u0438\u044f, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 1 \u044d\u043f\u043e\u0445\u0443 train","72350d57":"# \u041a\u043b\u0430\u0441\u0441 Dataset (\u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0441\u0442\u044c pytorch - \u043d\u0443\u0436\u043d\u043e \u043f\u0435\u0440\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0442\u044c \u043a\u043b\u0430\u0441\u0441 dataset)","ccbe90ff":"# \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445"}}