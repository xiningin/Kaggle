{"cell_type":{"baf3243a":"code","aa533bae":"code","5edca36a":"code","3e3ec4a1":"code","485527c4":"code","53ba1955":"code","f7840175":"code","9e75bad5":"code","423470ad":"code","9d671f8c":"code","f8fefb09":"code","37cb7b61":"code","bbc73f78":"code","cde0497b":"code","3297dbf5":"code","372eaffb":"code","a6bc6e62":"code","4fa57992":"code","1d826d89":"code","2cae0573":"code","096abc5c":"code","7e96fdd2":"code","3fcff2e4":"code","435da9ec":"code","47bdcc7d":"code","6e23ec18":"code","fdeb43f5":"code","f2cb5ab4":"code","5b729d26":"code","e53f0d86":"code","d11aec54":"code","f2136f47":"code","0281aa32":"code","c0799dd7":"markdown","33981b84":"markdown","fea3b0ad":"markdown","d3fc5afc":"markdown","08d03361":"markdown","ef0c2a8d":"markdown","3c776606":"markdown","8c9d4f0a":"markdown","9835cee5":"markdown","0a239e51":"markdown","52012399":"markdown","cbdfd6d5":"markdown","8c9d5c32":"markdown","a4d30bf9":"markdown","708bf621":"markdown","72171eb7":"markdown","d23e1c3f":"markdown","f4190112":"markdown","f4e5d23c":"markdown","993b76a2":"markdown","dc55b811":"markdown","58f114be":"markdown","b94b4045":"markdown","feac1425":"markdown"},"source":{"baf3243a":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","aa533bae":"# import all the required libraries\nimport pandas as pd\nimport numpy as np \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import metrics\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier","5edca36a":"# dataset courtesy Rishabh Misra & Kaggle\n# Link: https:\/\/www.kaggle.com\/rmisra\/news-category-dataset\ndata = pd.read_json(\"\/kaggle\/input\/news-category-dataset\/News_Category_Dataset_v2.json\", lines=True)","3e3ec4a1":"data.head()","485527c4":"# Let's see how many categories we have here\nprint(f\"Total unique categories are: {len(data['category'].value_counts())}\")\nprint(f\"Count of occurance of each category:\")\ndata['category'].value_counts()","53ba1955":"#check for Null Data\ndata.isnull().sum()","f7840175":"# Check of spaces in column headline - using enumerate\nspaces = []\nfor i, x in enumerate(data['headline']):\n    if type(x) == str:\n        if x.isspace():\n            spaces.append(i)\n        \nprint(len(spaces), 'spaces in index: ', spaces)","9e75bad5":"# Check of spaces in column short desc - using itertuples\nblanks = []  # start with an empty list\n\nfor i,cat,hl,au,l,sd,dt in data.itertuples():  # iterate over the DataFrame\n    if type(sd)==str:            # avoid NaN values\n        if sd.isspace():         # test 'review' for whitespace\n            blanks.append(i)     # add matching index numbers to the list\n        \nprint(len(blanks), 'blanks: ', blanks)","423470ad":"# Since the goal of this exercise if to identify category based on headline and short description, \n# we choose to merge them, as the vectorizer functions can't process multiple columns\nX = data['headline']+data['short_description']\ny = data['category']","9d671f8c":"X.head()","f8fefb09":"# Split the data into 70-30 i.e. test size of 30% to check the accuracy of the training\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=77)\n\n#Let's check the shape of the splitted data\nprint(f\"Training Data Shape: {X_train.shape}\")\nprint(f\"Testing Data Shape: {X_test.shape}\")","37cb7b61":"# Let's first try with Count Vectorizer from scikit learn\ncv = CountVectorizer()\n\nX_train_cv = cv.fit_transform(X_train)\nX_train_cv.shape","bbc73f78":"from sklearn.svm import LinearSVC\nclf = LinearSVC()\nclf.fit(X_train_cv,y_train)","cde0497b":"# Let's test it for the first 2 articles in the Test dataset\nX_test1 = X_test[0:2]\nprint(X_test1)","3297dbf5":"X_test1_cv = cv.transform(X_test1)\nclf.predict(X_test1_cv)","372eaffb":"# Transform the test data before predicting\nX_test_cv = cv.transform(X_test)","a6bc6e62":"# Form a prediction set\npredictions = clf.predict(X_test_cv)","4fa57992":"# Report the confusion matrix\nprint(metrics.confusion_matrix(y_test,predictions))\n# Print a classification report\nprint(metrics.classification_report(y_test,predictions))\n# Print the overall accuracy\nprint(metrics.accuracy_score(y_test,predictions))","1d826d89":"# single command to create a pipeline of activities...vectorize and classify the text, in this case\nclf_cvec_lsvc = Pipeline([('cvec', CountVectorizer()),\n                     ('clf', LinearSVC())])\n\n# Feed the training data through the pipeline\nclf_cvec_lsvc.fit(X_train, y_train)","2cae0573":"# Form a prediction set\n# No need to convert the test data. Classifier cretaed in the pipeline will take care of it\npredictions = clf_cvec_lsvc.predict(X_test)\n# Report the confusion matrix\nprint(metrics.confusion_matrix(y_test,predictions))\n# Print a classification report\nprint(metrics.classification_report(y_test,predictions))\n# Print the overall accuracy\nprint(metrics.accuracy_score(y_test,predictions))","096abc5c":"clf_tfidf_lsvc = Pipeline([('tfidf', TfidfVectorizer()),\n                     ('clf', LinearSVC())])\n\n# Feed the training data through the pipeline\nclf_tfidf_lsvc.fit(X_train, y_train)","7e96fdd2":"# Form a prediction set\npredictions = clf_tfidf_lsvc.predict(X_test)\n# Print the overall accuracy\nprint(metrics.accuracy_score(y_test,predictions))","3fcff2e4":"clf_tfidf_mnb = Pipeline([('tfidf', TfidfVectorizer()),\n                     ('clf', MultinomialNB())])\n\n# Feed the training data through the pipeline\nclf_tfidf_mnb.fit(X_train, y_train)  ","435da9ec":"# Form a prediction set\npredictions = clf_tfidf_mnb.predict(X_test)\n# Print the overall accuracy\nprint(metrics.accuracy_score(y_test,predictions))","47bdcc7d":"clf_tfidf_lr = Pipeline([('tfidf', TfidfVectorizer()),\n                     ('clf', LogisticRegression())])\n\n# Feed the training data through the pipeline\nclf_tfidf_lr.fit(X_train, y_train)","6e23ec18":"predictions = clf_tfidf_lr.predict(X_test)\n# Print the overall accuracy\nprint(metrics.accuracy_score(y_test,predictions))","fdeb43f5":"clf_tfidf_knc = Pipeline([('tfidf', TfidfVectorizer()),\n                     ('clf', KNeighborsClassifier())])\n\n# Feed the training data through the pipeline\nclf_tfidf_knc.fit(X_train, y_train)","f2cb5ab4":"predictions = clf_tfidf_knc.predict(X_test)\n# Print the overall accuracy\nprint(metrics.accuracy_score(y_test,predictions))","5b729d26":"clf_tfidf_rfc = Pipeline([('tfidf', TfidfVectorizer()),\n                     ('clf', RandomForestClassifier())])\n\n# Feed the training data through the pipeline\nclf_tfidf_rfc.fit(X_train, y_train)","e53f0d86":"predictions = clf_tfidf_rfc.predict(X_test)\n# Print the overall accuracy\nprint(metrics.accuracy_score(y_test,predictions))","d11aec54":"# Create list of StopWords\nimport nltk\nfrom nltk.corpus import stopwords\nstopwords = stopwords.words('english')\nprint(stopwords)","f2136f47":"clf_tfidf_lsvc2 = Pipeline([('tfidf', TfidfVectorizer(stop_words=stopwords)),\n                     ('clf', LinearSVC())])\n\n# Feed the training data through the pipeline\nclf_tfidf_lsvc2.fit(X_train, y_train)","0281aa32":"predictions = clf_tfidf_lsvc2.predict(X_test)\n# Print the overall accuracy\nprint(metrics.accuracy_score(y_test,predictions))","c0799dd7":"### Let's try Naive Baiyes","33981b84":"Just experienced that **TF-IDF is extremely faster compared to CountVectorizer** <br>\nLet's hope it yields better accuracy as well","fea3b0ad":"## Fit the model","d3fc5afc":"![image.png](https:\/\/raw.githubusercontent.com\/nadarsubash\/articles\/master\/white-moon-on-hands-3278643.jpg)\n*Photo by Gantas Vai\u010diul\u0117nas from Pexels*","08d03361":"**Ok. 54% Accuracy is bad :(**","ef0c2a8d":"*Both are predicted incorrectly :(*","3c776606":"**There is a slight decrease in the accuracy after removing the StopWords**\n\nWill explore other parameters that we can optimize. ","8c9d4f0a":"With an accuracy of 60%, **TF-IDF is fater and better**\n\nBut we need even better accuracy, let's try some other algorithm","9835cee5":"### Performance of various Classifiers\n**LinearSVC = 0.6043<br>\nMultinomialNB = 0.3437<br>\nLogisticRegression = 0.5845<br>\nK-NeighborsClassifier = 0.0641<br>\nRandomForestClassifier = 0.3959**<br><br>\n\nJury is out : **LinearSVC has the best accuracy of 60%**<br>\n\nLet's try to change some parameters <br>\nWe can start with removing the stop words from the articles before fitting the model. Let's see if that brings in any improvement","0a239e51":"Accuracy of Pipeline funtion is same as the deailed steps <br>\n**This means Pipeline works :)**","52012399":"## Prepare Training Data\nSplit the data into train & test sets","cbdfd6d5":"**Well, we can try some other method like TF-IDF!!**<br>\nBefore we try other methods, let's simplify the above steps.....using Pipeline","8c9d5c32":"## Identifying the Features","a4d30bf9":"# Predicting News Category Using NLP\n### based on the Headlines and Short Description of the Article ","708bf621":"*News category is Crime and Entertainment respectively. Let's see if our classifier is able to predict it correctly*","72171eb7":"## Text Feature Extraction with CountVectorizer\nNow time to vectorise the text data","d23e1c3f":"## K-Nearest Neighbors Classifier","f4190112":"#### above shape indicates that there are 140,597 artiles which has 145,283 unique relevant words *(assuming count vectorizer has removed the stop words)*","f4e5d23c":"## Logistic Regression","993b76a2":"## Random Forest","dc55b811":"## Data Pre-processing","58f114be":"Let's now test it for the entire Test dataset","b94b4045":"## Text Feature extraction with TF-IDF ","feac1425":"## Test the model\nLet's test the data and see the result "}}