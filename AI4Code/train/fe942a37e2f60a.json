{"cell_type":{"feb68579":"code","1be6cb4a":"code","f757045b":"code","76990c75":"code","8a4b478e":"code","9228e5c1":"code","32ab8793":"code","661588ea":"code","84ec7ca3":"code","0c05c6d6":"code","9a8457e2":"code","3e3d16d0":"code","d48f4743":"code","d6d8ae7f":"code","9ff00e61":"code","007f1cca":"code","92931d46":"code","15e7573f":"code","35421380":"code","12b980b3":"code","890e3236":"code","2e9e1dd9":"code","e5e46643":"code","16ff206b":"code","901d9d58":"code","1ca4148f":"code","cc0e34c6":"code","5f04ae40":"code","e3426684":"code","eb139106":"code","6b17782c":"code","2c75ff18":"code","e757c4e2":"code","7b29c022":"code","54a26460":"code","13a23143":"code","107a4b8d":"code","8416eb46":"code","bc3d001e":"code","a73f3f32":"code","d358af50":"code","f7b0cd09":"code","338f007d":"code","f7836b94":"code","077e391e":"code","31d20cfc":"code","2115f91c":"code","9e4e85ca":"code","f13b0ddd":"code","59214e62":"code","73447074":"code","12ee1272":"code","2ec4c08b":"code","afde119e":"code","1b0fe0f3":"code","c16a3b28":"code","7b3e0adc":"code","2b2852f4":"code","607389ab":"code","0c03ea90":"code","8bbc7be2":"code","ae169a66":"code","8395b1b9":"code","3aed727f":"code","df5768ab":"code","b22af2e5":"markdown","23f1f89b":"markdown","47508d9d":"markdown","b3d245f9":"markdown","63d244c2":"markdown","3b4516fc":"markdown","78a8d58a":"markdown","cbe8a72f":"markdown","692f4f63":"markdown","3a711006":"markdown","df7aca7c":"markdown","ae746a2b":"markdown","9afc259d":"markdown","ab54d285":"markdown","73f3693b":"markdown","e59d9087":"markdown","9edcd205":"markdown","fed4c808":"markdown","80c5682d":"markdown","a23a8b50":"markdown"},"source":{"feb68579":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","1be6cb4a":"\nfrom fastai.imports import *\nfrom fastai.structured import *\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom IPython.display import display\nfrom sklearn import metrics","f757045b":"import os\nprint(os.listdir(\"..\/input\"))\nPATH = '..\/input'","76990c75":"train = pd.read_csv(f'{PATH}\/train.csv', low_memory=False)","8a4b478e":"train","9228e5c1":"building_structure = pd.read_csv(f'{PATH}\/Building_Structure.csv', low_memory=False)\nbuilding_ownership = pd.read_csv(f'{PATH}\/Building_Ownership_Use.csv', low_memory=False)\ntest = pd.read_csv(f'{PATH}\/test.csv', low_memory=False)","32ab8793":"print(train.shape,'\\n',building_ownership.shape,'\\n', building_structure.shape)","661588ea":"train = train.merge(building_structure, on = 'building_id',how = 'left')\ntrain = train.merge(building_ownership, on = 'building_id', how = 'left')","84ec7ca3":"print(train.columns)\nprint(train.shape)","0c05c6d6":"train.drop(['district_id_x', 'district_id_y', 'vdcmun_id_x', 'vdcmun_id_y', 'ward_id_y'], axis=1, inplace=True)","9a8457e2":"print(train.shape,train.columns)","3e3d16d0":"test.shape","d48f4743":"test = test.merge(building_structure, on = 'building_id',how = 'left')\ntest = test.merge(building_ownership, on = 'building_id', how = 'left')\ntest.drop(['district_id_x', 'district_id_y', 'vdcmun_id_x', 'vdcmun_id_y', 'ward_id_y'], axis=1, inplace=True)\ntest.shape","d6d8ae7f":"#function to display all rows and columns\ndef display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n        display(df)","9ff00e61":"display_all(train.tail().T)\n","007f1cca":"display_all(train.describe(include='all').T)","92931d46":"display_all(train.describe(include='all').T)","15e7573f":"# Create table for missing data analysis\ndef draw_missing_data_table(df):\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    return missing_data","35421380":"draw_missing_data_table(train)","12b980b3":"#count familes have only 1 missing values we'll fill that\ntrain['count_families'].fillna(train['count_families'].mode()[0],inplace=True)","890e3236":"print(train['has_repair_started'].value_counts())\nprint(test['has_repair_started'].value_counts())\n","2e9e1dd9":"train['has_repair_started'].fillna(False,inplace=True)\ntest['has_repair_started'].fillna(False,inplace=True)","e5e46643":"print(train.columns.hasnans)\nprint(test.columns.hasnans)","16ff206b":"Y = {'Grade 1': 1, 'Grade 2': 2, 'Grade 3': 3, 'Grade 4': 4, 'Grade 5': 5}\ntrain['damage_grade'].replace(Y, inplace = True)\ntrain['damage_grade'].unique()","901d9d58":"train.dtypes","1ca4148f":"print(train.select_dtypes('object').nunique())\nprint(train.select_dtypes('object').nunique())","cc0e34c6":"#Remove column 'building_id' as it is unique for every row & doesnt have any impact\ntrain_building_id = train['building_id']\ntest_building_id = test['building_id']\ntrain.drop(['building_id'], axis=1, inplace=True)\ntest.drop(['building_id'], axis=1, inplace=True)","5f04ae40":"display_all(train)","e3426684":"train['count_floors_change'] = (train['count_floors_post_eq']\/train['count_floors_pre_eq'])\ntrain['height_ft_change'] = (train['height_ft_post_eq']\/train['height_ft_pre_eq'])\ntest['count_floors_change'] = (test['count_floors_post_eq']\/test['count_floors_pre_eq'])\ntest['height_ft_change'] = (test['height_ft_post_eq']\/test['height_ft_pre_eq'])\n\ntrain.drop(['count_floors_post_eq', 'height_ft_post_eq'], axis=1, inplace=True)\ntest.drop(['count_floors_post_eq', 'height_ft_post_eq'], axis=1, inplace=True)","eb139106":"sns.barplot(train['condition_post_eq'],train['damage_grade']);","6b17782c":"sns.barplot(train['plan_configuration'],train['damage_grade']);","2c75ff18":"train_cats(train)\napply_cats(test, train)","e757c4e2":"df, y, nas = proc_df(train, 'damage_grade', max_n_cat=6)","7b29c022":"test_df, _, _ = proc_df(test, na_dict=nas, max_n_cat=6)\n","54a26460":"print(test_df.shape, df.shape)","13a23143":"from sklearn.metrics import f1_score\n\ndef rmse(x,y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res = [m.score(x_train, y_train), m.score(x_test, y_test)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","107a4b8d":"from sklearn.model_selection import train_test_split\nx = df\nx_train, x_test, y_train, y_test = train_test_split(x, y, \ntest_size=0.2, random_state=0)","8416eb46":"print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)","bc3d001e":"set_rf_samples(100000)","a73f3f32":"m = RandomForestClassifier(n_jobs=-1)\n%time m.fit(x_train, y_train)\nprint(m.score(x_train, y_train))","d358af50":"print_score(m)","f7b0cd09":"m = RandomForestRegressor(n_estimators=150, min_samples_leaf=1, max_features=0.6, n_jobs=-1, oob_score=True)\n%time m.fit(x_train, y_train)\nprint_score(m)","338f007d":"fi = rf_feat_importance(m, df); fi[:10]","f7836b94":"fi.plot('cols', 'imp', figsize=(10,6), legend=False);\n","077e391e":"def plot_fi(fi): return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)","31d20cfc":"plot_fi(fi[:30]);","2115f91c":"to_keep = fi[fi.imp>0.001].cols; len(to_keep)","9e4e85ca":"to_keep","f13b0ddd":"def split_vals(a,n): return a[:n], a[n:]\ndf_keep = df[to_keep].copy()\n\nfrom scipy.cluster import hierarchy as hc\n\ncorr = np.round(scipy.stats.spearmanr(df_keep).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(16,10))\ndendrogram = hc.dendrogram(z, labels=df_keep.columns, orientation='left', leaf_font_size=16)\nplt.show()","59214e62":"correlations = df.corr()","73447074":"print('Most Positive Correlations:\\n', correlations.tail(10))\nprint('\\nMost Negative Correlations:\\n', correlations.head(10))","12ee1272":"imp_features=['height_ft_change', 'condition_post_eq', 'count_floors_change' , 'ward_id_x','age_building', 'plinth_area_sq_ft']\nscor = train[imp_features+['damage_grade']]\ndata_corrs = scor.corr()\ndata_corrs\nplt.figure(figsize = (8, 6))\n\n# Heatmap of correlations\nsns.heatmap(data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","2ec4c08b":"reset_rf_samples()","afde119e":"m = RandomForestRegressor(n_estimators=150, min_samples_leaf=1, max_features=0.6, n_jobs=-1, oob_score=True)\n%time m.fit(x_train, y_train)\nprint_score(m)","1b0fe0f3":"test_df.head(5)","c16a3b28":"ypreds = m.predict(test_df)","7b3e0adc":"ypreds = ypreds.round()","2b2852f4":"prediction=pd.DataFrame({'building_id': test_building_id, 'damage_grade':ypreds})","607389ab":"target = {1: 'Grade 1', 2: 'Grade 2', 3: 'Grade 3', 4: 'Grade 4', 5: 'Grade 5'}\nprediction.damage_grade.replace(target, inplace=True)\nprediction.to_csv('submission.csv', index=False)","0c03ea90":"prediction.head()","8bbc7be2":"from xgboost import XGBClassifier","ae169a66":"xgbc = XGBClassifier(n_estimators=100, learning_rate=0.2, max_depth=6, random_state=42) #random state = 42 as for Feature Imp above 0.01 there were 42 cols\nxgbc.fit(x_train, y_train)","8395b1b9":"print_score(xgbc)\npred_test_y = pd.Series(list(xgbc.predict(test_df)))\nprediction=pd.DataFrame({'building_id': test_building_id, 'damage_grade':pred_test_y})\ntarget = {1: 'Grade 1', 2: 'Grade 2', 3: 'Grade 3', 4: 'Grade 4', 5: 'Grade 5'}\nprediction.damage_grade.replace(target, inplace=True)\nprediction.to_csv('submission.csv', index=False)","3aed727f":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\nknn.fit(x_train, y_train)","df5768ab":"print_score(knn)","b22af2e5":"** Handling Missing Values **","23f1f89b":"** Splitting data **","47508d9d":"### Exploring the Data","b3d245f9":"has repair started has lot if missing values, we'll first check the importance of that variable and then decide what to do with those missing values(for now set it to false)\n\nWe'll  ** Convert the target variable into numeric format **","63d244c2":"This gives 75% accuracy on hackerearth","3b4516fc":"### Random Forest ###\n**Base Model**","78a8d58a":"**Metric F1 **","cbe8a72f":"After trying a lot of combinations of hyperparameters I managed to get 90% on training, 86% on validation (on sampled subset of 100000)\n\n### Feature Importance ###","692f4f63":"Now looking at the dendogram plot we can remove the varaible which are totaly dependent on each other like ward_id_x and the district_id and also count_floor_change but as we can see above they are important features so dropping them reduces the accuracy drastically.\n\nAlso keeping only the important features is decreasing the accuracy so we keep the same set of earlier chosen features\n","3a711006":"**Dropping the ID(not useful) column**","df7aca7c":"**Adding\/Extracting features from the existing columns**\n\nHere we have \n\ncount_floors_pre_eq|Number of floors that the building had before the earthquake\n\ncount_floors_post_eq|Number of floors that the building had after the earthquake\n\nheight_ft_pre_eq|Height of the building before the earthquake (in feet)\n\nheight_ft_post_eq|Height of the building after the earthquake (in feet)\n\nWe can clearly see that these varaible are not to be used directly but we should extract the meaning from the variable first and then use them.\nWe can calculate the changes in floor and height after the earthquake by subtracting first from the 2nd","ae746a2b":"This submission gives a 76.9% accuracy on hackerearth platform.With few variables removed and with xgboost the accuracy can be made to increase to 79+","9afc259d":"**Converting Object\/categorical into Dummy variable**\n\nWe'll choose one-hot encoding over numeric codes as here no object\/categorical varible is such that it has types like (high medium low) which do indicates (2,1,0) ","ab54d285":"We can see some missing values(has_repair_started \/ count_families) along with some categorical variable which we'll handle","73f3693b":"### Preprocessing","e59d9087":"At any point of time Ensembling works better than SVM as bags are created and distributed average of predictions of tree are taken.(although sometimes if the data if quite linear or fits perfectly for some poly function then SVM can have a upper hand) The reason here Randomforest gave better accuracy and prediction than xgboost was the data is bit noisy, and in such conditions RF works better than xgb.RFs train each tree independently, using a random sample of the data. This randomness helps to make the model more robust and less likely to overfit on the training data.Also xgb needs more number of enumarators(greater than 250 in this case atleast) for shooting up the accu, i trained on 100(Running on local machine with insuffient memory and above 150 the machine crashed).","9edcd205":"Merge the other dataset with test set too !","fed4c808":"Getting worst","80c5682d":"here we'll do one hot encoding on all except plan_configuration and condition_post_eq, as  condition post earthquake can be given codes (1,2,3...) where as plan config doesn't seem much useful now \nSo in proc_df which handles missing values and categorical variable(conversion into codes) we'll set max_n_cat  = 6 so that all the objects having unique values below 6 will be one hot encoded, and the variable above 6 will be given codes","a23a8b50":"Now let's merge training set with building structure and ownership on buildingID (we can see few overlaps which we can drop later)"}}