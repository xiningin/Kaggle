{"cell_type":{"e3f9e8c6":"code","e960fbd4":"code","58a6def7":"code","d900239e":"code","f8ec9645":"code","6beeaf4d":"code","dd327959":"code","cc345ddb":"code","b80268fd":"code","39fa9a92":"code","81d3fa46":"code","d897c4fb":"code","f48351ed":"code","c7a36c67":"code","afadfc74":"markdown","0a7b594c":"markdown","1f8689c1":"markdown","a6b9ae7c":"markdown","aa34e3dc":"markdown","2a245f8d":"markdown","b204404e":"markdown","be75312d":"markdown","e7af3310":"markdown","552c9f1b":"markdown","f5d9cb7b":"markdown","0bd352ea":"markdown","3fb06eb8":"markdown"},"source":{"e3f9e8c6":"import numpy as np # linear algebra\nimport os\nimport matplotlib.pyplot as plot\nfrom PIL import Image\nimport cv2\nfrom sklearn.cluster import KMeans\nimport random\n\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, BatchNormalization, Conv2D, MaxPooling2D, Dense, Dropout, LeakyReLU, UpSampling2D, concatenate\nfrom keras.optimizers import Adam, SGD\nfrom keras.callbacks import ModelCheckpoint","e960fbd4":"def LoadImage(name, path=\"..\/input\/cityscapes_data\/cityscapes_data\/train\",\n             rotation=0.0, flip=False, cut_bottom=58,\n             size=(256, 200)):\n    img = Image.open(path+\"\/\"+name)\n    img = np.array(img)\n    seg = img[:-cut_bottom, 256:]\n    img = img[:-cut_bottom, 0:256]\n    \n    for i in range(3):\n        zimg = img[:,:,i]\n        zimg = cv2.equalizeHist(zimg)\n        img[:,:,i] = zimg\n    \n    img = Image.fromarray(img).resize(size)\n    seg = Image.fromarray(seg).resize(size)\n    \n    \n\n    img = img.rotate(rotation)\n    seg = seg.rotate(rotation)\n\n    img = np.array(img)\n    seg = np.array(seg)\n\n    if flip:\n        img = img[:,::-1,:]\n        seg = seg[:,::-1,:]\n\n        #seg = np.round(seg\/255.0)\n    \n    return img\/255, seg\/255","58a6def7":"files =os.listdir(\"..\/input\/cityscapes_data\/cityscapes_data\/train\")[0:10]\n\ncolors = []\nfor file in files:\n    img, seg = LoadImage(file)\n    colors.append(seg.reshape(seg.shape[0]*seg.shape[1], 3))\ncolors = np.array(colors)\ncolors = colors.reshape((colors.shape[0]*colors.shape[1],3))\n\nkm = KMeans(13)\nkm.fit(colors)","d900239e":"def LayersToRGBImage(img):\n    colors = [(255,0,0), (0,255,0), (0,0,255),\n             (255,255,0), (255,0,255), (0,255,255),\n             (255,255,255), (200,50,0),(50,200,0),\n             (50,0,200), (200,200,50), (0,50,200),\n             (0,200,50), (0,0,0)]\n    \n    nimg = np.zeros((img.shape[0], img.shape[1], 3))\n    for i in range(img.shape[2]):\n        c = img[:,:,i]\n        col = colors[i]\n        \n        for j in range(3):\n            nimg[:,:,j]+=col[j]*c\n    nimg = nimg\/255.0\n    return nimg","f8ec9645":"def ColorsToClass(seg):\n    s = seg.reshape((seg.shape[0]*seg.shape[1],3))\n    s = km.predict(s)\n    s = s.reshape((seg.shape[0], seg.shape[1]))\n    \n    n = len(km.cluster_centers_)\n    \n    cls = np.zeros((seg.shape[0], seg.shape[1], n))\n    \n    for i in range(n):\n        m = np.copy(s)\n        m[m!=i] = 0\n        m[m!=0] = 1\n        \n        cls[:,:,i]=m\n        \n    return cls\n\nimg, seg = LoadImage(\"174.jpg\")\nseg2 = ColorsToClass(seg)\nseg2 = LayersToRGBImage(seg2)\ntotal = cv2.addWeighted(img, 0.6, seg2, 0.4, 0)\nplot.imshow(total[:,:,:])\nplot.show()","6beeaf4d":"def Generate(path=\"..\/input\/cityscapes_data\/cityscapes_data\/train\", batch_size=10,\n            maxangle=10.0):\n    \n    files = os.listdir(path)\n    while True:\n        imgs=[]\n        segs=[]\n        \n        for i in range(batch_size):\n            file = random.sample(files,1)[0]\n            \n            flip=False\n            if random.random() > 0.5:\n                flip=True\n            \n            angle = maxangle*(random.random()*2-1)\n            \n            img, seg = LoadImage(file, path, rotation=angle, flip=flip)\n            \n            seg = ColorsToClass(seg)\n            \n            imgs.append(img)\n            segs.append(seg)\n        yield np.array(imgs), np.array(segs)\n        \ngen = Generate()\nimgs, segs = next(gen)\n\nplot.subplot(121)\nplot.imshow(imgs[0])\nplot.subplot(122)\nplot.imshow(LayersToRGBImage(segs[0]))\nplot.show()","dd327959":"inp = Input(shape=(200, 256, 3))\n\nx1 = BatchNormalization()(inp)\nx1 = Conv2D(64, 12, activation=\"relu\", padding=\"same\")(x1)\nx1 = Conv2D(128, 12, activation=\"relu\", padding=\"same\")(x1)\np1 = MaxPooling2D()(x1)\n#p1 = Dropout(0.2)(p1)\n\n#x2 = BatchNormalization()(x1)\nx2 = Conv2D(128, 9, activation=\"relu\", padding=\"same\")(p1)\nx2 = Conv2D(128, 9, activation=\"relu\", padding=\"same\")(x2)\np2 = MaxPooling2D()(x2)\n#p2 = Dropout(0.2)(p2)\n\n#x3 = BatchNormalization()(x2)\nx3 = Conv2D(128, 6, activation=\"relu\", padding=\"same\")(p2)\nx3 = Conv2D(128, 6, activation=\"relu\", padding=\"same\")(x3)\np3 = MaxPooling2D()(x3)\n#p3 = Dropout(0.2)(p3)\n\n#x4 = BatchNormalization()(x3)\nx4 = Conv2D(128, 3, activation=\"relu\", padding=\"same\")(p3)\nx4 = Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x4)\n#x4 = MaxPooling2D()(x4)\n#x4 = Dropout(0.2)(x4)\n\nx5 = UpSampling2D()(x4)\nx5 = concatenate([x3, x5])\nx5 = Conv2D(128, 6, activation=\"relu\", padding=\"same\")(x5)\nx5 = Conv2D(128, 6, activation=\"relu\", padding=\"same\")(x5)\n#x5 = Dropout(0.2)(x5)\n\nx6 = UpSampling2D()(x5)\nx6 = concatenate([x2, x6])\nx6 = Conv2D(128, 6, activation=\"relu\", padding=\"same\")(x6)\nx6 = Conv2D(128, 6, activation=\"relu\", padding=\"same\")(x6)\n#x6 = Dropout(0.2)(x6)\n\nx7 = UpSampling2D()(x6)\nx7 = concatenate([x1, x7])\nx7 = Conv2D(13, 6, activation=\"relu\", padding=\"same\")(x7)\nx7 = Conv2D(13, 6, activation=\"softmax\", padding=\"same\")(x7)\n\n\n\nmodel = Model(inp, x7)\n\nopt = Adam(lr=0.0001)\nmodel.compile(optimizer=opt,\n             loss=\"categorical_crossentropy\",\n             metrics=[\"accuracy\"])\nmodel.summary()","cc345ddb":"train_gen = Generate()\nval_gen = Generate(\"..\/input\/cityscapes_data\/cityscapes_data\/val\")","b80268fd":"clb = [ModelCheckpoint(\"loss.h5\", save_best_only=True, verbose=0)]\n\nh = model.fit_generator(train_gen, epochs=1000, steps_per_epoch=10,\n                       validation_data=val_gen, validation_steps=10,\n                       callbacks=clb, verbose=0)","39fa9a92":"model.save(\"model.h5\")\nmodel = load_model(\"loss.h5\")","81d3fa46":"loss = h.history[\"val_loss\"]\nacc = h.history[\"val_acc\"]\n\nplot.figure(figsize=(12, 6))\nplot.subplot(211)\nplot.title(\"Val. Loss\")\nplot.plot(loss)\nplot.xlabel(\"Epoch\")\nplot.ylabel(\"Loss\")\n\nplot.subplot(212)\nplot.title(\"Val. Accuracy\")\nplot.plot(acc)\nplot.xlabel(\"Epoch\")\nplot.ylabel(\"Accuracy\")\n\nplot.tight_layout()\nplot.savefig(\"learn.png\", dpi=150)\nplot.show()","d897c4fb":"test_gen = Generate(\"..\/input\/cityscapes_data\/cityscapes_data\/val\")\nmax_show=20\nfor imgs, segs in test_gen:\n    p = model.predict(imgs)\n    for i in range(p.shape[0]):\n        if i > max_show:\n            break\n        _p = LayersToRGBImage(p[i])\n        _s = LayersToRGBImage(segs[i])\n        \n        predimg = cv2.addWeighted(imgs[i], 0.6, _p, 0.4, 0)\n        trueimg = cv2.addWeighted(imgs[i], 0.6, _s, 0.4, 0)\n        plot.figure(figsize=(12,6))\n        plot.subplot(121)\n        plot.title(\"Prediction\")\n        plot.imshow(predimg)\n        plot.axis(\"off\")\n        plot.subplot(122)\n        plot.title(\"True\")\n        plot.imshow(trueimg)\n        plot.axis(\"off\")\n        plot.tight_layout()\n        plot.savefig(\"pred_\"+str(i)+\".png\", dpi=150)\n        plot.show()\n    break","f48351ed":"from sklearn.metrics import roc_curve, balanced_accuracy_score\ntest_gen = Generate(\"..\/input\/cityscapes_data\/cityscapes_data\/val\", batch_size=200)\nmax_show=20\n\nbass = []\n\nfor imgs, segs in test_gen:\n    p = model.predict(imgs)\n    \n    plot.figure(figsize=(10, 10))\n    for i in range(p.shape[-1]):\n        fpr, tpr, _ = roc_curve(segs[:,:,:,i].ravel(), p[:,:,:,i].ravel())\n        \n        _p = np.round(p[:,:,:,i].ravel()).astype(np.int32)\n        bas = balanced_accuracy_score(segs[:,:,:,i].ravel(), _p)\n        \n        bass.append(bas)\n        \n        plot.subplot(4,4,i+1)\n        plot.plot(fpr, tpr)\n        plot.title(\"Class \"+str(i))\n        plot.xlabel(\"False positive rate\")\n        plot.ylabel(\"True positive rate\")\n    \n    plot.tight_layout()\n    plot.show()\n    \n    break","c7a36c67":"plot.figure(figsize=(8, 5))\nplot.bar(np.arange(0, len(bass)), bass)\nplot.xticks(np.arange(0, len(bass)))\nplot.ylabel(\"Balanced Accuracy\")\nplot.xlabel(\"Class\")\nplot.tight_layout()\nplot.savefig(\"bas.png\")\nplot.show()","afadfc74":"We can now visualize the training by looking at the validation loss and validation accuracy.","0a7b594c":"<h3>Color clustering<\/h3>\nDue to the compression of the JPEG standard, there are more than only a few discrete colors in the segmented image. To find the most imortant colors and identify similar colors we can use KMeans clustering. This also allows us to go from a color representation to a class representation.","1f8689c1":"<h1>Image Segmentation for Self-Driving Cars<\/h1>\n\nHi and welcome to my notebook explaining the basics about image segmentation for self-driving cars. Image segmentation is an exciting toptic that allows computers to classify regions of images as certain objects - an important task for self-driving cars.\n\n<h3>Importing the required libraries<\/h3>\nFirst of all we have to import the required libraries.\n<ul>\n    <li>Numpy - for linear algebra<\/li>\n    <li>os - for receiving information about files<\/li>\n    <li>matplotlib - for visualizing<\/li>\n    <li>PIL and CV2 - for image loading and editing<\/li>\n    <li>scikit-learn KMeans - for color clustering<\/li>\n    <li>random - for generating random numbers and choices<\/li>\n    <li>keras - a high level interface to tensorflow (for neural networks)<\/li>\n<\/ul>","a6b9ae7c":"Time to save the model and load the best model from the training run.","aa34e3dc":"<h3>Layers to RGB Image<\/h3>\nAfter clustering the image contains information about 13 different layers. Computer displays can only utilize the RGB colorspace. This function converts the layer representation to a color representation. This is just for visualizing.","2a245f8d":"<h3>The Convolutional Neural Network - UNet<\/h3>\nFor learning we are building a UNet type network. This is a special kind of CNN, which uses concatenate layers to feed data from previous layers to the layers close to the output. The network has a double-funnel shape. The throughfeeding of data makes sure that no data is lost. Otherwise the funnel shape would cause something similar to an autoencoder.","b204404e":"<h3>Training<\/h3>\n\nFinally we can start training. We will train for 1000 epochs and use a Model Checkpoint to save the model with the smalles validation loss.","be75312d":"I hope you enjoyed reading this short notebook demonstrating how to implement a simple image segmentation model for self-driving cars!","e7af3310":"The network has 9 million trainable parameters.\n\nThe next step is to create two generators:\n<ul>\n    <li>Training generator - for training data<\/li>\n    <li>Validation generator - to prevent overfitting<\/li>\n    <\/ul>","552c9f1b":"<h3>LoadImage<\/h3>\nThe function LoadImage loads and image from a file path and also allows simpe data augmentation options such as flipping and rotating. The images are side-by-side images of the raw image and a color coded segmented image. The function splits the image into seperate images and cuts the engine hood away.","f5d9cb7b":"<h3>Colors to Class<\/h3>\nThis function converts the discrete color representation (output of the color clustering) to a 13-dimensional class representation. This is useful for the machine learning algorithm later on. On the bottom you can see an example classification layed over the raw image.","0bd352ea":"<h3>The Results<\/h3>\n\nWe can now visualize how the CNN performs at the example of a few validation images. To be really sure there is no overfitting one could also have split of a test set from the training set.","3fb06eb8":"<h3>The Data Generator<\/h3>\nTo make training more memory efficient a generator is used for feeding the data to the deep learning algorithm. This generator creates batches of e.g. 10 raw-segmented image pairs at a time. It also uses image augmentation and randomly flips and rotates the images to increase the effective size of the dataset.\n\nAt the bottom you can see one such pair."}}