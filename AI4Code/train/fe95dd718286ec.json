{"cell_type":{"a1b7b1cd":"code","bab310ac":"code","14209a88":"code","6593ee35":"code","31e0aca5":"code","f05c561f":"code","96e613df":"code","7d44fbbb":"code","490edfd7":"code","2466edc2":"code","7a22ed1a":"code","1e11dcb2":"code","80a7c4e9":"code","022566be":"code","7431de05":"code","f0d928a1":"markdown","4235d722":"markdown","388cba6b":"markdown","0bcbaab9":"markdown","d5a51f30":"markdown","d74eb8be":"markdown"},"source":{"a1b7b1cd":"import numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport scipy as sp\n\nfrom sklearn import datasets, preprocessing\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.metrics import roc_curve, auc\n\nfrom sklearn.svm import SVC\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.datasets import make_classification\n\nfrom plotly import __version__\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nfrom plotly.graph_objs import Scatter, Figure, Layout, Histogram\n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","bab310ac":"train, test = pd.read_csv('..\/input\/train.csv') , pd.read_csv('..\/input\/test.csv')\n\n","14209a88":"train.head(10)","6593ee35":"# One-hot encode sex, class and derive a married variable\ntrain['female'] = train['Sex'].apply(lambda x: 1 if x == 'female' else 0)\ntrain['class_1'] = train['Pclass'].apply(lambda x: 1 if x == 1 else 0)\ntrain['class_2'] = train['Pclass'].apply(lambda x: 1 if x == 2 else 0)\ntrain['class_3'] = train['Pclass'].apply(lambda x: 1 if x == 3 else 0)\n\ntrain['married'] = train['Name'].apply(lambda x: 1 if 'MRS' in x.upper() else 0)\n# Let's engineer a feature\ntrain['n_names'] = train['Name'].apply(lambda x: len(x.split(' '))-1)\n\ntrain['embarked_C'] = train['Embarked'].apply(lambda x: 1 if x == 'C' else 0)\ntrain['embarked_S'] = train['Embarked'].apply(lambda x: 1 if x == 'S' else 0)\ntrain['embarked_Q'] = train['Embarked'].apply(lambda x: 1 if x == 'Q' else 0)\n\n\n\n# We need to replicate this for the Test data too\ntest['female'] = test['Sex'].apply(lambda x: 1 if x == 'female' else 0)\ntest['class_1'] = test['Pclass'].apply(lambda x: 1 if x == 1 else 0)\ntest['class_2'] = test['Pclass'].apply(lambda x: 1 if x == 2 else 0)\ntest['class_3'] = test['Pclass'].apply(lambda x: 1 if x == 3 else 0)\ntest['married'] = test['Name'].apply(lambda x: 1 if 'MRS' in x.upper() else 0)\ntest['n_names'] = test['Name'].apply(lambda x: len(x.split(' '))-1)\ntest['embarked_C'] = test['Embarked'].apply(lambda x: 1 if x == 'C' else 0)\ntest['embarked_S'] = test['Embarked'].apply(lambda x: 1 if x == 'S' else 0)\ntest['embarked_Q'] = test['Embarked'].apply(lambda x: 1 if x == 'Q' else 0)\n\n\n#We need to remove any NaN values\nfeatures = ['female', 'Age', 'Fare', 'class_1', 'class_2','class_3',\n            'married','n_names', 'embarked_C', 'embarked_Q', 'embarked_S']\noutput = 'Survived'\n\ntrain_all = train[features + [output]]\ntrain_all[train_all==np.inf]=np.nan\ntrain_all.dropna(inplace=True)\n\n# Let's also scale the age and fare while we are at it, this should minimize training time\nscaler = preprocessing.MinMaxScaler()\nscaler.fit(train_all[['Age', 'Fare']])\ntrain_all['Age_t'] = scaler.transform(train_all[['Age', 'Fare']])[:,0]\ntrain_all['Fare_t'] = scaler.transform(train_all[['Age', 'Fare']])[:,1]\n","31e0aca5":"\n\ntrain.groupby('Survived').describe().T","f05c561f":"train.boxplot(column='Age', by='Survived', figsize=(15,7))\ntrain.boxplot(column='Fare', by='Survived', figsize=(15,7))\n\nprint(sp.stats.ttest_ind(train.loc[train['Survived']==1, 'Age'].dropna(),\n                         train.loc[train['Survived']!=1, 'Age'].dropna()))\nprint(sp.stats.ttest_ind(train.loc[train['Survived']==1, 'Fare'].dropna(),\n                         train.loc[train['Survived']!=1, 'Fare'].dropna()))","96e613df":"# If we want to test categorical variables univariately we could use chi-squared test\ncont = pd.crosstab(train['Embarked'], train['Survived'])\nchsq = sp.stats.chi2_contingency(cont)\nprint('Chi^2: ' +  str(chsq[0]))\nprint('P-value: ' +  str(chsq[1]))\ncont\n","7d44fbbb":"#list of features I want to include\nfeatures = ['female', 'Age_t', 'Fare', 'class_1',\n            'married','n_names']\noutput = 'Survived'\n\n#Split the data set so we can validate our model\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_all[features], train_all[output], test_size=0.33, random_state=42)\n\n\n\n","490edfd7":"import seaborn as sns\ncorr = train_all.iloc[:,:20].corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(20, 20))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\ng = sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1.0, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot = True)","2466edc2":"# First using \nlogit_mod = sm.Logit(y_train, X_train)\nlogit_res = logit_mod.fit()\nprint(logit_res.summary2())","7a22ed1a":"# Take the exponential of the parameters and we get the odds \nlogit_res.params.apply(np.exp)","1e11dcb2":"X_test.head()","80a7c4e9":"# Logit function, this should work regardless of the number of variables, \n# as long as the params match the number of columns in test data (an they are the correct order)\n\n1\/(1 + np.exp(-sum((X_test.values * logit_res.params.values.T).T)))","022566be":"# just to show we get the same\nlogit_res.predict(X_test)","7431de05":"from sklearn.metrics import confusion_matrix,roc_curve, auc\nimport itertools\nimport numpy as np\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n# Create classifiers, istantiate the objects\nlr = LogisticRegression()\ngnb = GaussianNB()\nsvc = LinearSVC(C=1.0, random_state=42)\nrfc = RandomForestClassifier(n_estimators=100, random_state=42)\n\n\n# #############################################################################\n# Plot calibration plots\n\nplt.figure(figsize=(10, 10))\nfor clf, name in [(lr, 'Logistic'),\n                  (gnb, 'Naive Bayes'),\n                  (svc, 'Support Vector Classification'),\n                  (rfc, 'Random Forest')]:\n    clf.fit(X_train, y_train)\n    if hasattr(clf, \"predict_proba\"):\n        prob_pos = clf.predict_proba(X_test)[:, 1]\n    else:  # use decision function\n        prob_pos = clf.decision_function(X_test)\n        prob_pos = \\\n            (prob_pos - prob_pos.min()) \/ (prob_pos.max() - prob_pos.min())\n    fpr, tpr, _ = roc_curve(y_test, prob_pos)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label='%s (auc = %0.2f)' % (name, roc_auc))\n\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\nplt.show()\n    \n    \nfor clf, name in [(lr, 'Logistic'),\n              (gnb, 'Naive Bayes'),\n              (svc, 'Support Vector Classification'),\n              (rfc, 'Random Forest')]:\n        # Compute confusion matrix\n    cnf_matrix = confusion_matrix(y_test, clf.predict(X_test))\n    np.set_printoptions(precision=2)\n\n    class_names = ['Dead', 'Survived']\n    # Plot non-normalized confusion matrix\n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=class_names,\n                          title='Confusion matrix for %s' % name)\n\n\n","f0d928a1":"## Load data set","4235d722":"## Logistic regression\nEssentially logistic regression aims to maximise the separation of variable of interest into a binary classification.  Here we have a variable that is normally distributed.  We can look at the population as a whole or separated into classes.  As you can see in this example, there are two distinct populations.  What we can say from this is if we have a sample of unknown classification, but the value of the variable is 4, out of the two classes, it is more liklely that the sample belongs in the \"1\" group.  These is a very simple example, made easier by the fact that the base probability of being in one of the groups is 0.5.  \n\nLogistic regression is fully interpretable, the coeficients for each variable are the log(odds) of a 1 unit increase in the variable. I have used the stats_models implementation of logistic regression as this gives a better output to explain the model.","388cba6b":"## Feature Selection\n### Co-linearity \nThe first assumption we make with logistic regression is that the independence between each of the covariates.  As this can have an adverse affect on the optimisation of the co-efficient.  We can check this by looking at the correlation matrix for all the covariates.  As a rule of thumb, anything above 0.9 correlation (Pearson's r) would be considered bad, as this means 80% of the variance of one variable is explained by the other.","0bcbaab9":"# Data Science 101","d5a51f30":"We can classify each of the columns as categorical, ordinal, ratio\/interval and useless.  \n\nPassengerId, Ticket, look useless.  \n\nCabin looks like it's dependent on the Pclass, it may be useless in its current form.  \n\nName also looks useless but maybe we can engineer a feature from this, as we could determine marriage status of females, which could be important.  In the film, although not factually accurate, we seen husbands getting their wives to the front of the queue for a life boat.  We could also match surnames of married females to males to determine marriage status to fellow passengers.  We could also look at the length of the names, the more names they had, the more upper class they are.\n\nSex is relevant, from the line \"women and children first\", but we would need to one-hot encode this to be used in the model.  Age is an interval variable, so doesn't need trasformed.\n\nWhere they embarked is a categorical variable, this will need one-hot encoded to be used.\n\nPclass is an ordinal variable, we can approach this two ways, either keep as is, where we assume the probability of survival is the same between 1 and 2 as is between 2 and 3.  Or we could one-hot encode this too, so the distance between class can be estimated independently.\n\nFare and Age are interval data and can be used as is, but perhaps we could scale the fare using min\/max scaling.","d74eb8be":"### Interpretation\nIn the above example we can see that a female was 11 times more likely to have survived than a male. Also, a 1st class passenger was 13 times more likely to survive than the lower classes.  \n\nYou can also see that age has an effect, because I scaled it I've removed relevance to actual age.  But what we can say, as the odds is less than 1, that means the inverse is true.  1\/0.02 = 50, so we can say the youngest was 50 times more likely to have survived than the oldest.\n\nBut we knew this before, remember?  ***\"Women and Children first!!!\"***\n\n### Prediction\nNow we have the parameters, we can apply these parameters to a new set of data to generate a probability of survival.\n\nProbability is sometimes written as pi:\n\n$$\\log \\left({\\pi\\over 1-\\pi}\\right) = \\beta_0 + \\beta_1x$$\n\nThe equation can be rearranged into the **logistic function**:\n\n$$\\pi = \\frac{e^{\\beta_0 + \\beta_1x}} {1 + e^{\\beta_0 + \\beta_1x}}$$\n\nor \n\n$$\\pi = \\frac{1} {1 + e^{-\\beta_0 + \\beta_1x}}$$\n\nWe could use the predict function on the logistic regression object, but it's more fun to calculate manually"}}