{"cell_type":{"b9dc9fa2":"code","6d87e802":"code","58d9e33c":"code","6e6a69c7":"code","209f737f":"code","e632bc57":"code","bf51d68c":"code","8d3f07ca":"code","7cf53fa6":"code","645d7e49":"code","ab2579d6":"code","e8bb6e14":"code","09b453a6":"code","8758efb7":"code","5ab636f1":"code","ed8f1140":"code","8ca00d3c":"code","b8e63bff":"code","7a21bf03":"code","4ae9fe19":"code","7fc0816b":"code","88ff537a":"code","29e9c073":"code","09240b1e":"code","0fd236da":"code","f12ca90f":"code","b34adf98":"code","2f5d78e7":"markdown","f522307c":"markdown","5d85753b":"markdown","9220893d":"markdown","b80fac29":"markdown","0dc9b334":"markdown","4490c954":"markdown","d3fc5908":"markdown","e005403f":"markdown"},"source":{"b9dc9fa2":"import numpy as np \nimport pandas as pd \nimport tensorflow as tf\nfrom tensorflow.keras.layers import Activation, Dense, Embedding, GlobalAveragePooling1D\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords,wordnet\nstop = stopwords.words('english')\nfrom nltk.stem import WordNetLemmatizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, LSTM , Embedding, Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model, Sequential\nfrom keras.callbacks import EarlyStopping","6d87e802":"df = pd.read_csv('..\/input\/final-project\/train.csv')","58d9e33c":"df.info()","6e6a69c7":"df.head(10)","209f737f":"df = df[['Review_Title', 'Review', 'Rating', 'Recommended']]","e632bc57":"df.info()","bf51d68c":"df.isna().sum()","8d3f07ca":"df = df.fillna('')","7cf53fa6":"df.isna().sum()","645d7e49":"df['Reviews'] = df['Review_Title'].map(str) + df['Review'].map(str)","ab2579d6":"df.drop(['Review_Title','Review'], axis=1, inplace=True)","e8bb6e14":"\ndf['Reviews'] = df['Reviews'].str.replace('[^\\w\\s]','')\n#https:\/\/stackoverflow.com\/questions\/39782418\/remove-punctuations-in-pandas\n\ndf['Reviews'] = df['Reviews'].str.lower()\n\ndf['Reviews'] = df['Reviews'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n\ndf['Reviews'] = df['Reviews'].str.replace('\\d+', '')\n\n\n\n\n","09b453a6":"lemma = WordNetLemmatizer()\ndf['Reviews'] = [lemma.lemmatize(line) for line in df['Reviews']]","8758efb7":"df.head(10)","5ab636f1":"Ratings = df[['Rating']]","ed8f1140":"max_num_features = 100000\ntokenizer = Tokenizer(num_words=max_num_features)\ntokenizer.fit_on_texts(df['Reviews'])\nlist_tokenized_train = tokenizer.texts_to_sequences(df['Reviews'])\n\nmaxlen = 200\nX_tokenized = pad_sequences(list_tokenized_train, maxlen=maxlen)","8ca00d3c":"\nRatings2 = np.zeros((Ratings.shape[0], 5))\nRatings2[np.arange(Ratings.shape[0]), Ratings['Rating']-1] = 1\nRatings2 = pd.DataFrame(Ratings2,columns=['1','2','3','4','5'],dtype='int64')\n\n\nembed_size = 128\nnlp = Sequential()\nnlp.add(Embedding(max_num_features, embed_size))\nnlp.add(Bidirectional(LSTM(128, return_sequences = True)))\nnlp.add(GlobalMaxPool1D())\nnlp.add(Dense(20, activation=\"relu\"))\nnlp.add(Dense(5, activation=\"sigmoid\"))\nnlp.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nbatch_size = 100\nepochs = 10\nEarlyStopping(restore_best_weights=True)\nnlp.fit(X_tokenized,Ratings2, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n\ntf.keras.utils.plot_model(nlp)","b8e63bff":"max_num_features = 100000\ntokenizer = Tokenizer(num_words=max_num_features)\ntokenizer.fit_on_texts(df['Reviews'])\nlist_tokenized_train = tokenizer.texts_to_sequences(df['Reviews'])\n\nmaxlen = 170\nX_tokenized = pad_sequences(list_tokenized_train, maxlen=maxlen)\nRatings2 = df['Recommended']\n\nembed_size = 128\nnlp2 = Sequential()\nnlp2.add(Embedding(max_num_features, embed_size))\nnlp2.add(Bidirectional(LSTM(32, return_sequences = True)))\nnlp2.add(GlobalMaxPool1D())\nnlp2.add(Dense(20, activation=\"relu\"))\nnlp2.add(Dense(1, activation=\"sigmoid\"))\nnlp2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nbatch_size = 100\nepochs = 10\nEarlyStopping(restore_best_weights=True)\nnlp2.fit(X_tokenized,Ratings2, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n\ntf.keras.utils.plot_model(nlp2)","7a21bf03":"df_test=pd.read_csv('..\/input\/final-project\/test.csv')","4ae9fe19":"df_test = df_test[['Id', 'Review_Title', 'Review']]\n\n# fill the nan in features with ''(empty string)\ndf_test = df_test.fillna('like')\n\n# concatenate\ndf_test['Reviews'] = df_test['Review_Title'] + ' ' + df_test['Review']\n\ndf_test = df_test[['Id','Reviews']]\n\n\ndf['Reviews'] = df['Reviews'].str.replace('[^\\w\\s]','')\n#https:\/\/stackoverflow.com\/questions\/39782418\/remove-punctuations-in-pandas\n\ndf['Reviews'] = df['Reviews'].str.lower()\n\ndf['Reviews'] = df['Reviews'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n\ndf['Reviews'] = df['Reviews'].str.replace('\\d+', '')\n\n\nlist_tokenized_test = tokenizer.texts_to_sequences(df_test['Reviews'])\n\nmaxlen = 200\nX_test = pad_sequences(list_tokenized_test, maxlen=maxlen)","7fc0816b":"rating=nlp.predict(X_test)","88ff537a":"rating=rating.argmax(axis=1)+1\nrating","29e9c073":"recommended=nlp2.predict(X_test)","09240b1e":"recommended=recommended.round()\nrecommended=recommended.astype('int64')\nrecommended","0fd236da":"final=df_test","f12ca90f":"final['Rating']=rating\nfinal['Recommended']=recommended","b34adf98":"final[['Id', 'Rating', 'Recommended']].to_csv('ElnurTrial9.csv', index=False)","2f5d78e7":"we see that there are some missing data. Since we cannot know if these reviews are positive or not, let's fill them with empty string. Logically most of the reviews without comment should be high star numbers","f522307c":"let's now see how our data has changed","5d85753b":"now we need to choose, if we want to use stemmer or lemmatizer. I think they will not make much difference, so I will just use Lemmatizer","9220893d":"now we need to keep only the review and target related fields. Because other parts have not too big contribution to predict the results.","b80fac29":"now that we have new column, we don't need Review_Title and Review columns. Let's drop them","0dc9b334":"now that our data is filled, and there are no NaN values, lets move on to bag of words and training. Let us merge review_title and review columns because they are almost the same in importance and similarity","4490c954":"let us check the data again, if it is correct","d3fc5908":"now let us use tokenizer to utilize bag of words technique","e005403f":"cleaning up the text by removing the punctuation and lowercaseing the words will help our model to make better guesses."}}