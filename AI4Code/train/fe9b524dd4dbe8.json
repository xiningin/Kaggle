{"cell_type":{"d860422d":"code","5c0f1475":"code","0ec4e9cd":"code","c4f5a17d":"code","78c2199a":"code","c7085557":"code","7bf4b2c7":"code","5d5bff06":"code","8e0ae0c1":"code","57dd69e9":"code","36a4a86e":"code","ffca001b":"code","095aafc2":"code","f3e26b76":"code","b6a36d23":"code","52c59080":"code","53ab845d":"code","6f506e72":"code","c740f565":"code","fd14c069":"code","0a70dfc2":"code","f3d766fa":"code","1a4ea637":"code","09b070ed":"code","bf109192":"code","8b1daf1f":"markdown"},"source":{"d860422d":"#Goal : create word vector from Game of Throne dataset\nfrom __future__ import absolute_import, division, print_function # for dependency python 2 to 3\n# For word encoding\nimport codecs\n# Regex\nimport glob\n# Concurrency\nimport multiprocessing\n# Dealing with operating system like reading files\nimport os\n# Pretty Printing\nimport pprint\n# Regular Expression\nimport re\n# Natural Language  Toolkit\nimport nltk\nfrom nltk.corpus import stopwords\n# WOrd 2 vec\nfrom gensim.models import Word2Vec\n# Dimensional Reductionality\nimport sklearn.manifold\n#math\nimport numpy as np\n#plotting\nimport matplotlib.pyplot as plt\n#data processing \nimport pandas as pd\n# Data Visualization\nimport seaborn as sns\n\n%matplotlib inline","5c0f1475":"book_filenames = sorted(glob.glob(\"..\/input\/*.txt\"))","0ec4e9cd":"print(\"Books Found :\")\nbook_filenames","c4f5a17d":"corpus_raw = \"\"\nfor book_filename in book_filenames:\n    print(\"Reading '{0}'...\".format(book_filename))\n    with open(book_filename, \"rb\") as infile:\n        corpus_raw += str(infile.read())\n        \n        print(\"Corpus is now {0} characters long\". format(len(corpus_raw)))\n        print()\n        ","78c2199a":"text = corpus_raw\n\n# Preprocessing the data\ntext = re.sub(r'\\[[0-9]*\\]',' ',text)\ntext = re.sub(r'\\s+',' ',text)\ntext = text.lower()\ntext = text.strip()\ntext = re.sub(r'\\d',' ',text)\ntext = re.sub(r'\\s+',' ',text)","c7085557":"# Preparing the dataset\nsentences = nltk.sent_tokenize(text)","7bf4b2c7":"sentences","5d5bff06":"sentences = [nltk.word_tokenize(sentence) for sentence in sentences]","8e0ae0c1":"sentences","57dd69e9":"for i in range(len(sentences)):\n    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]","36a4a86e":"sentences","ffca001b":"# Training the Word2Vec model\nmodel = Word2Vec(sentences, min_count=1)","095aafc2":"model","f3e26b76":"words = model.wv.vocab","b6a36d23":"words","52c59080":"# Finding Word Vectors\nvector = model.wv['harry']","53ab845d":"vector","6f506e72":"# Most similar words\nsimilar = model.wv.most_similar('mcgonagall')","c740f565":"similar","fd14c069":"#distance, similarity, and ranking\ndef nearest_similarity_cosmul(start1, end1, end2):\n    similarities = model.wv.most_similar_cosmul(\n        positive=[end2, start1],\n        negative=[end1]\n    )\n    start2 = similarities[0][0] \n    print(\"{start1} is related to {end1}, as {start2} is related to {end2}\".format(**locals()))\n    return start2","0a70dfc2":"nearest_similarity_cosmul(\"harry\", \"professor\", \"snape\")\nnearest_similarity_cosmul(\"dumbledore\", \"elder\", \"wand\")\nnearest_similarity_cosmul(\"lupin\", \"james\", \"sirius\")","f3d766fa":"X = model[model.wv.vocab]","1a4ea637":"X","09b070ed":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\nplt.scatter(result[:, 0], result[:, 1])","bf109192":"words = list(model.wv.vocab)\nplt.figure(figsize=(12,5))\nfor i, word in enumerate(words):\n    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\nplt.show()","8b1daf1f":"**Plot Word Vectors Using PCA**"}}