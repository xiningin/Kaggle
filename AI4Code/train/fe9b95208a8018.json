{"cell_type":{"d9cb7e10":"code","bd8dc8d9":"code","5275bcc8":"code","45bd2056":"code","58a47a30":"code","5cc94acf":"code","ea0bac01":"code","bc387cb6":"code","37ec4f64":"code","5d235510":"code","878dcf50":"code","48fe9251":"code","4c668402":"code","e4fda004":"code","e224ef0b":"code","5554dac7":"code","5c5456bc":"code","cb92a106":"code","85f5710f":"code","ffe633ad":"code","6810e51c":"code","02c0d32c":"code","cc747bae":"markdown","2e49a904":"markdown"},"source":{"d9cb7e10":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bd8dc8d9":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","5275bcc8":"import re\nimport string\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModel","45bd2056":"print(tf.__version__)","58a47a30":"import nltk\nfrom nltk.stem import WordNetLemmatizer; wnl = WordNetLemmatizer()","5cc94acf":"from wordcloud import WordCloud, STOPWORDS\nwordcloud = WordCloud(width = 1600, height = 900, background_color = 'white', stopwords = STOPWORDS).generate(' '.join(train.text.tolist()).lower())\nplt.figure(figsize=(12, 7))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","ea0bac01":"def clean(text) :\n    text = ' '.join([wnl.lemmatize(k) for k in text.split() if k.lower() not in STOPWORDS])\n    text = re.sub('@\\S+', ' ', str(text).lower()).strip()\n    text = re.sub('https?:\\\/\\\/\\S*', ' ', str(text)).strip()\n    text = re.sub('&amp', ' ', str(text)).strip()\n    text = re.sub('[^A-Za-z]+', ' ', str(text)).strip()\n    text = re.sub('(^|\\s+)[a-z]($|\\s+)', ' ', str(text)).strip()\n    text = re.sub('(^|\\s+)rt($|s+)', ' ', str(text)).strip()\n    text = re.sub('\\s+', ' ', str(text)).strip()\n    return text","bc387cb6":"from tqdm.auto import tqdm; tqdm.pandas()\ntrain['text'] = train['text'].progress_apply(clean)\ntest['text'] = test['text'].progress_apply(clean)","37ec4f64":"train","5d235510":"wordcloud = WordCloud(width = 1600, height = 900, background_color = 'white', stopwords = STOPWORDS).generate(' '.join(train.text.tolist()).lower())\nplt.figure(figsize=(12, 7))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","878dcf50":"model_name = '..\/input\/huggingface-bert\/bert-base-uncased'\nMAXLEN = 64\nBATCH = 16","48fe9251":"tokenizer = AutoTokenizer.from_pretrained(model_name)\ntrain_tok = tokenizer.batch_encode_plus(train['text'].tolist(), max_length = MAXLEN,\n                                        truncation = True, padding = 'max_length',\n                                        add_special_tokens = True, return_tensors = 'np')\ny_train = np.array(train.target.values)\ndataset = tf.data.Dataset.from_tensor_slices((train_tok['input_ids'], train_tok['attention_mask'],\n                                              y_train))\ndef map_func(input_ids, masks, label) :\n    return {\n        'input_ids' : input_ids,\n        'attention_mask' : masks\n    }, label\ndataset = dataset.map(map_func)\ndataset = dataset.shuffle(2500).batch(batch_size = BATCH, drop_remainder = True)\nsplit = 0.7\nsize = int((train_tok.input_ids.shape[0] \/\/ BATCH) * split)\ntrain_ds = dataset.take(size)\nval_ds = dataset.skip(size)","4c668402":"model = TFAutoModel.from_pretrained(model_name)\ninput_ids = tf.keras.layers.Input(shape = (MAXLEN, ), name='input_ids', dtype = 'int32')\natt = tf.keras.layers.Input(shape = (MAXLEN, ), name = 'attention_mask', dtype = 'int32')\nembed = model(input_ids, attention_mask = att)[0]\nembed = embed[:, 0, :]\nx = tf.keras.layers.Dense(512, activation = 'relu')(embed)\ny = tf.keras.layers.Dense(1, activation = 'sigmoid')(x)\nbert_model = tf.keras.Model(inputs = [input_ids, att], outputs = y)\noptimizer = tf.keras.optimizers.Adam(lr = 1e-5)\nloss = tf.keras.losses.BinaryCrossentropy()\nacc = tf.keras.metrics.BinaryAccuracy()\nbert_model.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])\nhistory = bert_model.fit(train_ds,\n                         validation_data = val_ds,\n                         epochs = 10,\n                         batch_size = BATCH)","e4fda004":"acc = history.history['accuracy']\nloss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']","e224ef0b":"plt.figure(figsize = (12, 7))\nplt.subplot(3, 1, 1)\nplt.plot(acc, label = 'train acc')\nplt.plot(val_acc, label = 'val acc')\nplt.title('Accuracy')\nplt.legend()\n\nplt.subplot(3, 1, 2)\nplt.plot(loss, label = 'train loss')\nplt.plot(val_loss, label = 'val loss')\nplt.title('Loss')\nplt.legend()\nplt.show()","5554dac7":"bert_model.evaluate(val_ds)","5c5456bc":"a = np.array([1, 2, 3, 5, 7 ,9])\nnp.argmax(a)","cb92a106":"tok_test = tokenizer.batch_encode_plus(test.text.tolist(),\n                                       max_length = MAXLEN,\n                                       truncation = True,\n                                       padding = 'max_length',\n                                       return_tensors = 'tf')\ntest_ = {'input_ids' : tok_test['input_ids'], 'attention_mask' : tok_test['attention_mask']}\npred = bert_model.predict(test_)\npred = [i for j in pred for i in j]","85f5710f":"pred_int = []\nfor i in pred :\n    if i > 0.55 :\n        pred_int.append(1)\n    else :\n        pred_int.append(0)","ffe633ad":"sample = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nsample","6810e51c":"sample['target'] = pred_int\nsample","02c0d32c":"sample.to_csv('submission.csv', index = False)","cc747bae":"And it was scored 0.8099999","2e49a904":"Inspired from [Faressayah's Notebook](https:\/\/www.kaggle.com\/faressayah\/sentiment-model-with-tensorflow-transformers)"}}