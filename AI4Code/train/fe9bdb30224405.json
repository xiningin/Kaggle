{"cell_type":{"d848fcb7":"code","51bf916d":"code","5c885f9a":"code","57bce0b9":"code","2f1b3e78":"code","3bc66aea":"code","df0a29fc":"code","8bd6ef39":"code","1fd04115":"code","ce44b4e5":"code","152ac6f0":"code","796e8153":"code","a98394bc":"code","03f1fcb6":"code","1d420d95":"markdown","5ba1f639":"markdown"},"source":{"d848fcb7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n!pip install Livelossplot\nfrom livelossplot import PlotLossesKeras\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","51bf916d":"\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom PIL import Image\nimport scipy\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.losses import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.preprocessing.image import *\nfrom tensorflow.keras.utils import *\n# import pydot\n\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import *\nimport tensorflow.keras.backend as K\n\nfrom tqdm import tqdm, tqdm_notebook\nfrom colorama import Fore\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom glob import glob\nfrom skimage.io import *\n%config Completer.use_jedi = False\nimport time\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\nimport xgboost as xgb\n\nprint(\"All modules have been imported\")","5c885f9a":"info=pd.read_csv(\"..\/input\/mias-mammography\/Info.txt\",sep=\" \")\ninfo=info.drop('Unnamed: 7',axis=1)\ninfo.SEVERITY.fillna(0)","57bce0b9":"sns.set_style('darkgrid')\nfig, (ax1, ax2) = plt.subplots(1,2,figsize=(15,5))\nsns.barplot(x=info.BG.unique(),y=info.BG.value_counts(),palette='Blues_r',ax=ax1)\nsns.barplot(x=info.CLASS.unique(),y=info.CLASS.value_counts(),palette='Blues_r',ax=ax2)\n","2f1b3e78":"from PIL import Image\nimport glob\nx= []\nfor filename in sorted(glob.glob(\"..\/input\/mias-mammography\/all-mias\/*.pgm\")): \n    img=cv2.imread(filename)\n    img =cv2.resize(img,(224, 224))\n    x.append(img)\nfig=plt.figure(figsize=(15,15))\ncolumns = 3\nrows = 3\nfor i in range(1, columns*rows +1):\n    img = np.random.randint(10)\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(x[i])\nplt.show()","3bc66aea":"# Image Augmentation\nno_angles = 360\nurl = '\/kaggle\/input\/mias-mammography\/all-mias\/'\n\ndef save_dictionary(path,data):\n        print('saving catalog...')\n        #open('u.item', encoding=\"utf-8\")\n        import json\n        with open(path,'w') as outfile:\n            json.dump(str(data), fp=outfile)\n        # save to file:\n        print(' catalog saved')","df0a29fc":"# train_test_split_datagen=ImageDataGenerator(\"augmentations such as flip,brightness range,etc....\")\n# val_datagen=ImageDataGenerator(\"augmentations such as flip,brightness range,etc....\")\n# test_datagen=ImageDataGenerator(\"augmentations such as flip,brightness range,etc....\")\ndef read_image():\n        print(\"Reading images\")\n        import cv2\n        info = {}\n        for i in range(322):\n            if i<9:\n                image_name='mdb00'+str(i+1)\n            elif i<99:\n                image_name='mdb0'+str(i+1)\n            else:\n                image_name = 'mdb' + str(i+1)\n            image_address= url+image_name+'.pgm'\n            img = cv2.imread(image_address,1)\n            img = cv2.resize(img, (224,224))\n            rows, cols,channel = img.shape\n            info[image_name]={}\n            for angle in range(0,no_angles,8):\n                M = cv2.getRotationMatrix2D((cols \/ 2, rows \/ 2), angle, 1) \n                img_rotated = cv2.warpAffine(img, M, (cols, rows))\n                info[image_name][angle]=img_rotated\n        return (info)\n","8bd6ef39":"def read_lable():\n    print(\"Reading labels\")\n    filename = url+'Info.txt'\n    text_all = open(filename).read()\n    #print(text_all)\n    lines=text_all.split('\\n')\n    info={}\n    for line in lines:\n        words=line.split(' ')       \n        if len(words)>3:\n            if (words[3] == 'B'):\n                info[words[0]] = {}\n                for angle in range(0,no_angles,8):\n                    info[words[0]][angle] = 0\n            if (words[3] == 'M'):\n                info[words[0]] = {}\n                for  angle in range(0,no_angles,8):\n                    info[words[0]][angle] = 1\n    return (info)","1fd04115":"import numpy as np\nlable_info=read_lable()\nimage_info=read_image()\nids=lable_info.keys() \ndel lable_info['Truth-Data:']\nX=[]\nY=[]\nfor id in ids:\n    for angle in range(0,no_angles,8):\n        X.append(image_info[id][angle])\n        Y.append(lable_info[id][angle])\nX=np.array(X)\nY=np.array(Y)\nY=to_categorical(Y,2)\nx_train, x_test1, y_train, y_test1 = train_test_split(X, Y, test_size=0.3, random_state=42)\nx_val, x_test, y_val, y_test = train_test_split(x_test1, y_test1, test_size=0.3, random_state=42)\nprint(len(x_train),len(x_val),len(x_test))","ce44b4e5":"#Callbacks\nc2=tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\",\n    min_delta=0,\n    patience=6,\n    mode=\"auto\",\n    baseline=None,\n    restore_best_weights=True,\n)\n\nc3=tf.keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.1,\n    patience=6,\n    mode=\"auto\",\n    min_delta=0.0001,\n    cooldown=0,\n    min_lr=0.001\n)\nnClasses=2","152ac6f0":"base_Neural_Net= DenseNet201(input_shape=(224,224,3), weights='imagenet', include_top=False)\nmodel=Sequential()\nmodel.add(base_Neural_Net)\nmodel.add(Flatten())\nmodel.add(BatchNormalization())\nmodel.add(Dense(256,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2,activation='softmax'))\n\nfor layer in base_Neural_Net.layers:\n    layer.trainable = False","796e8153":"c1=PlotLossesKeras()\nmodel.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy','AUC'])\nmodel.fit(x_train,y_train,epochs=10,callbacks=[c1,c3],batch_size=16)","a98394bc":"model.evaluate(x_val,y_val,callbacks=[c2,c3],batch_size=16)","03f1fcb6":"print(\"Performance Report:\")\ny_pred9=model.predict_classes(x_test)\ny_test9=[np.argmax(x) for x in y_test]\ny_pred_prb9=model.predict_proba(x_test)\ntarget=[\"B\",\"M\"]\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test9, y_pred9),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test9, y_pred9, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test9,y_pred9, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test9, y_pred9, average='weighted'),4))\nprint('ROC AUC Score is :', np.round(metrics.roc_auc_score(y_test9, y_pred9,multi_class='ovo', average='weighted'),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test9, y_pred9,target_names=target))","1d420d95":"# DenseNet-201","5ba1f639":"\n# Validation Strategy:\n\n## Train Data Size= 0.7*Total_Data\n## Validation Data Size= 0.21* Total_Data\n## Test Data Size= 0.09 * Total_Data\n\n# Augmentaions Used:\n## Each Image is rotated through 45 angles\n"}}