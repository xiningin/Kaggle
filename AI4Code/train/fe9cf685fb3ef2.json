{"cell_type":{"c35220a7":"code","b8a8ddd2":"code","b65d4679":"code","27f43be8":"code","f8758f02":"code","2b966b92":"code","f4fb2bdb":"code","27ede255":"code","c1d41719":"code","575849cd":"code","075e1a49":"code","43eb310b":"code","84412420":"code","a7c311cd":"code","7eb7e992":"code","20463658":"code","4633d616":"code","62ac3bf6":"markdown","9f3af7a5":"markdown","160ec39d":"markdown","0f1e66be":"markdown","b0562058":"markdown","40b05832":"markdown","82cbcbb1":"markdown"},"source":{"c35220a7":"import seaborn as sns\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b8a8ddd2":"path = \"\/kaggle\/input\/vertebralcolumndataset\/\"\ndf1 = pd.read_csv(path+'column_2C.csv', delimiter=',')\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","b65d4679":"df1.head(5)","27f43be8":"sns.pairplot(df1, hue=\"class\", size=3, diag_kind=\"kde\")","f8758f02":"df1['class'] = df1['class'].map({'Normal': 0, 'Abnormal': 1})","2b966b92":"from sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score\nX = df1[['pelvic_incidence','pelvic_tilt','lumbar_lordosis_angle','sacral_slope', 'pelvic_radius','degree_spondylolisthesis']]\nY = df1['class']\n# split data into train and test sets\nseed = 2020\ntest_size = 0.33\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)","f4fb2bdb":"import xgboost as xgb\n\n# fit model no training data\nmodel = xgb.XGBClassifier()\nmodel.fit(X_train, y_train)\n# save model to file\nmodel.save_model(\"model.bst\")","27ede255":"# make predictions for test data\ny_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","c1d41719":"# make predictions proba for test data\ny_pred_prob = model.predict_proba(X_test)","575849cd":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\n\nfalse_pos_rate, true_pos_rate, proba = roc_curve(y_test, y_pred_prob[:, -1])\nplt.figure()\nplt.plot([0,1], [0,1], linestyle=\"--\") # plot random curve\nplt.plot(false_pos_rate, true_pos_rate, marker=\".\", label=f\"AUC = {roc_auc_score(y_test, predictions)}\")\nplt.title(\"ROC Curve\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlabel(\"False Positive Rate\")\nplt.legend(loc=\"lower right\")","075e1a49":"num_classes = 2\n\nfpr = dict()\ntpr = dict()\nthresholds = dict()\nroc_auc = dict()\n# Compute False Positive and True Positive Rates for each class\nfor i in range(num_classes):\n    fpr[i], tpr[i], thresholds[i] = roc_curve(y_test, y_pred_prob[:, -1], drop_intermediate=False)\n    roc_auc[i] = auc(fpr[i], tpr[i])","43eb310b":"J_stats = [None]*num_classes\nopt_thresholds = [None]*num_classes\n\n# Compute Youden's J Statistic for each class\nfor i in range(num_classes):\n    J_stats[i] = tpr[i] - fpr[i]\n    opt_thresholds[i] = thresholds[i][np.argmax(J_stats[i])]\n    print('Optimum threshold for classe ',i,': '+str(opt_thresholds[i]))\n    ","84412420":"optimal_proba_cutoff = sorted(list(zip(np.abs(true_pos_rate - false_pos_rate), y_pred_prob[:, -1])), key=lambda i: i[0], reverse=True)[0][1]\nroc_predictions = [1 if i >= optimal_proba_cutoff else 0 for i in y_pred_prob[:,-1]]","a7c311cd":"optimal_proba_cutoff","7eb7e992":"print(\"Accuracy Score Before Thresholding: {}\".format(accuracy_score(y_test, predictions)))\nprint(\"Precision Score Before Thresholding: {}\".format(precision_score(y_test, predictions)))\nprint(\"Recall Score Before Thresholding: {}\".format(recall_score(y_test, predictions)))\nprint(\"F1 Score Before Thresholding: {}\".format(f1_score(y_test, predictions)))\nprint(\"ROC AUC Score: {}\".format(roc_auc_score(y_test, y_pred_prob[:, -1])))","20463658":"print(\"Accuracy Score Before and After Thresholding: {}, {}\".format(accuracy_score(y_test, predictions), accuracy_score(y_test, roc_predictions)))\nprint(\"Precision Score Before and After Thresholding: {}, {}\".format(precision_score(y_test, predictions), precision_score(y_test, roc_predictions)))\nprint(\"Recall Score Before and After Thresholding: {}, {}\".format(recall_score(y_test, predictions), recall_score(y_test, roc_predictions)))\nprint(\"F1 Score Before and After Thresholding: {}, {}\".format(f1_score(y_test, predictions), f1_score(y_test, roc_predictions)) )","4633d616":"y_actual = pd.Series(y_test, name='Actual')\ny_predict_tf = pd.Series(roc_predictions, name='Predicted')\ndf_confusion = pd.crosstab(y_actual, y_predict_tf, rownames=['Actual'], colnames=['Predicted'], margins=True)\nprint (df_confusion)","62ac3bf6":"Running this example summarizes the performance of the model on the test set","9f3af7a5":"Bivariate relationship between the features","160ec39d":"Modeling","0f1e66be":"XGBoost","b0562058":"Confusion Matrix of Model (After Thresholding) \n\nWe can see that the new predictions have fewer false positives in the process. Recall score have improved.","40b05832":"End Notebook","82cbcbb1":"Obtain Optimal Probability Thresholds with ROC Curve \n\nIn this notebook, we will be using the Youden's J statistic, that is the distance between the ROC curve and the \"chance line\" - the ROC curve of a classifier that guesses randomly. The optimal threshold is that which maximises the J Statistic. We will be using the Youden's J statistic to obtain the optimal probability threshold and this method gives equal weights to both false positives and false negatives.\n\n"}}