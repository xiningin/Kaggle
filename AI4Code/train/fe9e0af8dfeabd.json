{"cell_type":{"f460754c":"code","4d86ea49":"code","11e6bc85":"code","e9026c7c":"code","a9e897ec":"code","0cfc6185":"code","00183d69":"code","102e307c":"code","97a63141":"code","46fb57ea":"code","2532e9ab":"code","0f0c7c2f":"code","0d87c460":"code","8625c5e6":"code","5c32866a":"code","32b4d376":"code","ada9fcea":"code","7bfb3fae":"code","4fa069f9":"code","813c740c":"code","a4f7c82c":"code","d46f1b2d":"code","4f0708a0":"code","a21277b6":"code","13fb534a":"code","4b504b88":"code","46f9215b":"code","7cf9dbf7":"code","7f67027c":"code","c663b290":"code","cf29cb34":"code","b7694115":"code","03d7bce0":"code","8c827d47":"code","8603c39c":"code","c23aa205":"code","9f3ff6e1":"code","dc25bc40":"code","dce704dc":"code","cb524c24":"code","cf9ef6d7":"code","0fccd8b7":"code","5ea82c59":"code","f4588e27":"code","5ebe0473":"code","cb3267a7":"code","ac9637ae":"code","29d9a263":"code","386bb47a":"code","e3c41eba":"code","26ece394":"code","1691f4a3":"code","8b7abb9a":"code","201a6c6e":"code","d24b2e87":"code","e257ef06":"code","9c43af16":"code","1dc58281":"code","f9e98542":"code","9a384015":"code","4036e992":"code","95c99042":"code","067f0ac0":"code","72420371":"code","d1700083":"code","d9b7867f":"code","9d211970":"code","2e86607c":"code","bd216782":"code","9f0ce8d8":"code","1c1273cc":"code","f999172e":"code","1bba80c4":"code","c23f5820":"code","bbba6e85":"code","62e82e4b":"code","4a522779":"code","7c3978ee":"code","34f246d9":"code","7f38d19f":"code","3aea444d":"code","69fe2058":"code","47b61048":"code","d9e315bd":"code","f3dbbf3e":"code","13d1220c":"code","95f181a1":"code","9907c7e2":"code","d9c104e9":"code","9927c82b":"code","b27309be":"code","6401fb6f":"code","a53ad432":"code","1f82f456":"code","6d3254cd":"code","f3f48418":"code","6610de34":"code","b0ef5a6b":"code","034a5122":"code","381e6454":"code","29e64695":"code","088696b3":"code","53bd5a66":"code","666625d1":"code","a1d6508a":"code","1d839350":"markdown","7f9330f0":"markdown","a9f6065b":"markdown","c9f3e54f":"markdown","835c0540":"markdown","48421ea5":"markdown","b714b6fe":"markdown","a258e799":"markdown","6ec6dce6":"markdown","1fa7697f":"markdown","4db167ed":"markdown","e4cf74d6":"markdown","2b522512":"markdown","7c581bb8":"markdown","879ad693":"markdown","fb33539e":"markdown","dea8e48f":"markdown","d864ba85":"markdown","0fbe1603":"markdown","20cd6ec9":"markdown","45038571":"markdown","b561e66f":"markdown","0d97f728":"markdown","f8617e9c":"markdown","5b22e2e0":"markdown","924d1ce8":"markdown","f07d88bb":"markdown","e2219fcb":"markdown","271131a0":"markdown","f8522f13":"markdown","a2765a17":"markdown","a3f2df6c":"markdown","0f8f8d60":"markdown","6b1f452b":"markdown","d9a76cab":"markdown","84d2fe35":"markdown","87f66142":"markdown","f6e7c5ac":"markdown","0a975c8f":"markdown","3d37fb2d":"markdown","783181f9":"markdown","36888a26":"markdown","675f07f8":"markdown","22b11ef6":"markdown","0df7c1b3":"markdown","f4762c60":"markdown","15b3575c":"markdown","80a6ec9f":"markdown","6bdc6590":"markdown","f321a18d":"markdown","1b4e4bf0":"markdown","adf7607d":"markdown","b8835de4":"markdown","cd7e54a9":"markdown","7a157cd5":"markdown","02a88cf9":"markdown","7e940931":"markdown","9bdb4059":"markdown","8a210718":"markdown","0f1f39c4":"markdown","f7360c0e":"markdown","ccc726dd":"markdown","b02cf94c":"markdown","9196d6f8":"markdown","0da1c4e8":"markdown","9d239351":"markdown","1f548f39":"markdown","9aab51bb":"markdown","359fdfe8":"markdown","d4ae6292":"markdown","871e1522":"markdown","93042f48":"markdown","baeacf20":"markdown"},"source":{"f460754c":"# Python \u22653.5 is required\nimport sys\nassert sys.version_info >= (3, 5)\n\n# Scikit-Learn \u22650.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\n\n# Common imports\nimport numpy as np\nimport os\n\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport ast\n\n%matplotlib inline \n#plotting directly without requering the plot()\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n\npd.set_option('display.max_columns', 500) #fixing the number of rows and columns to be displayed\npd.set_option('display.max_rows', 500)\n\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4d86ea49":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","11e6bc85":"# Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","e9026c7c":"dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n\ndef text_to_dict(df):\n    for column in dict_columns:\n        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n    return df\n        \ntrain = text_to_dict(train)\ntest = text_to_dict(test)","a9e897ec":"print(train.shape, test.shape) #looking at the shape of our dataframes","0cfc6185":"train.head() #taking a look at the first entries","00183d69":"##Merging the train and test dataset in order to have more data to train our model.\n\ntrain['source']='train' #creating a label for the training and testing set\ntest['source']='test'\n\ndata = pd.concat([train, test],ignore_index=True)\nprint (train.shape, test.shape, data.shape) #printing the shape","102e307c":"print(\"Belongs to a collection or not:\\n%s\" % (data['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0).value_counts()))","97a63141":"## Function that receives the dataset, reads the dictionary and create a new boolean column for each top 20 common value\ndef dict_one_hot_code(dataset,name_column, name_new_column):\n    list_of_values = list(dataset[name_column].apply(lambda x: [i['name'] for i in x] if x != {} else []).values) #create a list of each occurence of a given name\n    Counter([i for j in list_of_values for i in j]).most_common(20)\n    top_list_values = [m[0] for m in Counter([i for j in list_of_values for i in j]).most_common(20)]\n    for g in top_list_values: \n        dataset[name_new_column + \"_\" + g] = dataset[name_column].apply(lambda x: 1 if g in str(x) else 0)\n        \ndef counting_number_of_values_dict(dataset, name_column):\n    return dataset[name_column].apply(lambda x: len(x) if x!= {} else 0)","46fb57ea":"data['collection_name'] = data['belongs_to_collection'].apply(lambda x: x[0]['name'] if x != {} else \"None\")\ndata['is_part_of_collection'] = data['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0) \n#feature engineering a new column that has the len of the belongs_to_collection columns, by doing so, you attribute a weight to each particular movie, or zero if it doesn't belong to any\n\ndata.drop('belongs_to_collection', axis = 1, inplace=True)","2532e9ab":"data.head() #checking the changes ","0f0c7c2f":"#count the number of genres\ndata['genres'].apply(lambda x: x[0]['name'] if x != {} else 0).value_counts() #this counts for us the number of dicts containing the genre in our dataset and count the number of occurences","0d87c460":"data['genre'] = data['genres'].apply(lambda x: x[0]['name'] if x != {} else \"None\") # exactly the same code used for the collection belongs.\ndata.drop('genres', axis=1, inplace=True)","8625c5e6":"data['cast'][0]","5c32866a":"data['cast'][0][0]","32b4d376":"list_of_cast_names = list(data['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values) #create a list of each occurence of a given name\nCounter([i for j in list_of_cast_names for i in j]).most_common(20) #count every instance of each actor and sum them all to return back the 20 most common instances","ada9fcea":"#and let's also take a look at the total number of cast member for each movie\ndata['cast'].apply(lambda x: len(x) if x != {} else 0).value_counts()","7bfb3fae":"data['number_cast'] = counting_number_of_values_dict(data,'cast')","4fa069f9":"data.head()","813c740c":"list_of_cast_characters = list(data['cast'].apply(lambda x: [i['character'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_cast_characters for i in j]).most_common(15)","a4f7c82c":"top_cast_names = [m[0] for m in Counter([i for j in list_of_cast_names for i in j]).most_common(15)]\nfor g in top_cast_names: \n    data['cast_name_' + g] = data['cast'].apply(lambda x: 1 if g in str(x) else 0) #creating a boolean for the top 15 most common actors...","d46f1b2d":"data.head()","4f0708a0":"data.shape","a21277b6":"top_cast_characters = [m[0] for m in Counter([i for j in list_of_cast_characters for i in j]).most_common(15)]\nfor g in top_cast_characters:\n    data['cast_character_' + g] = data['cast'].apply(lambda x: 1 if g in str(x) else 0)","13fb534a":"data.drop('cast', axis=1, inplace=True)","4b504b88":"counting_number_of_values_dict(data,'crew').value_counts()","46f9215b":"data['crew_total'] = counting_number_of_values_dict(data, 'crew')","7cf9dbf7":"dict_one_hot_code(data,name_column='crew',name_new_column='crew_name')","7f67027c":"data.drop('crew', axis=1, inplace=True)","c663b290":"data.shape","cf29cb34":"data.head()","b7694115":"data['Keywords'][0]","03d7bce0":"counting_number_of_values_dict(data,'Keywords').value_counts()","8c827d47":"data['keywords_num'] = counting_number_of_values_dict(data,'Keywords')","8603c39c":"list_of_keywords = list(data['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)","c23aa205":"data['all_Keywords'] = data['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')","9f3ff6e1":"top_keywords = [m[0] for m in Counter([i for j in list_of_keywords for i in j]).most_common(30)]\nfor g in top_keywords:\n    data['keyword_' + g] = data['all_Keywords'].apply(lambda x: 1 if g in x else 0)","dc25bc40":"data.drop(['Keywords','all_Keywords'], axis=1, inplace=True)","dce704dc":"counting_number_of_values_dict(data,'spoken_languages').value_counts()","cb524c24":"languages = list(data['spoken_languages'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nCounter([i for j in languages for i in j]).most_common(15)","cf9ef6d7":"data['num_languages'] = counting_number_of_values_dict(data,'spoken_languages')\ndata['all_languages'] = data['spoken_languages'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_languages = [m[0] for m in Counter([i for j in languages for i in j]).most_common(30)]\nfor g in top_languages:\n    data['language_' + g] = data['all_languages'].apply(lambda x: 1 if g in x else 0)","0fccd8b7":"data.drop(['spoken_languages', 'all_languages'], axis=1, inplace=True)","5ea82c59":"data.shape #number of features has increased significantly","f4588e27":"counting_number_of_values_dict(data,'production_countries').value_counts()","5ebe0473":"data['num_countries'] = counting_number_of_values_dict(data, 'production_countries')\ndict_one_hot_code(data,name_column= 'production_countries', name_new_column='production_country')\n","cb3267a7":"data.head()","ac9637ae":"data.drop('production_countries', axis=1, inplace=True) #and finally, dropping the dictionary...","29d9a263":"counting_number_of_values_dict(data,'production_companies').value_counts()","386bb47a":"data['prod_companies_total_number'] = counting_number_of_values_dict(data,'production_companies')","e3c41eba":"dict_one_hot_code(data,'production_companies','all_production_companies')","26ece394":"data.drop('production_companies', axis=1, inplace=True)","1691f4a3":"data.shape","8b7abb9a":"data.head()","201a6c6e":"data['homepage'].isnull().value_counts() ","d24b2e87":"data['has_homepage'] = 0 #setting all the values to false as default\ndata.loc[data['homepage'].isnull() == False, 'has_homepage'] = 1\ndata.drop('homepage', axis=1, inplace=True)","e257ef06":"# Set the style of plots\nplt.style.use('fivethirtyeight')\n\nfig, ax = plt.subplots(2,2, figsize = (16, 6))\n\nplt.subplot(1, 2, 1)\nsns.countplot(y=data['genre'])\nplt.title('Distribution of genres')\nplt.subplot(1, 2, 2)\nsns.kdeplot(data['revenue'])\nplt.title('Distribution of revenues(nomal scale)')","9c43af16":"data['revenue_log'] = np.log1p(data['revenue']) #changing it to log1\nsns.kdeplot(data['revenue_log'])\nplt.title('Distribution of revenues in log scale')","1dc58281":"fig, ax = plt.subplots(figsize = (8, 10))\ndata['budget_log'] = np.log1p(data['budget'])\n\nplt.subplot(2,1,1)\nplt.scatter(x = data['budget'], y=data['revenue']);plt.ylabel('Revenue'); plt.xlabel('budget')\nplt.title('distribution of revenue X budget')\nplt.subplot(2,1,2)\nplt.scatter(x = data['budget_log'], y=data['revenue']);plt.ylabel('Revenue');plt.xlabel('budget')\nplt.title('distribution of revenue X budget(log scale)')\nplt.tight_layout()\n","f9e98542":"sns.kdeplot(data['budget_log'])\nplt.title('Density distribution of budget(log scale)')\nplt.tight_layout()","9a384015":"sns.catplot(x='has_homepage', y='revenue', data=data, aspect=1.5); plt.title('Has homepage X Revenue')\nsns.catplot(x='is_part_of_collection', y='revenue', data=data, aspect=1.5); plt.title('Part of colection X Revenue')","4036e992":"data_language = pd.DataFrame(data.groupby('original_language')[\"revenue\"].sum().sort_values(ascending=False).head(10))","95c99042":"data_language #showing only the first 10","067f0ac0":"data['is_english_original_language'] = 0 #setting all the values to false as default\ndata.loc[data['original_language'] == 'en', 'is_english_original_language'] = 1","72420371":"sns.catplot(x='is_english_original_language', y='revenue', data=data)\nplt.title('Revenue in relationship to english as original language')","d1700083":"data.drop('original_language', axis=1, inplace=True)","d9b7867f":"plt.scatter(x = data['popularity'], y=data['revenue'])\nplt.ylabel('revenue')\nplt.xlabel('popularity')\nplt.title('correlation of popularity X revenue')","9d211970":"data[['release_month','release_day','release_year']]=data['release_date'].str.split('\/',expand=True).replace(np.nan, -1).astype(int) #getting the month year and day using the string split function and the \/ as a delimiter; eg: 5\/25\/2015 -> month 5\/ day 25 \/ year 2015\ndata.loc[ (data['release_year'] <= 19) & (data['release_year'] < 100), \"release_year\"] += 2000 ## some rows have 4 digits for the year instead of 2, so the release year < 100 and > 100 is checking that\ndata.loc[ (data['release_year'] > 19)  & (data['release_year'] < 100), \"release_year\"] += 1900\n\nreleaseDate = pd.to_datetime(data['release_date']) #using the pandas to_datetime function to format the data, get a Series,  and store it in a variable that is gonna be used later to get the day of week and quarter\ndata['release_dayofweek'] = releaseDate.dt.dayofweek\ndata['release_quarter'] = releaseDate.dt.quarter","2e86607c":"plt.figure(figsize=(20, 8))\nplt.scatter(x='release_year', y='revenue', data=data); plt.title('Revenue X year released');","bd216782":"plt.figure(figsize=(15, 8))\nsns.countplot(data['release_dayofweek']); plt.title('Days of the week, from 0 - sunday,  to 6 - saturday')","9f0ce8d8":"## -1 is the nan values in the original data, that has been replaced by -1\nplt.figure(figsize=(15, 8))\nsns.countplot(data['release_month']); plt.title('Distribution of movies by month')","1c1273cc":"plt.figure(figsize=(15, 8))\nsns.countplot(data['release_day']); plt.title('Distribution of movies by days in a month')","f999172e":"plt.figure(figsize=(20, 10))\nplt.subplot(1, 3, 1)\nplt.hist(data['runtime'].fillna(0) \/ 60, bins=40) #filling the null values with 0\nplt.title('Distribution of length of film in hours');\nplt.subplot(1, 3, 2)\nplt.scatter(data['runtime'].fillna(0), data['revenue'])\nplt.title('runtime vs revenue');\nplt.subplot(1, 3, 3)\nplt.scatter(data['runtime'].fillna(0), data['popularity'])\nplt.title('runtime vs popularity');","1bba80c4":"plt.figure(figsize=(16, 14))\nplt.subplot(3, 3, 1)\nplt.scatter(data['number_cast'], data['revenue'])\nplt.title('Revenue X number of cast members')\nplt.subplot(3, 3, 2)\nplt.scatter(data['crew_total'], data['revenue'])\nplt.title('Revenue X number of crew members')\nplt.subplot(3, 3, 3)\nplt.scatter(data['num_languages'], data['revenue'])\nplt.title('Revenue X number of languages')\nplt.tight_layout(1.5)","c23f5820":"plt.figure(figsize=(16, 8))\nplt.subplot(2, 2, 1)\nplt.scatter(data['num_countries'], data['revenue'])\nplt.title('Revenue X number of countries')\nplt.subplot(2, 2, 2)\nplt.scatter(data['keywords_num'], data['revenue'])\nplt.title('Revenue X number of keywords')","bbba6e85":"data['title_length'] = data['original_title'].apply(lambda x: len(x) if x != '' else 0)","62e82e4b":"plt.figure(figsize=(20, 8))\nplt.scatter(data['title_length'], data['revenue']);plt.title('Revenue X title lenght')","4a522779":"corr = data.corr()\ncorr['revenue'].sort_values(ascending=False).head(20) #the top 20 features positively correlated to our target","7c3978ee":"corr['revenue'].sort_values(ascending=False).tail(20) #negatively correlated to our target","34f246d9":"data.isnull().sum().sort_values(ascending=False)","7f38d19f":"data.drop(['imdb_id', 'poster_path', 'release_date', 'status','budget_log','cast_character_','language_'], axis=1, inplace=True)","3aea444d":"data_dropping_names = data.drop(['original_title','overview','tagline','title'], axis=1)\n\ntrain = data_dropping_names[data_dropping_names['source'] == 'train'].copy()\ntest = data_dropping_names[data_dropping_names['source'] == 'test'].copy()\n\ntrain_labels = train['revenue_log'] #creating labels, our Y_train, gonna use the log as it works better for skewed data\n\ntrain.drop(['id', 'revenue', 'source', 'revenue_log'], axis=1, inplace=True) # dropping the target and id\n\ntest_final = test.drop(['id', 'source','revenue','revenue_log'], axis=1) #this is the final test, the dataset give for our prediction, notice that I am dropping the revenue column here that has been created when we merged the two datasets together","69fe2058":"print(train.shape, train_labels.shape,test_final.shape)","47b61048":"train.isnull().sum()","d9e315bd":"#this pipeline is gonna be use for numerical atributes and standard scaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler,RobustScaler, MinMaxScaler\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('robust_scaler', RobustScaler()),\n        #('minmax_scaler', MinMaxScaler())\n    ])","f3dbbf3e":"#let's create this function to make it easier and clean to fit the model and use the cross_val_score and obtain results\nimport time #implementing in this function the time spent on training the model\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import  KFold\nfrom catboost import CatBoostRegressor\nimport lightgbm as lgb\nimport xgboost as xgb\nimport eli5\nimport gc\n\nn_fold = 10 #number of folds that our function is gonna use and split the training set accordingly\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n\ndef train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):\n\n    prediction = np.zeros(X_test.shape[0]) #initializing the prediction matrix with zeros, with the number of training examples in X_test\n    scores = [] #this list is gonna be used to store all the scores across different folds\n    feature_importance = pd.DataFrame() #initializing this dataframe, it's gonna be used to plot the features importance.\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        if model_type == 'sklearn': #if the model type is sklearn then\n            X_train, X_valid = X[train_index], X[valid_index]\n        else:\n            X_train, X_valid = X.values[train_index], X.values[valid_index]\n        y_train, y_valid = y[train_index], y[valid_index]\n        \n        if model_type == 'lgb': \n            model = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n                    verbose=1000, early_stopping_rounds=200)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test.values), ntree_limit=model.best_ntree_limit)\n\n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = mean_squared_error(y_valid, y_pred_valid)\n            \n            y_pred = model.predict(X_test)\n            \n        if model_type == 'cat':\n            model = CatBoostRegressor(iterations=20000,  eval_metric='RMSE', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        scores.append(mean_squared_error(y_valid, y_pred_valid) ** 0.5)\n        \n        prediction += y_pred #summing all the prediction which is gonna later be divided by the number of folds   \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction \/= n_fold # all the predictions divided by the number of folds(getting the average value)\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    # Clean up memory\n    gc.enable()\n    del model, y_pred_valid, X_test,X_train,X_valid, y_pred, y_train\n    gc.collect()\n\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] \/= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return scores, prediction, feature_importance \n        return scores, prediction\n    else:\n        return scores, prediction\n    ","13d1220c":"train_dummies = pd.get_dummies(train)\ntest_dummies = pd.get_dummies(test)\ntrain_dummies, test_dummies = train_dummies.align(test_dummies, axis=1, join='inner')","95f181a1":"print(train_dummies.shape, test_dummies.shape)","9907c7e2":"########## LGB ########\nparams = {\n          'num_leaves': 30,\n         'min_data_in_leaf': 20,\n         'objective': 'regression',\n         'max_depth': 6,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.2,\n}\nscore_lgb, prediction_lgb, _ = train_model(train_dummies, test_dummies, train_labels, params=params, model_type='lgb', plot_feature_importance=True)","d9c104e9":"score_lgb.sort(reverse=True)\ndictvalues = {'RMSE_lgb': score_lgb}","9927c82b":"plt.plot(dictvalues['RMSE_lgb'], label = 'RMSE_lgb')\nplt.legend()","b27309be":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['revenue'] = np.expm1(prediction_lgb)\nsub.to_csv(\"lgb_model.csv\", index=False)","6401fb6f":"xgb_params = {'eta': 0.01,\n              'objective': 'reg:linear',\n              'max_depth': 7,\n              'subsample': 0.8,\n              'colsample_bytree': 0.8,\n              'eval_metric': 'rmse',\n              'seed': 11,\n              'silent': True}\nscore_xgb, prediction_xgb = train_model(train_dummies, test_dummies, train_labels, params=xgb_params, model_type='xgb', plot_feature_importance=True)","a53ad432":"score_xgb.sort(reverse=True)\ndictvalues.update({'RMSE_XGB': score_xgb})","1f82f456":"plt.plot(dictvalues['RMSE_lgb'], label = 'RMSE_lgb')\nplt.plot(dictvalues['RMSE_XGB'], label = 'RMSE_XGB')\nplt.legend()","6d3254cd":"sub['revenue'] = np.expm1(prediction_xgb)\nsub.to_csv(\"XGB_model.csv\", index=False)","f3f48418":"cat_params = {'learning_rate': 0.002,\n              'depth': 5,\n              'l2_leaf_reg': 10,\n              'colsample_bylevel': 0.8,\n              'bagging_temperature': 0.2,\n              'od_type': 'Iter',\n              'od_wait': 100,\n              'random_seed': 11,\n              'allow_writing_files': False}\nscore_cat, prediction_cat = train_model(train_dummies, test_dummies, train_labels, params=cat_params, model_type='cat', plot_feature_importance=True)","6610de34":"score_cat.sort(reverse=True)\ndictvalues.update({'RMSE_CAT': score_cat})\nplt.plot(dictvalues['RMSE_lgb'], label = 'RMSE_lgb')\nplt.plot(dictvalues['RMSE_XGB'], label = 'RMSE_XGB')\nplt.plot(dictvalues['RMSE_CAT'], label = 'RMSE_CAT')\nplt.legend()","b0ef5a6b":"sub['revenue'] = np.expm1(prediction_cat)\nsub.to_csv(\"cat_model.csv\", index=False)","034a5122":"sub['revenue'] = np.expm1((prediction_lgb + prediction_xgb + prediction_cat) \/ 3)\nsub.to_csv(\"combined.csv\", index=False)","381e6454":"## loading relevant models for this part of the notebook\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam, SGD\nfrom keras.layers import Dense, Activation, Dropout, Input, concatenate,BatchNormalization\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras.regularizers import l2\nfrom keras.callbacks import EarlyStopping","29e64695":"########### creating a function ir order to make easier to test different models ###############\n\ndef build_model(input_shape,n_hidden=10, n_neurons=30, optimizer = SGD(3e-3,momentum=0.9)): # the function has the stochastic gradient descent with momemntum as the default, you might wanna use it and see the results\n    model = Sequential()\n    options = {\"input_shape\": input_shape}\n    for layer in range(n_hidden): # fixed number of neurons for each hidden layer\n        BatchNormalization() #calling batch normalization before the activation...\n        model.add(Dense(n_neurons, activation=\"relu\",\n                           kernel_regularizer=l2(0.01), **options)) #using the relu activation and L2 regularization, I couldn't make this NN works better and the model didn't generalize well\n        options = {}\n    BatchNormalization() #adding BN before the output layer\n    model.add(Dense(1, **options)) \n    model.compile(loss=\"mean_squared_error\", optimizer=optimizer)\n    return model","088696b3":"train_dummies_prepared = num_pipeline.fit_transform(train_dummies)","53bd5a66":"model_NN = build_model(input_shape= train_dummies_prepared.shape[1:], n_hidden=6,optimizer=Adam(3e-5)) #gonna use 6 hidden layers and Adam ","666625d1":"model_history = model_NN.fit(train_dummies_prepared, train_labels, epochs=2500, batch_size=5, validation_split=0.1,callbacks=[EarlyStopping(patience=100)])","a1d6508a":"pd.DataFrame(model_history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 50) # set the vertical range to [0-1]","1d839350":"### I would love to improve this neural network, any sort of feedback is welcome...","7f9330f0":"## Cool, our rather simple pipeline has been create, now I am gonna implement a function that is an \"all-in-one\" function, which has been implemented in this kernel [here](https:\/\/www.kaggle.com\/artgor\/eda-feature-engineering-and-model-interpretation)\n* this function receives different models, parameters, and the Kfold that is gonna be used to train and validate the model using different folds.","a9f6065b":"## *** Feature Engineering and Data Cleaning ***\n\n#### One hot coding, creating boolean columns depending on the weight of values \n* in order to make it easier, it's gonna be created a function to read the dictionaries and create the boolean columns for each most common values","c9f3e54f":"***","835c0540":"** finally, on to the last dictionary, production companies. **\n* I am not sure whether the number of production companies a movie has gives it a better chance of success, gotta check this later..","48421ea5":"## we have a really strong, linear and positive, relationship between revenue and budget, and the density distribution shows two different aggregates, what does that mean? an assymetry in the distribution of budgets? that's what our intuition tell us, isn't it?\n\n*** \n\n\n* is there a relationship between having a homepage and reveue? And orginal language? how does it influences the revenue?","b714b6fe":"** instead of using the name of the homepage, it would be better to create a column has_homepage, indicating whether or not a given movie has a homepage or not **","a258e799":"## the function implemented above creates a validation set for each Kfold, depending on the number of splits you want. I set it to 10 splits.","6ec6dce6":"** last step has increase the number of features significantly, but not drammatically, and the models are going to find relationships way faster with boolean columns like so **","1fa7697f":"**looking at the first entry to have an idea of how the data is**","4db167ed":"Quite similar both models, don't you think? In fact, in small datasets as this one, XGB tends to outpeform lgb...\n* next, catboost...","e4cf74d6":"**And again, next step I got the idea and code from this kernel ** [here](https:\/\/www.kaggle.com\/artgor\/eda-feature-engineering-and-model-interpretation)\n\n##### I am not gonna get the gender out of the cast column, for our model, I believe what is the most important is the actor's name and kind of character","2b522512":"* taking a look at the distribution of different genres and the density plot of revenue.","7c581bb8":"* #### By looking at the size of each set, what one might say is that as the test dataset is relativaly large in comparison with our training set, we must try to do our best to not overfit and generalize our model the best","879ad693":"### Now, plotting our RMSE","fb33539e":"* taking a look at the homepage column","dea8e48f":"#### Clearly, as could've been expected, revenue has been increasing in time...\n\n* Continuing checking what's the distribution for days of the week and days of the month.","d864ba85":"The code below has been take form this kernel: [here](https:\/\/www.kaggle.com\/artgor\/eda-feature-engineering-and-model-interpretation)","0fbe1603":"** 15 person have been casted in 525 different movies **","20cd6ec9":"** creating a column with all the keywords presents in the dictionary **","45038571":"* Now on the spoken languages column","b561e66f":"***","0d97f728":"## 1. ** loading relevant modules **","f8617e9c":"* creating a column with the number of contries participating in the movie","5b22e2e0":"** Like pretty much all the other features where we have sums, higher revenues are correlated with titles within a normal range, less than 30**\n* This is for the original title, now, what about the actual title, is there gonna be a difference?\n#TODO","924d1ce8":"**As be can be seen, english as an original language is correlated to a higher revenue. I am gonna drop the column original language now**","f07d88bb":"** that's interesting, isn't? having a website have a positive correlation to revenue **\n** it's also quite informative the fact that most of our data is populated with english movies, gonna create a new category, is_english_original_language, and see how it correlates to revenue **\n\n","e2219fcb":"***\nWrapping up our EDA, it might be informative to take a look at the linear correlation among our features.","271131a0":"***\n# This next part, I am using keras and building a simple neural net regressor. I tried to use it with a scikit learn wrapper but I couldn't make it work, for some reason, I am gonna try it again later. This won't get a good score, but it's for educational purposes. It's fun! \n***","f8522f13":"** As one could have expected, distribution of revenues is skewed, and the most representative genre is comedy**\n* it's gonna be used the log 1 for our revenue, so we are gonna have a better idea of the distribution","a2765a17":"## Models\n##### Start modeling  ","a3f2df6c":"**Now let's take a look at other dictionaries we have in this dataset**","0f8f8d60":"#### missing values won't be a problem, tagline is gonna be drop, overview as well... runtime can be filled with the median value.\n* dropping some columns that are not really important for our models","6b1f452b":"* Starting testing our models, first is gonna be LGB","d9a76cab":"* Creating a simple numerical pipeline to impute and scale the data","84d2fe35":"* LOADING OUR TRAINING SET AND TESTING SET and preparing the dictionary data in our dataframes\n\n* code from this kernel [here](https:\/\/www.kaggle.com\/gravix\/gradient-in-a-box)","87f66142":"** There is a positive relationship between the length of the movie and revenue, there's also a positive relationship between length and popularity but it is less pronounced**","f6e7c5ac":"***","0a975c8f":"** that looks more promissing, despite having a long tail to the right, it's better distributed now, it's gonna help our analysis for now on...**\n* we are gonna gonna use log1 for budget as well, and we are gonna see how budget is ditributed and wha's the relationship with revenue ","3d37fb2d":"### As it was done for collections, only the name of the genre is importante, and popularity of this given genre. Notice that we have 23 movies that the genre have not been listed(value 0)","783181f9":"** that's a lot of data, only the relevant characters would be important **","36888a26":"Similar to what was done to the cast column, it's gonna be done to the crew column\n","675f07f8":"** plenty of information in those plots: **\n* number of keywords from 0 - 40 is the strongly present in this data, whereas we have two movies witch a low revenue and a high number of keywords\n* number of countries have also a negative correlation the more countries we have\n* the same thing goes for number of languages","22b11ef6":"** Checking the first entry for keywords, we have different keywords for each movie, similar to what has been done for the other dictionaries, I am gonna keep only the most common keywords**\n* eg: Hot Tube is not a recurrent keyword, let's check","0df7c1b3":"### We have some features in this dataset that has descriptive information, those columns have valuable information that may help our models. I am gonna try simply getting the length of those features and see whether it has a positive relationship with our target and to what extent","f4762c60":"#### now, The popularity rating of a particular title on IMDb indicates how much that title's page has been visited in the current week on IMDb in it's area of interest. \n* plotting how popularity impacts revenue","15b3575c":"* the vast majority of the data does not belong to a collection, so we might as well create a boolean column **has_collection** that gives us this information\n* moreover, we don't need the dictionary, but only the name of the collection is enough","80a6ec9f":"* Finally, dropping the **Keywords** and **All Keywords** column","6bdc6590":"** following the same logic for other dictionaries, gonna create a column with the total number of keywords for each movie **","f321a18d":"* Creating a column for the number of the languages spoken for each movie.","1b4e4bf0":"* creating also a training set with dummies, as well as align it with the testing set to avoid different columns","adf7607d":"***","b8835de4":"* crew total number for each movie","cd7e54a9":"***","7a157cd5":"** IMO, it's enough the name of crew members and tota crew members as it has been created, next, I am gonna create booean columns for each of top 15 crew members and drop the crew column **","02a88cf9":"**Finally we can drop the cast column and next, we are going to take a look at the crew column**","7e940931":"#### 3 more dictionaries to extract relevant information for our models","9bdb4059":"##### Again, using the same code that has been used for the cast, let's take a look at the top 15 most common crew members","8a210718":"## Now that data has been prepared, a brief data analysis in order to derive some insights out of our dataset relate to the revenue, our target","0f1f39c4":"##### release_date has some problems that need to be fixed, let's address this now","f7360c0e":"## using all the features previously created, such as number of crew members, number of cast, number of companies and so on","ccc726dd":"* same thing will be done with top 15 most common characters","b02cf94c":"Gonna create a column with the number of cast members for each movie.","9196d6f8":"* Now on to the production_countries","0da1c4e8":"#### popularity has a positive correlation to revenue, as it could have been guessed.","9d239351":"* some columns have dictionaries inside of it, such **as belongs to a collection**\n* before any sort of data cleaning, let's merge the two datasets together to avoid code repetition","1f548f39":"* next, take a look at cast","9aab51bb":"** Same code as it has been used before for the other dicts, it's getting the list of keywords to be used with the counter method to get the 30 most common, then create a boolean column for each keyword**","359fdfe8":"gonna create a crew_total","d4ae6292":"# TMDB Box Office Prediction Challenge,\n## in this kernel,We're gonna work with a database containing different information about film industries. \n### Could we create a model that helps estimate how to improve movies in order to generate better revenue?","871e1522":"* checking the total number of countries participating in each movies ","93042f48":"* gonna combine all the prediction and divide by 3, honestly, I don't like this approach, this is only for kaggle....","baeacf20":"#### this code is gonna change the dict_columns list below, replacing the NAN values for {}\n** ast.literal_eval: **  Safely evaluate an expression node or a string containing a Python literal or container display. "}}