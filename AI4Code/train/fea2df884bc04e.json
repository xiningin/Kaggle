{"cell_type":{"191ba882":"code","c48282c1":"code","ffc0b0b7":"code","6a3f9e1f":"code","a79d0a77":"code","5f70f946":"code","9b789036":"code","0d531822":"code","77e877f9":"code","7a41ce6e":"code","d47c5525":"code","8619767d":"code","0550f32c":"code","ce3e9ef0":"code","3b6f6d95":"code","d76c4f08":"code","c9984e80":"code","18b59253":"code","76dd53d2":"code","fe85cc16":"code","b6a33a60":"code","a9a48112":"code","ab4f9037":"code","9ae25bd5":"code","0072e1c8":"code","f8d25b1d":"code","49e65b26":"code","1806b94f":"code","15258ec2":"code","d1a2cf09":"code","0535e5f5":"code","05488e62":"code","c3a7cf5d":"markdown","ab770e30":"markdown","b8c1d8bb":"markdown","638e06b8":"markdown","900e5056":"markdown","ea4df3b0":"markdown","4b4d8928":"markdown","b7de3f98":"markdown","f40b8534":"markdown","76cf5def":"markdown","0894f8e9":"markdown","6992233b":"markdown"},"source":{"191ba882":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.models import Sequential\n\nimport numpy as np\nimport cv2 as cv\nimport matplotlib.pyplot as plt\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix","c48282c1":"#path of the different folders\ntrain_path = '..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/train\/'\nval_path = '..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/val\/'\ntest_path = '..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/test\/'","ffc0b0b7":"#labels\nlabels=['NORMAL', 'PNEUMONIA']","6a3f9e1f":"#setting image size\nimage_size = 256","a79d0a77":"def process_data(data_dir, labels, image_size):              #creating the training data\n    \n    images = []\n    for label in labels:\n        dir = os.path.join(data_dir,label)\n\n        class_num = labels.index(label)\n\n        for image in os.listdir(dir):    #going through all the images in different folders and resizing them\n            if image == '.DS_Store':\n                continue\n            image_read = cv.imread(os.path.join(dir,image),cv.IMREAD_GRAYSCALE)\n            image_resized = cv.resize(image_read,(image_size,image_size))\n            images.append([image_resized,class_num])\n    return np.array(images)","5f70f946":"train = process_data(train_path, labels, image_size)\ntest = process_data(test_path, labels, image_size)\nval = process_data(val_path, labels, image_size)","9b789036":"train.shape","0d531822":"print('number of images in the training set:', train.shape[0])\nprint('number of images in the validation set:', val.shape[0])\nprint('number of images in the test set:', test.shape[0])","77e877f9":"X = []\ny = []\n\nfor feature, label in train:\n    X.append(feature)          #appending all images\n    y.append(label)            #appending all labels\n\nfor feature, label in val:\n    X.append(feature)\n    y.append(label)","7a41ce6e":"#Reshaping data in order to feed it into the Neural Network\nX = np.array(X).reshape(-1, image_size, image_size, 1)\ny = np.array(y)\ny = np.expand_dims(y, axis =1)","d47c5525":"#Concatenate train and val data\n\nX_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.2, shuffle=True)\n\nprint('number of images in training set:', X_train.shape[0])\nprint('number of images in validation set:', X_val.shape[0])","8619767d":"X_train.shape","0550f32c":"count_normal = len([y for y in y_train if y == 0])\ncount_pneumonia = len([y for y in y_train if y == 1])\n\nprint('Count of NORMAL images in train:', count_normal)\nprint('Count of PNEUMONIA images in train:', count_pneumonia)","ce3e9ef0":"plt.imshow(train[2000][0], cmap='gray')\nprint(labels[train[2000][1]])   ","3b6f6d95":"plt.imshow(train[1000][0], cmap='gray')\nprint(labels[train[1000][1]])","d76c4f08":"classifier = Sequential()","c9984e80":"classifier.add(Conv2D(filters=32, kernel_size=(3,3), strides=1, input_shape=(image_size,image_size,1), activation='relu', padding='same'))\nclassifier.add(BatchNormalization())\nclassifier.add(Conv2D(filters=32, kernel_size=(3,3), strides=1, input_shape=(image_size,image_size,1), activation='relu', padding='same'))\nclassifier.add(BatchNormalization())   \nclassifier.add(MaxPooling2D(2,2))                                       \n\nclassifier.add(Conv2D(filters=64, kernel_size=(3,3), strides=1, input_shape=(image_size,image_size,1), activation='relu', padding='same'))\nclassifier.add(BatchNormalization())\nclassifier.add(Conv2D(filters=64, kernel_size=(3,3), strides=1, input_shape=(image_size,image_size,1), activation='relu', padding='same'))\nclassifier.add(BatchNormalization())   \nclassifier.add(MaxPooling2D(2,2))      \n\nclassifier.add(Conv2D(filters=128, kernel_size=(3,3), strides=1, input_shape=(image_size,image_size,1), activation='relu', padding='same'))\nclassifier.add(BatchNormalization())\nclassifier.add(Conv2D(filters=128, kernel_size=(3,3), strides=1, input_shape=(image_size,image_size,1), activation='relu', padding='same'))\nclassifier.add(BatchNormalization())   \nclassifier.add(MaxPooling2D(2,2))      \n\nclassifier.add(Conv2D(filters=256, kernel_size=(3,3), strides=1, input_shape=(image_size,image_size,1), activation='relu', padding='same'))\nclassifier.add(BatchNormalization())\nclassifier.add(Conv2D(filters=256, kernel_size=(3,3), strides=1, input_shape=(image_size,image_size,1), activation='relu', padding='same'))\nclassifier.add(BatchNormalization())   \nclassifier.add(MaxPooling2D(2,2))      \n\nclassifier.add(Flatten())     \n\nclassifier.add(Dense(units=512, activation='relu'))\nclassifier.add(Dropout(0.4))\n\nclassifier.add(Dense(units=512, activation='relu'))\nclassifier.add(Dropout(0.3))\n\nclassifier.add(Dense(units=512, activation='relu'))\nclassifier.add(Dropout(0.1))\n\nclassifier.add(Dense(units=1, activation='sigmoid'))\n","18b59253":"classifier.summary()","76dd53d2":"batch_size = 4\n\ntrain_gen = ImageDataGenerator(rotation_range=10,\n                                   horizontal_flip = True,\n                                   width_shift_range=0.1,\n                                   height_shift_range=0.1,\n                                   rescale=1.,\n                                   zoom_range=0.2,\n                                   fill_mode='nearest',\n                                   cval=0)\n\ntrain_generator = train_gen.flow(X_train,y_train,batch_size)\nsteps_per_epoch = X_train.shape[0]\/\/batch_size","fe85cc16":"#Callback allowing to save the best performing model\ncheckpoint = ModelCheckpoint('COTI_to_the_moon.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto')","b6a33a60":"classifier.compile(optimizer = Adam(lr = 0.00001), loss = 'binary_crossentropy', metrics = ['accuracy']) ","a9a48112":"history=classifier.fit(train_generator, validation_data=(X_val, y_val), steps_per_epoch = steps_per_epoch, epochs= 10,\n                       callbacks = [checkpoint])","ab4f9037":"plt.plot(history.history['loss'],label='loss')\nplt.plot(history.history['val_loss'],label='val_loss')\nplt.legend()","9ae25bd5":"plt.plot(history.history['accuracy'],label='accuracy')\nplt.plot(history.history['val_accuracy'],label='val_accuracy')\nplt.legend()","0072e1c8":"final_model = tf.keras.models.load_model('COTI_to_the_moon.h5')","f8d25b1d":"X_test = []\ny_test = []\n# Preproccessing test set\nfor feature, label in test:\n    X_test.append(feature)\n    y_test.append(label)\n#Reshaping data in order to feed it into the Neural Network\nX_test = np.array(X_test).reshape(-1, image_size, image_size, 1)\ny_test = np.array(y_test)\ny_test = np.expand_dims(y_test, axis =1)","49e65b26":"pred = final_model.predict(X_test, batch_size = 8)\n","1806b94f":"pred_final = np.where(pred>0.5,1,0)\n","15258ec2":"X_test.shape","d1a2cf09":"# Get the confusion matrix\nCM = confusion_matrix(y_test, pred_final)\n\nfig, ax = plot_confusion_matrix(conf_mat=CM ,  figsize=(8,8))\nplt.title('Confusion matrix')\nplt.xticks(range(2), ['Normal','Pneumonia'], fontsize=10)\nplt.yticks(range(2), ['Normal','Pneumonia'], fontsize=10)\nplt.show()\n","0535e5f5":"def perf_measure(y_test, pred_final):\n    TP = 0\n    FP = 0\n    TN = 0\n    FN = 0\n\n    for i in range(len(pred_final)): \n        if y_test[i]==pred_final[i]==1:\n           TP += 1\n        if y_test[i]==0 and y_test[i]!=pred_final[i]:\n           FP += 1\n        if y_test[i]==pred_final[i]==0:\n           TN += 1\n        if y_test[i]==1 and y_test[i]!=pred_final[i]:\n           FN += 1\n\n    return(TP, FP, TN, FN)","05488e62":"tp, fp, tn ,fn = perf_measure(y_test,pred_final)\n\nprecision = tp\/(tp+fp)\nrecall = tp\/(tp+fn)\nf_score = (2*precision*recall)\/(precision+recall)\n\nprint(\"Recall of the model is {:.2f}\".format(recall))\nprint(\"Precision of the model is {:.2f}\".format(precision))\nprint(\"F-Score is {:.2f}\".format(f_score))","c3a7cf5d":"## 3. Resizing validation set","ab770e30":"## 4. Visualize some chest X-Ray images","b8c1d8bb":"## 6. Fitting the model","638e06b8":"## 9. Conclusion\n\nOur model has an accuracy of 95.8% on the validation set.\n\nOn the hold out dataset, the model is performing with an F-Score of 0.91 but what really matters is that it has a recall of 96%!\n\nMeaning that we're not detecting 4% of the malignant patient. On the test dataset, out of 390 malignant patients, the model detected 378 pneumonia.\n\nThe trade off is that our model predicted that 27.7% of patient had pneumonia where in fact they were healthy (False Positive).\n","900e5056":"## 8. Plotting the Confusion Matrix","ea4df3b0":"## 2. Creating the dataset","4b4d8928":"## 7. Make prediction","b7de3f98":"## 6. Compiling the model","f40b8534":"## 5. Creating the CNN Model","76cf5def":"Our validation set only contains 16 images which is way to small. That's why we will merge the training set and validation set together and resplit them into a 80% training set and 20% validation set.","0894f8e9":"## 5. Data augmentation\n\n[Data augmentation](https:\/\/machinelearningmastery.com\/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks\/) is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset.\n\n","6992233b":"# Zoidberg2.0\n\n## 0. Project Description\n\n### Objective\n\nUsing a dataset of chest X-Ray image, we have to implement a computer vision model in order to detect pneumonia.\n\nWe decided to **maximize recall** and not accuracy. The reason is that we want our model to detect as much pneumonia case as possible. We prefer having a false positive (the model detects pneumonia but in realaity the patient is healthy) case than a false negative case (the model detects a normal case but in reality the patient has pneumonia)\n\n### The dataset\n\nWe're having a dataset split into three folders:\n\n- test: 234 normal, 390 pneumonia\n- train: 1341 normal, 3875 pneumonia\n- validation: 8 normal, 8 pneumonia\n\n### Machine Learning or DeepLearning?\n\nSince our data are unstructured data (image) and because we want to classify image which is a computer vision field, we decided to use DeepLearning. \n\nMachine Learning and classification algorithm as logistic regression, CART or Random Forest are suitable for structured data but will hardly perform better than Computer Vision's DeepLearning algorithm.\n\n### Picking the DeepLearning model\n\n\nWe decided to opt for a Convolutional Neural Network (CNN) model. The reason of this choice is that in today's current state, CNN has been proven to be the way-to-go model for computer vision, be it image classification or object detection. Residual Network (ResNet) could also work but our computer are not powerful enough to train Deep Resnet.\n\n### Agreeing in the term used in the notenook\n\n\u2013 Training set: A set of examples used for learning, that is to fit the parameters of the classifier.\n\n\u2013 Validation set: A set of examples used to tune the parameters of a classifier, for example to choose the number of hidden units in a neural network.\n\n\u2013 Test set: A set of examples used only to assess the performance of a fully-specified classifier.\n\n\n## 1. Importations des modules"}}