{"cell_type":{"c3bc7706":"code","30388e3c":"code","94a7aed1":"code","d29c6557":"code","3b3ee735":"code","b66f26a5":"code","1476c993":"code","24fa984f":"code","f6841ff3":"code","3c57638b":"code","7d8adae9":"code","dde5334c":"code","08ded592":"code","61e0b6e6":"code","189d4704":"code","866d2b82":"code","cbeb8b9c":"code","ba70faa0":"code","b766cc75":"code","375479a8":"code","0bf39c9b":"code","124645bb":"code","141ce073":"code","cdbfc500":"code","39e94d17":"code","52261e9b":"code","d0d7792d":"code","c1d02845":"code","1f0b20ba":"code","85c19464":"code","b218c4c9":"code","db56952c":"code","b49b97c2":"code","f6268bcd":"code","fcfc5100":"code","e7f4db7a":"code","358fa648":"code","bb29d19e":"code","ed6084b5":"code","ede8f64b":"code","43f91153":"code","8d068af5":"code","9aa14248":"code","2e8439dd":"code","6d871bbc":"code","acaa21ab":"code","2cd45b30":"code","63af2ac1":"code","c3b1c6bd":"code","cf96abe1":"code","4b374c55":"code","0be25384":"code","13069dc9":"code","98e39826":"code","f6d3ba54":"code","ee794869":"markdown","edd1bf2e":"markdown","65646065":"markdown","d64d5e8f":"markdown","edfaec1a":"markdown","e279c34f":"markdown","13eac6fd":"markdown","452166fb":"markdown","e2e4806b":"markdown","e777163c":"markdown","39b05344":"markdown","c5c12185":"markdown","de780f3a":"markdown","5cafba9a":"markdown","99f0f198":"markdown","ad02ab3c":"markdown","67153e1a":"markdown","a3349910":"markdown","82a8bc2e":"markdown","f31717b6":"markdown","3c15dec1":"markdown","7428ddff":"markdown","1eff8a07":"markdown","fda72036":"markdown","d3a4f4f0":"markdown","62356040":"markdown","cd08fc54":"markdown","cf223f19":"markdown","a5fcf339":"markdown","16bfb59b":"markdown","6401a9c2":"markdown","40f6d5e5":"markdown","4f8c7326":"markdown","98d17c0c":"markdown","12f99063":"markdown","86786827":"markdown","6a9e3777":"markdown","eb6e8699":"markdown","4efb2c55":"markdown","c099880b":"markdown","3cb4fad9":"markdown","42fc9d3a":"markdown","6e0d37b8":"markdown","46df341c":"markdown","edc73057":"markdown","09a2f458":"markdown","60035766":"markdown","8936af68":"markdown","b03773d0":"markdown","13e8cf54":"markdown","b696c71d":"markdown","b6acafb5":"markdown","283ecd89":"markdown","bdffd134":"markdown","46d307dd":"markdown","786ca679":"markdown","d8beeca4":"markdown","819bdc28":"markdown","705c7096":"markdown","8b1f91d9":"markdown","6008a9d2":"markdown","6c6253ce":"markdown","16aa28bd":"markdown"},"source":{"c3bc7706":"# Any results you write to the current directory are saved as output.\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n        \n# Importing Libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas_profiling import ProfileReport\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nfrom sklearn.model_selection import train_test_split # for spliting dataset\nfrom sklearn.feature_extraction.text import CountVectorizer # bow-->1gram and 2 gram\nfrom sklearn.feature_extraction.text import TfidfVectorizer # tf-idf\nfrom gensim.models import Word2Vec  # w2v\nfrom gensim.models import KeyedVectors # to understanding w2v using google pre trained model\nfrom sklearn.metrics import accuracy_score # to check the accuracy of model\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score # k-fold cv\nfrom sklearn.metrics import classification_report\nimport pickle\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom tqdm import tqdm\nimport re\nimport nltk\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport string\neng_stopwords = stopwords.words('english')\nfrom nltk.stem import PorterStemmer\nfrom sklearn.metrics import confusion_matrix,log_loss\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.preprocessing import StandardScaler","30388e3c":"# Loading CSV file\ndata = pd.read_csv(\"..\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv\",index_col=0)\n\n# Using only 'Review text' and 'Rating' and descarding other columns\ndata = data[['Review Text','Rating']] \n\n# Converting into binary classification problem\ndata = data[data['Rating']!=3]\ndata['Rating'] = data['Rating'].apply(lambda x: 0 if x<3 else 1)\n\n# Shape of dataset\nprint(\"Shape of the dataset:\",data.shape)\n\n# Overview of data\nprint(\"\\nOverview of data: \")\ndata.info","94a7aed1":"# Pandas Profiling : Really good library to get the overview EDA.\nprofile = ProfileReport(data, title='Pandas Profiling Report',minimal=False, html={'style':{'full_width':True}})\nprofile.to_widgets()","d29c6557":"# Finding missing values\nprint(f\"Number of Missing values: \\n{data.isnull().sum()}\\n\")\n\nprint(f\"number of duplicated reviews: {sum(data[data['Review Text'].notnull()].duplicated(['Review Text'],keep='first'))}\")","3b3ee735":"# Duplicate Review text example\ndata[data['Review Text'].notnull()][data[data['Review Text'].notnull()].duplicated(['Review Text'],keep=False)]","b66f26a5":"# Removing datapoints having missing values and duplicate Review text\ndata_after_drop = data[data['Review Text'].notnull()]\ndata_after_drop = data_after_drop.drop_duplicates(['Review Text'],keep='first')\n\nprint(f\"percentage of data remaing after dopping missing values and duplicate reviews: { round((data_after_drop.shape[0]\/data.shape[0])*100,3)} %\")","1476c993":"# Classlabel value counts\n\ntemp  = data_after_drop['Rating'].value_counts()\nprint(pd.DataFrame({'Class label(sentiment)':temp.index, \"values_counts\":temp.values,\"distribution percentage\":temp.values\/sum(temp.values) }))\n\n# Ploting distribution of classlabel\nsn.countplot(data_after_drop['Rating'])\nplt.show()","24fa984f":"num_of_words = data_after_drop['Review Text'].apply(lambda x: len(str(x).split()))\n\n# Ploting\nsn.distplot(num_of_words[data_after_drop['Rating']==1],label = 'Positive Sentiments')\nsn.distplot(num_of_words[data_after_drop['Rating']==0],label = 'Negative Sentiments')\nplt.legend()\nplt.title(\"Distribution of Number of words in Reviews text \")\nplt.show()","f6841ff3":"\"\"\"sn.boxplot(num_of_words,hue=data_after_drop['Rating'])\nplt.show()\"\"\"","3c57638b":"# refer: https:\/\/www.datacamp.com\/community\/tutorials\/wordcloud-python\n\ntext_train = \" \".join(word for word in data_after_drop['Review Text'])\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text_train)\n\n# Display the generated image:\nplt.figure(figsize=(11,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title(\"WordCloud of Review Text\\n\")\nplt.axis(\"off\")\nplt.show()","7d8adae9":"# WordCloud hue by class label\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6))\n\n# For Positive Sentiment \ntext = \" \".join(word for word in data_after_drop[data_after_drop['Rating']==1]['Review Text'])\n# Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text)\n\n# Display the generated image:\nax1.imshow(wordcloud, interpolation='bilinear')\nax1.set(title='WordCloud of Positive Text\\n')\nax1.axis(\"off\")\n\n# -------------------------------------------------------------------------------------------------\n\n# For Negative Sentiment \ntext = \" \".join(word for word in data_after_drop[data_after_drop['Rating']==0]['Review Text'])\n# Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text)\n\n# Display the generated image:\nax2.imshow(wordcloud, interpolation='bilinear')\nax2.set(title='WordCloud of Negative Review Text\\n')\nax2.axis(\"off\")\nplt.show()","dde5334c":"# Preprocessing Functions\n# credit : https:\/\/www.kaggle.com\/urvishp80\/quest-encoding-ensemble\n\n#======================================================================================================================================\n# Return the number of links and text without html tags \n# Also return the counts of 'number of lines'  and remove it\ndef strip_html(text):\n    \"\"\" Return theclean text (without html tags) \"\"\"\n    \n    # Removing HTML tags\n    text = re.sub(r'http[s]?:\/\/\\S+',\" \",text)\n    \n    # finding number of lines using regex and counting it and remove it\n    text = re.sub(r'\\n', \" \",text)\n    \n    return  text\n\n\n#======================================================================================================================================\nmispell_dict = {\"aren't\" : \"are not\",\"can't\" : \"cannot\",\"couldn't\" : \"could not\",\"couldnt\" : \"could not\",\"didn't\" : \"did not\",\"doesn't\" : \"does not\",\n                \"doesnt\" : \"does not\",\"don't\" : \"do not\",\"hadn't\" : \"had not\",\"hasn't\" : \"has not\",\"haven't\" : \"have not\",\"havent\" : \"have not\",\n                \"he'd\" : \"he would\",\"he'll\" : \"he will\",\"he's\" : \"he is\",\"i'd\" : \"i would\",\"i'd\" : \"i had\",\"i'll\" : \"i will\",\"i'm\" : \"i am\",\n                \"isn't\" : \"is not\",\"it's\" : \"it is\",\"it'll\":\"it will\",\"i've\" : \"I have\",\"let's\" : \"let us\",\"mightn't\" : \"might not\",\n                \"mustn't\" : \"must not\",\"shan't\" : \"shall not\",\"she'd\" : \"she would\",\"she'll\" : \"she will\",\"she's\" : \"she is\",\"shouldn't\" : \"should not\",\n                \"shouldnt\" : \"should not\",\"that's\" : \"that is\",\"thats\" : \"that is\",\"there's\" : \"there is\",\"theres\" : \"there is\",\"they'd\" : \"they would\",\n                \"they'll\" : \"they will\",\"they're\" : \"they are\",\"theyre\":  \"they are\",\"they've\" : \"they have\",\"we'd\" : \"we would\",\"we're\" : \"we are\",\n                \"weren't\" : \"were not\",\"we've\" : \"we have\",\"what'll\" : \"what will\",\"what're\" : \"what are\",\"what's\" : \"what is\",\"what've\" : \"what have\",\n                \"where's\" : \"where is\",\"who'd\" : \"who would\",\"who'll\" : \"who will\",\"who're\" : \"who are\",\"who's\" : \"who is\",\"who've\" : \"who have\",\n                \"won't\" : \"will not\",\"wouldn't\" : \"would not\",\"you'd\" : \"you would\",\"you'll\" : \"you will\",\"you're\" : \"you are\",\"you've\" : \"you have\",\n                \"'re\": \" are\",\"wasn't\": \"was not\",\"we'll\":\" will\",\"didn't\": \"did not\",\"tryin'\":\"trying\"}\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\ndef replace_typical_misspell(text):\n    \n    text = text.lower()\n    \n    \n    \"\"\"De-Concatenation of words and correction of misspelled words\"\"\"\n    mispellings, mispellings_re = _get_mispell(mispell_dict)\n\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\n\n#======================================================================================================================================\n# removing non_alpha_numeric character and removing all the special character words\ndef non_alpha_numeric_remove(text):  \n    \n    # removing all non alpha char \n    text = re.sub(r\"[^A-Za-z]\", \" \",text)\n    \n    return text \n\n#======================================================================================================================================\n\n# function to remove all the stopwords and words having lengths less than 3\ndef remove_stop_words_and_punc(text) :\n    \n    \"\"\" \n    Remove all the stopwords \n    \n    \"\"\"\n    # removing the words from the stop words list: 'no', 'nor', 'not'\n    stops = set(stopwords.words(\"english\"))\n    stops.remove('no')\n    stops.remove('nor')\n    stops.remove('not')\n    \n    clean_text = []\n    for word in text.split():\n        if word not in stops and len(word)>3:        \n            clean_text.append(word)\n        \n    clean_text = \" \".join(clean_text)\n    \n    return(clean_text)\n\n#======================================================================================================================================\n# function for stemming of words in text\ndef stem(text):\n    stemmer = PorterStemmer()\n    result = \" \".join([ stemmer.stem(word) for word in text.split(\" \")])\n    return result\n\n#======================================================================================================================================\n#======================================================================================================================================\n# Final text cleaning funtion  \ndef clean_text(text):\n    \"\"\"\n    This function sequentially execute all the cleaning and preprocessing function and finaly gives cleaned text.\n    Input: Boolean values of extra_features, strip_html, count_all_cap_words_and_lower_it, replace_typical_misspell, count_non_alpha_numeric_and_remove, remove_stop_words_and_punc, stem\n            (by default all the input values = True)\n    \n    return: clean text\n    \n    \"\"\"\n    \n    # remove html tags\n    clean_text = strip_html(text)  \n    \n    # de-concatenation of words\n    clean_text = replace_typical_misspell(clean_text)\n     \n    # Count the number of non alpha numeric character and remove it\n    clean_text = non_alpha_numeric_remove(clean_text)\n    \n    # removing Stopwords and the words length less than 3(As these words mostly tend to redundant words) excpect 'C' and 'R'and 'OS' <-- programing keywords\n    clean_text = remove_stop_words_and_punc(clean_text)\n    \n    # stemming ( use only for BOW or TFIDF represention. Not effective for word embedding like w2v or glove)\n    clean_text = stem(clean_text)\n\n    return clean_text","08ded592":"# Preprocessing \ncleaned_review_text = data_after_drop['Review Text'].apply(lambda x: clean_text(x))\n\n# Sample\ni=15\nprint(f\"\\nBefore Preprocessing\\n{'='*20}\")\nprint(data_after_drop['Review Text'].iloc[i])\n\nprint(f\"\\nAfter Preprocessing\\n{'='*20}\")\nprint(cleaned_review_text.iloc[i])","61e0b6e6":"# Calculating the length of text before and after preprocessing\nlen_after_cleaning = cleaned_review_text.apply(lambda x : len(x.split()))\nlen_before_cleaning = data_after_drop['Review Text'].apply(lambda x : len(x.split()))\n    \n# ploting\nplt.figure(figsize=(9,6))\nsn.distplot(len_before_cleaning, label=f'Review text before cleaning')\nsn.distplot(len_after_cleaning, label=f'Review text after cleaning')\nplt.title(f\" Distribution of number of words of Review text before v\/s after preprocessing\\n\",fontsize=15)\nplt.ylabel(\"distribtion\")\nplt.xlabel(f\"number of words in Review text\")\nplt.legend()\nplt.grid()\nplt.show()","189d4704":"\"\"\"# Calculating the length of text before and after preprocessing\nlen_after_cleaning = cleaned_review_text[data_after_drop['Rating']==1].apply(lambda x : len(x.split()))\nlen_before_cleaning = cleaned_review_text[data_after_drop['Rating']==0].apply(lambda x : len(x.split()))\n\n# ploting\nplt.figure(figsize=(9,6))\nsn.distplot(len_before_cleaning, label=f'Review text before cleaning')\nsn.distplot(len_after_cleaning, label=f'Review text after cleaning')\nplt.title(f\" Distribution of number of words of Review text before v\/s after preprocessing\\n\",fontsize=15)\nplt.ylabel(\"distribtion\")\nplt.xlabel(f\"number of words in Review text\")\nplt.legend()\nplt.grid()\nplt.show()\"\"\"\n\nprint(\"number of words after preprocessing hue by class label\")","866d2b82":"# WordCloud hue by class label\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6))\n\n# For Positive Sentiment \ntext = \" \".join(word for word in cleaned_review_text[data_after_drop['Rating']==1])\n# Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text)\n\n# Display the generated image:\nax1.imshow(wordcloud, interpolation='bilinear')\nax1.set(title='WordCloud of Positive Text\\n')\nax1.axis(\"off\")\n\n# -------------------------------------------------------------------------------------------------\n\n# For Negative Sentiment \ntext = \" \".join(word for word in cleaned_review_text[data_after_drop['Rating']==0])\n# Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text)\n\n# Display the generated image:\nax2.imshow(wordcloud, interpolation='bilinear')\nax2.set(title='WordCloud of Negative Review Text\\n')\nax2.axis(\"off\")\nplt.show()","cbeb8b9c":"top = Counter([item for sublist in cleaned_review_text[data_after_drop['Rating']==1] for item in str(sublist).split()])\ntemp = pd.DataFrame(top.most_common(20))\ntemp = temp.iloc[1:,:]\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Purples')","ba70faa0":"top = Counter([item for sublist in cleaned_review_text[data_after_drop['Rating']==0] for item in str(sublist).split()])\ntemp = pd.DataFrame(top.most_common(20))\ntemp = temp.iloc[1:,:]\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Purples')","b766cc75":"# train test split\n\nX = data_after_drop['Review Text']\ny = data_after_drop['Rating']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)\n\nprint(f\"Shape of X_train: {X_train.shape}\")\nprint(f\"Shape of y_train: {y_train.shape}\")\nprint(f\"Shape of X_val: {X_val.shape}\")\nprint(f\"Shape of y_val: {y_val.shape}\")","375479a8":"# Preprocessing for BOW and TFIDF\nX_train_clean_text = X_train.apply(lambda x: clean_text(x))\nX_val_clean_text = X_val.apply(lambda x: clean_text(x))","0bf39c9b":"from sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer(ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None)\nx_train_bow_unigram = count_vect.fit_transform(X_train_clean_text)\nx_val_bow_unigram = count_vect.transform(X_val_clean_text)\n\nprint(f\"shape of features after BOW Feture extraction: {x_train_bow_unigram.shape}\")\n\n# Sparsity of BOW-unigram\nsparsiry_bow = (len(x_train_bow_unigram.toarray().nonzero()[0]) \/ len(np.nonzero(x_train_bow_unigram.toarray()==0)[0]))*100\nprint(f\"Sparsity of BOW: {round(sparsiry_bow,5)}%\")","124645bb":"# BOW feature representaiona\nbow_unigram_feature_representation = pd.DataFrame(data = x_train_bow_unigram.toarray(), columns = count_vect.get_feature_names())\nbow_unigram_feature_representation.head()","141ce073":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vect = TfidfVectorizer(ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None)\nx_train_tfidf_unigram = tfidf_vect.fit_transform(X_train_clean_text)\nx_val_tfidf_unigram = tfidf_vect.transform(X_val_clean_text)\n\nprint(f\"shape of features after BOW Feture extraction: {x_train_tfidf_unigram.shape}\")\n\n# Sparsity of TFIDF-unigram\nsparsiry_tfidf = (len(x_train_tfidf_unigram.toarray().nonzero()[0]) \/ len(np.nonzero(x_train_tfidf_unigram.toarray()==0)[0]))*100\nprint(f\"Sparsity of TFIDF: {round(sparsiry_tfidf,5)}%\")","cdbfc500":"# Tf-IDF feature representaion\ntfidf_unigram_feature_representation = pd.DataFrame(data = x_train_tfidf_unigram.toarray(), columns = tfidf_vect.get_feature_names())\ntfidf_unigram_feature_representation.head()","39e94d17":"# Text Preprocessing funtion for word2vec\ndef clean_text_for_embedding(text):\n    \"\"\"\n    This function sequentially execute all the cleaning and preprocessing function and finaly gives cleaned text.\n    Input: Boolean values of extra_features, strip_html, count_all_cap_words_and_lower_it, replace_typical_misspell, count_non_alpha_numeric_and_remove, remove_stop_words_and_punc, stem\n            (by default all the input values = True)\n    \n    return: clean text \"\"\"\n    \n    # remove html tags\n    clean_text = strip_html(text)  \n    \n    # de-concatenation of words\n    clean_text = replace_typical_misspell(clean_text)\n     \n    # Count the number of non alpha numeric character and remove it\n    clean_text = non_alpha_numeric_remove(clean_text)\n    \n    # removing Stopwords and the words length less than 3(As these words mostly tend to redundant words) excpect 'C' and 'R'and 'OS' <-- programing keywords\n    clean_text = remove_stop_words_and_punc(clean_text)\n\n    return clean_text","52261e9b":"# Preprocessing for word2vec embedding for train and test review text\nX_train_review_text_for_embedding = X_train.apply(lambda x: clean_text_for_embedding(x))\nX_val_review_text_for_embedding = X_val.apply(lambda x: clean_text_for_embedding(x))\n\n\n# Sample\ni=15\nprint(f\"\\nBefore Preprocessing\\n{'='*20}\")\nprint(X_train.iloc[i])\n\nprint(f\"\\nAfter Preprocessing\\n{'='*20}\")\nprint(X_train_review_text_for_embedding.iloc[i])","d0d7792d":"import operator \nimport gensim\nfrom gensim.models import KeyedVectors\n\n# Train the genisim word2vec model with our own custom corpus\n# CBOW -> sg = 0\n\n# Convering text in list of list of train reviews text\nlist_of_sent_train = X_train_review_text_for_embedding.apply(lambda x: x.split()).values\n\n# Convering text in list of list of val reviews text\nlist_of_sent_val = X_val_review_text_for_embedding.apply(lambda x: x.split()).values\n\n# Traing W2V\nmodel_cbow = Word2Vec(sentences= list_of_sent_train, min_count=3, sg=0, workers= 3,size=100) # Default setting","c1d02845":"# Vocab after training\nwords = model_cbow.wv.vocab.keys()\nprint(\"Number of words in vocab\",len(words),\"\\n\\n\")\nprint(words,sep='\\n')","1f0b20ba":"# Top similar word\nmodel_cbow.similar_by_word(\"good\")","85c19464":"'''\n    -->procedure to make avg w2v of each reviews\n    \n    1. find the w2v of each word\n    2. sum-up w2v of each word in a sentence\n    3. divide the total w2v of sentence by total no. of words in the sentence\n'''\n\n# vocablary of w2v model of e-commerce dataset\nvocab=model_cbow.wv.vocab\n\n\n#------------------------------------------------------------------------------------------------------------\n## average Word2Vec for train reviews\n# compute average word2vec for each review.\ntrain_w2v_cbow = [] # the avg-w2v for each sentence\/review in train dataset is stored in this list\n\nlist_of_sent_train = X_train_review_text_for_embedding.apply(lambda x: x.split()).values\n\nfor sent in list_of_sent_train: # for each review\/sentence\n    sent_vec = np.zeros(100) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        if word in vocab:\n            vec = model_cbow.wv[word]\n            sent_vec += vec\n            cnt_words += 1\n    if cnt_words != 0:\n        sent_vec \/= cnt_words\n    train_w2v_cbow.append(sent_vec)\n\nprint(\"Number of datapoints in train: \",len(train_w2v_cbow))\n\n\n#------------------------------------------------------------------------------------------------------------\n\n## average Word2Vec for val reviews\n# compute average word2vec for each review.\nval_w2v_cbow = [] # the avg-w2v for each sentence\/review in train dataset is stored in this list\n\nlist_of_sent_train = X_val_review_text_for_embedding.apply(lambda x: x.split()).values\n\nfor sent in list_of_sent_val: # for each review\/sentence\n    sent_vec = np.zeros(100) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        if word in vocab:\n            vec = model_cbow.wv[word]\n            sent_vec += vec\n            cnt_words += 1\n    if cnt_words != 0:\n        sent_vec \/= cnt_words\n    val_w2v_cbow.append(sent_vec)\n\nprint(\"Number of datapoints in val: \",len(val_w2v_cbow))","b218c4c9":"# Standard scaling of W2V\nsc = StandardScaler()\ntrain_w2v_sc = sc.fit_transform(train_w2v_cbow)\nval_w2v_sc = sc.transform(val_w2v_cbow)\n\n\n## Example : Review text is encoded into 100 dim vector space\nprint(f\"\\n Before encoding: \\n{'='*20}\\n {X_train_review_text_for_embedding.iloc[0]}\")\nprint(f\"\\n After encoding: \\n{'='*20}\\n {train_w2v_sc[0]}\")","db56952c":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\n\n# PCA for visualisation\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(train_w2v_sc)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2'])\n\n# Ploting\nsns.scatterplot(x='principal component 1', y='principal component 2', hue=y_train.values, style=None, size=None, data=principalDf)\nplt.show()","b49b97c2":"# Loading Glove(pretrained) model\nGLOVE_EMBEDDING_PATH = '..\/input\/glove840b300dtxt\/glove.840B.300d.txt'\n\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n    \nembeddings_index = load_embeddings(GLOVE_EMBEDDING_PATH)","f6268bcd":"## Building vocubulary from our Quest Data\ndef build_vocab(sentences, verbose =  True):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n\n#=========================================================================================================\nimport operator \n## This is a common function to check coverage between our quest data and the word embedding\ndef check_coverage(vocab,embeddings_index):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) \/ len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k \/ (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x","fcfc5100":"##Apply the vocab function to get the words and the corresponding counts\nsentences = X_train_review_text_for_embedding.apply(lambda x: x.split()).values\nvocab = build_vocab(sentences)\n\nprint(f\"\\nFor cleaned_question_body_for_embedding: \\n{'-'*40}\")\noov = check_coverage(vocab,embeddings_index)\n\n## List 10 out of vocabulary word\nprint(f\"\\nTop 10 out of vocabulary word: \\n{'-'*30}\")\noov[:10]","e7f4db7a":"#------------------------------------------------------------------------------------------------------------\n## average Word2Vec usnig pretrained model(GLOVE) for train reviews\n# compute average word2vec for each review.\ntrain_w2v_pretrained = [] # the avg-w2v for each sentence\/review in train dataset is stored in this list\n\nlist_of_sent_train = X_train_review_text_for_embedding.apply(lambda x: x.split()).values\n\nfor sent in list_of_sent_train: # for each review\/sentence\n    sent_vec = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        if word in vocab:\n            try:\n                vec = embeddings_index[word]\n                sent_vec += vec\n                cnt_words += 1\n            \n            except:\n                pass\n            \n    if cnt_words != 0:\n        sent_vec \/= cnt_words\n    train_w2v_pretrained.append(sent_vec)\n\nprint(\"Number of datapoints in train: \",len(train_w2v_pretrained))\n\n\n#------------------------------------------------------------------------------------------------------------\n\n## average Word2Vec for val reviews\n# compute average word2vec for each review.\nval_w2v_pretrained = [] # the avg-w2v for each sentence\/review in train dataset is stored in this list\n\nlist_of_sent_train = X_val_review_text_for_embedding.apply(lambda x: x.split()).values\n\nfor sent in list_of_sent_val: # for each review\/sentence\n    sent_vec = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        if word in vocab:\n            try:     \n                vec = embeddings_index[word]\n                sent_vec += vec\n                cnt_words += 1\n                \n            except:\n                pass\n    if cnt_words != 0:\n        sent_vec \/= cnt_words\n    val_w2v_pretrained.append(sent_vec)\n\nprint(\"Number of datapoints in val: \",len(val_w2v_pretrained))","358fa648":"# Standard scaling of W2V\nsc = StandardScaler()\ntrain_w2v_pretrained_sc = sc.fit_transform(train_w2v_pretrained)\nval_w2v_pretrained_sc = sc.transform(val_w2v_pretrained)\n\n\n## Example : Review text is encoded into 100 dim vector space\nprint(f\"\\n Before encoding: \\n{'='*20}\\n {X_train_review_text_for_embedding.iloc[0]}\")\nprint(f\"\\n After encoding: \\n{'='*20}\\n {train_w2v_pretrained_sc[0]}\")","bb29d19e":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\n\n# PCA for visualisation\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(train_w2v_pretrained_sc)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2'])\n\n# Ploting\nsns.scatterplot(x='principal component 1', y='principal component 2', hue=y_train.values, style=None, size=None, data=principalDf)\nplt.show()","ed6084b5":"# Applying Logistic Regression with L2 regularization on BOW\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\n\n\"\"\"\ny_true : array, shape = [n_samples] or [n_samples, n_classes]\nTrue binary labels or binary label indicators.\n\ny_score : array, shape = [n_samples] or [n_samples, n_classes]\nTarget scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of\ndecisions (as returned by \u201cdecision_function\u201d on some classifiers). \nFor binary y_true, y_score is supposed to be the score of the class with greater label.\n\n\"\"\"\n\ntrain_loss = []\ncv_loss = []\nc_range=[10e-5,10e-4,10e-3,10e-2,1,10,10e1,10e2,10e3]\nfor i in c_range:\n    \n    clf=LogisticRegression(penalty='l2', C=i,class_weight='balanced')\n    clf.fit(x_train_bow_unigram, y_train)\n    \n    # Predicting\n    y_train_pred = clf.predict_proba(x_train_bow_unigram)\n    y_cv_pred = clf.predict_proba(x_val_bow_unigram)\n    \n    # Loss metric storing\n    train_loss.append(log_loss(y_train, y_train_pred))\n    cv_loss.append(log_loss(y_val, y_cv_pred))\n    \n    \n# Visualising and finding optimal parameter \nplt.plot(np.arange(1,10,1), train_loss, label='Train loss')\nplt.plot(np.arange(1,10,1), cv_loss, label='CV loss')\nplt.xticks( np.arange(1,10,1), (10e-5, 10e-4, 10e-3, 10e-2, 10e-1, 10e0, 10e1, 10e2, 10e3))\nplt.legend()\nplt.xlabel(\"alpha: hyperparameter\")\nplt.ylabel(\"log loss\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()\n\n\n\n## Training using Optimal hyperparemeter\n# using optimum_k to find generalistion loss\n\noptimum_c = c_range[np.argmin(cv_loss)] #optimum 'alpha'\n\n# Naive Bayes training\nprint(f\"Traing using optimal alpha:  {c_range[np.argmin(cv_loss)]}\\n\")\nclf=LogisticRegression(penalty='l2', C=optimum_c,class_weight='balanced')\nclf.fit(x_train_bow_unigram, y_train)\n    \ny_pred = clf.predict(x_val_bow_unigram)\ny_pred_proba = clf.predict_proba(x_val_bow_unigram)\n\n# Result track\naccuracy = accuracy_score(y_val,y_pred)\nbal_accuracy = balanced_accuracy_score(y_val,y_pred)\nlogloss = log_loss(y_val,y_pred_proba)\nprint(f'\\nGenearalisation log_loss: {logloss:.3f}')\nprint(f\"\\nGeneralisation Accuracy: {(round(accuracy,2))*100}%\")\nprint(f\"\\nGeneralisation Balance accuracy: {(round(bal_accuracy,2))*100}%\")\nprint(f'\\nmisclassification percentage: {(1-accuracy)*100:.2f}%')\n\n\n#ploting confusion matrix\nsn.heatmap(confusion_matrix(y_pred,y_val),annot=True, fmt=\"d\",linewidths=.5)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted values')\nplt.ylabel('Actual values')\nplt.show()\n# Classification Report\nprint(\"\\n\\nclassification report:\\n\",classification_report(y_val,y_pred)) ","ede8f64b":"from lime import lime_text\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom lime.lime_text import LimeTextExplainer\n\n# Countvectoriser\ncount_vect = CountVectorizer(ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None)\nx_train_bow_unigram = count_vect.fit_transform(X_train_clean_text)\nx_val_bow_unigram = count_vect.transform(X_val_clean_text)\n\n# Model Training\nclf=LogisticRegression(penalty='l2', C=1.0)\nclf.fit(x_train_bow_unigram,y_train)\n\n# Creating Pipeline\npipe = make_pipeline(count_vect, clf)\n\n# LimeTextExplainer \nclass_names = ['Negative sentiment','Positive Sentiment']\nexplainer = LimeTextExplainer(class_names=class_names, kernel_width=25, verbose=True, feature_selection='auto', bow=True)","43f91153":"# Prediction\nidx = 115\nexp = explainer.explain_instance(text_instance = X_val_clean_text.iloc[idx], classifier_fn = pipe.predict_proba, num_features=10,num_samples=5000,distance_metric='cosine')\nprint('\\nDocument id: %d' % idx)\nprint(f\"Before Preprocessing\\n{'-'*20}\\n{X_val.iloc[idx]}\\n\")\nprint(f\"After Preprocessing\\n{'-'*20}\\n{X_val_clean_text.iloc[idx]}\\n\")\n\nprint('Probability(Negative sentiment) =',pipe.predict_proba([X_val_clean_text.iloc[idx]])[0,0])\nprint('Probability(Positive sentiment) =',pipe.predict_proba([X_val_clean_text.iloc[idx]])[0,1])\nprint('True class: %s' % class_names[y_train[idx]])\n\n# Local Explanability\n%matplotlib inline\nfig = exp.as_pyplot_figure()","8d068af5":"exp.show_in_notebook(text=True)","9aa14248":"# Prediction\nidx = 115\nexp = explainer.explain_instance(text_instance = X_val_clean_text.iloc[idx], classifier_fn = pipe.predict_proba, num_features=10,num_samples=5000,distance_metric='cosine')\nprint('\\nDocument id: %d' % idx)\nprint(f\"Before Preprocessing\\n{'-'*20}\\n{X_val.iloc[idx]}\\n\")\nprint(f\"After Preprocessing\\n{'-'*20}\\n{X_val_clean_text.iloc[idx]}\\n\")\n\nprint('Probability(Negative sentiment) =', pipe.predict_proba([X_val_clean_text.iloc[idx]])[0,0])\nprint('Probability(Positive sentiment) =', pipe.predict_proba([X_val_clean_text.iloc[idx]])[0,1])\nprint('True class: %s' % class_names[y_train[idx]])\n\n# Local Explanability\n%matplotlib inline\nfig = exp.as_pyplot_figure()","2e8439dd":"exp.show_in_notebook(text=True)","6d871bbc":"# Prediction\nidx = 25\n\n\nexp = explainer.explain_instance(text_instance = X_val_clean_text.iloc[idx], classifier_fn = pipe.predict_proba, num_features=10,num_samples=5000,distance_metric='cosine')\nprint('\\nDocument id: %d' % idx)\nprint(f\"Before Preprocessing\\n{'-'*20}\\n{X_val.iloc[idx]}\\n\")\nprint(f\"After Preprocessing\\n{'-'*20}\\n{X_val_clean_text.iloc[idx]}\\n\")\n\nprint('Probability(Negative sentiment) =', pipe.predict_proba([X_val_clean_text.iloc[idx]])[0,0])\nprint('Probability(Positive sentiment) =', pipe.predict_proba([X_val_clean_text.iloc[idx]])[0,1])\nprint('True class: %s' % class_names[y_train.iloc[idx]])\n\n# Local Explanability\n%matplotlib inline\nfig = exp.as_pyplot_figure()\n\n","acaa21ab":"exp.show_in_notebook(text=True)","2cd45b30":"# Applying SMOTE to balance the dataset\n# imbalanced-learn API: https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.over_sampling.SMOTE.html\nfrom imblearn.over_sampling import SMOTE\n\n# transform the dataset\noversample = SMOTE(sampling_strategy='auto',random_state=None,k_neighbors=5,n_jobs=None)\nX_res_bow, y_res = oversample.fit_resample(x_train_bow_unigram, y_train)\n\n# summarize the new class distribution\ncounter = Counter(y_res)\nprint(\"After applying SMOTE: \",counter)","63af2ac1":"train_loss = []\ncv_loss = []\nalpha_range = [10e-5,10e-4,10e-3,10e-2,1,10,10e1,10e2,10e3]\nfor i in alpha_range:\n    \n    # Training\n    clf = LogisticRegression(penalty='l2', C=i,class_weight='balanced')\n    clf.fit(X_res_bow, y_res)\n    \n    # Predicting\n    y_train_pred = clf.predict_proba(X_res_bow)\n    y_cv_pred = clf.predict_proba(x_val_bow_unigram)\n    \n    # Loss metric storing\n    train_loss.append(log_loss(y_res,y_train_pred))\n    cv_loss.append(log_loss(y_val, y_cv_pred))\n\n    \n# Visualising and finding optimal parameter \nplt.plot(np.arange(1,10,1), train_loss, label='Train loss')\nplt.plot(np.arange(1,10,1), cv_loss, label='CV loss')\nplt.xticks( np.arange(1,10,1), (10e-5, 10e-4, 10e-3, 10e-2, 10e-1, 10e0, 10e1, 10e2, 10e3))\nplt.legend()\nplt.xlabel(\"alpha: hyperparameter\")\nplt.ylabel(\"log loss\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()\n\n\n## Training using Optimal hyperparemeter\n# using optimum_k to find generalistion loss\n\noptimum_c = c_range[np.argmin(cv_loss)] #optimum 'alpha'\n\n# Naive Bayes training\nprint(f\"Traing using optimal alpha:  {c_range[np.argmin(cv_loss)]}\\n\")\nclf=LogisticRegression(penalty='l2', C=optimum_c,class_weight='balanced')\nclf.fit(X_res_bow, y_res)\n\ny_pred = clf.predict(x_val_bow_unigram)\ny_pred_proba = clf.predict_proba(x_val_bow_unigram)\n\n# Result track\naccuracy = accuracy_score(y_val,y_pred)\nbal_accuracy = balanced_accuracy_score(y_val,y_pred)\nlogloss = log_loss(y_val,y_pred_proba)\nprint(f'\\nGenearalisation log_loss: {logloss:.3f}')\nprint(f\"\\nGeneralisation Accuracy: {(round(accuracy,2))*100}%\")\nprint(f\"\\nGeneralisation Balance accuracy: {(round(bal_accuracy,2))*100}%\")\nprint(f'\\nmisclassification percentage: {(1-accuracy)*100:.2f}%')\n\n\n#ploting confusion matrix\nsn.heatmap(confusion_matrix(y_pred,y_val),annot=True, fmt=\"d\",linewidths=.5)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted values')\nplt.ylabel('Actual values')\nplt.show()\n# Classification Report\nprint(\"\\n\\nclassification report:\\n\",classification_report(y_val,y_pred)) ","c3b1c6bd":"# Applying Logistic Regression with L2 regularization on BOW\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\n\n\"\"\"\ny_true : array, shape = [n_samples] or [n_samples, n_classes]\nTrue binary labels or binary label indicators.\n\ny_score : array, shape = [n_samples] or [n_samples, n_classes]\nTarget scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of\ndecisions (as returned by \u201cdecision_function\u201d on some classifiers). \nFor binary y_true, y_score is supposed to be the score of the class with greater label.\n\n\"\"\"\n\ntrain_loss = []\ncv_loss = []\nc_range=[10e-5,10e-4,10e-3,10e-2,1,10,10e1,10e2,10e3]\nfor i in c_range:\n    \n    clf=LogisticRegression(penalty='l2', C=i,class_weight='balanced')\n    clf.fit(x_train_tfidf_unigram, y_train)\n    \n    # Predicting\n    y_train_pred = clf.predict_proba(x_train_tfidf_unigram)\n    y_cv_pred = clf.predict_proba(x_val_tfidf_unigram)\n    \n    # Loss metric storing\n    train_loss.append(log_loss(y_train, y_train_pred))\n    cv_loss.append(log_loss(y_val, y_cv_pred))\n    \n    \n# Visualising and finding optimal parameter \nplt.plot(np.arange(1,10,1), train_loss, label='Train loss')\nplt.plot(np.arange(1,10,1), cv_loss, label='CV loss')\nplt.xticks( np.arange(1,10,1), (10e-5, 10e-4, 10e-3, 10e-2, 10e-1, 10e0, 10e1, 10e2, 10e3))\nplt.legend()\nplt.xlabel(\"alpha: hyperparameter\")\nplt.ylabel(\"log loss\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()\n\n\n\n## Training using Optimal hyperparemeter\n# using optimum_k to find generalistion loss\n\noptimum_c = c_range[np.argmin(cv_loss)] #optimum 'alpha'\n\n# Naive Bayes training\nprint(f\"Traing using optimal alpha:  {c_range[np.argmin(cv_loss)]}\\n\")\nclf=LogisticRegression(penalty='l2', C=optimum_c,class_weight='balanced')\nclf.fit(x_train_tfidf_unigram, y_train)\n    \ny_pred = clf.predict(x_val_tfidf_unigram)\ny_pred_proba = clf.predict_proba(x_val_tfidf_unigram)\n\n# Result track\naccuracy = accuracy_score(y_val,y_pred)\nbal_accuracy = balanced_accuracy_score(y_val,y_pred)\nlogloss = log_loss(y_val,y_pred_proba)\nprint(f'\\nGenearalisation log_loss: {logloss:.3f}')\nprint(f\"\\nGeneralisation Accuracy: {(round(accuracy,2))*100}%\")\nprint(f\"\\nGeneralisation Balance accuracy: {(round(bal_accuracy,2))*100}%\")\nprint(f'\\nmisclassification percentage: {(1-accuracy)*100:.2f}%')\n\n\n#ploting confusion matrix\nsn.heatmap(confusion_matrix(y_pred,y_val),annot=True, fmt=\"d\",linewidths=.5)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted values')\nplt.ylabel('Actual values')\nplt.show()\n# Classification Report\nprint(\"\\n\\nclassification report:\\n\",classification_report(y_val,y_pred)) ","cf96abe1":"from lime import lime_text\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom lime.lime_text import LimeTextExplainer\n\n# TFIDF vectoriser\ntfidf_vect = TfidfVectorizer(ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None)\nx_train_tfidf_unigram = tfidf_vect.fit_transform(X_train_clean_text)\nx_val_tfidf_unigram = tfidf_vect.transform(X_val_clean_text)\n\n# Model Training\nclf=LogisticRegression(penalty='l2', C=1.0)\nclf.fit(x_train_tfidf_unigram,y_train)\n\n# Creating Pipeline\npipe = make_pipeline(tfidf_vect, clf)\n\n# LimeTextExplainer \nclass_names = ['Negative sentiment','Positive Sentiment']\nexplainer = LimeTextExplainer(class_names=class_names, kernel_width=25, verbose=True, feature_selection='auto', bow=True)","4b374c55":"# Prediction\nidx = 115\nexp = explainer.explain_instance(text_instance = X_val_clean_text.iloc[idx], classifier_fn = pipe.predict_proba, num_features=10,num_samples=5000,distance_metric='cosine')\nprint('\\nDocument id: %d' % idx)\nprint(f\"Before Preprocessing\\n{'-'*20}\\n{X_val.iloc[idx]}\\n\")\nprint(f\"After Preprocessing\\n{'-'*20}\\n{X_val_clean_text.iloc[idx]}\\n\")\n\nprint('Probability(Negative sentiment) =',pipe.predict_proba([X_val_clean_text.iloc[idx]])[0,0])\nprint('Probability(Positive sentiment) =',pipe.predict_proba([X_val_clean_text.iloc[idx]])[0,1])\nprint('True class: %s' % class_names[y_train[idx]])\n\n# Local Explanability\n%matplotlib inline\nfig = exp.as_pyplot_figure()","0be25384":"exp.show_in_notebook(text=True)","13069dc9":"# Applying Logistic Regression with L2 regularization on BOW\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\n\n\"\"\"\ny_true : array, shape = [n_samples] or [n_samples, n_classes]\nTrue binary labels or binary label indicators.\n\ny_score : array, shape = [n_samples] or [n_samples, n_classes]\nTarget scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of\ndecisions (as returned by \u201cdecision_function\u201d on some classifiers). \nFor binary y_true, y_score is supposed to be the score of the class with greater label.\n\n\"\"\"\n\ntrain_loss = []\ncv_loss = []\nc_range=[10e-5,10e-4,10e-3,10e-2,1,10,10e1,10e2,10e3]\nfor i in c_range:\n    \n    clf=LogisticRegression(penalty='l2', C=i,class_weight='balanced')\n    clf.fit(train_w2v_sc, y_train)\n    \n    # Predicting\n    y_train_pred = clf.predict_proba(train_w2v_sc)\n    y_cv_pred = clf.predict_proba(val_w2v_sc)\n    \n    # Loss metric storing\n    train_loss.append(log_loss(y_train, y_train_pred))\n    cv_loss.append(log_loss(y_val, y_cv_pred))\n    \n    \n# Visualising and finding optimal parameter \nplt.plot(np.arange(1,10,1), train_loss, label='Train loss')\nplt.plot(np.arange(1,10,1), cv_loss, label='CV loss')\nplt.xticks( np.arange(1,10,1), (10e-5, 10e-4, 10e-3, 10e-2, 10e-1, 10e0, 10e1, 10e2, 10e3))\nplt.legend()\nplt.xlabel(\"alpha: hyperparameter\")\nplt.ylabel(\"log loss\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()\n\n\n\n## Training using Optimal hyperparemeter\n# using optimum_k to find generalistion loss\n\noptimum_c = c_range[np.argmin(cv_loss)] #optimum 'alpha'\n\n# Naive Bayes training\nprint(f\"Traing using optimal alpha:  {c_range[np.argmin(cv_loss)]}\\n\")\nclf=LogisticRegression(penalty='l2', C=optimum_c,class_weight='balanced')\nclf.fit(train_w2v_sc, y_train)\n    \ny_pred = clf.predict(val_w2v_sc)\ny_pred_proba = clf.predict_proba(val_w2v_sc)\n\n# Result track\naccuracy = accuracy_score(y_val,y_pred)\nbal_accuracy = balanced_accuracy_score(y_val,y_pred)\nlogloss = log_loss(y_val,y_pred_proba)\nprint(f'\\nGenearalisation log_loss: {logloss:.3f}')\nprint(f\"\\nGeneralisation Accuracy: {(round(accuracy,2))*100}%\")\nprint(f\"\\nGeneralisation Balance accuracy: {(round(bal_accuracy,2))*100}%\")\nprint(f'\\nmisclassification percentage: {(1-accuracy)*100:.2f}%')\n\n\n#ploting confusion matrix\nsn.heatmap(confusion_matrix(y_pred,y_val),annot=True, fmt=\"d\",linewidths=.5)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted values')\nplt.ylabel('Actual values')\nplt.show()\n# Classification Report\nprint(\"\\n\\nclassification report:\\n\",classification_report(y_val,y_pred)) ","98e39826":"# Applying Logistic Regression with L2 regularization on BOW\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\n\n\"\"\"\ny_true : array, shape = [n_samples] or [n_samples, n_classes]\nTrue binary labels or binary label indicators.\n\ny_score : array, shape = [n_samples] or [n_samples, n_classes]\nTarget scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of\ndecisions (as returned by \u201cdecision_function\u201d on some classifiers). \nFor binary y_true, y_score is supposed to be the score of the class with greater label.\n\n\"\"\"\n\ntrain_loss = []\ncv_loss = []\nc_range=[10e-5,10e-4,10e-3,10e-2,1,10,10e1,10e2,10e3]\nfor i in c_range:\n    \n    clf=LogisticRegression(penalty='l2', C=i,class_weight='balanced')\n    clf.fit(train_w2v_pretrained_sc, y_train)\n    \n    # Predicting\n    y_train_pred = clf.predict_proba(train_w2v_pretrained_sc)\n    y_cv_pred = clf.predict_proba(val_w2v_pretrained_sc)\n    \n    # Loss metric storing\n    train_loss.append(log_loss(y_train, y_train_pred))\n    cv_loss.append(log_loss(y_val, y_cv_pred))\n    \n    \n# Visualising and finding optimal parameter \nplt.plot(np.arange(1,10,1), train_loss, label='Train loss')\nplt.plot(np.arange(1,10,1), cv_loss, label='CV loss')\nplt.xticks( np.arange(1,10,1), (10e-5, 10e-4, 10e-3, 10e-2, 10e-1, 10e0, 10e1, 10e2, 10e3))\nplt.legend()\nplt.xlabel(\"alpha: hyperparameter\")\nplt.ylabel(\"log loss\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()\n\n\n\n## Training using Optimal hyperparemeter\n# using optimum_k to find generalistion loss\n\noptimum_c = c_range[np.argmin(cv_loss)] #optimum 'alpha'\n\n# Naive Bayes training\nprint(f\"Traing using optimal alpha:  {c_range[np.argmin(cv_loss)]}\\n\")\nclf=LogisticRegression(penalty='l2', C=optimum_c,class_weight='balanced')\nclf.fit(train_w2v_pretrained_sc, y_train)\n    \ny_pred = clf.predict(val_w2v_pretrained_sc)\ny_pred_proba = clf.predict_proba(val_w2v_pretrained_sc)\n\n# Result track\naccuracy = accuracy_score(y_val,y_pred)\nbal_accuracy = balanced_accuracy_score(y_val,y_pred)\nlogloss = log_loss(y_val,y_pred_proba)\nprint(f'\\nGenearalisation log_loss: {logloss:.3f}')\nprint(f\"\\nGeneralisation Accuracy: {(round(accuracy,2))*100}%\")\nprint(f\"\\nGeneralisation Balance accuracy: {(round(bal_accuracy,2))*100}%\")\nprint(f'\\nmisclassification percentage: {(1-accuracy)*100:.2f}%')\n\n\n#ploting confusion matrix\nsn.heatmap(confusion_matrix(y_pred,y_val),annot=True, fmt=\"d\",linewidths=.5)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted values')\nplt.ylabel('Actual values')\nplt.show()\n# Classification Report\nprint(\"\\n\\nclassification report:\\n\",classification_report(y_val,y_pred)) ","f6d3ba54":"from prettytable import PrettyTable\nx = PrettyTable()\nx.field_names = [\"S.No\",\"Model(text featurization)\", \"generalization log loss\", \"generalization AUC(%age) \", \"generalization balance accuracy\",]\n\nx.add_row([1, \"Logistic Reg(BOW)\", 0.259, 90, 83])\nx.add_row([2, \"Logistic Reg(BOW) + SMOTE\", 0.274, 89, 78])\n\nx.add_row([3, \"Logistic Reg(TFIDF)\", 0.239, 90, 84])\n\nx.add_row([4, \"Logistic Reg(W2V_train) \", 0.396, 81, 81])\n\nx.add_row([5, \"Logistic Reg(W2V_pretrain)\", 0.361, 84, 83])\n\nprint(x)","ee794869":"Observation:\n* Class labels are high imbalance. Using accuracy as metric is really not a good idea to check the performance of model.","edd1bf2e":"## 1.2. Machine Learning Formulation\n\n#### Its a simple Binary Classification \n![image.png](attachment:image.png)\n\n### 1.2.1. Evaluation Metric\n\n#### 1. Logloss ass primary metric. (want to penalise based on probability score of each class and also for better interpretability of model for both of the class)\n#### 2. Classification Report as secondary metric (include accuracy, precision, recall, f1_score)  ","65646065":"#### Building vocubulary from our Dataset\nRefer: https:\/\/www.kaggle.com\/phoenix9032\/quest-preprocessing-data-for-embedding","d64d5e8f":"# 1. About Data, Machine Learning Formulation and EDA","edfaec1a":"#### AVG W2V Feature Exraction","e279c34f":"![](http:\/\/)![image.png](attachment:image.png)","13eac6fd":"### Train Test Split","452166fb":"### 3.2 Word2vec \n\n#### problem with BOW or TFIDF approach\n\n![image.png](attachment:image.png)\n","e2e4806b":"### 1.3.5. WordCloud hue by class label","e777163c":"### 1.3.1. Pandas Profiling\n\nLearning Resource for pandas profiling:\n\n1. https:\/\/towardsdatascience.com\/a-better-eda-with-pandas-profiling-e842a00e1136\n\n2. https:\/\/github.com\/pandas-profiling\/pandas-profiling\n","39b05344":" ### 4.4 Logistic Regression on W2V(trained on own corpus)","c5c12185":"### 2.2 Top Frequency of words in negative class","de780f3a":"# Sentiment classification on E-commerce dataset\n----\n----\n\n## Workflow\n-----\n\n### 1. About Data, Machine Learning Formulation and EDA\n\n### 2. Preprocessing\n\n### 3. Feature Engineering\n\n### 4. Modeling and hypertuining\n\n### 5. Result and deploying prediction pipeline\n","5cafba9a":"## 1.1.1. Loading the dataset and dataset Overview\n","99f0f198":"### 4.3. Logistic Regression on TFIDF","ad02ab3c":"###  Importing llibraries","67153e1a":"#### Check Coverage for review_text_for_embedding","a3349910":"#### W2V Embeddings using CBOW","82a8bc2e":"----","f31717b6":"### 1.3.3. Distribution of Number of words in Reviews text","3c15dec1":"#### Visualising AVG W2V pretrained model using PCA","7428ddff":"#### Intuition of W2V \n\n![image.png](attachment:image.png)","1eff8a07":"# 4. Modeling","fda72036":"### N-gram Explaination\n![image.png](attachment:image.png)","d3a4f4f0":"### LIME (Logistic Regression on BOW)\n\n1. Preprocessed all the text\n2. Split the data into train and text\n3. Create Pipeline of feature_extraction(count_vect) and model(Logistic reg.)\n4. Create LimeTextExplainer object\n5. Explain the prediction of text_instance given by classifier_fn ","62356040":"----","cd08fc54":"Word2vec Visualiser: http:\/\/projector.tensorflow.org\/\n\n#### W2V try to capture the semantic of word with respect to its neighborhood s.t. distance between simantically similar words is less than distance between non similar.simantical words.\nExample\n1. (kids,child) --> simantically similar\n2. (kids, baseball) --> simantically unsimilar\n\n![image.png](attachment:image.png)","cf223f19":"****","a5fcf339":"#### Visualizing Reviews text using Avg W2V CBOW","16bfb59b":"#### Preprocessing for BOW and TFIDF","6401a9c2":"#### Example 1","40f6d5e5":"### 1.3.2 Missing Values and Duplicate values","4f8c7326":"### 3.2. Word2vec - pretrained (Glove)","98d17c0c":"### 4.1. Logistic Regression on BOW","12f99063":"#### Preprocessing for Embeddings","86786827":"### 2.1. Distribution  number of word in text (before and after preprocessing)","6a9e3777":"Refer: https:\/\/towardsdatascience.com\/3-basic-approaches-in-bag-of-words-which-are-better-than-word-embeddings-c2cbc7398016\n\n![image.png](attachment:image.png)","eb6e8699":"### 2.2 Top Frequency of words in positive class","4efb2c55":"## 2. Preprocessing\n\n1. tokenize and lowering words\n2. Decontraction (mis spelled words)\n3. Cleaning(html tags remove, punctuation remove etc)\n4. stopwords remove\n5. lemmatization or stemming\n5. Unnecessary words remove(word_len<3)\n","c099880b":"## 1.3. EDA\n\n* Dataset Overview\n* Distribution of classlabel\n* Missing Values and Duplicate values\n* Number of words in review text distribution\n* WordCloud (Visualisation of words frequency)\n* Frequency distribution (Using countvectoriser ngram) ","3cb4fad9":"### 1.3.4. Word Cloud","42fc9d3a":"#### Example 1","6e0d37b8":"### Explanation . . .. ","46df341c":"### 3.2. TF-IDF\n\nhttps:\/\/www.slideshare.net\/hadyelsahar\/word-embedings-why-the-hype-55769273\n\n![image.png](attachment:image.png)\n","edc73057":"#### Example 2","09a2f458":"### 2.1. WordCloud after preprocessing (hue by class label)","60035766":"### LIME (Logistic regression on TFIDF)\n\n1. Preprocessed all the text\n2. Split the data into train and text\n3. Create Pipeline of feature_extraction(TFIDF_vect) and model(Logistic reg.)\n4. Create LimeTextExplainer object\n5. Explain the prediction of text_instance given by classifier_fn ","8936af68":"### 1.3.2 Distribution of class labels(sentiments)","b03773d0":"## 1.1 About Dataset\n\n\n#### Dataset Source: https:\/\/www.kaggle.com\/nicapotato\/womens-ecommerce-clothing-reviews\n\n\n### Context:\nWelcome. This is a Women\u2019s Clothing E-Commerce dataset revolving around the reviews written by customers. Its nine supportive features offer a great environment to parse out the text through its multiple dimensions. Because this is real commercial data, it has been anonymized, and references to the company in the review text and body have been replaced with \u201cretailer\u201d.\n\n\n### Content:\n\n#### This dataset includes 23486 rows and 10 feature variables. Each row corresponds to a customer review, and includes the variables:\n\n**Clothing ID:** Integer Categorical variable that refers to the specific piece being reviewed.\n\n**Age:** Positive Integer variable of the reviewers age.\n\n**Title:** String variable for the title of the review.\n\n**Review Text:** String variable for the review body.\n\n**Rating:** Positive Ordinal Integer variable for the product score granted by the customer from 1 Worst, to 5 Best.\n\n**Recommended IND:** Binary variable stating where the customer recommends the product where 1 is recommended, 0 is not recommended.\n\n**Positive Feedback Count:** Positive Integer documenting the number of other customers who found this review positive.\n\n**Division Name:** Categorical name of the product high level division.\n\n**Department Name:** Categorical name of the product department name.\n\n**Class Name:** Categorical name of the product class name.\n\n\n### Note: Iam only using *'Review Text'* as features and *'Rating'* as class labels and converting into binary classification problem.\n#### {1: positive sentiment, 0: Negative sentiment}","13e8cf54":"#### AVG W2V using pretrained model","b696c71d":"#### 3.1.1. BOW - unigram ","b6acafb5":"----","283ecd89":"#### Standard scaling of Avg W2V (Trained on own corpus)","bdffd134":"#### Correct Prediction","46d307dd":"### 4.2. Logistic Regression on BOW + SMOTE","786ca679":" ### 4.5. Logistic Regression on W2V pretrained(GLOVE)","d8beeca4":"## 3. Feature Engineering or Feature Extraction\n\n#### 3.1. Bag of Words (countvectoriser)\n#### 3.2. TF-IDF\n#### 3.3. Word2Vec pretrained\n#### 3.4. Word2Vec trained on own corpus","819bdc28":"### 3.1. Bag of Words (BOW)\nRefer: https:\/\/www.slideshare.net\/hadyelsahar\/word-embedings-why-the-hype-55769273\n\n![image.png](attachment:image.png)","705c7096":"### 3.2. Word2vec trained on our own corpus","8b1f91d9":"#### Incorrect prediction","6008a9d2":"****","6c6253ce":"## RESULT","16aa28bd":"#### 3.1.2. BOW - bigram"}}