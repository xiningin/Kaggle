{"cell_type":{"0f7956c4":"code","6c788dde":"code","bf66ff97":"code","884417c9":"code","45976016":"code","e971776d":"code","16fdcf86":"code","868632a9":"code","779b4c56":"code","2107bf5f":"code","5b3434de":"code","48ee8f4a":"code","f40486b9":"code","119c1166":"code","6b1edb66":"code","9cc300f5":"markdown","44e0b7c5":"markdown","756bce06":"markdown","3d56c37b":"markdown","d6b718af":"markdown","f271b5c6":"markdown","5d8af8ad":"markdown","87a74c93":"markdown","56242282":"markdown","52820d8d":"markdown","9999b8d1":"markdown","4019a716":"markdown","f87dc5f1":"markdown","f5f8c168":"markdown","07e658c6":"markdown"},"source":{"0f7956c4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (8,6) # setting a default preferred size for plots\nplt.style.use('fivethirtyeight')\nsns.set(style='darkgrid')\nimport scipy\nfrom scipy import stats","6c788dde":"df = pd.read_csv('..\/input\/mobility\/train.csv')\ndf","bf66ff97":"df.drop('Trip_ID', axis=1,inplace=True); target = 'Surge_Pricing_Type'\nnumfeat, catfeat = list(df.select_dtypes(include=np.number)), list(df.select_dtypes(exclude=np.number)); numfeat.remove(target)\ndf","884417c9":"df.info()","45976016":"import missingno as msno; msno.bar(df, figsize=(16,6)); plt.show()","e971776d":"df[numfeat].skew()","16fdcf86":"for col in catfeat:\n    df[col].fillna(df[col].mode()[0], inplace=True)\nfor col in numfeat:\n    df[col].fillna(df[col].median(), inplace=True)","868632a9":"f, a = plt.subplots(2,4, figsize=(24,12))\na = a.flatten().T\nfor i, col in enumerate(df[numfeat].columns):\n    sns.distplot(df[col],ax=a[i],kde=False).set_title('Skew: {:.4f}'.format(df[col].skew()))\nplt.show()","779b4c56":"f, a = plt.subplots(2,4, figsize=(24,12))\na = a.flatten().T\nfor i, col in enumerate(df[numfeat].columns):\n    stats.probplot(df[col], plot=a[i])\n    a[i].set_title(col)\nplt.show()","2107bf5f":"## REMOVING OBSERVATIONS BASED ON CLASSIFICATION IN 'EXAMPLE OUTLIERS' FOR FEATURE 'Var1'\ndf.drop(df[df['Var1']>160].index).reset_index(drop=True)","5b3434de":"## TRANSFORMING THE FEATURE TO TREAT OUTLIERS USING BOXCOX TRANSFORMATION FOR FEATURE 'Var3'\ntemp = pd.Series(stats.boxcox(df['Var3'],lmbda=stats.boxcox_normmax(df['Var3'])))\nstats.probplot(temp, plot=plt); plt.show()","48ee8f4a":"## TREATING OUTLIERS AS MISSING VALUES FOR FEATURE 'Life_Style_Index'\ndf['Life_Style_Index'].where(df['Life_Style_Index']<4.0, df['Life_Style_Index'].median())","f40486b9":"sns.pairplot(df[numfeat]); plt.show()","119c1166":"sns.scatterplot('Var2', 'Var3', data=df, hue = target).set_title('Correlation: {:.4f}'.format(df['Trip_Distance'].corr(df['Life_Style_Index']))); plt.show()","6b1edb66":"from scipy.spatial import ConvexHull\n\nextremes = df[['Var2', 'Var3']].to_numpy()\n\nhull = ConvexHull(extremes)\n\n# print(extremes[hull.vertices])\n\nplt.plot(df[\"Var2\"], df[\"Var3\"], 'ok')\nplt.plot(extremes[hull.vertices, 0], extremes[hull.vertices,1], 'r--', lw = 2)\nplt.plot(extremes[hull.vertices, 0], extremes[hull.vertices,1], 'ro', lw = 2)\nplt.show()","9cc300f5":"# A Message to readers\n\nGiven the importance of Data Preprocessing in ML, I as a beginner used to be inconsiderate of its effect in model performance. Neglecting it always put me behind in competitions until I explored the methods comprehensively and learned what data science really is. I aim to cover the methods in my notebooks so that I can enlist the prevalent methods for my and your future reference.\n\nBelow is my take on how handle outliers in your data. My target is to make it comprehensive and include all the research done in the field. Yet the notebook is not extensive but will be will in the near future. Stay tuned for future versions. \n\n\nPlease provide your suggestions and compliments in comments! Feel to point out any mistakes I might have made. They help a lot in the learning process!!\n\nUpvote and follow the notebook for updates on future versions! Happy Learning :)","44e0b7c5":"So simple and pretty already.\n\nUnderstand what it tells? Well, obviously...","756bce06":"## Now, what is this doing for us? Thats for you to answer. I will upload the answer in the next version of the notebook which will also include:\n* ###  Complete bi-variate and multivariate analysis\n* ###  Un-supervised learning for outlier-inlier clustering\n* ###  Outliers and Marginal Objects Detection\n* ###  Do categorical variables have any outliers?\n\n### I am sure we are gonna learn a lot from this notebook. I would learn from your experience and suggestions and you might learn a bit from my research and coverage of the topic so crucial to Data Preprocessing! See ya!","3d56c37b":"So, these was the most basic (and comprehensive) way of treating outliers based on univariate analysis of numeric features. Doing so might yield you better results already. But we aren't done yet, **are we?**\n\n## Detecting (more) outliers using Multivariate Analysis\nYour data might still have outliers so we are not done yet! How do we detect them in the first place?\n\nWell, we are still left with bivariate (involving two variables\/features) analysis of the data. These will the last kind of analysis we'll be able to plot (unless you delve into 3D plotting or 2D plots with hue).\n\nStarting off with bivariate visualizations we realize that we can now have \n* Numeric-Numeric analysis\n* and Numeric-Categorical analysis \n\nfor which we will use scatterplots and boxplots respectively.\n\n### Scatterplots\nA scatterplot is a graphic tool used to display the relationship between two quantitative variables. They are nice to the eye and easy to interpret and draw conclusions from. Lets see how!\n","d6b718af":"### Normal Probability Plot\n#### The normal probability plot is formed by:\n   *  Vertical axis: Ordered response values\n   *  Horizontal axis: Normal order statistic medians\nWell, what do they mean? Check it out [here](https:\/\/online.stat.psu.edu\/stat501\/lesson\/4\/4.6).\n   \nThe crux is - **The further the points vary from this line, the greater the indication of departures from normality.**","f271b5c6":"### Dont like tabular ugly datas hence dont understand a thing?\n[This](https:\/\/www.kaggle.com\/twinkle0705\/a-comprehensive-guide-to-handle-missing-values) notebook tells you about beautiful insightful visualizations for missing values. Lets be done with it and move to the main methods of outlier handling.","5d8af8ad":"### Distribution plots","87a74c93":"See, the numeric features do not have much skew. We can safely assume mean ~= median and fill with median. \n\nYou get what I am saying? Well, obviously...","56242282":"# IMPORTS & necessary data preprocessing before getting into Outlier handling","52820d8d":"Do you see any outliers for any feature? Well, with plots it can be a bit difficult more so when the data is huge. Plus there is an argument of artificial vs natural outliers. How do we go on about deciding which outlier is which? \n\nIt is hard to identify outliers if the data range is not specified.","9999b8d1":"## Detecting outliers using Univariate Analysis\n\nThere are numerous ways one can go about detecting outliers. The one which we usually use - visualizations of univariate, bivariate, trivariate.....multivariate, and statistical calculations. \n\nUnivariate visualizations would be the case of checking the distribution of data. \n\n","4019a716":"# Now, outliers - what are they?\nOutlier is defined as an observation point that is distant from the mainstream data. The presence of outliers can break a model\u2019s analysis ability. \nOne of the most efficient ways to identify outliers may be data visualization. But we'll go beyond that in this notebook.\n\n### Definition of Hawkins [Hawkins 1980]:\n\u201cAn outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism\u201d\n\n\nUsually, features are expected to follow some statistical process - a generating mechanism. A lot of times we observe the feature distributions deviating from this \"generating mechanism\". \n\nML models generally expect **Normal Distribution** of the data hence we proceed assuming our features should be Normally Distributed. \n\n*(Ever wonder \"WHY NORMAL DISTRIBUTION EVERYTIME!!!\"? Here's [C\u00e9dric Villani](https:\/\/www.springer.com\/birkhauser?SGWID=0-40290-6-986722-0) in his [TED Talk explaining why](https:\/\/www.youtube.com\/watch?v=Kc0Kthyo0hU)).*\n\nOutlier handling involves 2 steps:-\n 1. Detecting the outliers\n 2. Treating the detected outliers\n \nLets proceed with various methods of detection of the outliers. ","f87dc5f1":"References:-\n*  [Sets up the pipeline for you](https:\/\/www.analyticsvidhya.com\/blog\/2016\/01\/guide-data-exploration\/)\n*  [A great comprehensive resource](https:\/\/www.itl.nist.gov\/div898\/handbook\/eda\/section3\/eda35h.htm) to learn statistical methods\n*  [Pandas documentation](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/frame.html) comes in very handy\n*  [Scipy Stats documentation](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.probplot.html) is worth a read after your stats lecture\n*  [Google Scholar search on published papers](https:\/\/scholar.google.co.in\/scholar?q=outlier+detection+papers&hl=en&as_sdt=0&as_vis=1&oi=scholart) ","f5f8c168":"Now, the assumption that out data should be Normally Distributed as the generating mechanism was so, we can clearly see from the probability plots that there exist values which are responsible for deviation from Normal Distribution. \n\nSo, one can nitpick values from the visualizations and classify them as \"Outliers\" and treat accordingly.\n\n#### Example:-\n*  Similarly, values beyond 160 can be treated as **Outliers** for feature 'Var1'.\n*  It can be clearly seen that values between 50 and 60, and values beyond 150 are responsible for the deviation and hence can be classified as **Outliers** for feature 'Var3'.\n*  Values beyond 4.0 are **Outliers** for feature 'Life_Style_Index' and so on for other features too..\n\n## Treatment of outliers\n*  Removing observations which involve these outliers \n*  Or transform the data \n*  Or treat them as we would treat missing values and impute accordingly","07e658c6":"# Problem Statement\nThe problem statement was posted on [Analytics Vidhya](https:\/\/datahack.analyticsvidhya.com\/contest\/all\/) as a competition - [JanataHack: Mobility Analytics](https:\/\/datahack.analyticsvidhya.com\/contest\/janatahack-mobility-analytics\/). \nYou can access the data there while I will post the gist of the problem statement which will make you ready to go!\n\nWelcome to Sigma Cab Private Limited - a cab aggregator service. Their customers can download their app on smartphones and book a cab from any where in the cities they operate in. They, in turn search for cabs from various service providers and provide the best option to their client across available options. They have been in operation for little less than a year now. During this period, they have captured surge_pricing_type from the service providers.\n\nYou have been hired by Sigma Cabs as a Data Scientist and have been asked to build a predictive model, which could help them in predicting the surge_pricing_type pro-actively. This would in turn help them in matching the right cabs with the right customers quickly and efficiently.\n\n![image.png](attachment:image.png)\n\nI had submitted my score and got a rank of roughly 170 (as far as I remember). I would share the approach in a future notebook and stick with outlier handling in this one!\n\n### It is a multi-class classification problem."}}