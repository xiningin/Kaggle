{"cell_type":{"b73246e8":"code","7e03535c":"code","c62dbfc3":"code","c5d278a5":"code","6feed53d":"code","6f9665b0":"code","2be517c5":"code","01d569bb":"code","c2d822cd":"code","9ed12c86":"code","40d703f5":"code","a9efd9d7":"code","7077d662":"code","75bea67e":"code","deac5cf4":"code","fe769f41":"code","e9fe17ac":"code","501fe09a":"code","5f011c18":"code","3dba2a1a":"code","8f9590c8":"code","bbbd3579":"code","71b267fe":"code","15279efe":"code","fc7c14bb":"code","7368aace":"code","6992762c":"code","aec207d3":"code","e2ea14f0":"code","e8e3f475":"code","d74933e8":"code","2af08248":"code","44bdc009":"code","b9dac626":"code","a0d25451":"code","f2555ba1":"code","5cf25828":"code","660c10dc":"code","3a95fef7":"code","2fac6833":"code","06581d5c":"code","0cc45aee":"code","3ea5aa2a":"code","f089b075":"code","61511d2b":"code","f89c3864":"code","57488c55":"code","09554e77":"code","464c6d59":"code","03a567d5":"code","54373c8a":"code","e492f82b":"code","49202a56":"code","56686bb1":"code","647acdfa":"markdown","775ad8cd":"markdown","cac45d75":"markdown","6ec97840":"markdown","c87af01b":"markdown","c0dc08bb":"markdown","c17b19cd":"markdown","1fb432e1":"markdown","8456c54b":"markdown","114b82f4":"markdown"},"source":{"b73246e8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7e03535c":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","c62dbfc3":"df = pd.read_csv('..\/input\/fraud-detection-bank-dataset-20k-records-binary\/fraud_detection_bank_dataset.csv')\ndf.head(5)","c5d278a5":"df.info()","6feed53d":"df.describe()","6f9665b0":"df.isnull().sum()","2be517c5":"df.columns","01d569bb":"df.shape","c2d822cd":"#plt.figure(figsize=(16, 6))\n#heat_map = sns.heatmap(df.corr(), cmap = \"coolwarm\")","9ed12c86":"X = df.iloc[:,0:-1]\nX.head(5)","40d703f5":"y = df.iloc[:, -1]\ny","a9efd9d7":"print ('Data Set:', X.shape,  y.shape)","7077d662":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\n","75bea67e":"from sklearn.neighbors import KNeighborsClassifier","deac5cf4":"k = 4\n#Train Model and Predict  \nknn_4 = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nknn_4","fe769f41":"yhat = knn_4.predict(X_test)\nyhat[0:5]","e9fe17ac":"from sklearn import metrics\nprint(\"Train set Accuracy: \", metrics.accuracy_score(y_train, knn_4.predict(X_train)))\nprint(\"Test set Accuracy: \", metrics.accuracy_score(y_test, yhat))","501fe09a":"Ks = 10\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\n\nfor n in range(1,Ks):\n    \n    #Train Model and Predict  \n    knn = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat=knn.predict(X_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n\n    \n    std_acc[n-1]=np.std(yhat==y_test)\/np.sqrt(yhat.shape[0])\n\nmean_acc","5f011c18":"plt.plot(range(1,Ks),mean_acc,'g')\nplt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\nplt.fill_between(range(1,Ks),mean_acc - 3 * std_acc,mean_acc + 3 * std_acc, alpha=0.10,color=\"green\")\nplt.legend(('Accuracy ', '+\/- 1xstd','+\/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Neighbors (K)')\nplt.tight_layout()\nplt.show()","3dba2a1a":"print( \"The best accuracy was with\", mean_acc.max(), \"with k=\", mean_acc.argmax()+1) ","8f9590c8":"k = 1\nknn_1 = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nyhat_1 = knn_1.predict(X_test)\nprint(\"Train set Accuracy: \", metrics.accuracy_score(y_train, knn_1.predict(X_train)))\nprint(\"Test set Accuracy: \", metrics.accuracy_score(y_test, yhat_1))","bbbd3579":"from sklearn.metrics import jaccard_score\njaccard_score(y_test, yhat_1,pos_label=0)","71b267fe":"from sklearn.metrics import f1_score\nf1_score(y_test, yhat_1, average='weighted') ","15279efe":"Results = pd.DataFrame({'Algorithm': [],'Accuracy_score': [], 'J-score': [], \"F-1 score\":[], \"log_loss\":[]})","fc7c14bb":"res = pd.DataFrame({'Algorithm': ['KNN'],'Accuracy_score': [metrics.accuracy_score(y_test, yhat_1)], \"J-score\":[jaccard_score(y_test, yhat_1)], \"F-1 score\":[f1_score(y_test, yhat_1, average='weighted')]})\n \nResults = Results.append(res)","7368aace":"from sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier","6992762c":"DTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4)\nDTree","aec207d3":"# Training and Predicting\n\nDTree.fit(X_train,y_train)\npredTree = DTree.predict(X_test)","e2ea14f0":"print (predTree [0:5])\nprint (y_test [0:5])","e8e3f475":"# Evaluation\nprint(\"DecisionTrees's Accuracy: \", metrics.accuracy_score(y_test, predTree))","d74933e8":"clf = DTree.fit(X, y)","2af08248":"plt.figure(figsize=(16, 6))\ntree.plot_tree(clf, filled = True)\n","44bdc009":"print(tree.export_text(clf))","b9dac626":"jaccard_score(y_test, predTree,pos_label=0)","a0d25451":"f1_score(y_test, predTree, average='weighted') ","f2555ba1":"res = pd.DataFrame({'Algorithm': ['DTree'],'Accuracy_score': [metrics.accuracy_score(y_test, predTree)], \"J-score\":[jaccard_score(y_test, predTree)], \"F-1 score\":[f1_score(y_test, predTree, average='weighted')]})\n \nResults = Results.append(res)","5cf25828":"# Training\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nlr = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\nlr","660c10dc":"# Predicting\nyhat_lr = lr.predict(X_test)\nyhat_lr","3a95fef7":"# Predicting with probabilities\nyhat_prob_lr = lr.predict_proba(X_test)\nyhat_prob_lr","2fac6833":"# Evaluation\nfrom sklearn.metrics import jaccard_score\njaccard_score(y_test, yhat_lr,pos_label=0)","06581d5c":"# Confusion Matrix\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\nprint(confusion_matrix(y_test, yhat_lr, labels=[1,0]))","0cc45aee":"# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, yhat_lr, labels=[1,0])\nnp.set_printoptions(precision=2)\n\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['Target =1','Target=0'],normalize= False,  title='Confusion matrix')","3ea5aa2a":"print (classification_report(y_test, yhat_lr))","f089b075":"# logloss\nfrom sklearn.metrics import log_loss\nlog_loss(y_test, yhat_prob_lr)","61511d2b":"print(\"DLogistic Regression Accuracy: \", metrics.accuracy_score(y_test, yhat_lr))","f89c3864":"res = pd.DataFrame({'Algorithm': ['Logistic Regression'],'Accuracy_score': [metrics.accuracy_score(y_test, yhat_lr)], \"J-score\":[jaccard_score(y_test, yhat_lr)], \"F-1 score\":[f1_score(y_test, yhat_lr, average='weighted')], \"log_loss\":[log_loss(y_test, yhat_prob_lr)]})\n \nResults = Results.append(res)","57488c55":"# Training\nfrom sklearn import svm\nsvm = svm.SVC(kernel='rbf')\nsvm.fit(X_train, y_train) ","09554e77":"# Predicting\nyhat_svm = svm.predict(X_test)\nyhat_svm [0:5]","464c6d59":"# Evaluation\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, yhat_svm, labels=[1,0])\nnp.set_printoptions(precision=2)\n\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['Target =1','Target=0'],normalize= False,  title='Confusion matrix')","03a567d5":"from sklearn.metrics import f1_score\nf1_score(y_test, yhat_svm, average='weighted') ","54373c8a":"jaccard_score(y_test, yhat_svm)","e492f82b":"print(\"SVM Accuracy: \", metrics.accuracy_score(y_test, yhat_svm))","49202a56":"res = pd.DataFrame({'Algorithm': ['SVM'],'Accuracy_score': [metrics.accuracy_score(y_test, yhat_svm)], \"J-score\":[jaccard_score(y_test, yhat_svm)], \"F-1 score\":[f1_score(y_test, yhat_svm, average='weighted')]})\n \nResults = Results.append(res)","56686bb1":"Results","647acdfa":"#### Predicting","775ad8cd":"#### Accuracy Evaluation","cac45d75":"#### k = 4","6ec97840":"## Classification","c87af01b":"## Decision Tree","c0dc08bb":"#### Training","c17b19cd":"#### K nearest neighbor (KNN)","1fb432e1":"## Logistic Regression","8456c54b":"## SVM","114b82f4":"#### Accuracy of KNN for different k values"}}