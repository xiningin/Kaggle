{"cell_type":{"d6d85b37":"code","36b4ec26":"code","ee9cf9f8":"code","4e0885f8":"code","a783ab79":"code","30578cbf":"code","43e6b682":"code","163463c3":"code","b3add949":"code","be6cdb5b":"code","4fb2c01e":"code","2bab10fd":"code","de5cb057":"code","eb8f31e2":"code","b67a801b":"code","977f2e7a":"code","cca02926":"code","d1d81ff2":"code","48ab4f59":"code","df08e038":"code","950b93b5":"code","b173307d":"code","e3db6a60":"code","3c04186f":"code","93c8325e":"code","8bd7aadb":"code","7c37f9b8":"code","6a3403e7":"code","2d276a3a":"code","3b4a4912":"code","23b2b0f4":"code","2c29716e":"code","81e7f960":"code","ad843912":"code","1d98afc2":"code","d901428c":"code","f195e53d":"code","ed0aa04f":"markdown","c6c27965":"markdown","e30aa7c7":"markdown","3f04371c":"markdown","dfe73d1c":"markdown","11de3b92":"markdown","1a87bce1":"markdown","0cbce335":"markdown","20a73ac5":"markdown","3faf9b81":"markdown","6b0e6358":"markdown"},"source":{"d6d85b37":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","36b4ec26":"train = pd.read_csv('..\/input\/sentiment-dataset-for-afghanistan-sd4a\/SD4A-clean.csv')\ntrain.head().style.set_properties(**{'background-color':'black',\n                                     'color': '#03e8fc'})","ee9cf9f8":"train.isnull().sum()","4e0885f8":"train = train.rename(columns={'Wed Jun 13 06:41:31 +0000 2018':'date', '1': 'polarity', 'assure afghans of bright future once u.s invaders by reuters news': 'text'})","a783ab79":"train.head()","30578cbf":"# Extract insights from the excerpt variable\nimport spacy","43e6b682":"# Initialise spacy\nnlp = spacy.load('en_core_web_sm')","163463c3":"# Perform initial test on one text\nsample = train.head(5)\nsample\nsample1 = train.loc[0, 'text']","b3add949":"# Create the spacy doc item for review\ndoc = nlp(sample1)\ndoc","be6cdb5b":"# Reviewing the token, lemma and stopword for each token (item)\nprint(f\"Token \\t\\tLemma \\t\\tStopword\".format('Token', 'Lemma', 'Stopword'))\nprint(\"-\"*40)\n# Review the first 20 values to test the output\nfor token in doc[:20]:\n    print(f\"{str(token)}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\\t\\t{len(token)}\")","4fb2c01e":"# A few different options for stopwords, spacy and nltk. Lets compare\nimport nltk\nfrom nltk.corpus import stopwords","2bab10fd":"# Comparison of the stop words available\nprint(f\"NLTK : {len(stopwords.words('english'))} \\n {stopwords.words('english')}\")\nprint(f\"Spacy : {len(nlp.Defaults.stop_words)} \\n {nlp.Defaults.stop_words}\")\n\n# Compare the differences\nnltk_set = set(stopwords.words('english'))\nspacy_set = set(nlp.Defaults.stop_words)\n\n# Union - all values\nunion = nltk_set.union(spacy_set)\n# Intersection - seen in both sets\ninter = nltk_set.intersection(spacy_set)\nprint(f\"Seen in both : {len(inter)} \\n {inter}\")\n# Remainder - differences between sets\nnltk_extra = nltk_set - inter\nspacy_extra = spacy_set - inter\nprint(f\"Extra NLTK : {len(nltk_extra)} \\n {nltk_extra}\")\nprint(f\"Extra Spacy : {len(spacy_extra)} \\n {spacy_extra}\")","de5cb057":"from sklearn.feature_extraction.text import TfidfTransformer \nfrom sklearn.feature_extraction.text import CountVectorizer","eb8f31e2":"#instantiate CountVectorizer() \ncv=CountVectorizer() \n\n# this steps generates word counts for the words in the sample doc\nword_count_vector=cv.fit_transform(sample.text)\n\nword_count_vector.shape","b67a801b":"# Compute the IDF values\ntfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \ntfidf_transformer.fit(word_count_vector)","977f2e7a":"# print idf values \ndf_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"]) \n \n# sort ascending \ndf_idf.sort_values(by=['idf_weights'])\n\ndf_idf.describe()","cca02926":"# Time to compute the TFIDF\n# count matrix \ncount_vector=cv.transform(sample.text) \n \n# tf-idf scores \ntf_idf_vector=tfidf_transformer.transform(count_vector)","d1d81ff2":"feature_names = cv.get_feature_names() \n \n#get tfidf vector for first document \nfirst_document_vector=tf_idf_vector[0] \n \n#print the scores \ndf = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \ndf.sort_values(by=[\"tfidf\"],ascending=False)","48ab4f59":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# settings that you use for count vectorizer will go here \ntfidf_vectorizer=TfidfVectorizer(use_idf=True) \n \n# just send in all your docs here \ntfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(sample.text)","df08e038":"# get the first vector out (for the first document) \nfirst_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] \n \n# place tf-idf values in a pandas data frame \ndf = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"]) \ndf.sort_values(by=[\"tfidf\"],ascending=False)","950b93b5":"tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n \n# just send in all your docs here. Here is Sample\nfitted_vectorizer=tfidf_vectorizer.fit(sample.text)\ntfidf_vectorizer_vectors=fitted_vectorizer.transform(sample.text)\ndf = pd.DataFrame(tfidf_vectorizer_vectors.T.todense(), index=fitted_vectorizer.get_feature_names(), columns=sample['date'])\ndf.head()\ndf.columns\ndf.shape\ndf_out = df.loc[:, ['Wed Jun 13 06:42:06 +0000 2018']]\ndf_out.sort_values(by=['Wed Jun 13 06:42:06 +0000 2018'], ascending=False)","b173307d":"# Test on all training data (=df)\n# just send in all your docs here\nfitted_vectorizer=tfidf_vectorizer.fit(train.text)\ntfidf_vectorizer_vectors=fitted_vectorizer.transform(train.text)\ndf = pd.DataFrame(tfidf_vectorizer_vectors.T.todense(), index=fitted_vectorizer.get_feature_names(), columns=train['date'])\ndf.head()\ndf.columns\ndf.shape\ndf_out = df.loc[:, ['Wed Jun 13 06:42:06 +0000 2018']]\ndf_out.sort_values(by=['Wed Jun 13 06:42:06 +0000 2018'], ascending=False)","e3db6a60":"# Lets create a dictionary to review the key phrase outputs\nfrom collections import defaultdict, Counter\n\n# Returns integers that map to parts of speech\ncounts_dict = doc.count_by(spacy.attrs.IDS['POS'])\n\n# Print the human readable part of speech tags\nfor pos, count in counts_dict.items():\n    human_readable_tag = doc.vocab[pos].text\n    print(human_readable_tag, count)","3c04186f":"pos_counts = defaultdict(Counter)\nfor token in doc:\n    pos_counts[token.pos][token.orth] += 1\n    \nfor pos_id, counts in sorted(pos_counts.items()):\n    pos = doc.vocab.strings[pos_id]\n    for orth_id, count in counts.most_common():\n        print(pos, count, doc.vocab.strings[orth_id], len(doc.vocab.strings[orth_id]))","93c8325e":"# Expanding named entities\nfor entity in doc.ents:\n    print(entity.text, entity.label_)\n\n# Analyze syntax\nprint(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\nprint(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\nprint(\"Number of sentences\", len([*doc.sents]))\nprint(\"Sentiment\", doc.sentiment)\n\n# Understand the length of sentences\nfor sent in doc.sents:\n    print(sent.start_char, sent.end_char, (sent.end_char - sent.start_char))","8bd7aadb":"from spacy import displacy\n\n# Display the entities within a sentence\ndisplacy.render(doc, style='ent', jupyter=True)","7c37f9b8":"# Visualise the dependencies within a sentence\ndisplacy.render(doc, style='dep', jupyter=True)","6a3403e7":"# Lets apply the nlp instance to each text\ntrain['text_scy'] = train['text'].apply(nlp)","2d276a3a":"# Check the data type for the updated column\ntype(train.loc[0, 'text_scy'])\ntrain.head()","3b4a4912":"# Reviewing a different row\ntrain.loc[1,'text_scy']","23b2b0f4":"# Create the class methods required to run the analysis\nclass NLPMethods():\n    # Create constructor for the class\n#     def __init__():\n    \n    # Number of sentences\n    def number_sentences(self, nlp_text):\n        return len([*nlp_text.sents])\n    \n    # Average length of sentence\n    def average_sentence_length(self, nlp_text):\n        sent_length = list()\n        for sent in nlp_text.sents:\n            sent_length.append(sent.end_char - sent.start_char)\n        return np.mean(sent_length)\n    \n    # Part of speech tags\n    def part_of_speech_tags(self, nlp_text):\n        counts_dict = nlp_text.count_by(spacy.attrs.IDS['POS'])\n        counts_dict1 = {}\n        # Extract the text that matches to the POS value\n        for k, v in counts_dict.items():\n            counts_dict1[nlp_text.vocab[k].text] = v\n        return counts_dict1\n    \n    # Number of spaces\n    def number_spaces(self, nlp_text):\n        dict_pos = self.part_of_speech_tags(nlp_text)\n        if dict_pos.get('SPACE') != None:\n            space = dict_pos.get('SPACE')\n        else:\n            space = 0\n        return space\n    \n    # Part of speech tags - including the word counts\n    def word_counts(self, nlp_text):\n        pos_counts = defaultdict(Counter)\n        for token in nlp_text:\n            pos_counts[token.pos][token.orth] += 1\n        \n        # Create dictionary for the word counts\n        word_counts_dict = {}\n        for pos_id, counts in sorted(pos_counts.items()):\n            pos = nlp_text.vocab.strings[pos_id]\n            for orth_id, count in counts.most_common():\n                word_counts_dict[nlp_text.vocab.strings[orth_id]] = {'count':count, \n                                                                     'length':len(nlp_text.vocab.strings[orth_id]), \n                                                                     'pos':pos}\n        return word_counts_dict\n    \n    # Number of words\n    def number_words(self, nlp_text):\n        dict_word_counts = self.word_counts(nlp_text)\n        return len(dict_word_counts.items())\n    \n    # Longest word\n    def longest_word(self, nlp_text):\n        dict_word_counts = self.word_counts(nlp_text)\n        df = pd.DataFrame(dict_word_counts).T.reset_index().rename(columns={'index':'variable'})\n        return max(df['length'])","2c29716e":"# Add columns for the spacy doc\ntrain['num_sentences'] = train['text_scy'].apply(NLPMethods().number_sentences)\ntrain['avg_sentence_length'] = train['text_scy'].apply(NLPMethods().average_sentence_length)\ntrain['pos_dict'] = train['text_scy'].apply(NLPMethods().part_of_speech_tags)\ntrain['num_space'] = train['text_scy'].apply(NLPMethods().number_spaces)\ntrain['wc_dict'] = train['text_scy'].apply(NLPMethods().word_counts)\ntrain['num_words'] = train['text_scy'].apply(NLPMethods().number_words)\ntrain['longest_word'] = train['text_scy'].apply(NLPMethods().longest_word)","81e7f960":"train.sample(5)","ad843912":"# Review the max value target variable. We don't target to return a single line max val\n#max_val = np.max(train['polarity'])\n#train_max = train.loc[(train['polarity']==max_val), :]\n#train_max","1d98afc2":"# Check the reason for the largest target value\n#We don't have largest target. Hence, I kept the same value 2829 \ntype(train.loc[2829, 'text_scy'])\ntrain.loc[2829, 'text_scy']\ntrain.loc[2829, 'text']","d901428c":"#I commented since we don't have target or min value target.\n# Review the min value target variable\n#min_val = np.min(train['target'])\n#train_min = train.loc[(train['target']==min_val), :]\n#train_min","f195e53d":"#Though we don't have smallest target I kept the same value\n# Check the reason for the smallest target value\ntype(train.loc[1705, 'text_scy'])\ntrain.loc[1705, 'text_scy']","ed0aa04f":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Tfidfvectorizer Usage<\/span><\/h1><br>","c6c27965":"#That below will take some time.","e30aa7c7":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Review stop words<\/span><\/h1><br>","3f04371c":"#Code by James McNeill  https:\/\/www.kaggle.com\/datajmcn\/nlp-read-eda\/notebook","dfe73d1c":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Review Tfidftransformer and Tfidfvectorizer<\/span><\/h1><br>","11de3b92":"#Another one snippet that takes time","1a87bce1":"![](https:\/\/pbs.twimg.com\/media\/E6EFuIPXEAA9IXP.jpg)twitter.com","0cbce335":"#SPACE value appears to correspond to the new line.","20a73ac5":"#Thanks James McNeill for the code https:\/\/www.kaggle.com\/datajmcn\/nlp-read-eda\/notebook","3faf9b81":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Create the datasets for training the models<\/span><\/h1><br>","6b0e6358":"#Lower the IDF value the more common the value is"}}