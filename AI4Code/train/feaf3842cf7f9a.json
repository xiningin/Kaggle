{"cell_type":{"cc00331d":"code","d14682ac":"code","9535d8e5":"code","28e11605":"code","5bd779b6":"code","0476fd6a":"code","e8562554":"code","6f53f440":"code","84ea05b5":"code","421b3f9e":"code","03538e28":"code","4b08c8c2":"code","d5e9f9d8":"code","1a18d992":"code","4f9ad9a9":"code","80d66557":"code","2028ad0d":"code","41ec18ea":"code","84ed6ea7":"code","a2473f64":"code","2c99e0e2":"code","1bc88897":"code","17b1a8ad":"code","a7535faa":"code","62751a7b":"code","c68427fa":"code","c8b48450":"code","412fc8a0":"code","2d81abfb":"code","38ad5a58":"code","d2819fc4":"code","2cbf969c":"code","50073b09":"code","153943c6":"code","cfeb2859":"code","ff0a0cc1":"code","c9222b59":"code","c34df7c8":"code","041774cd":"code","8ebd0f8d":"code","5aedf725":"code","cbdb0948":"code","fff94496":"code","dc061d80":"code","4792136f":"code","3b528014":"code","1a243921":"code","2e42bc06":"code","a56cda54":"code","48ce8510":"code","eb5c8ade":"code","63a0b9d6":"markdown","b49b57b8":"markdown","9fec7b51":"markdown","bf8cb505":"markdown","af9f8f85":"markdown","3c7bd795":"markdown","45a5faad":"markdown","f0b6a33c":"markdown","33c0bf35":"markdown","586277b4":"markdown","5f504483":"markdown","31bfaa93":"markdown","b5ad05ad":"markdown","00051056":"markdown","11a7cf76":"markdown","f413f423":"markdown","711adbd0":"markdown","43ced4c0":"markdown","1ec0146a":"markdown","f50b74b1":"markdown","e8484050":"markdown","c7083dab":"markdown","4228b665":"markdown","e5c73fb9":"markdown","1a242f67":"markdown","33b0f2d3":"markdown","2cd31b97":"markdown","6dcf0d71":"markdown","6733c520":"markdown","a17d8928":"markdown","fa562d96":"markdown","5a5882a6":"markdown","16f5d343":"markdown","01d4ad0a":"markdown","5259a80f":"markdown","99de0ce2":"markdown","38a1b770":"markdown","0e7f1202":"markdown"},"source":{"cc00331d":"# import necessary modules\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport os\nimport warnings\nfrom scipy import stats\nfrom scipy.stats import norm, skew, probplot \n\nwarnings.filterwarnings('ignore')","d14682ac":"from sklearn.model_selection import train_test_split\n#\n# read csv files\n#df_train = pd.read_csv('..\/input\/train.csv') ; df_test = pd.read_csv('..\/input\/test.csv')\ndf_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv') \ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf_train['SalePrice(k$)'] = df_train['SalePrice']\/1e3\ndf_train['logSalePrice'] = np.log(df_train[\"SalePrice\"])\ndf_train, df_holdout = train_test_split(df_train, test_size=0.2, random_state=59)\ndf_train, df_holdout = df_train.reset_index(drop=True), df_holdout.reset_index(drop=True)\n#df_train_clean = df_train.copy() ; df_test_clean = df_test.copy()\n#\n# create other target variables\nNtrain = df_train.shape[0]\nNholdout = df_holdout.shape[0]\nNtest = df_test.shape[0]\n#\nsave_plot = True\nchoice_text = True","9535d8e5":"df_train.head()","28e11605":"# distribution of a given feature\ndef plot_targetdistrib(y,plotname,plottitle):\n    plt.figure(0,figsize=[15,5])\n    plt.subplots_adjust(wspace=0.2, hspace=0.5)\n    plt.subplot(1,2,1)\n    (mu, sigma) = norm.fit(y)\n    #print( 'mu = {:.2f} and sigma = {:.2f}'.format(mu, sigma))\n    sns.distplot(y, fit=norm)\n    plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n                loc='best')\n    plt.ylabel('Frequency')\n    plt.title(plottitle)\n    # QQ-plot wrt normal distribution\n    plt.subplot(1,2,2)\n    res = probplot(y, plot=plt)\n    plt.show()\n    #plt.savefig('figures\/'+plotname+'.pdf',bbox_inches='tight',transparent=True)\n    plt.close(0)","5bd779b6":"# make a correlation plot between target variable and features\ndef corr_plot(df,plotname):\n    corrmat = df.drop(['SalePrice(k$)', 'logSalePrice'], axis=1).corr()\n    plt.figure(0,figsize=[20,14])\n    k = 90 #number of variables for heatmap\n    cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n    cm = np.corrcoef(df[cols].values.T)\n    #sns.set(font_scale=1.25)\n    hm = sns.heatmap(cm, cbar=True, square=True, yticklabels=cols.values, xticklabels=cols.values, annot=True, fmt='.2f', annot_kws={'size': 8})\n    plt.title(plotname)\n    #plt.savefig('figures\/'+plotname+'.pdf',bbox_inches='tight',transparent=True)\n    plt.show()\n    plt.close(0)","0476fd6a":"def plot_scatter(x,y,xtitle,ytitle,plottitle):\n    plt.figure(0) #,figsize=[7,7]\n    plt.xlabel(xtitle)\n    plt.ylabel(ytitle)\n    plt.scatter(x, y)\n    plt.show()\n    #plt.savefig('figures\/'+plottitle+'.pdf',bbox_inches='tight',transparent=True)\n    plt.close(0)","e8562554":"# impute features depending on their context (see above)\ndef impute_features(df):\n    # NaN values in categorical variables: \"None\" information \n    list_none = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', \\\n                 'GarageQual', 'GarageCond', 'GarageFinish', 'GarageType', \\\n                 'BsmtExposure', 'BsmtFinType2', 'BsmtQual', 'BsmtCond', 'BsmtFinType1', 'BsmtFinType2', \\\n                 'MasVnrType', 'MSSubClass', 'Utilities']\n    df[list_none] = df[list_none].fillna('None')\n    #\n    # NaN values in categorical variables: most frequent\n    list_none = ['MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType']\n    mf_imputer = SimpleImputer(strategy='most_frequent')\n    df[list_none] = mf_imputer.fit_transform(df[list_none])\n    #\n    # Functional: NA means typical\n    df['Functional'] = df[\"Functional\"].fillna(\"Typ\")\n    #\n    # NaN values for integer features related to garage or basement: replaced by 0\n    list_0 = ['GarageCars', 'GarageArea', \n              'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath',\n              'MasVnrArea']\n    df[list_0] = df[list_0].fillna(0)\n    #\n    # NaN values for size feature: replaced by the average\n    list_mean = ['LotFrontage', 'GarageYrBlt']\n    mean_imputer = SimpleImputer(strategy='mean')\n    df[list_mean] = mean_imputer.fit_transform(df[list_mean]).astype('int64')\n    #\n    return df","6f53f440":"# create new features\ndef create_features(df):\n    df['TotLivArea'] = df['GrLivArea']+df['1stFlrSF']+df['2ndFlrSF']+df['TotalBsmtSF']\n    df['GardenArea'] = df['LotArea'] - (df['GrLivArea']+df['GarageArea'])\n    df['TotBath'] = df['BsmtFullBath']+0.5*df['BsmtHalfBath']+df['FullBath']+0.5*df['HalfBath']\n    df['YrBltAndRemod'] = df ['YearBuilt']+df['YearRemodAdd']\n    df['TotPorchArea'] = (df['OpenPorchSF'] + df['3SsnPorch'] +\n                          df['EnclosedPorch'] + df['ScreenPorch'] +\n                          df['WoodDeckSF'])\n    #\n    df['haspool'] = df['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n    df['has2ndfloor'] = df['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n    df['hasgarage'] = df['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n    df['hasbsmt'] = df['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    df['hasfireplace'] = df['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n    #\n    return df","84ea05b5":"# Run a GridSearchCV on a specified dataset with a given model and return the model object\ndef score_model(model_tupple, model_param, X, y, verbose):\n    modelname, model = model_tupple\n    steps = [('scaler', RobustScaler()), #StandardScaler()), #\n             (modelname, model)]\n    parameters = model_param #{'ridge__alpha' : np.logspace(0,4,10)}\n    pipeline = Pipeline(steps)\n    grid_obj = GridSearchCV(pipeline, param_grid=parameters,cv=5,\\\n                            scoring='neg_mean_squared_error',\n                            return_train_score=True,verbose=verbose,n_jobs=4) #, \n    grid_obj.fit(X, y)\n    return grid_obj #pd.DataFrame(grid_obj.cv_results_), grid_obj.best_estimator_, grid_obj.best_params_, np.sqrt(-grid_obj.best_score_)","421b3f9e":"# run a GridSearCV for a list of models with a given list of parameters\ndef rungrid(list_models, list_params, X, y):\n    list_results = [] ; list_bestestimator = [] ; list_bestparam = [] ; list_bestscore = []\n    for im, model in enumerate(list_models):\n        print('Running %s model' % (model[0]))\n        model_tupple = model #('ridge', Ridge())\n        model_param = list_params[im] #{'ridge__alpha' : np.logspace(0,4,10)}\n        grid_obj = score_model(model_tupple, model_param, X, y, 1)\n        result = pd.DataFrame(grid_obj.cv_results_)\n        bestest = grid_obj.best_estimator_\n        bestparam = grid_obj.best_params_\n        bestscore = np.sqrt(-grid_obj.best_score_)\n        list_results.append(result) ; list_bestestimator.append(bestest)\n        list_bestparam.append(bestparam) ; list_bestscore.append(bestscore)\n        list_results[-1].to_csv('results_'+model_tupple[0]+'.csv')\n        print(\"%s score: %.3f obtained for parameter value(s)\" % \n              (model_tupple[0], list_bestscore[-1]))\n        print(list_bestparam[-1])\n    return list_results, list_bestestimator, list_bestparam, list_bestscore\n","03538e28":"plot_targetdistrib(df_train['SalePrice'],'distrib_saleprice','SalePrice distribution')","4b08c8c2":"plot_targetdistrib(df_train['logSalePrice'],'distrib_logsaleprice','log(SalePrice) distribution')","d5e9f9d8":"len(df_train.drop(['SalePrice', 'SalePrice(k$)', 'logSalePrice','Id'],axis=1).columns)","1a18d992":"for icol, col in enumerate(df_train.columns.sort_values()):\n    if len(df_train[col].unique()) < 20: \n        print(col, df_train[col].unique()) \n    else:\n        print(col,' has too many unique values')","4f9ad9a9":"df_train_na = (df_train.isnull().sum()\/len(df_train)*100.).sort_values(ascending=False)\ndf_train_na = df_train_na.drop(df_train_na[df_train_na == 0].index)\nif choice_text:\n    print(df_train_na)","80d66557":"corr_plot(df_train,'Feature correlation Table')\n","2028ad0d":"# plot the target as function of features\ndef scatter_target_features(df,target, list_features,plotname,y_target):\n    plt.figure(0,figsize=[20,5]) #5*len(list_features)\n    plt.subplots_adjust(wspace=0.2, hspace=0.5)\n    for ifeat, feature in enumerate(list_features):\n        plt.subplot(1,len(list_features),ifeat+1)\n        plt.xscale('linear')\n        plt.xlabel(feature)\n        plt.ylabel(y_target)\n        plt.xlim([0.,df[feature].max()*1.1])\n        plt.title(plotname)\n        plt.scatter(df[feature], df[target])\n    plt.title(plotname)\n    plt.show()\n    #plt.savefig('figures\/'+plotname+'.pdf',bbox_inches='tight',transparent=True)\n    plt.close(0)","41ec18ea":"# plot the target as function of features as boxplot\ndef boxplot_target_features(df,target, list_features,plotname,y_target):\n    plt.figure(0,figsize=[20,5])\n    plt.subplots_adjust(wspace=0.2, hspace=0.5)\n    for ifeat, feature in enumerate(list_features):\n        plt.subplot(1,len(list_features),ifeat+1)\n        plt.xticks(rotation=90);\n        data = pd.concat([df[target], df[feature]], axis=1)\n        fig = sns.boxplot(x=feature, y=target, data=data)\n    #plt.savefig('figures\/'+plotname+'.pdf',bbox_inches='tight',transparent=True)\n    plt.title(plotname)\n    plt.show()\n    plt.close(0)\n","84ed6ea7":"list_features2plot = ['LotArea', 'GrLivArea', '1stFlrSF', 'GarageArea'] #, 'BedroomAbvGr']\nscatter_target_features(df_train,'SalePrice(k$)', list_features2plot,'SalePrice vs Area features','Sale Price (k\\$)')","a2473f64":"list_features2plot = ['BedroomAbvGr', 'TotRmsAbvGrd', 'FullBath', 'GarageCars']\nboxplot_target_features(df_train,'SalePrice(k$)', list_features2plot,'SalePrice vs Size features','Sale Price (k\\$)')","2c99e0e2":"list_features2plot = ['OverallQual', 'CentralAir', 'KitchenQual', 'ExterQual']\nboxplot_target_features(df_train,'SalePrice(k$)', list_features2plot,'SalePrice vs Quality features','Sale Price (k\\$)')","1bc88897":"list_features2plot = ['YearBuilt']\nboxplot_target_features(df_train,'SalePrice(k$)', list_features2plot,'SalePrice vs YearBuilt','Sale Price (k\\$)')","17b1a8ad":"from sklearn.impute import SimpleImputer\n#\n# convert some integer features to categories\nlist_cat = ['MSSubClass', 'MoSold', 'YrSold']\nfor cat2 in list_cat:\n    df_train[cat2] = df_train[cat2].apply(str)\n    df_test[cat2] = df_test[cat2].apply(str)\n    df_holdout[cat2] = df_holdout[cat2].apply(str)\n#\n# replace NaN values depending on the context\ndf_train = impute_features(df_train)\ndf_test = impute_features(df_test)\ndf_holdout = impute_features(df_holdout)\n#\n# check that the dataframe no longer contains NaN \ndf_train_na_aft = (df_train.isnull().sum()\/len(df_train)*100.).sort_values(ascending=False)\ndf_train_na_aft = df_train_na_aft.drop(df_train_na_aft[df_train_na_aft == 0].index)\nif choice_text:\n    print(df_train_na_aft)","a7535faa":"from scipy.special import boxcox1p\n# print skewed features\nnumeric_feats = df_test.dtypes[df_test.dtypes != \"object\"].index.drop('Id')\nskewed_feats = df_train[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew_before' :skewed_feats})\n#\n# transform skewed features with box cox transform\nskewness = skewness[abs(skewness) > 0.75]\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    df_train[feat] = boxcox1p(df_train[feat], lam)\n    df_test[feat] = boxcox1p(df_test[feat], lam)\n    df_holdout[feat] = boxcox1p(df_holdout[feat], lam)\nskewed_feats = df_train[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness['Skew_after'] = pd.DataFrame({'Skew_after' :skewed_feats})\nif choice_text:\n    print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n    print(\"\\nSkew in numerical features: \\n\")\n    print(skewness.head(50))\n","62751a7b":"# create a small set of features describing the most important properties of houses\n# sl_prop = ['BedroomAbvGr', 'TotRmsAbvGrd', 'GrLivArea', 'CentralAir', 'ExterQual', \\\n#            'KitchenQual', 'OverallCond', 'OverallQual', 'YearBuilt', \\\n#           'GarageCond', 'GarageCars', 'LotArea', 'PoolArea', 'MSZoning',\\\n#           'SalePrice','logSalePrice','SalePrice(k$)']\n# df_train_sub = df_train[sl_prop]\n#\n# create new features out of existing ones\ndf_train = create_features(df_train)\ndf_test = create_features(df_test)\ndf_holdout = create_features(df_holdout)\n#df_train_sub.head()\n#\n# list features of different types\nfeature_cat = list(df_train.columns[df_train.dtypes == 'O'].sort_values())\nfeature_cont = list(df_train.columns[df_train.dtypes == 'float64'].drop(['SalePrice(k$)','logSalePrice']))\nfeature_int = list(df_train.columns[df_train.dtypes == 'int64'].drop(['SalePrice']))\n","c68427fa":"from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n#\n# One-Hot Encoding with pandas get_dummies\nall_data = pd.concat((\n           df_train.drop(['SalePrice','SalePrice(k$)','logSalePrice'], axis=1), \n           df_holdout.drop(['SalePrice','SalePrice(k$)','logSalePrice'], axis=1),\n           df_test\n           )).reset_index(drop=True)\nall_data = pd.get_dummies(all_data) #, prefix='ohe_cat')\ndf_train_oh = all_data[:Ntrain].join(df_train[['SalePrice','SalePrice(k$)','logSalePrice']])\ndf_holdout_oh = all_data[Ntrain:Ntrain+Nholdout]#.join(df_holdout[['SalePrice']])\ndf_test_oh = all_data[Ntrain+Nholdout:]\n#\n# Label Encoding\nle = LabelEncoder() ; \ndf_train_le = df_train.copy() ; df_test_le = df_test.copy() ; df_holdout_le = df_holdout.copy() #; df_train_sub_le = df_train_sub.copy() ; \nfor feature in feature_cat:\n    df_train_le[feature] = le.fit_transform(df_train_le[feature])\n    df_holdout_le[feature] = le.fit_transform(df_holdout_le[feature])\n    df_test_le[feature] = le.fit_transform(df_test_le[feature])","c8b48450":"from sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.metrics import make_scorer, accuracy_score, mean_squared_error as MSE, mean_squared_log_error as MSLE\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.svm import SVR, LinearSVR\n","412fc8a0":"train_sets = [('Label Encoded', df_train_le.drop(['SalePrice','SalePrice(k$)','Id'],axis=1)), \n              ('One-Hot', df_train_oh.drop(['SalePrice','SalePrice(k$)','Id'],axis=1))]\n#train_set_name = ['Full Label-Encoded', 'Full One-Hot']\n#\nlist_results = [] ; list_bestestimator = [] ; list_bestparam = [] ; list_bestscore = [] ; list_params = []\nfor it, trainset in enumerate(train_sets):\n    #print((trainset[1].columns))\n    X_all = trainset[1].drop(['logSalePrice'], axis=1)\n    y_all = trainset[1]['logSalePrice']\n    model_tupple = ('ridge', Ridge())\n    model_param = {'ridge__alpha' : np.logspace(0,4,10)}\n    grid_obj = score_model(model_tupple, model_param, X_all, y_all, 0)\n    result = pd.DataFrame(grid_obj.cv_results_)\n    bestest = grid_obj.best_estimator_\n    bestparam = grid_obj.best_params_\n    bestscore = np.sqrt(-grid_obj.best_score_)\n    list_results.append(result) ; list_bestestimator.append(bestest)\n    list_bestparam.append(bestparam) ; list_bestscore.append(bestscore)\n    print(\"%s score: %.3f obtained for parameter value %.3f\" % \n          (trainset[0], list_bestscore[it], list_bestparam[it]['ridge__alpha'])) ","2d81abfb":"def plot_coeff(bestest,df,plottitle,plotname):\n    coef = pd.Series(bestest.named_steps.ridge.coef_, \\\n                     index = df.drop(['logSalePrice'],axis=1).columns).sort_values() #,'SalePrice','SalePrice(k$)'\n    plt.figure(0,figsize=[20,5])\n    plt.xticks(rotation=90)\n    plt.ylabel('Feature coefficient')\n    plt.title(plottitle) #clf_name[im])\n    coef.plot(kind='bar')\n    #plt.savefig('figures\/'+plotname+'.pdf',bbox_inches='tight',transparent=True)\n    plt.show()\n    plt.close(0)","38ad5a58":"plot_coeff(list_bestestimator[0],train_sets[0][1],train_sets[0][0],'coeff_ridge')","d2819fc4":"def plot_coeff_corr(bestest,df,plottitle,plotname):\n    df2 = df.drop(['SalePrice'], axis=1)\n    #print(len(df2.columns), len(bestest.named_steps.ridge.coef_))\n    coef = pd.Series(bestest.named_steps.ridge.coef_, \\\n                     index = df2.columns)# .drop(['logSalePrice'],axis=1) .sort_values() ,'SalePrice','SalePrice(k$)'\n    corrmat = df.corr() # .drop(['logSalePrice'], axis=1) 'SalePrice(k$)', \n    concat = pd.concat([coef,corrmat['SalePrice']], axis=1)\n    concat.columns = ['coef', 'corr']\n    # plot the coefficients\n    plt.figure(0)\n    plt.xlabel('Feature coefficient')\n    plt.ylabel('Correlation coefficient')\n    plt.scatter(concat['coef'], concat['corr'])\n    plt.show()\n    #plt.savefig('figures\/'+plotname+'.pdf',bbox_inches='tight',transparent=True)\n    plt.close(0)","2cbf969c":"plot_coeff_corr(list_bestestimator[0],df_train_le.drop(['logSalePrice','SalePrice(k$)','Id'],axis=1),train_sets[0][0],'coeff_vs_corr')","50073b09":"linreg_models = [('ridge', Ridge()),\n               ('lasso', Lasso(max_iter=1e7)), \n               ('elasticnet', ElasticNet(max_iter=1e7))]\nlinreg_params = [{'ridge__alpha' : np.logspace(0,4,10)},\n               {'lasso__alpha' : np.logspace(-4,0,10)},\n               {'elasticnet__alpha' : np.logspace(-4,2,10),\n                'elasticnet__l1_ratio' : np.array([0.6,0.7,0.8,0.9,1.])}]","153943c6":"# run the grid\nlinreg_results = [] ; linreg_bestestimator = [] ; linreg_bestparam = [] ; linreg_bestscore = []\nX_all = df_train_oh.drop(['SalePrice','SalePrice(k$)','logSalePrice','Id'], axis=1)\ny_all = df_train_oh['logSalePrice']\nlinreg_results, linreg_bestestimator, linreg_bestparam, linreg_bestscore = \\\n    rungrid(linreg_models, linreg_params, X_all, y_all)","cfeb2859":"# make predictions on test data\n# linreg_score = []\n# for im, model in enumerate(linreg_models):\n#     df_holdout['SalePrice_pred('+model[0]+')'] = np.exp(linreg_bestestimator[im].predict(df_holdout_oh.drop('Id', axis=1)))\n#     df_test['logSalePrice_pred('+model[0]+')'] = (linreg_bestestimator[im].predict(df_test_oh.drop('Id', axis=1)))\n#     df_test['SalePrice_pred('+model[0]+')'] = np.exp(linreg_bestestimator[im].predict(df_test_oh.drop('Id', axis=1)))\n#     #\n#     score_holdout = np.sqrt(MSLE(df_holdout['SalePrice'], df_holdout['SalePrice_pred('+model[0]+')']))\n#     linreg_score.append((model[0],score_holdout))\n#     print('RMSLE score of '+model[0]+' on hold-out set: %.3f' % (score_holdout))\n#     #\n#     df_test[['Id', 'SalePrice_pred('+model[0]+')']].set_index('Id').to_csv('predictions_'+str(model[0])+'.csv')","ff0a0cc1":"def makepred(list_models, test, test_oh, holdout, holdout_oh, list_score):\n    #linreg_score = []\n    for namemod, model in list_models:\n        holdout['SalePrice_pred('+namemod+')'] = np.exp(model.predict(holdout_oh.drop('Id', axis=1)))\n        test['logSalePrice_pred('+namemod+')'] = (model.predict(test_oh.drop('Id', axis=1)))\n        test['SalePrice_pred('+namemod+')'] = np.exp(model.predict(test_oh.drop('Id', axis=1)))\n        #\n        score_holdout = np.sqrt(MSLE(holdout['SalePrice'], holdout['SalePrice_pred('+namemod+')']))\n        list_score.append((namemod,score_holdout))\n        print('RMSLE score of '+namemod+' on hold-out set: %.3f' % (score_holdout))\n        #\n        testcsv = test.copy()\n        testcsv['SalePrice'] = testcsv['SalePrice_pred('+namemod+')']\n        testcsv[['Id', 'SalePrice']].set_index('Id').to_csv('predictions_'+str(namemod)+'.csv')","c9222b59":"list_models = list(zip([model[0] for model in linreg_models], linreg_bestestimator)) ; linreg_score = []\nmakepred(list_models, df_test, df_test_oh, df_holdout, df_holdout_oh, linreg_score)","c34df7c8":"# plot the score of the models as function of hyperparameter\ndef plot_score_1d(plotname, plottitle, list_results):\n    plt.figure(0,figsize=[15,5])\n    plt.subplots_adjust(wspace=0.35, hspace=0.5)\n    for idf, df in enumerate(list_results):\n        Nparams = len(df.params[0])\n        param_name = list(df.params[0].keys())\n        #\n        plt.subplot(1,len(list_results),idf+1)\n        plt.title(plottitle[idf]) #clf_name[im])\n        #plt.ylim(0,0.06)\n        plt.xscale('log')\n        plt.ylabel('Score')\n        if Nparams == 1:\n            plt.xlabel(param_name[0]) #[x for x in model_param.keys()][0])\n            param_value = ([([x for x in df['params'][i].values()][0]) for i in range(len(df['params']))])\n            plt.plot(param_value, -df['mean_train_score'],ls='-',color='b',label='Train cont')\n            plt.plot(param_value, -df['mean_test_score'],ls='-',color='r',label='Test cont')\n        else:\n            plt.xlabel(param_name[0]) #[x for x in model_param.keys()][0])\n            list_param2 = sorted(set([row.params[param_name[1]] for ir, row in df.iterrows()])) \n            for ip2, param2 in enumerate(list_param2):\n                list_param1 = sorted(set([row.params[param_name[0]] for ir, row in df.iterrows()])) \n                mask = [row.params[param_name[1]] == param2 for ir, row in df.iterrows()]\n                plt.plot(list_param1, -df[mask]['mean_train_score'],ls='-',color='b',label='Train cont '+str(param2))\n                plt.plot(list_param1, -df[mask]['mean_test_score'],ls='-',color='r',label='Test cont '+str(param2))\n        plt.legend()\n    plt.show()\n    #plt.savefig('figures\/'+plotname+'.pdf',bbox_inches='tight',transparent=True)\n    plt.close(0)","041774cd":"# plot the scores for the four dataframes\nplot_score_1d('score_linearregressors', [i[0] for i in list_models], list_results)","8ebd0f8d":"plot_scatter(df_holdout['SalePrice'], df_holdout['SalePrice_pred('+linreg_models[-1][0]+')'], 'SalePrice', 'SalePrice_pred', 'comp_saleprice_linreg')","5aedf725":"plot_targetdistrib(np.log(df_test['SalePrice_pred('+linreg_models[-1][0]+')']),'distrib_logsaleprice_test','log(SalePrice) distribution')","cbdb0948":"list_features2plot = ['LotArea', 'GrLivArea', 'TotLivArea'] #, 'BedroomAbvGr']\ndf_test['logSalePrice_pred('+linreg_models[-1][0]+')']\nscatter_target_features(df_test,'SalePrice_pred('+list_models[-1][0]+')', list_features2plot,'saleprice_area_testset','Sale Price (k\\$)')","fff94496":"import xgboost as xgb\nfrom sklearn.ensemble import RandomForestRegressor as rfr, GradientBoostingRegressor as gbr\nfrom sklearn.model_selection import TimeSeriesSplit, GridSearchCV\nfrom lightgbm import LGBMRegressor as lgbm\n\n#\n# define list of models and parameters\nboost_models = [('RF', \n                 rfr(criterion='mse',\n                     n_jobs=-1,\n                     random_state = 59\n                    )),\n               ('GBR', \n                gbr(loss='huber', \n                    #learning_rate=0.05, \n                    #max_features='sqrt',\n                    #min_samples_leaf=15, \n                    #min_samples_split=10,\n                    random_state = 59,\n                    verbose=1\n                   )),\n               ('LGBM', \n                lgbm(objective='regression', \n                     #num_leaves=4,\n                     #learning_rate=0.01, \n                     #n_estimators=5000,\n                     #max_bin=200, \n                     #bagging_fraction=0.75,\n                     #bagging_freq=5, \n                     #bagging_seed=7,\n                     #feature_fraction=0.2,\n                     #feature_fraction_seed=7,\n                     verbose=1,\n                    )),\n               ('XGB', \n                xgb.XGBRegressor(objective='reg:linear',\n                                 #learning_rate=0.01, \n                                 #n_estimators=3460,\n                                 #max_depth=3, \n                                 #min_child_weight=0,\n                                 #gamma=0, \n                                 #subsample=0.7,\n                                 #colsample_bytree=0.7,\n                                 nthread=-1,\n                                 #scale_pos_weight=1, seed=27,\n                                 #reg_alpha=0.00006\n                                ))\n              ]\n\nboost_params = [{'RF__max_depth' : [2, 4, 6, 8, 10, 12], #\n                'RF__n_estimators' : [10, 30, 100, 300, 1000, 3000, 10000]},\n               {'GBR__max_depth' : [2, 4, 6, 8, 10, 12], #\n                'GBR__n_estimators' : [10, 30, 100, 300, 1000, 3000, 10000]},\n               {'LGBM__max_depth' : [2, 4, 6, 8, 10, 12], #\n                'LGBM__n_estimators' : [10, 30, 100, 300, 1000, 3000, 10000]},\n               {'XGB__max_depth' : [2, 4, 6, 8, 10, 12], #\n                'XGB__n_estimators' : [10, 30, 100, 300, 1000, 3000, 10000]} #[5, 10, 20, 50, 100, 200]\n              ]\n# ","dc061d80":"# run the grid\nboost_results = [] ; boost_bestestimator = [] ; boost_bestparam = [] ; boost_bestscore = []\nX_all = df_train_oh.drop(['SalePrice','SalePrice(k$)','logSalePrice','Id'], axis=1)\ny_all = df_train_oh['logSalePrice']\nboost_results, boost_bestestimator, boost_bestparam, boost_bestscore = \\\n    rungrid(boost_models, boost_params, X_all, y_all)","4792136f":"list_models = list(zip([model[0] for model in boost_models], boost_bestestimator)) ; boost_score = []\nmakepred(list_models, df_test, df_test_oh, df_holdout, df_holdout_oh, boost_score)","3b528014":"# plot the scores for the four dataframes\nplot_score_1d('score_boostingregressors', [i[0] for i in list_models], boost_results)","1a243921":"#plot_scatter(df_holdout['SalePrice'], df_holdout['SalePrice_pred('+list_models[-1][0]+')'], 'SalePrice', 'SalePrice_pred', 'comp_saleprice_linreg')","2e42bc06":"#plot_targetdistrib(np.log(df_test['SalePrice_pred('+model[0]+')']),'distrib_logsaleprice_test','log(SalePrice) distribution')","a56cda54":"#xgb.plot_importance(list_bestest[num_xgb][isd], show_values=False,\u2423  \u2192xlim=None, height=0.8, max_num_features=8, ax=axes[isd]) ; axes[isd].  \u2192set_title(str(storedept))","48ce8510":"#list_features2plot = ['LotArea', 'GrLivArea', 'TotLivArea'] #, 'BedroomAbvGr']\n#df_test['logSalePrice_pred('+list_models[-1][0]+')']\n#scatter_target_features(df_test,'logSalePrice_pred('+list_models[-1][0]+')', list_features2plot,'saleprice_area_testset','Sale Price (k\\$)')","eb5c8ade":"list_stack = ['ridge', 'lasso', 'elasticnet', 'RF', 'GBR', 'LGBM', 'XGB']\nlist_res = ['SalePrice_pred(%s)' % (mod) for mod in list_stack]\ndf_test['SalePrice_pred(stacking)'] = df_test[list_res].mean(axis=1)\ndf_testcsv = df_test[['Id', 'SalePrice_pred(stacking)']].copy()\ndf_testcsv['SalePrice'] = df_testcsv['SalePrice_pred(stacking)']\ndf_testcsv[['Id', 'SalePrice']].set_index('Id').to_csv('predictions_stacking.csv')","63a0b9d6":"Let's now clean the dataframe by imputing NaN values depending on the type of features.","b49b57b8":"Let's apply now several bagging and boosting models.","9fec7b51":"# 7. Bagging and Boosting\n<a name=\"7\">\nlink\n<\/a>","bf8cb505":"The log(SalePrice) distribution is close to be gaussian! That's great!","af9f8f85":"This dataset contains 79 features of different categories (objects, int, and float). By looking at their description provided in Kaggle, we can evaluate how they caracterize the housing properties (building size, building condition, and location), and our expectation about the variable influence on SalePrice.","3c7bd795":"Let's list the values given by each feature to explore what they are talking about. If there are more than 20 values, it likely means that the features are continuous variables.","45a5faad":"# 4. Label Encoding vs One-Hot Encoding\n<a name=\"4\">\nlink\n<\/a>","f0b6a33c":"The SalePrice target variable is not normally distributed, and is right skewed. Let's perform a log-transformation of the target variable to make it normally distributed. ","33c0bf35":"## Useful functions","586277b4":"Let's now transform the character features into numerical ones using Label-Encoding and One-Hot Encoding. Note that I first concat the three (train, holdout, test) dataframes first before performing the transformation so the encoding takes all values into account for the transformation.","5f504483":"Not surprisingly, the features with the highest coefficients also tend to be those that showed the highest correlation with the SalePrice target variable: overall quality, ground living area, or garage size.","31bfaa93":"## Data Importation","b5ad05ad":"Before applying several methods to the One-Hot encoded dataset, let's plot the feature coefficients obtained with the Ridge model applied on the Label Encoded dataset and compare them with their correlation coefficients with the SalePrice target feature.","00051056":"### Other functions","11a7cf76":"The dataframe no longer contains NaN values, that's great !","f413f423":"### Plotting functions","711adbd0":"Let's apply several simple regression models on our One-Hot encoded dataset. ","43ced4c0":"- **Features describing the size of the house**\n    - *Area of various parts of the house*: 1stFlrSF, 2ndFlrSF, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, GrLivArea, LowQualFinSF, MasVnrArea, TotalBsmtSF\n    - *Number of all rooms of all types*: BedroomAbvGr, BsmtFullBath, BsmtHalfBath, Fireplaces, FullBath, HalfBath, Kitchen, TotRmsAbvGrd\n- **Features describing the equipment of the house**\n    - CentralAir, Electrical, Foundation, Heating, MasVnrType, RoofMatl, RoofStyle, Utilities\n- **Features describing the condition\/quality of the house**\n    - *Basement condition*: BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtQual\n    - *Exterior condition*: ExterCond, ExterQual, Exterior1st, Exterior2nd, ExterQual\n    - *Quality of the equipment*: FireplaceQu,  HeatingQC, KitchenQual\n    - *Condition of the house*: OverallCond, OverallQual, YearBuilt, YearRemodAdd, Functional\n    - *Type of the house*: HouseStyle, MSSubClass, BldgType\n- **Features describing the condition\/quality of the lot (garage, pool, porch, garden)**\n    - *Condition of the garage*: GarageCond, GarageFinish, GarageQual, GarageType, GarageYrBlt\n    - *Condition of the yard*: LandContour, LandSlope, LotConfig, PoolQC, LotShape, Fence, PavedDrive\n- **Features describing the size of the lot (garage, pool, porch, garden)**\n    - *Size of the porch*: 3SsnPorch, EnclosedPorch, OpenPorchSF, ScreenPorch, WoodDeckSF\n    - *Size of the garage*: GarageArea, GarageCars\n    - *Size of the lot*: LotArea, PoolArea\n- **Features describing the location**\n    - Alley, Condition1, Condition2, LotFrontage, MSZoning, Neighborhood\n- **Misc features**\n    - MiscFeature, MiscVal, MoSold, SaleCondition, SaleType, Street, YrSold","1ec0146a":"Let's put here several functions that will be used throughout the notebook. ","f50b74b1":"Let's now create new features that would give a more general (and hopefully more accurate) description of the houses.","e8484050":"## Exploring the data features","c7083dab":"As expected, the score of the One-Hot dataset seems to better, the One-Hot will likely give better performances !","4228b665":"Let's list the correlation between the target variable (SalePrice) and the (numeric) features.","e5c73fb9":"## Exploring the features and their values","1a242f67":"Let's start our *quantitative* analysis by comparing the results of a Ridge model applied on our Label Encoded data and One-Hot encoded data to estimate which encoding results in the best performance.","33b0f2d3":"## Data cleaning","2cd31b97":"# 5. Regression with Linear models\n<a name=\"5\">\nlink\n<\/a>","6dcf0d71":"## Distribution of the target variable","6733c520":"Let's now check if features are skewed, and perform some box cox transformation (here using the boxcox1p function) to make them more normally distributed. ","a17d8928":"This work is largely inspired by these great notebooks:\n\n- [Stacked Regressions : Top 4% on LeaderBoard](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard) by Serigne\n- [Comprehensive data exploration with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) by Pedro Marcelino","fa562d96":"### [1. Data Importation](#1)\n\n### [2. Exploratory Data Analysis (EDA)](#2)\n\n### [3. Data cleaning and feature engineering](#3)\n\n### [4. Label-Encoded vs One-Hot Encoded](#4)\n\n### [5. Regression with Linear models](#5)\n\n### [6. Support Vector Machines (SVM)](#6)\n\n### [7. Bagging and Boosting](#7)\n\n### [8. Conclusions and Perspectives](#8)\n","5a5882a6":"Several features contain a lot of NaN values, we'll have to take this into account later. ","16f5d343":"Let's now list the fraction of NaN values in descending order.","01d4ad0a":"## Label and One-Hot encoding","5259a80f":"# 8. Stacking\n<a name=\"8\">\nlink\n<\/a>","99de0ce2":"# 3. Data Cleaning and Feature Engineering\n<a name=\"3\">\nlink\n<\/a>","38a1b770":"# 1. Data importation and useful functions\n<a name=\"1\">\nlink\n<\/a>\n","0e7f1202":"# 2. Exploratory Data Analysis\n<a name=\"2\">\nlink\n<\/a>\n"}}