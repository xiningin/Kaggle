{"cell_type":{"3e4a3381":"code","c92b1df4":"code","2b5e60e3":"code","900b4a4e":"code","13d4a3b8":"code","abfe945a":"code","31822d06":"code","304602d2":"code","7ccd703a":"code","7fd0fc57":"code","16960d81":"code","d077e456":"code","eab5affa":"code","62bc6d4d":"code","c035ee84":"code","9617ae2a":"code","85badbd9":"code","f80811c0":"code","c34c30f9":"code","7ecbff5e":"code","37dfcb10":"code","dfdac896":"code","9d5b7bf6":"code","204ac787":"code","c67dbdc0":"code","0be153df":"code","fe646c65":"code","4d8e666b":"code","886f359b":"markdown","1c6d69dc":"markdown","f196b092":"markdown","913e5f6f":"markdown","736048a5":"markdown","508edb0b":"markdown","37f2e445":"markdown","e9cb4c37":"markdown","ec90e890":"markdown","9422828e":"markdown","1731f2a6":"markdown","b8795bb9":"markdown","62959650":"markdown","ada17387":"markdown","42071576":"markdown"},"source":{"3e4a3381":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c92b1df4":"import sklearn\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier","2b5e60e3":"train = pd.read_csv('\/kaggle\/input\/kpitmovies\/train_data.csv')\n","900b4a4e":"train.head(2)","13d4a3b8":"basic_features = ['runtime', 'budget', 'revenue', 'vote_count']\nbasic_X = train[basic_features]\nbasic_y = train['target']","abfe945a":"basic_X.head()","31822d06":"basic_y.head()","304602d2":"assert basic_X.shape[0] == basic_y.shape[0]","7ccd703a":"basic_X_train, basic_X_validate, basic_y_train, basic_y_validate = train_test_split(basic_X, basic_y)","7fd0fc57":"logres =  LogisticRegression()\nlogres.fit(basic_X_train, basic_y_train)\nlogres_y_pred = logres.predict(basic_X_validate)","16960d81":"print('Accuracy \/ train:\\t',cross_val_score(logres, basic_X_train, basic_y_train).mean())\nprint('Accuracy \/ validation:  ',accuracy_score(logres_y_pred, basic_y_validate))","d077e456":"tree =  DecisionTreeClassifier()\ntree.fit(basic_X_train, basic_y_train)\ntree_y_pred = tree.predict(basic_X_validate)","eab5affa":"print('Accuracy \/ train:\\t', cross_val_score(tree, basic_X_train, basic_y_train).mean())\nprint('Accuracy \/ validation:  ', accuracy_score(basic_y_validate,tree_y_pred))","62bc6d4d":"knn = KNeighborsClassifier()\nknn.fit(basic_X_train, basic_y_train)\nknn_y_pred = knn.predict(basic_X_validate)","c035ee84":"print('Accuracy \/ train:\\t', cross_val_score(knn, basic_X_train, basic_y_train).mean())\nprint('Accuracy \/ validation:  ', accuracy_score(basic_y_validate,knn_y_pred))","9617ae2a":"poll = VotingClassifier(estimators=[('logres', logres),('dt', tree),('knn', knn)], weights=[1, 1, 1], voting='hard')\npoll.fit(basic_X_train, basic_y_train)\npoll_y_pred = poll.predict(basic_X_validate)","85badbd9":"print('Accuracy \/ train:\\t', cross_val_score(poll, basic_X_train, basic_y_train).mean())\nprint('Accuracy \/ validation:  ', accuracy_score(basic_y_validate,poll_y_pred))","f80811c0":"test = pd.read_csv('\/kaggle\/input\/kpitmovies\/test_data.csv').drop(61)\ntest.head(2)","c34c30f9":"test.movie_id = test.movie_id.astype('int')","7ecbff5e":"basic_X_test = test[basic_features]\nbasic_X_test.head(2)","37dfcb10":"tree_prediction = tree.predict(basic_X_test)\npoll_prediction = poll.predict(basic_X_test)","dfdac896":"#tree_prediction","9d5b7bf6":"submission = pd.read_csv('\/kaggle\/input\/kpitmovies\/sample_submission.csv')","204ac787":"submission.head()","c67dbdc0":"#test.movie_id.values","0be153df":"submission.movie_id = test.movie_id.values\nsubmission.target = tree_prediction","fe646c65":"#submission.to_csv('tree_baseline.csv', index=False)","4d8e666b":"submission.target = poll_prediction\n#submission.to_csv('poll_baseline.csv', index=False)","886f359b":"#### KNN","1c6d69dc":"#### Decision Tree","f196b092":"### How to submit","913e5f6f":"...And run your best models on it!","736048a5":"##### Now, you can explore sample submission and use it as an example of valid formatting!","508edb0b":"### Finally, just a one more advice for you: improve your model gradually, step by step\n#### Otherwise after two-three hours of brain-crushing work (or already after midnight) you can find out that the result of your final submission ended with an error","37f2e445":"#### Voting Ensemble","e9cb4c37":"### A few ways to improve your score without any additional features\n\n1. try Random Forest - an ensemble of decision trees\n2. use GridSearchCV - try to do some tuning on your models\n    - some hyper-params for LogRes: \n      - C - use 0.001, 0.01, 0.1, 1, 10, 100, 1000 or simply np.logspace(-3, 3, 7)\n      - penalty (if you understand your intentions)\n    - ... on Decision Tree:\n      - max_depth - use None, 5, 10, 15 or whatever you like,or simply range \/ np.arange \/ np.linspace\n      - min_samples_split - use 2 (default), 3, 5, i donno...\n      - max_features - use None, 'auto', 'sqrt', 'log2', 2, 4, 5, 10, ...\n    - ... on Random Forest:\n      - n_estimators\n      - max_depth - as for Decision Tree\n      - min_samples_split - as for Decision Tree\n      - max_features - as for Decision Tree\n    - ... on KNeighbors:\n      - n_neighbors - 3, 5 (default), 7, etc\n      - weights - \u2018uniform\u2019, \u2018distance\u2019, custom\n      - p & metric (2 & minkowski, 1 & minkowski... really need this? read the docs)\n3. do some tune on polling model (weights, soft polling)\n4. try to scale features (use minmax or robust scaler)\n5. try to understand features... maybe you'll found it simply stupid just to multiply some features on some numbers to be in a linear relation with the target variable? maybe you have an answer how to use some of them to build a more intelligent features? For example, is the quality of the film really directly dependent on the duration of the film? And in what, the longer the better, or vice versa?\n6. generate new features based on current set! maybe feature 'runtime * budget \/ vote_count'? or feature 'sqrt (vote_count^2 - runtime^2)'?\n7. level Expert: do EDA to find if outliers or NA have the influence to the results","ec90e890":"### ... and with additional features\n\n**so, still unexplored features:**\n1. movie_id (used for submission not for models)\n2. title\n3. director\t\n4. language\n5. release_date\n6. genres\n7. production_countries\n8. production_companies\n9. keywords\n10. cast","9422828e":"**i'm gonna recommend to start with:**\n- release_date (parse date and pull out from it anything you want)\n- director (do OneHotEncoding, or maybe do some research to find the most top-scoring directors and generate new binary feature)\n- language (some people say 'es' cinema has the best scores... are they right?)\n\n**other features require a greater level of investigation and investment...**\n- genres (your steps: parse, find unique, do OHE for them, write 1s for those of OHE_columns which are in current film's genres list, ha-ha)\n- keywords (for a warm start, you can try the same pipeline like for the genres)\n- production_companies (for a warm start, you can try the same pipeline like for the genres)\n- production_countries (maybe the more countries participate in production of a film the better score it will got?)\n\n**and for grandmasters:**\n- title (impress me)\n- cast","1731f2a6":"#### Logistic Regression","b8795bb9":"## Basic Model: only features that need little or no preprocessing","62959650":"##### ...Select exactly the same features you used before","ada17387":"##### Now you need to change default data to yours!","42071576":"##### Firstly, open *test_data*..."}}