{"cell_type":{"379ae552":"code","2bc90eb9":"code","19c2a670":"code","7f46bb43":"code","3133a6f0":"code","b07b78c5":"code","34836ad0":"code","0547f249":"code","29608b35":"code","9b2e3647":"code","297d4f95":"code","ff557deb":"code","f90254bb":"code","c383be42":"code","31f42dd7":"code","4b61e574":"code","ce938e1f":"code","a27a4f37":"code","46a6db5e":"code","c1315c95":"code","fea731be":"code","68e9c225":"code","96734598":"code","6427be10":"code","a0a556df":"code","4cbb7cff":"code","1d7eb79d":"code","3cdeccb2":"code","ea640fbd":"code","ddb3aa2f":"code","60a2a852":"code","4d26662b":"code","b4dbcdb0":"code","6a4a5581":"code","f9e2769d":"code","355f75a9":"code","53ccd477":"code","c369451a":"code","7242a76f":"code","d31cb116":"code","62849e4b":"code","97ab1077":"code","971ab3cb":"code","6bb9c61c":"code","4650bb08":"code","07c35d65":"code","de26c57d":"code","1d229245":"code","2220181f":"code","0fae26a1":"code","45a83b2a":"code","c17f0fb9":"code","8b07ead3":"code","6feb3067":"code","75b2f78e":"code","7ec38121":"code","3c0d3ce3":"code","6b0d542c":"code","73bc082c":"code","86d6900c":"code","f324e179":"code","4c5b071a":"code","678f1e1e":"code","f861b0fb":"code","fe4c4334":"code","6ec048de":"code","ea5c5629":"code","b3330db6":"markdown","dbeaad19":"markdown","70891397":"markdown","1a8d3d8a":"markdown","21cc7351":"markdown","ec136932":"markdown","b1017077":"markdown","3b7bbf48":"markdown","acef9d26":"markdown","8cceefef":"markdown","bcba139c":"markdown","565c5036":"markdown","44f361bd":"markdown","bb93c26c":"markdown","7f66f6d7":"markdown","abee6ea6":"markdown","37e585d5":"markdown","c0ef4068":"markdown","ee8ef03a":"markdown","7fedeb63":"markdown","5334f309":"markdown","c8885db5":"markdown","77e1310f":"markdown","777f4618":"markdown","3f67d87c":"markdown","8c45b52b":"markdown","cb0a2934":"markdown","cbc77b4a":"markdown","a2e75d24":"markdown","dbe4d870":"markdown","1e148129":"markdown","a885dfd2":"markdown","ce0bf214":"markdown","ff305812":"markdown","a6d4321c":"markdown"},"source":{"379ae552":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport numpy as np\nfrom scipy import stats\nsns.set()\npd.set_option('display.max_columns', 60)\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2bc90eb9":"\n# Read the csv file\ndf = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')","19c2a670":"# First and last 5 rows of the data\ndisplay(df.head())\ndf.tail()","7f46bb43":"print('The Stroke data has {0} rows and {1} columns'.format(df.shape[0],df.shape[1]))","3133a6f0":"# Since this is a classification problem, let's investigate the proportion of stroke variables.\ndf['stroke'].value_counts()","b07b78c5":"# remove Id columns and investigate column types\ndf = df.drop(['id'], axis=1)\ndf.info()","34836ad0":"# investigate the means, medians, min-max of the data.\ndf.describe()","0547f249":"missings = pd.DataFrame(columns=['Columns','Missing','Percentage'])","29608b35":"for x in df.columns:\n    if df[x].isna().sum() >0:\n        missings = missings.append({'Columns': x ,'Missing': df[x].isna().sum(), \n                                    'Percentage':(df[x].isna().sum()\/len(df[x])*100)}, ignore_index=True)","9b2e3647":"missings","297d4f95":"#first import KNN imputer\nfrom sklearn.impute import KNNImputer","ff557deb":"# check the describe line for the mean of the BMI\n#mean 28 std 7.8","f90254bb":"imputer = KNNImputer(n_neighbors=5)\nimputed = pd.DataFrame(imputer.fit_transform(df.iloc[:,[1,7,8,10]]), columns=['age','avg_glucose_levels','bmi','stroke'])\n\n","c383be42":"# lets see if the mean was changed\ndisplay(imputed.describe())\nimputed.info()","31f42dd7":"df['bmi'] = imputed['bmi']","4b61e574":"df","ce938e1f":"df['has_diabetes'] = [(1 if i >125 else 0) for i in df['avg_glucose_level']] \ndf['is_obese'] = [1 if i>=30 else 0 for i in df['bmi']]","a27a4f37":"df","46a6db5e":"# possible outliers are in Age and BMI columns as mentioned above. ","c1315c95":"# Reference to Ceren \u0130yim github, link : https:\/\/github.com\/cereniyim\/Tree-Classification-ML-Model\ndef outlier_function(df, col_name):\n    ''' this function detects first and third quartile and interquartile range for a given column of a dataframe\n    then calculates upper and lower limits to determine outliers conservatively\n    returns the number of lower and uper limit and number of outliers respectively\n    '''\n    first_quartile = np.percentile(df[col_name], 25)\n    third_quartile = np.percentile(df[col_name], 75)\n    IQR = third_quartile - first_quartile\n                      \n    upper_limit = third_quartile+(1.5*IQR)\n    lower_limit = first_quartile-(1.5*IQR)\n    outlier_count = 0\n                      \n    for value in df[col_name].tolist():\n        if (value < lower_limit) | (value > upper_limit):\n            outlier_count +=1\n    return lower_limit, upper_limit, outlier_count","fea731be":"numerics= df.select_dtypes(include='float64')\nfor column in numerics.columns:\n    if outlier_function(numerics, column)[2] > 0:\n        print(\"There are {} outliers in {}\".format(outlier_function(df, column)[2], column))","68e9c225":"#I want to investigate age columns additionally, although there was no outliers detected. The ages below 1 may be problem\nage_invest = df[df['work_type']=='children']\ndisplay(age_invest)\nage_invest.describe()","96734598":"# I'm going to remove outliers by glucose levels in which we had 166 outliers.\ndf = df[(df['avg_glucose_level'] > outlier_function(df, 'avg_glucose_level')[0]) &\n              (df['avg_glucose_level'] < outlier_function(df, 'avg_glucose_level')[1])]\n","6427be10":"df.shape","a0a556df":"df.describe()","4cbb7cff":"#Before graphs, lets see the means of the two groups (stroke and healthy)\n\nmeans = df.loc[:,['age','avg_glucose_level','bmi','stroke']]\ncounts = df.loc[:,['gender', 'hypertension','heart_disease','ever_married','work_type','Residence_type','smoking_status',\n                  'has_diabetes','is_obese','stroke']]\ndisplay(df['stroke'].value_counts())\ndisplay(means.groupby(means['stroke']).mean())\ndisplay(counts.groupby(counts['stroke']).sum())","1d7eb79d":"print('the percentage of stroke in the data set is :', sum(df['stroke']==1)*100\/len(df))","3cdeccb2":"fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(30,10))\n\nsns.countplot(df['stroke'], ax = ax1)\nax1.set_xticklabels( ['No Stroke', 'Stroke'])\nax1.set_title('Numbers of stroke and healthy individuals')\nax1.text( s = df['stroke'].value_counts()[0],\n         x = 0,\n         y = (df['stroke'].value_counts()[0]) * 0.8,\n        fontsize = 20)\n\nax1.text( s = df['stroke'].value_counts()[1],\n         x = 1,\n         y = (df['stroke'].value_counts()[1]),\n        fontsize = 20)\n\nsns.violinplot(x=df['stroke'], y=df['age'], ax = ax2)\nax2.set_title('Age Proportions of Healthy and Individuals with Stroke')\n\n\nsns.histplot(data = df, x = 'age', hue='stroke', ax = ax3)\nminy_lim, y_lim = plt.ylim()\nax3.axvline(df[df['stroke']==1]['age'].mean(), linestyle='--', color='r')\nax3.axvline(df[df['stroke']==0]['age'].mean(), linestyle='--')\nax3.text(s = f\"Mean Age (Stroke) : \\n {df[df['stroke']==1]['age'].mean():.2f}\",\n         y = y_lim * 0.75, x =df[df['stroke']==1]['age'].mean() )\nax3.text(s = f\"Mean Age : \\n {df[df['stroke']==0]['age'].mean():.2f}\",\n         y = y_lim * 0.9, x =df[df['stroke']==0]['age'].mean()+3 )\nax3.set_title('Age Proportions of Healthy and Individuals with Stroke')\n\nplt.show()","ea640fbd":"strokes = df[df['stroke']==1]\nno_stroke = df[df['stroke']==0]\nfig, ax = plt.subplots(5,2, figsize=(20,12))\n\nax[0,0].pie(strokes['hypertension'].value_counts(normalize=True),\n           autopct= '%1.1f%%',\n           shadow = True,\n            labels = ['no hypertension', 'hypertension'],\n           startangle = 90,\n           explode = [0,0.2])\nax[0,0].set_title('Hypertension Percentages of Clients with a Stroke')\n\nax[0,1].pie(no_stroke['hypertension'].value_counts(normalize=True),\n           autopct= '%1.1f%%',\n           shadow = True,\n           startangle = 90,\n           explode = [0,0.2])\nax[0,1].set_title('Hypertension Percentages of Clients without a Stroke')\n\nax[1,0].pie(strokes['gender'].value_counts(normalize=True),\n           autopct= '%1.1f%%',\n           shadow = True,\n           labels = ['male', 'female'],\n           startangle = 90,\n           explode = [0,0.2])\nax[1,0].set_title('Gender Percentages of Clients with a Stroke')\n\nax[1,1].pie(no_stroke['gender'].value_counts(normalize=True),\n           autopct= '%1.1f%%',\n           shadow = True,\n            labels = ['male', 'female', 'other'],\n           startangle = 90,\n           )\nax[1,1].set_title('Gender Percentages of Clients without a Stroke')\n\n\nax[2,0].pie(strokes['ever_married'].value_counts(normalize=True),\n           autopct= '%1.1f%%',\n           shadow = True,\n            labels = ['yes', 'no'],\n           startangle = 90,\n           explode = [0,0.2])\nax[2,0].set_title('Marital Status Percentages of Clients with a Stroke')\n\n\nax[2,1].pie(no_stroke['ever_married'].value_counts(normalize=True),\n           autopct= '%1.1f%%',\n           shadow = True,\n            labels = ['yes', 'no'],\n           startangle = 90,\n           explode = [0,0.2])\nax[2,1].set_title('Marital Status Percentages of Clients without a Stroke')\n\nax[3,0].pie(strokes['smoking_status'].value_counts(normalize=True),\n           autopct= '%1.1f%%',\n           shadow = True,\n           labels = ['never smoked', 'formerly smokes', 'smokes','unkown'],\n           startangle = 90,\n           #explode = [0,0.2]\n           )\nax[3,0].set_title('Smoking Status Percentages of Clients with a Stroke')\n\nax[3,1].pie(no_stroke['smoking_status'].value_counts(normalize=True),\n           autopct= '%1.1f%%',\n           shadow = True,\n          labels = ['never smoked', 'formerly smokes', 'smokes','unkown'],\n           startangle = 90,\n           #explode = [0,0.2]\n           )\nax[3,1].set_title('Smoking Status Percentages of Clients without a Stroke')\n\nax[4,0].pie(strokes['is_obese'].value_counts(normalize=True),\n           autopct= '%1.1f%%',\n           shadow = True,\n           labels = ['not obese', 'obese'],\n           startangle = 90,\n           #explode = [0,0.2]\n           )\nax[4,0].set_title('Obesity Percentages of Clients with a Stroke')\n\nax[4,1].pie(no_stroke['is_obese'].value_counts(normalize=True),\n           autopct= '%1.1f%%',\n           shadow = True,\n            labels = ['not obese', 'obese'],\n           startangle = 90,\n           #explode = [0,0.2]\n           )\nax[4,1].set_title('Obesity Percentages of Clients without a Stroke')\n\n\nplt.show()","ddb3aa2f":"fig, (ax1,ax2) = plt.subplots(1,2, figsize=(30,10))\n\nsns.histplot(data = df, x= 'bmi', hue= 'stroke', ax = ax1)\nax1.set_title('BMI distributions of  stroke and healthy individuals')\nax1.text( s = 'Stroke mean (red) : ' \"{:.2f}\".format(strokes['bmi'].mean()),\n         x = 33,\n         y = 250,\n       fontsize = 20)\n\nax1.text( s = 'No stroke mean (blue): ' \"{:.2f}\".format(no_stroke['bmi'].mean()),\n         x = 33,\n         y = 225,\n       fontsize = 20)\nax1.axvline(strokes['bmi'].mean(), linestyle='--', color='r')\nax1.axvline(no_stroke['bmi'].mean(), linestyle='--', color='b')\n\nsns.violinplot(x=df['stroke'], y=df['bmi'], ax = ax2)\nax2.set_title('Age Proportions of Healthy and Individuals with Stroke')\n\n\n\nplt.show()\n# the clients who had a stroke gathered around 30 bmi, means are close","60a2a852":"fig , (ax1,ax2) = plt.subplots(1,2, figsize=(12,6))\nsns.countplot(x= 'work_type', hue= 'stroke', data=df, ax=ax1)\n_ = ax1.set_xticklabels(labels = df['work_type'].unique(),rotation=60, ha='right')\nax1.set_title('Work Types Among Different Diagnosis')\n\nsns.countplot(x= 'Residence_type', hue= 'stroke', data=df, ax = ax2)\n_ = ax2.set_title('Stroke Counts among Residence Type')\n\nplt.show()","4d26662b":"from sklearn.preprocessing import LabelEncoder","b4dbcdb0":"lab_enc= LabelEncoder()","6a4a5581":"# Let's see (don't scroll back to top, lazy style) the columns to be encoded\nfor x in df.select_dtypes(include='object'):\n    print(df[x].value_counts())","f9e2769d":"lab_enc_data= df.loc[:,['gender','ever_married','Residence_type','work_type','smoking_status']]\nfor x in lab_enc_data.columns:\n    lab_enc_data[x]=lab_enc.fit_transform(lab_enc_data[x])","355f75a9":"# I know this is the hard way, but i want to preserv the df and after append new columns after. \n# First remove these columns from df, then append lab_enc_data to the df. I will do this with a for loop\nfor x in lab_enc_data.columns:\n    df[x]=lab_enc_data[x]","53ccd477":"df.head()","c369451a":"def bayesian_dist(column, data):\n    \"\"\"this function gets column name (str) and dataframe (str), \n    returns distribution plot of the column, skewness and kurtosis\"\"\"\n    sns.distplot(data[column])\n    plt.title(x)\n    plt.show()\n    plt.show()\n    print('skewness: ', stats.skew(data[column]))\n    print('kurtosis: ', stats.kurtosis(data[column]))\n    \n\ndef normal_visual(column, df):\n    \"\"\"This function gets column and dataframe as str.\n    Return \n    Shapiro Wilk test and Kolmogorov-Smirnov test results,\n    distplot, skewness and kurtosis of the column\n    \"\"\"\n    bayesian_dist(column, df)\n    print('*'* 30)\n    print(column, 'Shapiro-Wilk test t score: ', \"{:.2f}\".format(stats.shapiro(df[x])[0]))\n    print(column, 'Shapiro-Wilk test p value: ', \"{:.2f}\".format(stats.shapiro(df[x])[1]))\n    print('*'*30)\n    print(column, 'Kolmogorov-Smirnov t score: ', \"{:.2f}\".format(stats.kstest(df[x],'norm', args=(df[x].mean(),\n                                                                                                   df[x].std()))[0]))\n    print(column, 'Kolmogorov-Smirnov t score: ', \"{:.2f}\".format(stats.kstest(df[x],'norm', args=(df[x].mean(),\n                                                                                                   df[x].std()))[1]))\n    \n    ","7242a76f":"for x in means.columns:\n    if x != 'stroke':\n        normal_visual(x, df)","d31cb116":"# boxcox transformation \n\nfor x in means.columns:\n    if x != 'stroke':\n        means[x] = stats.boxcox(df[x])[0]","62849e4b":"for x in means.columns:\n    if x != 'stroke':\n        normal_visual(x, means)","97ab1077":"for x in means.columns:\n    if x != 'stroke':\n        df[x]=means[x]\n\n# and check if it's ok\ndf.head()","971ab3cb":"from sklearn.model_selection import train_test_split\ntarget = df['stroke']\npredictors = df.drop('stroke', axis=1)\nx_train, x_test, y_train, y_test = train_test_split(predictors,target, train_size=0.7,\n                                                    random_state= 42, stratify = target.values)\n\n\ndisplay(x_train.shape)\ndisplay(y_train.shape)\ndisplay(x_test.shape)\ndisplay(y_test.shape)","6bb9c61c":"# SMOTE method for class imbalance, \nX = df.drop('stroke', axis=1)\ny= df['stroke']\nfrom imblearn.over_sampling import SMOTE\noversample = SMOTE(sampling_strategy= 0.4, random_state=42)\nX_sm,y_sm = oversample.fit_resample(x_train,y_train,)\nprint('Data shapes before oversampling were {0} and {1}'.format(x_train.shape, y_train.shape))\nprint('Data shapes after oversampling are {0} and {1}'.format(X_sm.shape, y_sm.shape))\ny_sm.value_counts()","4650bb08":"# undersampling \nfrom imblearn.under_sampling import NearMiss\nnearmiss = NearMiss(sampling_strategy = 0.7)\nX_us, y_us = nearmiss.fit_resample(X_sm, y_sm)\nprint('Data shapes after oversampling were {0} and {1}'.format(X_sm.shape, y_sm.shape))\nprint('Data shapes after undersampling are {0} and {1}'.format(X_us.shape, y_us.shape))\ny_us.value_counts()","07c35d65":"\nfrom sklearn.metrics import accuracy_score\n\n#ML algoritms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n\n#Performance metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import cross_val_score","de26c57d":"# Non-oversampled data ML\nmodel_accuracy = pd.DataFrame(columns=['Model','Accuracy'])\nmodels = {\"LR\": LogisticRegression(),\n          \"NB\": GaussianNB(),\n          \"KNN\" : KNeighborsClassifier(),\n          \"DT\" : DecisionTreeClassifier(),\n          'RFC' : RandomForestClassifier(),\n          'ABC' : AdaBoostClassifier(),\n          'GBC' : GradientBoostingClassifier(),\n          'DTC' : DecisionTreeClassifier(),\n          }\n\nfor test, clf in models.items():\n    clf.fit(x_train, y_train)\n    y_pred = clf.predict(x_test)\n    acc = accuracy_score(y_test,y_pred)\n    train_pred = clf.predict(x_train)\n    train_acc = accuracy_score(y_train, train_pred)\n    print( test + ' scores')\n    print(acc)\n    print(classification_report(y_test, y_pred))\n    print(confusion_matrix(y_test, y_pred))\n    print('*' * 100)\n    model_accuracy = model_accuracy.append({'Model': test, 'Accuracy': acc, 'Train_acc': train_acc}, ignore_index=True)","1d229245":"model_accuracy","2220181f":"#resampled data train_test_split\nresX_train, resX_test, resy_train, resy_test = train_test_split(X_us, y_us, train_size= 0.7,\n                                                               random_state=42)\n\nprint(resX_train.shape, resX_test.shape)\nprint(resy_train.shape, resy_test.shape)","0fae26a1":"# oversampled data ML\nmodel_accuracy = pd.DataFrame(columns=['Model','Accuracy'])\nmodels = {\"LR\": LogisticRegression(),\n          \"NB\": GaussianNB(),\n          \"KNN\" : KNeighborsClassifier(),\n          \"DT\" : DecisionTreeClassifier(),\n          'RFC' : RandomForestClassifier(),\n          'ABC' : AdaBoostClassifier(),\n          'GBC' : GradientBoostingClassifier(),\n          'DTC' : DecisionTreeClassifier(),\n          }\n\nfor test, clf in models.items():\n    clf.fit(resX_train, resy_train)\n    y_pred = clf.predict(resX_test)\n    acc = accuracy_score(resy_test,y_pred)\n    train_pred = clf.predict(resX_train)\n    train_acc = accuracy_score(resy_train, train_pred)\n    print( test + ' scores')\n    print(acc)\n    print(classification_report(resy_test,y_pred))\n    print(confusion_matrix(resy_test,y_pred))\n    print('*' * 100)\n    model_accuracy = model_accuracy.append({'Model': test, 'Accuracy': acc, 'Train_acc': train_acc}, ignore_index=True)","45a83b2a":"model_accuracy.sort_values('Accuracy', ascending=False)","c17f0fb9":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nrfc = RandomForestClassifier()\n\n\n# number of estimators , default = 100\nn_estimators = [int(x) for x in np.linspace(start=10, stop=100, num=10)]\n# number of  features for every split, default= auto, we have options sqrt(same as auto), log2, None( = n_features)\nmax_features = ['auto','log2']\n#maximum depth for trees\nmax_depth = [2,4,6,8,10, None]\n#min_samples_leaf\nmin_samples_leaf = [1,2]\n#min_samples_split\nmin_samples_split = [2,5,10]\n#bootstrap\nbootstrap = [True, False]\n\n#Create random grid\nparam_grid = {'n_estimators': n_estimators,\n             'max_features' : max_features,\n             'max_depth' : max_depth,\n             'min_samples_leaf' : min_samples_leaf,\n             'min_samples_split' : min_samples_split,\n             'bootstrap' : bootstrap}\n\n","8b07ead3":"rf_grid = RandomizedSearchCV(estimator= rfc, param_distributions = param_grid, n_iter= 100,  cv=3, n_jobs=2, verbose=2)","6feb3067":"rf_grid.fit(X_sm, y_sm)\nrf_grid.best_params_","75b2f78e":"# number of estimators , default = 100\nn_estimators = [29,30,31]\n# number of  features for every split, default= auto, we have options sqrt(same as auto), log2, None( = n_features)\nmax_features = [1,3,5]\n#maximum depth for trees\nmax_depth = [ None, 2, 5]\n#min_samples_leaf\nmin_samples_leaf = [1]\n#min_samples_split\nmin_samples_split = [1,2,3]\n#bootstrap\nbootstrap = [False]\n\n#Create random grid\nparam_grid = {'n_estimators': n_estimators,\n             'max_features' : max_features,\n             'max_depth' : max_depth,\n             'min_samples_leaf' : min_samples_leaf,\n             'min_samples_split' : min_samples_split,\n             'bootstrap' : bootstrap}","7ec38121":"gr_grid = GridSearchCV(estimator= rfc, param_grid = param_grid,cv=3, n_jobs=-1, verbose=2)","3c0d3ce3":"gr_grid.fit(X_sm, y_sm)\ngr_grid.best_params_ , gr_grid.best_score_","6b0d542c":"rfc = RandomForestClassifier(bootstrap= False, max_depth=None, max_features= 3, min_samples_leaf = 1, \n                            min_samples_split=3, n_estimators= 31)\nrfc.fit(resX_train, resy_train)\ny_pred = rfc.predict(resX_test)\nacc = accuracy_score(resy_test,y_pred)\nprint(acc)\nprint(accuracy_score(rfc.predict(resX_train), resy_train))\nprint(classification_report(resy_test, y_pred))\nprint(confusion_matrix(resy_test, y_pred))","73bc082c":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score","86d6900c":"fpr, tpr, threshold = roc_curve(resy_test, y_pred)\nplt.plot(fpr, tpr)\nplt.title('ROC Curve for Stroke Prediction')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()\n\n","f324e179":"roc_auc_score(resy_test,y_pred)","4c5b071a":"feature_imp = rfc.feature_importances_","678f1e1e":"feature_imp","f861b0fb":"sns.barplot(X_sm.columns, feature_imp)\nplt.xticks(rotation=60)","fe4c4334":"features = X_sm.loc[:,['age','work_type','Residence_type','avg_glucose_level','bmi','smoking_status','is_obese']]","6ec048de":"resX_train, resX_test, resy_train, resy_test = train_test_split(features, y_sm, test_size=0.3, random_state=42)","ea5c5629":"rfc.fit(resX_train, resy_train)\ny_pred = rfc.predict(resX_test)\nacc = accuracy_score(resy_test,y_pred)\nprint(acc)\nprint(classification_report(resy_test, y_pred))\nprint(confusion_matrix(resy_test, y_pred))","b3330db6":"# Discussion","dbeaad19":"They seem ok for now. Now append these new columns to the df dataframe","70891397":"# Scaling","1a8d3d8a":"Outliers done, now it is time to do some visualization","21cc7351":"We will accept the data normally distributed if Skewness is in range -0.5,0.5 and Kurtosis -3,3\n\nSkewness and kurtosis seem ok (except bmi), but histograms doesn't show normal dist. \n\nlet's investigate p values. p>0.05 is accepted as Normal dist. So age and bmi might be normally distributed, however glucose level is not. Now let's see Kolmogorov-smirnov test results which is an other way to examine ND.\n\nKS test results also indicate no ND of these data. \n\nWe'll use Box-Cox transformation for these variables.","ec136932":"## Investigating Target Variable","b1017077":"## Observations of Plots\n\nAs we can see from the pie charts;\n\n* Clients with a stroke has higher hypertension rates than the clients with no stroke\n\n* We cannot tell anything from the gender distrubitions; however according to literature Male gender has higher chance to have a stroke \n\n* Let's make a bias here : IF YOU ARE MARRIED, YOU ARE MORE LIKELY TO HAVE A STROKE :) \n        \n        Well it is hard to say that. \n        \n        Let's recall the histogram that we plot for age distributions. The clients with stroke had higher age means than the clients with no stroke. It is more likely someone to get married after a certain age.  So , I wouldn't say that marriage affects the possibility to have a stroke or not.\n        \n* Nearly all groups has similar proportions, however smokers are higher in the ones had a stroke. And again, it is hard to conclude 'Smoking increases your chances to have a stroke' from this dataset. \n\n* And yet again, we cannot make an assumption from the obesity levels. Maybe we can create labels from obesity data. ","3b7bbf48":"Age column seem to have an issue. 0.08 years of age might be wrong, we'll dive into that.  Also, 10 BMI would be wrong too, but we need to see if it's ok or not. I really wished that data had weight an height informations as well.","acef9d26":"### All data visuals","8cceefef":"We can see clearly best parameters for Random Forest Classifier model. ","bcba139c":"4861 vs 249. We have Class Imbalance here. From what I learnt (tnx google and youtube),  We can use Spread SubSampling or  Synthetic Minority Over-sampling Technique (SMOTE). Spread SubSampling means that we will delete some rows which had 0 (no stroke) value. However I don't think this is a good idea for this dataset, because the number of  participants with stroke is 227; and we need to delete nearly 4400 rows. So I think it is better to use SMOTE and have a decent number of data\n\nWe'll handle this after dealing with missings.","565c5036":"According to Global burden disease reports Stroke prevelance in the world is 1,180.40 per 100,000 population. Which is 1.12%. \n\nReference: https:\/\/www.world-stroke.org\/assets\/downloads\/WSO_Fact-sheet_15.01.2020.pdf\n\nHowever, to make appropriate predictions, we need more data from Stroke feature. As mentioned above I will use SMOTE to estimate\/create more data.\n","44f361bd":"We have no missing, and mean and std seems not changed ( we can ignore the changes on the decimals).\nChange BMI column of the df data.","bb93c26c":"# Stroke Prediction\nStroke is the 2nd highest cause of mortality in the world (WHO). Even if an individual survives a stroke, it is common that the individual has severe symptoms such as spasticity, cognitive problems etc.\n\nSince stroke is a major health problem, it is crucial to know the risk factors that cause a stroke.In this dataset, we will investigate these factors and build a model that predicts stroke.\n\nThe data set contains 11 features and 1 target variable. The target variable is 'Stroke' column, which is binary data 0: No stroke, 1 : Stroke.  \n\nAnyway, let's start digging.\n","7f66f6d7":"The data types seem good. No changes needed","abee6ea6":"Accuracy scores are good, ranging between 0.90 to 0.94. Seem's Ok right?\n\n#### NO !!\n\nLets investigate the confusion matrices:\n\nAll models are good at predicting 'no stroke'. On the other hand , predicting 'Stroke' is on the ground.. However, the model needs to predict 'stroke' which is 4% in total population.  With these True negative predictions these models are useless. \n\nLet's try the models with oversampled data.\n","37e585d5":"# Model Interpretation","c0ef4068":"## Feature Importance evaluation","ee8ef03a":"## Outlier detection ","7fedeb63":"#### Importing packages ","5334f309":"It seems, stroke is morelikely to occur at older ages. (yes, i know this is too obvious)","c8885db5":"People who had a stroke is older, has higher blood sugar rates and slightly higher BMI. ","77e1310f":"This was a good exercise for me , and I've learnt about BoxCox transformation, Class Imbalance and SMOTE and nearmiss method. And I also practiced my model interpretation,  bayesian distribution- skewness-kurtosis values, hyperparameter tuning and tried Feature Importance(way to go btw)\n\nAnyways,\nRandom Forest Classification did a good job on predicting clients with Stroke. Not only accuracy scores are good, additionally Precision, Recall, F1-scores and ROC curve and AUC score are also good.\n\nPS: \nI learned that I made a mistake using SMOTE on all the data. So I changed the process as follows:\nSplit the data into train and test datasets\nUsed SMOTE to increase 'Stroke' values in train df, then used nearmiss to decrease 'No stroke' value. This way I prevented the estimation of stroke data too much. \nThen trained the model and made a prediction.\n\nAlso, we sse the random forest classifier model overfit. ","777f4618":"Feature importance didn't changed the model accuracy significantly. ","3f67d87c":"## Data Visualition","8c45b52b":"Let's see;\nThe data consists of 5110 rows and 12 columns. The first column is 'ID' column, we might want to remove this. Then, the other columns seems to be categorical variables except Age, glucose level and BMI. We will get the info of the data, just in case if there is any unsuitable types.\n\nAfter that, we should investigate missing values if there is any; and see the proportion of the missings.","cb0a2934":"### ROC Curves and AUC\n\nKeeping on model evaluation.","cbc77b4a":"It is obvious that I was wrong about the age issue; the ages below 0 is months converted to years, and these values are children's data. So I would keep age data as it is\n\nTLDR: age columns is good, back to outlier removal.","a2e75d24":"Work Type : I don't know how things work in the country where the data gathered, But I assume private sector and running your own work is a stresfull thing to earn a position in competative environment, rather than government job. \n\nResidence : Seems where you live is not that important on having a stroke","dbe4d870":"basicly we imputed BMI values accoring to the neigborhoods; KNN imputer will get nearest 5 columns according to age, avg_glucose_levels and stroke; and returned their means for the missing rows.","1e148129":" ### Random Forest to go\n As we can see RFC has a accuracy score of 95%, and also precision-recall and f1-scores are 95%. By data oversampling, we handled Class Imbalance. \n \n Let's try Hyperparameter Tuning for the RFC algoritm","a885dfd2":"#### Creating new features: \nhas_diabetes and \nis_obese\n\nAccording to DSM VI glucose lvl above 126 accepted as diabetes mellitus.\n\nBMI > 30 is obese","ce0bf214":"only BMI column has missings and it's really a low proportion of the data. We will use KNN imputation to predict the missings , or easily we will fill them with means; but i want to learn KNN here, so I will use it :)","ff305812":"Appending completed.\n\nOne more thing to do. It would be better to handle earlier, but anyway. Let's investigate the normal distribution of the numeric data.","a6d4321c":"## Dealing with Class Imbalance with SMOTE"}}