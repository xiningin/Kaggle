{"cell_type":{"ae52e84f":"code","d87164d4":"code","93356521":"code","b5adb535":"code","ea5d5e2a":"code","fef5e220":"code","4341c75a":"code","21ada2a3":"code","9f0df20e":"code","67a58cb1":"code","f0c095e8":"code","e30ba5a0":"code","6aae0740":"code","e0659768":"code","a5e75c2e":"code","2d006893":"code","2b6d6daf":"code","b3d7d88c":"code","6b77199e":"code","8dd84f47":"code","964c26a4":"code","96ff0ea7":"code","d82af486":"code","dfebe814":"code","c0be7e5c":"code","41711715":"code","6efc5365":"code","d7ad0300":"code","7b193f69":"code","c35ea141":"code","e4f3bfaf":"code","2352b5e9":"code","6da9ee60":"code","48918a28":"code","c869e208":"code","77d3d0c1":"code","01b7784b":"code","a9f7869e":"code","745273b3":"code","53f73de4":"code","b85e6fba":"code","1fd8bd2f":"code","2d24da6b":"code","eca54449":"code","c32b60fe":"code","48f5a6aa":"code","301211a7":"code","20bf693c":"code","36562430":"code","c1740ec6":"code","5a72dd56":"code","da33bc1a":"code","1c90d6f8":"code","d819059f":"code","1cf4fcb9":"code","a4c86aad":"code","449d1c4e":"code","929a2f6e":"code","086f5a77":"code","35c2f9f1":"code","afd77876":"code","02c8da2c":"code","dedd7d0a":"code","8e947d4b":"code","2f28d0d3":"code","c94026d3":"code","05d7b64c":"code","c97a0e9d":"code","b10333e5":"code","849e95ff":"code","5397053d":"code","df5f00fa":"code","d69dccd7":"code","7dfde7e3":"code","4f5ba6dc":"code","e386c62b":"code","034f137b":"code","9acae9d2":"code","b9571b86":"code","0d93acc6":"code","ffed774e":"code","145fc988":"code","b5f814b4":"code","3536a601":"code","4654002a":"code","945e02a2":"code","d1c74119":"code","7df83c36":"code","a66725c6":"code","5f6da197":"code","4f577215":"code","5207458c":"code","252700de":"code","42f7e7a6":"code","9632a321":"code","f712eab0":"code","700c087f":"code","5ce21b85":"code","188a119d":"code","6d780b5e":"code","cdc9c629":"code","86dc3899":"code","b7804f3f":"code","24e0e4d5":"code","c1c0539e":"code","533f1a40":"code","a61bdc82":"code","a0907b94":"code","bce14b3a":"code","26d4d40a":"code","e3ae82ab":"code","6235318b":"code","333b29e1":"code","812ed72d":"code","4f76fc96":"code","f37e05ba":"code","5649cd74":"code","6c46a09e":"code","d429a396":"code","90494081":"code","b6ac7a0c":"code","ce8573bb":"code","9a0f45dc":"markdown","a3b24dc3":"markdown","38b1ecb4":"markdown","390939c2":"markdown","351d2b8d":"markdown","06235bcf":"markdown","d1408307":"markdown","4fe4f7c6":"markdown","c262fe3d":"markdown","460ce273":"markdown","e88a99dc":"markdown","1df85e7b":"markdown","7ee722eb":"markdown"},"source":{"ae52e84f":"import gc\nimport glob\nimport os\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pprint\nimport warnings\nfrom skimage import feature\nimport itertools\nfrom keras.layers import *\nimport lightgbm as lgb\nfrom sklearn import preprocessing\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm, tqdm_notebook\nimport cv2\nimport os\nfrom keras.models import Sequential, Model, load_model\nfrom keras.layers import Input, Reshape, concatenate, Dense, GlobalAveragePooling1D, BatchNormalization, Flatten, \\\n    Dropout, PReLU, LeakyReLU, Embedding\nfrom keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, GRU, LSTM, Bidirectional\nfrom keras.utils import np_utils\nfrom keras.callbacks import EarlyStopping\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Layer, Flatten\nfrom keras import initializers, regularizers, constraints\n# from keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import Callback, ModelCheckpoint, LearningRateScheduler\nfrom keras.optimizers import Adam, RMSprop, Nadam\nimport keras.backend as K\nimport tensorflow as tf\nfrom collections import defaultdict\nfrom keras.preprocessing import text, sequence\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.layers import Input, Dense, \\\n    Embedding, SpatialDropout1D, concatenate, CuDNNGRU, CuDNNLSTM, Dropout, TimeDistributed, Activation\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom nltk.stem.porter import PorterStemmer\n\n%matplotlib inline\n\nnp.random.seed(seed=1337)\nwarnings.filterwarnings('ignore')\n\nsplit_char = '\/'","d87164d4":"train = pd.read_csv('..\/input\/petfinder-adoption-prediction\/train\/train.csv')\ntest = pd.read_csv('..\/input\/petfinder-adoption-prediction\/test\/test.csv')\nsample_submission = pd.read_csv('..\/input\/petfinder-adoption-prediction\/test\/sample_submission.csv')\nlen_train = len(train)","93356521":"HD_cate = [\n    'Type', \n    'Breed1', 'Breed2', 'Color1', 'Color2', 'Color3', 'FurLength',\n    'Vaccinated', 'Dewormed', 'Sterilized', 'Health', 'Quantity', 'State', \n    'main_breed_Type', 'main_breed_BreedName', 'second_breed_Type', 'second_breed_BreedName', 'breed_id_cross'\n    ,'breed_type_cross'\n]\n\n\nkeep_cate = [\n    'Type', \n    'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2', 'Color3', \n     'FurLength', 'Vaccinated', 'Dewormed', 'Sterilized', 'Health', 'Quantity', 'State', \n    'main_breed_Type', 'main_breed_BreedName', 'second_breed_Type', 'second_breed_BreedName', 'breed_id_cross'\n    ,'breed_type_cross'\n    ]\nkeep_float = [\n    'Age',\n    'MaturitySize',\n    'Fee',\n    'VideoAmt', 'PhotoAmt','sentiment_magnitude',\n    'sentiment_score', 'sentiment_magnitude_sum', 'sentiment_score_sum', 'sentiment_magnitude_mean', 'sentiment_score_mean', \n    'sentiment_magnitude_var', 'sentiment_score_var', 'metadata_annots_score_SUM', 'metadata_annots_score_MEAN', 'metadata_annots_score_VAR', \n    'metadata_color_score_SUM', 'metadata_color_score_MEAN', 'metadata_color_score_VAR', 'metadata_color_pixelfrac_SUM', 'metadata_color_pixelfrac_MEAN', \n    'metadata_color_pixelfrac_VAR', 'metadata_crop_conf_SUM', 'metadata_crop_conf_MEAN', 'metadata_crop_conf_VAR', 'metadata_crop_importance_SUM', \n    'metadata_crop_importance_MEAN', 'metadata_crop_importance_VAR',\n    'TFIDF_Description_0', 'TFIDF_Description_1', 'TFIDF_Description_2', 'TFIDF_Description_3', 'TFIDF_Description_4', \n    'TFIDF_Description_5', 'TFIDF_Description_6', 'TFIDF_Description_7', 'TFIDF_Description_8', 'TFIDF_Description_9', \n    'TFIDF_Description_10', 'TFIDF_Description_11', 'TFIDF_Description_12', 'TFIDF_Description_13', 'TFIDF_Description_14', \n    'TFIDF_Description_15', 'TFIDF_metadata_annots_top_desc_0', 'TFIDF_metadata_annots_top_desc_1', 'TFIDF_metadata_annots_top_desc_2', \n    'TFIDF_metadata_annots_top_desc_3', 'TFIDF_metadata_annots_top_desc_4', 'TFIDF_metadata_annots_top_desc_5', 'TFIDF_metadata_annots_top_desc_6', \n    'TFIDF_metadata_annots_top_desc_7', 'TFIDF_metadata_annots_top_desc_8', 'TFIDF_metadata_annots_top_desc_9', \n    'TFIDF_metadata_annots_top_desc_10', 'TFIDF_metadata_annots_top_desc_11', 'TFIDF_metadata_annots_top_desc_12', \n    'TFIDF_metadata_annots_top_desc_13', 'TFIDF_metadata_annots_top_desc_14', 'TFIDF_metadata_annots_top_desc_15', \n    'TFIDF_sentiment_entities_0', 'TFIDF_sentiment_entities_1', 'TFIDF_sentiment_entities_2', 'TFIDF_sentiment_entities_3',\n    'TFIDF_sentiment_entities_4', 'TFIDF_sentiment_entities_5', 'TFIDF_sentiment_entities_6', 'TFIDF_sentiment_entities_7', \n    'TFIDF_sentiment_entities_8', 'TFIDF_sentiment_entities_9', 'TFIDF_sentiment_entities_10', 'TFIDF_sentiment_entities_11', \n    'TFIDF_sentiment_entities_12', 'TFIDF_sentiment_entities_13', 'TFIDF_sentiment_entities_14', 'TFIDF_sentiment_entities_15', \n    'IMG_SVD_0', 'IMG_SVD_1', 'IMG_SVD_2', 'IMG_SVD_3', 'IMG_SVD_4', 'IMG_SVD_5', 'IMG_SVD_6', 'IMG_SVD_7', 'IMG_SVD_8', 'IMG_SVD_9', 'IMG_SVD_10', 'IMG_SVD_11', \n    'IMG_SVD_12', 'IMG_SVD_13', 'IMG_SVD_14', 'IMG_SVD_15', 'IMG_SVD_16', 'IMG_SVD_17', 'IMG_SVD_18', 'IMG_SVD_19', 'IMG_SVD_20', 'IMG_SVD_21', 'IMG_SVD_22', \n    'IMG_SVD_23', 'IMG_SVD_24', 'IMG_SVD_25', 'IMG_SVD_26', 'IMG_SVD_27', 'IMG_SVD_28', 'IMG_SVD_29', 'IMG_SVD_30', 'IMG_SVD_31',\n    'average_blue_sum', 'average_blue_mean', 'average_blue_var', 'average_green_sum', 'average_green_mean', 'average_green_var', \n    'average_red_sum', 'average_red_mean', 'average_red_var', 'img_average_sum', 'img_average_mean', 'img_average_var', \n    'blur_sum', 'blur_mean', 'blur_var', 'image_size_sum', 'image_size_mean', 'image_size_var', 'width_sum', 'width_mean', \n    'width_var', 'height_sum', 'height_mean', 'height_var',\n    'RescuerID_COUNT', 'Length_Description', 'Length_metadata_annots_top_desc', 'Lengths_sentiment_entities', \n    'Length_Description_2', 'Length_metadata_annots_top_desc_2', 'Lengths_sentiment_entities_2', \n    'resultion','ppi','txt_pred','img_pred','tablur_pred'\n    ]","b5adb535":"print('hello') ","ea5d5e2a":"# here is the agg helper\ndef agg_helper(X, base, other):\n    tmp2 = X.groupby(base).agg({other:['max','min','mean']})\n    tmp2.columns = pd.Index([base +'_' + e[0] + \"_\" + e[1].upper() for e in tmp2.columns.tolist()])\n    X = X.merge(tmp2, on = [base],how = 'left')\n    return X","fef5e220":"from keras.applications.densenet import preprocess_input, DenseNet121","4341c75a":"def resize_to_square(im):\n    old_size = im.shape[:2]\n    ratio = float(img_size)\/max(old_size)\n    new_size = tuple([int(x*ratio) for x in old_size])\n    im = cv2.resize(im, (new_size[1], new_size[0]))\n    delta_w = img_size - new_size[1]\n    delta_h = img_size - new_size[0]\n    top, bottom = delta_h\/\/2, delta_h-(delta_h\/\/2)\n    left, right = delta_w\/\/2, delta_w-(delta_w\/\/2)\n    color = [0, 0, 0]\n    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n    return new_im\n\ndef load_image(path, pet_id):\n    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n    new_image = resize_to_square(image)\n    new_image = preprocess_input(new_image)\n    return new_image","21ada2a3":"img_size = 256\nbatch_size = 256","9f0df20e":"from keras.models import Model\nfrom keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\nimport keras.backend as K\ninp = Input((256,256,3))\nbackbone = DenseNet121(input_tensor = inp, \n                       weights=\"..\/input\/densenet-keras\/DenseNet-BC-121-32-no-top.h5\",\n                       include_top = False)\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\nx = AveragePooling1D(4)(x)\nout = Lambda(lambda x: x[:,:,0])(x)\n\nm = Model(inp,out)","67a58cb1":"pet_ids = train['PetID'].values\nn_batches = len(pet_ids) \/\/ batch_size + 1\n\nfeatures = {}\nfor b in tqdm(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"..\/input\/petfinder-adoption-prediction\/train_images\/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]","f0c095e8":"train_feats = pd.DataFrame.from_dict(features, orient='index')\ntrain_feats.columns = [f'pic_{i}' for i in range(train_feats.shape[1])]","e30ba5a0":"pet_ids = test['PetID'].values\nn_batches = len(pet_ids) \/\/ batch_size + 1\n\nfeatures = {}\nfor b in tqdm(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"..\/input\/petfinder-adoption-prediction\/test_images\/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]","6aae0740":"del m\ngc.collect()","e0659768":"test_feats = pd.DataFrame.from_dict(features, orient='index')\ntest_feats.columns = [f'pic_{i}' for i in range(test_feats.shape[1])]","a5e75c2e":"train_feats = train_feats.reset_index()\ntrain_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)\n\ntest_feats = test_feats.reset_index()\ntest_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)","2d006893":"all_ids = pd.concat([train, test], axis=0, ignore_index=True, sort=False)[['PetID']]\nall_ids.shape","2b6d6daf":"all_ids.head(2)","b3d7d88c":"features_df = pd.concat([train_feats, test_feats], axis=0)","6b77199e":"n_components = 32\nsvd_ = TruncatedSVD(n_components=n_components, random_state=1337)\n\nfeatures = features_df[[f'pic_{i}' for i in range(256)]].values","8dd84f47":"svd_col = svd_.fit_transform(features)\nsvd_col = pd.DataFrame(svd_col)\nsvd_col = svd_col.add_prefix('IMG_SVD_')\nimg_features = pd.concat([all_ids, svd_col], axis=1)","964c26a4":"img_features.shape","96ff0ea7":"img_row = pd.concat([all_ids.reset_index(drop = True), features_df[[f'pic_{i}' for i in range(256)]].reset_index(drop = True)], axis=1)","d82af486":"img_row.head()","dfebe814":"labels_breed = pd.read_csv('..\/input\/petfinder-adoption-prediction\/breed_labels.csv')\nlabels_state = pd.read_csv('..\/input\/petfinder-adoption-prediction\/color_labels.csv')\nlabels_color = pd.read_csv('..\/input\/petfinder-adoption-prediction\/state_labels.csv')","c0be7e5c":"labels_breed.head()","41711715":"labels_state.head()","6efc5365":"labels_color.head()","d7ad0300":"train_image_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/train_images\/*.jpg'))\ntrain_metadata_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/train_metadata\/*.json'))\ntrain_sentiment_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/train_sentiment\/*.json'))\n\nprint(f'num of train images files: {len(train_image_files)}')\nprint(f'num of train metadata files: {len(train_metadata_files)}')\nprint(f'num of train sentiment files: {len(train_sentiment_files)}')\n\n\ntest_image_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/test_images\/*.jpg'))\ntest_metadata_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/test_metadata\/*.json'))\ntest_sentiment_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/test_sentiment\/*.json'))\n\nprint(f'num of test images files: {len(test_image_files)}')\nprint(f'num of test metadata files: {len(test_metadata_files)}')\nprint(f'num of test sentiment files: {len(test_sentiment_files)}')","7b193f69":"# Images:\ntrain_df_ids = train[['PetID']]\nprint(train_df_ids.shape)\n\n# Metadata:\ntrain_df_ids = train[['PetID']]\ntrain_df_metadata = pd.DataFrame(train_metadata_files)\ntrain_df_metadata.columns = ['metadata_filename']\ntrain_metadata_pets = train_df_metadata['metadata_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\ntrain_df_metadata = train_df_metadata.assign(PetID=train_metadata_pets)\nprint(len(train_metadata_pets.unique()))\n\npets_with_metadatas = len(np.intersect1d(train_metadata_pets.unique(), train_df_ids['PetID'].unique()))\nprint(f'fraction of pets with metadata: {pets_with_metadatas \/ train_df_ids.shape[0]:.3f}')\n\n# Sentiment:\ntrain_df_ids = train[['PetID']]\ntrain_df_sentiment = pd.DataFrame(train_sentiment_files)\ntrain_df_sentiment.columns = ['sentiment_filename']\ntrain_sentiment_pets = train_df_sentiment['sentiment_filename'].apply(lambda x: x.split(split_char)[-1].split('.')[0])\ntrain_df_sentiment = train_df_sentiment.assign(PetID=train_sentiment_pets)\nprint(len(train_sentiment_pets.unique()))\n\npets_with_sentiments = len(np.intersect1d(train_sentiment_pets.unique(), train_df_ids['PetID'].unique()))\nprint(f'fraction of pets with sentiment: {pets_with_sentiments \/ train_df_ids.shape[0]:.3f}')","c35ea141":"# Images:\ntest_df_ids = test[['PetID']]\nprint(test_df_ids.shape)\n\n# Metadata:\ntest_df_metadata = pd.DataFrame(test_metadata_files)\ntest_df_metadata.columns = ['metadata_filename']\ntest_metadata_pets = test_df_metadata['metadata_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\ntest_df_metadata = test_df_metadata.assign(PetID=test_metadata_pets)\nprint(len(test_metadata_pets.unique()))\n\npets_with_metadatas = len(np.intersect1d(test_metadata_pets.unique(), test_df_ids['PetID'].unique()))\nprint(f'fraction of pets with metadata: {pets_with_metadatas \/ test_df_ids.shape[0]:.3f}')\n\n# Sentiment:\ntest_df_sentiment = pd.DataFrame(test_sentiment_files)\ntest_df_sentiment.columns = ['sentiment_filename']\ntest_sentiment_pets = test_df_sentiment['sentiment_filename'].apply(lambda x: x.split(split_char)[-1].split('.')[0])\ntest_df_sentiment = test_df_sentiment.assign(PetID=test_sentiment_pets)\nprint(len(test_sentiment_pets.unique()))\n\npets_with_sentiments = len(np.intersect1d(test_sentiment_pets.unique(), test_df_ids['PetID'].unique()))\nprint(f'fraction of pets with sentiment: {pets_with_sentiments \/ test_df_ids.shape[0]:.3f}')","e4f3bfaf":"class PetFinderParser(object):\n    \n    def __init__(self, debug=False):\n        \n        self.debug = debug\n        self.sentence_sep = ' '\n        \n        self.extract_sentiment_text = False\n    \n    def open_json_file(self, filename):\n        with open(filename, 'r', encoding='utf-8') as f:\n            json_file = json.load(f)\n        return json_file\n        \n    def parse_sentiment_file(self, file):\n        \"\"\"\n        Parse sentiment file. Output DF with sentiment features.\n        \"\"\"\n        \n        file_sentiment = file['documentSentiment']\n        file_entities = [x['name'] for x in file['entities']]\n        file_entities = self.sentence_sep.join(file_entities)\n        \n        file_sentences_sentiment = [x['sentiment'] for x in file['sentences']]\n        \n        file_sentences_sentiment = pd.DataFrame.from_dict(\n            file_sentences_sentiment, orient='columns')\n        file_sentences_sentiment_df = pd.DataFrame(\n            {\n                'magnitude_sum': file_sentences_sentiment['magnitude'].sum(axis=0),\n                'score_sum': file_sentences_sentiment['score'].sum(axis=0),\n                'magnitude_max': file_sentences_sentiment['magnitude'].max(axis=0),\n                'score_max': file_sentences_sentiment['score'].max(axis=0),\n                'magnitude_min': file_sentences_sentiment['magnitude'].min(axis=0),\n                'score_min': file_sentences_sentiment['score'].min(axis=0),\n                'magnitude_mean': file_sentences_sentiment['magnitude'].mean(axis=0),\n                'score_mean': file_sentences_sentiment['score'].mean(axis=0),\n                'magnitude_var': file_sentences_sentiment['magnitude'].var(axis=0),\n                'score_var': file_sentences_sentiment['score'].var(axis=0),\n            }, index=[0]\n        )\n        \n        df_sentiment = pd.DataFrame.from_dict(file_sentiment, orient='index').T\n        df_sentiment = pd.concat([df_sentiment, file_sentences_sentiment_df], axis=1)\n            \n        df_sentiment['entities'] = file_entities\n        df_sentiment = df_sentiment.add_prefix('sentiment_')\n        \n        return df_sentiment\n    \n    def parse_metadata_file(self, file):\n        \"\"\"\n        Parse metadata file. Output DF with metadata features.\n        \"\"\"\n        \n        file_keys = list(file.keys())\n        \n        if 'labelAnnotations' in file_keys:\n            file_annots = file['labelAnnotations']\n            file_top_score = np.asarray([x['score'] for x in file_annots]).mean()\n            file_top_desc = [x['description'] for x in file_annots]\n        else:\n            file_top_score = np.nan\n            file_top_desc = ['']\n        \n        file_colors = file['imagePropertiesAnnotation']['dominantColors']['colors']\n        file_crops = file['cropHintsAnnotation']['cropHints']\n\n        file_color_score = np.asarray([x['score'] for x in file_colors]).mean()\n        file_color_pixelfrac = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n\n        file_crop_conf = np.asarray([x['confidence'] for x in file_crops]).mean()\n        \n        if 'importanceFraction' in file_crops[0].keys():\n            file_crop_importance = np.asarray([x['importanceFraction'] for x in file_crops]).mean()\n        else:\n            file_crop_importance = np.nan\n\n        df_metadata = {\n            'annots_score': file_top_score,\n            'color_score': file_color_score,\n            'color_pixelfrac': file_color_pixelfrac,\n            'crop_conf': file_crop_conf,\n            'crop_importance': file_crop_importance,\n            'annots_top_desc': self.sentence_sep.join(file_top_desc)\n        }\n        \n        df_metadata = pd.DataFrame.from_dict(df_metadata, orient='index').T\n        df_metadata = df_metadata.add_prefix('metadata_')\n        \n        return df_metadata\n    \n\ndef extract_additional_features(pet_id, mode='train'):\n    \n    sentiment_filename = f'..\/input\/petfinder-adoption-prediction\/{mode}_sentiment\/{pet_id}.json'\n    try:\n        sentiment_file = pet_parser.open_json_file(sentiment_filename)\n        df_sentiment = pet_parser.parse_sentiment_file(sentiment_file)\n        df_sentiment['PetID'] = pet_id\n    except FileNotFoundError:\n        df_sentiment = []\n\n    dfs_metadata = []\n    for ind in range(1,200):\n        metadata_filename = '..\/input\/petfinder-adoption-prediction\/{}_metadata\/{}-{}.json'.format(mode, pet_id, ind)\n        try:\n            metadata_file = pet_parser.open_json_file(metadata_filename)\n            df_metadata = pet_parser.parse_metadata_file(metadata_file)\n            df_metadata['PetID'] = pet_id\n            dfs_metadata.append(df_metadata)\n        except FileNotFoundError:\n            break\n    if dfs_metadata:\n        dfs_metadata = pd.concat(dfs_metadata, ignore_index=True, sort=False)\n    dfs = [df_sentiment, dfs_metadata]\n    return dfs\n\npet_parser = PetFinderParser()","2352b5e9":"debug = False\ntrain_pet_ids = train.PetID.unique()\ntest_pet_ids = test.PetID.unique()\n\nif debug:\n    train_pet_ids = train_pet_ids[:1000]\n    test_pet_ids = test_pet_ids[:500]\n\n\ndfs_train = Parallel(n_jobs=-1, verbose=1)(\n    delayed(extract_additional_features)(i, mode='train') for i in train_pet_ids)\n\ntrain_dfs_sentiment = [x[0] for x in dfs_train if isinstance(x[0], pd.DataFrame)]\ntrain_dfs_metadata = [x[1] for x in dfs_train if isinstance(x[1], pd.DataFrame)]\n\ntrain_dfs_sentiment = pd.concat(train_dfs_sentiment, ignore_index=True, sort=False)\ntrain_dfs_metadata = pd.concat(train_dfs_metadata, ignore_index=True, sort=False)\n\nprint(train_dfs_sentiment.shape, train_dfs_metadata.shape)\n\n\ndfs_test = Parallel(n_jobs=-1, verbose=1)(\n    delayed(extract_additional_features)(i, mode='test') for i in test_pet_ids)\n\ntest_dfs_sentiment = [x[0] for x in dfs_test if isinstance(x[0], pd.DataFrame)]\ntest_dfs_metadata = [x[1] for x in dfs_test if isinstance(x[1], pd.DataFrame)]\n\ntest_dfs_sentiment = pd.concat(test_dfs_sentiment, ignore_index=True, sort=False)\ntest_dfs_metadata = pd.concat(test_dfs_metadata, ignore_index=True, sort=False)\n\nprint(test_dfs_sentiment.shape, test_dfs_metadata.shape)","6da9ee60":"test_dfs_sentiment.head()","48918a28":"train_dfs_metadata.head()","c869e208":"aggregates = ['sum', 'mean', 'var', 'min', 'max']\nsent_agg = ['sum'] #['sum', 'mean', 'var', 'min', 'max']#['sum']\n\n\n# Train\ntrain_metadata_desc = train_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntrain_metadata_desc = train_metadata_desc.reset_index()\ntrain_metadata_desc[\n    'metadata_annots_top_desc'] = train_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\nprefix = 'metadata'\ntrain_metadata_gr = train_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in train_metadata_gr.columns:\n    if 'PetID' not in i:\n        train_metadata_gr[i] = train_metadata_gr[i].astype(float)\ntrain_metadata_gr = train_metadata_gr.groupby(['PetID']).agg(aggregates)\ntrain_metadata_gr.columns = pd.Index([f'{c[0]}_{c[1].upper()}' for c in train_metadata_gr.columns.tolist()])\ntrain_metadata_gr = train_metadata_gr.reset_index()\n\n\ntrain_sentiment_desc = train_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntrain_sentiment_desc = train_sentiment_desc.reset_index()\ntrain_sentiment_desc[\n    'sentiment_entities'] = train_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\nprefix = 'sentiment'\ntrain_sentiment_gr = train_dfs_sentiment.drop(['sentiment_entities'], axis=1)\nfor i in train_sentiment_gr.columns:\n    if 'PetID' not in i:\n        train_sentiment_gr[i] = train_sentiment_gr[i].astype(float)\ntrain_sentiment_gr = train_sentiment_gr.groupby(['PetID']).agg(sent_agg)\ntrain_sentiment_gr.columns = pd.Index([f'{c[0]}' for c in train_sentiment_gr.columns.tolist()])\ntrain_sentiment_gr = train_sentiment_gr.reset_index()\n\n\n# Test\ntest_metadata_desc = test_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntest_metadata_desc = test_metadata_desc.reset_index()\ntest_metadata_desc[\n    'metadata_annots_top_desc'] = test_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\nprefix = 'metadata'\ntest_metadata_gr = test_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in test_metadata_gr.columns:\n    if 'PetID' not in i:\n        test_metadata_gr[i] = test_metadata_gr[i].astype(float)\ntest_metadata_gr = test_metadata_gr.groupby(['PetID']).agg(aggregates)\ntest_metadata_gr.columns = pd.Index([f'{c[0]}_{c[1].upper()}' for c in test_metadata_gr.columns.tolist()])\ntest_metadata_gr = test_metadata_gr.reset_index()\n\n\ntest_sentiment_desc = test_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntest_sentiment_desc = test_sentiment_desc.reset_index()\ntest_sentiment_desc[\n    'sentiment_entities'] = test_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\nprefix = 'sentiment'\ntest_sentiment_gr = test_dfs_sentiment.drop(['sentiment_entities'], axis=1)\nfor i in test_sentiment_gr.columns:\n    if 'PetID' not in i:\n        test_sentiment_gr[i] = test_sentiment_gr[i].astype(float)\ntest_sentiment_gr = test_sentiment_gr.groupby(['PetID']).agg(sent_agg)\ntest_sentiment_gr.columns = pd.Index([f'{c[0]}' for c in test_sentiment_gr.columns.tolist()])\ntest_sentiment_gr = test_sentiment_gr.reset_index()","77d3d0c1":"# Train merges:\ntrain_proc = train.copy()\ntrain_proc = train_proc.merge(\n    train_sentiment_gr, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_metadata_gr, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_metadata_desc, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_sentiment_desc, how='left', on='PetID')\n\n# Test merges:\ntest_proc = test.copy()\ntest_proc = test_proc.merge(\n    test_sentiment_gr, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_metadata_gr, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_metadata_desc, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_sentiment_desc, how='left', on='PetID')\n\nprint(train_proc.shape, test_proc.shape)\nassert train_proc.shape[0] == train.shape[0]\nassert test_proc.shape[0] == test.shape[0]","01b7784b":"train_proc.head(2)","a9f7869e":"train_breed_main = train_proc[['Breed1']].merge(\n    labels_breed, how='left',\n    left_on='Breed1', right_on='BreedID',\n    suffixes=('', '_main_breed'))\n\ntrain_breed_main = train_breed_main.iloc[:, 2:]\ntrain_breed_main = train_breed_main.add_prefix('main_breed_')\n\ntrain_breed_second = train_proc[['Breed2']].merge(\n    labels_breed, how='left',\n    left_on='Breed2', right_on='BreedID',\n    suffixes=('', '_second_breed'))\n\ntrain_breed_second = train_breed_second.iloc[:, 2:]\ntrain_breed_second = train_breed_second.add_prefix('second_breed_')\n\n\ntrain_proc = pd.concat(\n    [train_proc, train_breed_main, train_breed_second], axis=1)\n\n\ntest_breed_main = test_proc[['Breed1']].merge(\n    labels_breed, how='left',\n    left_on='Breed1', right_on='BreedID',\n    suffixes=('', '_main_breed'))\n\ntest_breed_main = test_breed_main.iloc[:, 2:]\ntest_breed_main = test_breed_main.add_prefix('main_breed_')\n\ntest_breed_second = test_proc[['Breed2']].merge(\n    labels_breed, how='left',\n    left_on='Breed2', right_on='BreedID',\n    suffixes=('', '_second_breed'))\n\ntest_breed_second = test_breed_second.iloc[:, 2:]\ntest_breed_second = test_breed_second.add_prefix('second_breed_')\n\n\ntest_proc = pd.concat(\n    [test_proc, test_breed_main, test_breed_second], axis=1)\n\nprint(train_proc.shape, test_proc.shape)","745273b3":"train_proc.head()","53f73de4":"X = pd.concat([train_proc, test_proc], ignore_index=True, sort=False)","b85e6fba":"print(X.columns)","1fd8bd2f":"X['breed_type_cross'] = X['main_breed_BreedName'].fillna('none') +'_' +  X['second_breed_BreedName'].fillna('none')\nX['breed_id_cross'] = X['main_breed_Type'].fillna('none').astype(str) +'_' +  X['second_breed_Type'].fillna('none').astype(str)","2d24da6b":"X.head()","eca54449":"X_temp = X.copy()\n\ntext_columns = ['Description', 'metadata_annots_top_desc', 'sentiment_entities']\ncategorical_columns = ['main_breed_BreedName', 'second_breed_BreedName','breed_id_cross','breed_type_cross']\n\nto_drop_columns = ['PetID', 'Name', 'RescuerID']","c32b60fe":"#Name feature\ndef no_name(x):\n    if x == 'n' or 'No_name' in x:\n        return 1\n    return 0\nX_temp['Name'] = X_temp['Name'].fillna('n')\nX_temp['Name'] =  X_temp['Name'].apply(lambda x:no_name(str(x)))\nX_temp['Name_length'] = X_temp['Name'].apply(lambda x:len(str(x)))\nX_temp['Name_length_split'] = X_temp['Name'].apply(lambda x:len(str(x).split()))\ndef have_num(x):\n    if any(char.isdigit() for char in x):\n        return 1\n    else:\n        return 0\nX_temp['Name_length_if_num'] = X_temp['Name'].apply(lambda x:have_num(str(x)))\ndef kitty(x):\n    if 'kitty' in x.lower():\n        return 1\n    else:\n        return 0\nX_temp['Name_if_kitty'] = X_temp['Name'].apply(lambda x:kitty(str(x)))\ndef puppy(x):\n    if 'puppy' in x.lower():\n        return 1\n    else:\n        return 0\nX_temp['Name_if_puppy'] = X_temp['Name'].apply(lambda x:puppy(str(x)))\n","48f5a6aa":"X_temp[[ 'Color1', 'Color2',\n       'Color3','breed_type_cross','breed_id_cross','main_breed_Type','second_breed_Type']].head(3)","301211a7":"rescuer_count = X.groupby(['RescuerID'])['PetID'].count().reset_index()\nrescuer_count.columns = ['RescuerID', 'RescuerID_COUNT']\n\nX_temp = X_temp.merge(rescuer_count, how='left', on='RescuerID')","20bf693c":"for i in categorical_columns:\n    X_temp.loc[:, i] = pd.factorize(X_temp.loc[:, i])[0]","36562430":"X_text = X_temp[text_columns]\n\nfor i in X_text.columns:\n    X_text.loc[:, i] = X_text.loc[:, i].fillna('none')","c1740ec6":"X_temp['Length_Description'] = X_text['Description'].map(len)\nX_temp['Length_metadata_annots_top_desc'] = X_text['metadata_annots_top_desc'].map(len)\nX_temp['Lengths_sentiment_entities'] = X_text['sentiment_entities'].map(len)\nX_TXT = X_text[['Description', 'metadata_annots_top_desc', 'sentiment_entities']].copy()","5a72dd56":"X_temp['Length_Description_2'] = X_text['Description'].apply(lambda x: len(x.split()))\nX_temp['Length_metadata_annots_top_desc_2'] = X_text['metadata_annots_top_desc'].apply(lambda x: len(x.split()))\nX_temp['Lengths_sentiment_entities_2'] = X_text['sentiment_entities'].apply(lambda x: len(x.split()))\n","da33bc1a":"X_temp['Length_metadata_annots_top_desc'].mean()","1c90d6f8":"X_temp['Lengths_sentiment_entities'].mean()","d819059f":"print(\"hello word\")","1cf4fcb9":"print(X_temp.columns)","a4c86aad":"import string\nimport re\nfrom keras.preprocessing.text import Tokenizer\n\ndef spacing_punctuation(text):\n    \"\"\"\n    add space before and after punctuation and symbols\n    \"\"\"\n    regular_punct = list(string.punctuation)\n    extra_punct = [\n        ',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&',\n        '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3',\n        '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',\n        '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', '\u201c', '\u2605', '\u201d',\n        '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be',\n        '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc',\n        '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2',\n        '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb',\n        '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8',\n        '\u00b9', '\u2264', '\u2021', '\u221a', '\u00ab', '\u00bb', '\u00b4', '\u00ba', '\u00be', '\u00a1', '\u00a7', '\u00a3', '\u20a4']\n    all_punct = ''.join(sorted(list(set(regular_punct + extra_punct))))\n    re_tok = re.compile(f'([{all_punct}])')\n    return re_tok.sub(r' \\1 ', text)\n\ndef tokenize(df_text, max_features):\n    # preprocess\n    df_text = df_text.apply(spacing_punctuation)\n    # tokenizer\n    tokenizer = Tokenizer(\n        num_words=max_features,\n        filters='',\n        lower=False,\n        split=' ')\n    # fit to data\n    tokenizer.fit_on_texts(list(df_text))\n    return tokenizer","449d1c4e":"df_text = pd.concat([X_TXT['Description'], X_TXT['metadata_annots_top_desc'],X_TXT['sentiment_entities']],axis=0).reset_index(drop=True)\ntokenizer = tokenize(df_text, max_features=160000)","929a2f6e":"import xgboost as xgb\n\nxgb_params = {\n    'eval_metric': 'rmse',\n    'seed': 1337,\n    'eta': 0.005,\n    'subsample': 0.8,\n    'colsample_bytree': 0.85,\n    'tree_method': 'gpu_hist',\n    'device': 'gpu',\n    'silent': 1,\n}\n\ndef run_xgb(params, X_train, X_test):\n    n_splits = 10\n    verbose_eval = 1000\n    num_rounds = 60000\n    early_stop = 500\n\n    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1337)\n\n    oof_train = np.zeros((X_train.shape[0]))\n    oof_test = np.zeros((X_test.shape[0], n_splits))\n\n    i = 0\n\n    for train_idx, valid_idx in kf.split(X_train, X_train['AdoptionSpeed'].values):\n\n        X_tr = X_train.iloc[train_idx, :]\n        X_val = X_train.iloc[valid_idx, :]\n\n        y_tr = X_tr['AdoptionSpeed'].values\n        X_tr = X_tr.drop(['AdoptionSpeed'], axis=1)\n\n        y_val = X_val['AdoptionSpeed'].values\n        X_val = X_val.drop(['AdoptionSpeed'], axis=1)\n\n        d_train = xgb.DMatrix(data=X_tr, label=y_tr, feature_names=X_tr.columns)\n        d_valid = xgb.DMatrix(data=X_val, label=y_val, feature_names=X_val.columns)\n\n        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n        model = xgb.train(dtrain=d_train, num_boost_round=num_rounds, evals=watchlist,\n                         early_stopping_rounds=early_stop, verbose_eval=verbose_eval, params=params)\n\n        valid_pred = model.predict(xgb.DMatrix(X_val, feature_names=X_val.columns), ntree_limit=model.best_ntree_limit)\n        test_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_test.columns), ntree_limit=model.best_ntree_limit)\n\n        oof_train[valid_idx] = valid_pred\n        oof_test[:, i] = test_pred\n\n        i += 1\n    return oof_train, oof_test","086f5a77":"tabular_temp = X_temp[[i for i in X_temp.columns if i not in to_drop_columns +['Description','metadata_annots_top_desc', 'sentiment_entities'] ] ] \nfor i in HD_cate:\n    if i in tabular_temp.columns:\n        tabular_temp[i] = tabular_temp.groupby(i)[i].transform('count') \nX_tabular_temp = tabular_temp[len_train:].drop(['AdoptionSpeed'], axis = 1)","35c2f9f1":"tablur_train, tablur_test = run_xgb(xgb_params,tabular_temp[:len_train], X_tabular_temp)\nX_temp['tablur_pred'] = np.concatenate([tablur_train,tablur_test.mean(axis = 1)])","afd77876":"del tabular_temp, X_tabular_temp\ngc.collect()","02c8da2c":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Ridge\nfrom scipy.sparse import hstack\n\nprint('Tfidf word vector')\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 3),\n    max_features=20000)\nword_vectorizer.fit(df_text)\ntrain_desc_word_features = word_vectorizer.transform(X_TXT[:len_train]['Description'])\ntest_desc_word_features = word_vectorizer.transform(X_TXT[len_train:]['Description'])\ntrain_meta_word_features = word_vectorizer.transform(X_TXT[:len_train]['metadata_annots_top_desc'])\ntest_meta_word_features = word_vectorizer.transform(X_TXT[len_train:]['metadata_annots_top_desc'])\ntrain_sent_word_features = word_vectorizer.transform(X_TXT[:len_train]['sentiment_entities'])\ntest_sent_word_features = word_vectorizer.transform(X_TXT[len_train:]['sentiment_entities'])\n\nprint('Tfidf char vector')\nchar_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='char',\n    stop_words='english',\n    ngram_range=(2, 5),\n    max_features=50000)\nchar_vectorizer.fit(df_text)\ntrain_desc_char_features = char_vectorizer.transform(X_TXT[:len_train]['Description'])\ntest_desc_char_features = char_vectorizer.transform(X_TXT[len_train:]['Description'])\ntrain_meta_char_features = char_vectorizer.transform(X_TXT[:len_train]['metadata_annots_top_desc'])\ntest_meta_char_features = char_vectorizer.transform(X_TXT[len_train:]['metadata_annots_top_desc'])\ntrain_sent_char_features = char_vectorizer.transform(X_TXT[:len_train]['sentiment_entities'])\ntest_sent_char_features = char_vectorizer.transform(X_TXT[len_train:]['sentiment_entities'])\n\ntrain_features = hstack([\n    train_desc_word_features, train_desc_char_features, \n    train_meta_word_features, train_meta_char_features, \n    train_sent_word_features, train_sent_char_features, ]).tocsr()\ntest_features = hstack([\n    test_desc_word_features, test_desc_char_features, \n    test_meta_word_features, test_meta_char_features, \n    test_sent_word_features, test_sent_char_features, ]).tocsr()\n\ndef run_txt(X_train, X_test):\n    n_splits = 10\n    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1337)\n    oof_train = np.zeros((X_train.shape[0]))\n    oof_test = np.zeros((X_test.shape[0], n_splits))\n    i = 0\n    for train_idx, valid_idx in kf.split(train_features, X_train['AdoptionSpeed'].values):\n        X_tr,X_val= train_features[train_idx], train_features[valid_idx]\n        y_tr = X_train.iloc[train_idx, :]['AdoptionSpeed'].values\n        y_val = X_train.iloc[valid_idx, :]['AdoptionSpeed'].values\n        model = Ridge(alpha=20, copy_X=True, fit_intercept=True, solver='auto',max_iter=100,normalize=False, random_state=0,  tol=0.0025)\n        model.fit(X_tr,y_tr)\n        valid_pred = model.predict(X_val)#, feature_names=X_val.columns), ntree_limit=model.best_ntree_limit)\n        test_pred = model.predict(test_features)#(xgb.DMatrix(X_test, feature_names=X_test.columns), ntree_limit=model.best_ntree_limit)\n        oof_train[valid_idx] = valid_pred\n        oof_test[:, i] = test_pred\n        i += 1\n    oof_test = oof_test.mean(1)\n    return oof_train, oof_test","dedd7d0a":"oof_train_txt, oof_test_txt = run_txt(X_temp[:len_train][['AdoptionSpeed']], X_temp[len_train:][['AdoptionSpeed']])\nX_temp['txt_pred'] = np.concatenate([oof_train_txt,oof_test_txt])","8e947d4b":"X_temp['txt_pred']","2f28d0d3":"del train_features, test_features, train_desc_word_features,test_desc_char_features,train_meta_char_features,test_meta_char_features\ndel train_sent_char_features,test_desc_word_features, test_meta_word_features, test_sent_word_features, test_sent_char_features,\ngc.collect()","c94026d3":"def load_word_embedding(filepath):\n    \"\"\"\n    given a filepath to embeddings file, return a word to vec\n    dictionary, in other words, word_embedding\n\n    E.g. {'word': array([0.1, 0.2, ...])}\n    \"\"\"\n    def _get_vec(word, *arr):\n        return word, np.asarray(arr, dtype='float32')\n\n    print('load word embedding ......')\n    try:\n        word_embedding = dict(_get_vec(*w.split(' ')) for w in open(filepath))\n    except UnicodeDecodeError:\n        word_embedding = dict(_get_vec(*w.split(' ')) for w in open(\n            filepath, encoding=\"utf8\", errors='ignore'))\n    # sanity check word vector length\n    words_to_del = []\n    for word, vec in word_embedding.items():\n        if len(vec) != 300:\n            words_to_del.append(word)\n    for word in words_to_del:\n        del word_embedding[word]\n    return word_embedding","05d7b64c":"from nltk.stem.porter import PorterStemmer\n\ndef create_embedding_weights(word_index, word_embedding,\n                             max_features, paragram=False):\n    print('create word embedding weights ......')\n    count = 0\n    stemmer = PorterStemmer()\n    # get entire embedding matrix\n    mat_embedding = np.stack(word_embedding.values())\n    # get shape\n    a, b = min(max_features, len(word_index)) + 1, mat_embedding.shape[1]\n    print('embedding weights matrix with shape: ({}, {})'.format(a, b))\n    # init embedding weight matrix\n    embedding_mean, embedding_std = mat_embedding.mean(), mat_embedding.std()\n    del mat_embedding\n    gc.collect()\n    embedding_weights = np.random.normal(embedding_mean, embedding_std, (a, b))\n    # mapping\n    for word, idx in word_index.items():\n        if idx >= a:\n            continue\n        word_vec = word_embedding.get(word, None)\n        if word_vec is None:\n            word_vec = word_embedding.get(word.lower(), None)\n            if word_vec is None:\n                 word_vec = word_embedding.get(stemmer.stem(word), None)\n            if word_vec is not None:\n                count += 1\n        if word_vec is not None:\n            embedding_weights[idx] = word_vec\n    print(\"{} number has been replaced\".format(count))\n    return embedding_weights","c97a0e9d":"GLOVE_PATH = '..\/input\/glove840b300dtxt\/glove.840B.300d.txt'\nglove_word_embed = load_word_embedding(GLOVE_PATH)","b10333e5":"embedding_matrix = create_embedding_weights(tokenizer.word_index, glove_word_embed, 160000, False)  # noqa","849e95ff":"del glove_word_embed\ngc.collect()","5397053d":"n_components = 16\ntext_features = []\n\n# Generate text features:\nfor i in X_text.columns:\n    \n    # Initialize decomposition methods:\n    print(f'generating features from: {i}')\n    tfv = TfidfVectorizer(min_df=2,  max_features=None,\n                          strip_accents='unicode', analyzer='word', token_pattern=r'(?u)\\b\\w+\\b',\n                          ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1)\n    svd_ = TruncatedSVD(\n        n_components=n_components, random_state=1337)\n    \n    tfidf_col = tfv.fit_transform(X_text.loc[:, i].values)\n    \n    svd_col = svd_.fit_transform(tfidf_col)\n    svd_col = pd.DataFrame(svd_col)\n    svd_col = svd_col.add_prefix('TFIDF_{}_'.format(i))\n    text_features.append(svd_col)\n    \n    tfv = TfidfVectorizer(min_df=2,  max_features=None,\n                          strip_accents='unicode', analyzer='char', \n                          ngram_range=(2, 5), use_idf=1, smooth_idf=1, sublinear_tf=1)\n    svd_ = TruncatedSVD(\n        n_components=n_components, random_state=1337)\n    \n    tfidf_col = tfv.fit_transform(X_text.loc[:, i].values)\n    \n    svd_col = svd_.fit_transform(tfidf_col)\n    svd_col = pd.DataFrame(svd_col)\n    svd_col = svd_col.add_prefix('TFIDF_char_{}_'.format(i))\n    text_features.append(svd_col)\n    \ntext_features = pd.concat(text_features, axis=1)\n\nX_temp = pd.concat([X_temp, text_features], axis=1)\n\nfor i in X_text.columns:\n    X_temp = X_temp.drop(i, axis=1)","df5f00fa":"X_temp = X_temp.merge(img_features, how='left', on='PetID')","d69dccd7":"X_img_row_temp = X_temp[['PetID','AdoptionSpeed']].merge(img_row, how='left', on='PetID')","7dfde7e3":"# run image\nX_img_row_temp = X_img_row_temp.drop(['PetID'], axis = 1)\nX_text_image = X_img_row_temp[len_train:].drop(['AdoptionSpeed'], axis = 1)","4f5ba6dc":"img_train, img_test = run_xgb(xgb_params,X_img_row_temp[:len_train],X_text_image)\nX_temp['img_pred'] = np.concatenate([img_train,img_test.mean(axis = 1)])","e386c62b":"X_temp['img_pred'].head()","034f137b":"from PIL import Image\ntrain_df_ids = train[['PetID']]\ntest_df_ids = test[['PetID']]\n\ntrain_df_imgs = pd.DataFrame(train_image_files)\ntrain_df_imgs.columns = ['image_filename']\ntrain_imgs_pets = train_df_imgs['image_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\n\ntest_df_imgs = pd.DataFrame(test_image_files)\ntest_df_imgs.columns = ['image_filename']\ntest_imgs_pets = test_df_imgs['image_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\n\ntrain_df_imgs = train_df_imgs.assign(PetID=train_imgs_pets)\ntest_df_imgs = test_df_imgs.assign(PetID=test_imgs_pets)\n","9acae9d2":"def getSize(filename):\n    st = os.stat(filename)\n    return st.st_size\n\ndef getDimensions(filename):\n    img_size = Image.open(filename).size\n    return img_size \n\ndef color_analysis(img):\n    # obtain the color palatte of the image \n    palatte = defaultdict(int)\n    for pixel in img.getdata():\n        palatte[pixel] += 1\n    \n    # sort the colors present in the image \n    sorted_x = sorted(palatte.items(), key=operator.itemgetter(1), reverse = True)\n    light_shade, dark_shade, shade_count, pixel_limit = 0, 0, 0, 25\n    for i, x in enumerate(sorted_x[:pixel_limit]):\n        if all(xx <= 20 for xx in x[0][:3]): ## dull : too much darkness \n            dark_shade += x[1]\n        if all(xx >= 240 for xx in x[0][:3]): ## bright : too much whiteness \n            light_shade += x[1]\n        shade_count += x[1]\n        \n    light_percent = round((float(light_shade)\/shade_count)*100, 2)\n    dark_percent = round((float(dark_shade)\/shade_count)*100, 2)\n    return light_percent, dark_percent\n\ndef perform_color_analysis(filename, flag = 'white'):\n    im = Image.open(filename) #.convert(\"RGB\")\n    \n    # cut the images into two halves as complete average may give bias results\n    size = im.size\n    halves = (size[0]\/2, size[1]\/2)\n    im1 = im.crop((0, 0, size[0], halves[1]))\n    im2 = im.crop((0, halves[1], size[0], size[1]))\n\n    try:\n        light_percent1, dark_percent1 = color_analysis(im1)\n        light_percent2, dark_percent2 = color_analysis(im2)\n    except Exception as e:\n        return None\n\n    light_percent = (light_percent1 + light_percent2)\/2 \n    dark_percent = (dark_percent1 + dark_percent2)\/2 \n    if flag == 'black':\n        return dark_percent\n    elif flag == 'white':\n        return light_percent\n    else:\n        return None\n\ndef average_pixel_width(img):\n    im = Image.open(img)    \n    #im_array = np.asarray(im.convert(mode='L'))\n    #edges_sigma1 = feature.canny(im_array, sigma=3)\n    #apw = (float(np.sum(edges_sigma1)) \/ (im.size[0]*im.size[1]))\n    size = list(im.size)\n    #size.append(apw*100)\n    return size\n\ndef get_blurrness_score(image):\n    image = cv2.imread(image)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    fm = cv2.Laplacian(image, cv2.CV_64F).var()\n    return fm\n\ndef get_average_color(img):\n    img = cv2.imread(img)\n    average_color = [img[:, :, i].mean() for i in range(img.shape[-1])]\n    image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    fm = cv2.Laplacian(image, cv2.CV_64F).var()\n    average_color.append(fm)\n    return average_color","b9571b86":"\navg_blur_result = Parallel(n_jobs=-1, verbose=1)(\n    delayed(get_average_color)(i) for i in train_df_imgs['image_filename'])\n\ntrain_df_imgs['average_red'] = [x[0] for x in avg_blur_result]\ntrain_df_imgs['average_green'] = [x[1] for x in avg_blur_result]\ntrain_df_imgs['average_blue'] = [x[2] for x in avg_blur_result]\ntrain_df_imgs['blur'] = [x[3] for x in avg_blur_result]","0d93acc6":"w_h_average = Parallel(n_jobs=-1, verbose=1)(\n    delayed(average_pixel_width)(i) for i in train_df_imgs['image_filename'])\n\ntrain_df_imgs['width'] = [x[0] for x in w_h_average]\ntrain_df_imgs['height'] = [x[1] for x in w_h_average]\ntrain_df_imgs['resultion'] = train_df_imgs['width'] * train_df_imgs['height'] \n#train_df_imgs['img_average'] = [x[2] for x in w_h_average]\ntrain_df_imgs['image_size'] = train_df_imgs['image_filename'].apply(getSize)\ntrain_df_imgs['ppi'] = train_df_imgs['resultion'] \/ train_df_imgs['image_size']","ffed774e":"avg_blur_result = Parallel(n_jobs=-1, verbose=1)(\n    delayed(get_average_color)(i) for i in test_df_imgs['image_filename'])\n\ntest_df_imgs['average_red'] = [x[0] for x in avg_blur_result]\ntest_df_imgs['average_green'] = [x[1] for x in avg_blur_result]\ntest_df_imgs['average_blue'] = [x[2] for x in avg_blur_result]\ntest_df_imgs['blur'] = [x[3] for x in avg_blur_result]\n","145fc988":"w_h_average = Parallel(n_jobs=-1, verbose=1)(\n    delayed(average_pixel_width)(i) for i in test_df_imgs['image_filename'])\n\ntest_df_imgs['width'] = [x[0] for x in w_h_average]\ntest_df_imgs['height'] = [x[1] for x in w_h_average]\ntest_df_imgs['resultion'] = test_df_imgs['width'] * test_df_imgs['height'] \n\n#test_df_imgs['img_average'] = [x[2] for x in w_h_average]\ntest_df_imgs['image_size'] = test_df_imgs['image_filename'].apply(getSize)\ntest_df_imgs['ppi'] = test_df_imgs['resultion'] \/ test_df_imgs['image_size']","b5f814b4":"train_df_imgs.head()","3536a601":"aggs = {\n    'average_blue': ['sum', 'mean', 'var','min','max'],\n    'average_green': ['sum', 'mean', 'var','min','max'],\n    'average_red': ['sum', 'mean', 'var','min','max'],\n#    'img_average': ['sum', 'mean', 'var'],\n#    'black': ['sum', 'mean', 'var'],\n    'blur': ['sum', 'mean', 'var','min','max'],\n    'image_size': ['sum', 'mean', 'var','min','max'],\n    'width': ['sum', 'mean', 'var','min','max'],\n    'height': ['sum', 'mean', 'var','min','max'],\n    'ppi': ['sum', 'mean', 'var','min','max'],\n    'resultion': ['sum', 'mean', 'var','min','max'],\n\n}\n\nagg_train_imgs = train_df_imgs.groupby('PetID').agg(aggs)\nnew_columns = [\n    k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n]\nagg_train_imgs.columns = new_columns\nagg_train_imgs = agg_train_imgs.reset_index()\n\nagg_test_imgs = test_df_imgs.groupby('PetID').agg(aggs)\nnew_columns = [\n    k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n]\nagg_test_imgs.columns = new_columns\nagg_test_imgs = agg_test_imgs.reset_index()\n\nagg_imgs = pd.concat([agg_train_imgs, agg_test_imgs], axis=0).reset_index(drop=True)","4654002a":"X_temp = X_temp.merge(agg_imgs, how='left', on='PetID')","945e02a2":"X_temp = X_temp.drop(to_drop_columns, axis=1)","d1c74119":"X_temp","7df83c36":"for x in [\n    'Breed1','Breed2',# 'Color1', 'Color2', 'Color3', 'FurLength',\n    #'Quantity', 'State', \n    'main_breed_BreedName', 'second_breed_Type', 'second_breed_BreedName',\n    'breed_type_cross'\n]: #'txt_pred','img_pred','tablur_pred'\n    X_temp = agg_helper(X_temp, x,'txt_pred')\n    X_temp = agg_helper(X_temp, x,'img_pred')\n    X_temp = agg_helper(X_temp, x,'tablur_pred')\n    X_temp = agg_helper(X_temp, x,'height_mean')\n    X_temp = agg_helper(X_temp, x,'Age')\n    X_temp = agg_helper(X_temp, x,'image_size_mean')\n    X_temp = agg_helper(X_temp, x,'resultion_mean')\n    X_temp = agg_helper(X_temp, x,'ppi_mean')\n    ","a66725c6":"for i in HD_cate:\n    if i in X_temp.columns:\n        X_temp[i] =  X_temp.groupby(i)[i].transform('count') ","5f6da197":"X_train = X_temp.loc[np.isfinite(X_temp.AdoptionSpeed), :]\nX_test = X_temp.loc[~np.isfinite(X_temp.AdoptionSpeed), :]\n\nX_test = X_test.drop(['AdoptionSpeed'], axis=1)\n\nassert X_train.shape[0] == train.shape[0]\nassert X_test.shape[0] == test.shape[0]\n\ntrain_cols = X_train.columns.tolist()\ntrain_cols.remove('AdoptionSpeed')\n\ntest_cols = X_test.columns.tolist()\n\nassert np.all(train_cols == test_cols)","4f577215":"print(test_cols)","5207458c":"X_train_non_null = X_train.fillna(-1)\nX_test_non_null = X_test.fillna(-1)","252700de":"len_train = len(X_train_non_null)\ndata = X_train_non_null.append(X_test_non_null).reset_index(drop=True)","42f7e7a6":"import scipy as sp\n\nfrom collections import Counter\nfrom functools import partial\nfrom math import sqrt\n\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix\n\n\n# FROM: https:\/\/www.kaggle.com\/myltykritik\/simple-lgbm-image-features\n\n# The following 3 functions have been taken from Ben Hamner's github repository\n# https:\/\/github.com\/benhamner\/Metrics\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              \/ num_scored_items)\n            d = pow(i - j, 2.0) \/ pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] \/ num_scored_items\n            denominator += d * expected_count \/ num_scored_items\n\n    return (1.0 - numerator \/ denominator)","9632a321":"class OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n    \n    def _kappa_loss(self, coef, X, y):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return -cohen_kappa_score(y, preds, weights='quadratic')\n    \n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X = X, y = y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n    \n    def predict(self, X, coef):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return preds\n    \n    def coefficients(self):\n        return self.coef_['x']","f712eab0":"\nkeep_cate = [i for i in keep_cate if i in data.columns]\n#keep_float = [i for i in keep_float if i in data.columns]\nkeep_float = [i for i in data.columns if i not in keep_cate + ['Description','metadata_annots_top_desc','sentiment_entities','AdoptionSpeed']]\n\npad_title = 200\npad_desc = 200\npad_sent = 50\n\nfor i in keep_cate:\n    encoder = preprocessing.LabelEncoder()    \n    data[i] = encoder.fit_transform(data[i].astype(str)) \n    \nfor i in keep_float:\n    stanscaler = preprocessing.StandardScaler()\n    stanscaler.fit(data[i].values.reshape(-1, 1).astype(np.float32))\n    data[i] = stanscaler.transform(data[i].values.reshape(-1, 1).astype(np.float32))\n    \ndef get_keras_data(data, y=None):\n    X = {}\n    for feat in keep_cate + keep_float:\n        if feat in data.columns:\n            X[feat] = np.array(data[feat])\n    X['Description'] = pad_sequences(tokenizer.texts_to_sequences(data['Description'].apply(spacing_punctuation)), maxlen=pad_title)\n    X['metadata_annots_top_desc'] = pad_sequences(tokenizer.texts_to_sequences(data['metadata_annots_top_desc'].apply(spacing_punctuation)), maxlen=pad_desc)\n    X['sentiment_entities'] = pad_sequences(tokenizer.texts_to_sequences(data['sentiment_entities'].apply(spacing_punctuation)), maxlen=pad_sent)\n    if y is not None:\n        y = y.values.reshape(-1, 1)\n    return X, y","700c087f":"keep_cate","5ce21b85":"from sklearn.metrics import mean_squared_error\nfrom keras.callbacks import Callback\n\nclass RMSEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0, batch_size=2000)\n            score = np.sqrt(mean_squared_error(self.y_val, y_pred))\n            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch + 1, score))\n\nclass ModelWrapper(object):\n    def __init__(self):\n        pass\n\n    def fit_predict(self, X_train, y_train, X_val, y_val):\n        pass\n\n    def predict(self, X_test):\n        pass\n\nclass DNNModelWrapper(ModelWrapper):\n    def __init__(self, get_dnn_model, batch_size,\n                 epochs, get_keras_data):\n        self.get_dnn_model = get_dnn_model\n        self.batch_size = batch_size\n        self.epochs = epochs\n        self.get_keras_data = get_keras_data\n\n    def fit_predict(self, X_train, y_train, X_val, y_val):\n        self.dnn_model = self.get_dnn_model(len(X_train), self.batch_size, self.epochs)\n\n        print(\"get_keras_data.....\")\n        X_train, y_train = self.get_keras_data(X_train, y_train)\n        X_val, y_val = self.get_keras_data(X_val, y_val)\n\n        RMSE = RMSEvaluation(validation_data=(X_val, y_val), interval=1)\n        ES = EarlyStopping(monitor='val_loss',\n                              patience=2,\n                              verbose=0, mode='auto')\n        mcp_save = ModelCheckpoint('mdl_wts.h5', save_best_only=True, monitor='val_loss', mode='min')\n        \n        self.dnn_model.fit(X_train, y_train,\n                  batch_size=self.batch_size,\n                  callbacks=[RMSE, ES, mcp_save],\n                  validation_data=(X_val, y_val),\n                  verbose=1, epochs=self.epochs)\n        self.dnn_model.load_weights('mdl_wts.h5') #= load_model('mdl_wts.h5', custom_objects={'Attention':Attention})\n        val_pred = self.dnn_model.predict(X_val)\n        return val_pred.reshape(-1)\n\n    def predict(self, X):\n        X, _ = self.get_keras_data(X)\n        y_pred = self.dnn_model.predict(X, batch_size=2000)\n\n        return y_pred.reshape(-1)\n","188a119d":"max_dict = dict((feat,  np.max(data[feat]) + 1) for feat in keep_cate)","6d780b5e":"class Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n        if self.bias:\n            eij += self.b\n        eij = K.tanh(eij)\n        a = K.exp(eij)\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim\n","cdc9c629":"\ndef get_dnn_model2(n_train, batch_size, epochs):\n    conts_size = 12\n    inputs = [Input(shape=[1], name=feat) for feat in keep_cate + keep_float]\n    embeds = []\n    for i, feat in enumerate(keep_cate):\n        embeds.append(Flatten()(Embedding(max_dict[feat], conts_size)(inputs[i])))\n\n\n    y_dot = []\n    for field1, field2 in itertools.combinations([i for i in range(len(keep_cate))], 2):\n        embed1 = embeds[field1]\n        embed2 = embeds[field2]        \n        y_dot.append(\n            Dot(1, normalize=False)([embed1, embed2])\n     )\n    y_dot_cat = concatenate(y_dot)  ##### None * (F*K)\n\n\n    for i, feat in enumerate(keep_float):\n        embeds.append(inputs[i + len(keep_cate)])\n    title = Input(shape=[pad_title], name=\"Description\")\n    embed_title = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n                            weights=[embedding_matrix], trainable= False)(title)\n\n\n    embed_title = SpatialDropout1D(0.2)(embed_title)\n    embed_title = Bidirectional(CuDNNGRU(96, return_sequences=True))(embed_title)\n    att_pool = Attention(pad_title)(embed_title)\n    max_pool2 = GlobalMaxPooling1D()(embed_title)\n    embed_title = concatenate([att_pool, max_pool2])\n    \n    description = Input(shape=[pad_desc], name=\"metadata_annots_top_desc\")\n    embed_description = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n                            weights=[embedding_matrix], trainable= False)(description)\n\n\n    embed_description = SpatialDropout1D(0.2)(embed_description)\n    embed_description = Bidirectional(CuDNNGRU(128, return_sequences=True))(embed_description)\n    att_pool = Attention(pad_desc)(embed_description)\n    max_pool2 = GlobalMaxPooling1D()(embed_description)\n    embed_description = concatenate([att_pool,max_pool2])\n\n    sentiment_entities = Input(shape=[pad_sent], name=\"sentiment_entities\")\n    embed_sentiment_entities = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n                            weights=[embedding_matrix], trainable= False)(sentiment_entities)\n\n\n    embed_sentiment_entities = SpatialDropout1D(0.2)(embed_sentiment_entities)\n    embed_sentiment_entities = Bidirectional(CuDNNGRU(128, return_sequences=True))(embed_sentiment_entities)\n    att_pool = Attention(pad_sent)(embed_sentiment_entities)\n    max_pool2 = GlobalMaxPooling1D()(embed_sentiment_entities)\n    embed_sentiment_entities = concatenate([att_pool,max_pool2])\n        \n    inputs += [title, description,sentiment_entities]\n    embeds += [embed_title, embed_description, embed_sentiment_entities]\n    \n    out = concatenate(embeds)\n    out = concatenate([out, y_dot_cat])\n    out = Dense(300)(out)\n    out = Activation('relu')(out)\n    out = Dropout(0.2)(out)\n    out = BatchNormalization()(out)\n    out = Dense(1, activation='linear', name='output')(out)\n    model = Model(inputs, out)\n\n    exp_decay = lambda init, fin, steps: (init \/ fin) ** (1 \/ (steps - 1)) - 1\n    steps = int(n_train \/ batch_size) * epochs\n    lr_init, lr_fin = 0.001, 0.0005\n    lr_decay = exp_decay(lr_init, lr_fin, steps)\n    optimizer = Adam(lr=lr_init, decay=lr_decay)\n    model.compile(optimizer=optimizer, loss=\"mse\")\n    return model\n\n\n\n    \ndef get_dnn_model(n_train, batch_size, epochs):\n    conts_size = 10\n    inputs = [Input(shape=[1], name=feat) for feat in keep_cate + keep_float]\n    embeds = []\n    for i, feat in enumerate(keep_cate):\n        #print(feat,max_dict[feat])\n        if feat in data.columns:\n            embeds.append(Flatten()(Embedding(max_dict[feat], conts_size)(inputs[i])))\n\n    for i, feat in enumerate(keep_float):\n        if feat in data.columns:\n            embeds.append(inputs[i + len(keep_cate)])\n    '''\n    title = Input(shape=[pad_title], name=\"title\")\n    embed_title = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n                            weights=[embedding_matrix], trainable=False)(title)\n\n    description = Input(shape=[pad_desc], name=\"description\")\n    embed_description = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n                            weights=[embedding_matrix], trainable=False)(description)\n\n\n    embed_title = SpatialDropout1D(0.2)(embed_title)\n    embed_title = Bidirectional(CuDNNGRU(20, return_sequences=True))(embed_title)\n    #embed_title = GlobalAveragePooling1D()(embed_title)\n    embed_title = Conv1D(32, kernel_size=2, padding=\"valid\", kernel_initializer=\"he_uniform\")(embed_title)\n    avg_pool = GlobalAveragePooling1D()(embed_title)\n    max_pool = GlobalMaxPooling1D()(embed_title)\n    embed_title = concatenate([avg_pool, max_pool])\n\n\n    embed_description = SpatialDropout1D(0.2)(embed_description)\n    embed_description = Bidirectional(CuDNNGRU(80, return_sequences=True))(embed_description)\n    #embed_description = GlobalAveragePooling1D()(embed_description)\n    embed_description = Conv1D(64, kernel_size=2, padding=\"valid\", kernel_initializer=\"he_uniform\")(embed_description)\n    avg_pool = GlobalAveragePooling1D()(embed_description)\n    max_pool = GlobalMaxPooling1D()(embed_description)\n    embed_description = concatenate([avg_pool, max_pool])\n\n    inputs += [title, description]\n    embeds += [embed_title, embed_description]\n    '''\n    out = concatenate(embeds)\n    out = Dense(512)(out)\n    out = BatchNormalization()(out)\n    out = Activation('relu')(out)\n    out = Dense(1, activation='linear', name='output')(out)\n    model = Model(inputs, out)\n\n    exp_decay = lambda init, fin, steps: (init \/ fin) ** (1 \/ (steps - 1)) - 1\n    steps = int(n_train \/ batch_size) * epochs\n    lr_init, lr_fin = 0.001, 0.0005\n    lr_decay = exp_decay(lr_init, lr_fin, steps)\n    optimizer = Adam(lr=lr_init, decay=lr_decay)\n    model.compile(optimizer=optimizer, loss=\"mse\")\n    return model\n\ndef rmse(y_true, y_pred):\n    return mean_squared_error(y_true, y_pred)\n\n\ndef get_oof(model, X, y, X_test,\n            k_fold, metric=rmse, save=False,\n            name=None, is_df=False):\n    ntrain, ntest = X.shape[0], X_test.shape[0]\n\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n\n    #kf = KFold(ntrain, n_folds=k_fold, shuffle=True)\n    kf = StratifiedKFold(n_splits=k_fold, shuffle=True, random_state=1337)\n    oof_test_skf = np.empty((k_fold, ntest))\n    errors = []\n    #kf.split(X_train, X_train['AdoptionSpeed'].values)\n    for i, (train_index, val_index) in enumerate(kf.split(X,y)):\n        \n        print(f'Fold {i}..........')\n        if is_df:\n            X_train, y_train, X_val, y_val = \\\n                X.loc[train_index], y.loc[train_index], X.loc[val_index], y.loc[val_index]\n        else:\n            X_train, y_train, X_val, y_val = \\\n                X[train_index], y[train_index], X[val_index],y[val_index]\n\n        val_pred = model.fit_predict(X_train, y_train, X_val, y_val)\n        error = metric(val_pred, y_val)\n        print(f\"{i}th error is {error}\")\n        errors.append(error)\n        oof_train[val_index] = val_pred\n        oof_test_skf[i, :] = model.predict(X_test)\n\n    print(f\"mean error is {np.mean(errors)}\")\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","86dc3899":"X = data[:len_train]\nX_test = data[len_train:]\nX = pd.concat([X,X_TXT[:len_train]], axis = 1)\nX_test = pd.concat([X_test,X_TXT[len_train:]], axis = 1)\n\nprint(len(X), type(X))\nprint(len(X_test))\n\ny =  X_train['AdoptionSpeed']\nk_fold = 10\nbatch_size = 300\nepochs = 10\n\ndnn_wrapper = DNNModelWrapper(get_dnn_model2, batch_size, epochs, get_keras_data)\ndnn_oof_train, dnn_oof_test = get_oof(dnn_wrapper, X, y,\n                                      X_test, k_fold,\n                                      save=True, name=\"dnn_v1\",\n                                      is_df=True)\n","b7804f3f":"def get_fea_importance(clf, feature_name):\n    gain = clf.feature_importance('gain')\n    importance_df = pd.DataFrame(\n        {'feature':clf.feature_name(),\n    'split':clf.feature_importance('split'),\n    'gain':100 * gain \/ gain.sum(),\n    }).sort_values('gain',ascending=False)\n    return importance_df\n    \ndef run_lgb(params, X_train, X_test):\n    n_splits = 10\n    verbose_eval = 1000\n    num_rounds = 60000\n    early_stop = 500\n\n    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1337)\n\n    oof_train = np.zeros((X_train.shape[0]))\n    oof_test = np.zeros((X_test.shape[0], n_splits))\n    i = 0\n    feature_importance_df = pd.DataFrame()\n\n    for train_idx, valid_idx in kf.split(X_train, X_train['AdoptionSpeed'].values):\n\n        X_tr = X_train.iloc[train_idx, :]\n        X_val = X_train.iloc[valid_idx, :]\n\n        y_tr = X_tr['AdoptionSpeed'].values\n        X_tr = X_tr.drop(['AdoptionSpeed'], axis=1)\n\n        y_val = X_val['AdoptionSpeed'].values\n        X_val = X_val.drop(['AdoptionSpeed'], axis=1)\n        lgb_train =  lgb.Dataset(X_tr, y_tr)#, feature_names=X_tr.columns)\n        lgb_val = lgb.Dataset(X_val, y_val)#, feature_names=X_val.columns)\n        \n        #d_train = xgb.DMatrix(data=X_tr, label=y_tr, feature_names=X_tr.columns)\n        #d_valid = xgb.DMatrix(data=X_val, label=y_val, feature_names=X_val.columns)\n        clf = lgb.train(params, lgb_train , valid_sets = lgb_val,feature_name = list(X_tr.columns),\n                            verbose_eval=verbose_eval, early_stopping_rounds=early_stop, num_boost_round=num_rounds)\n        \n#         watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n#         model = xgb.train(dtrain=d_train, num_boost_round=num_rounds, evals=watchlist,\n#                          early_stopping_rounds=early_stop, verbose_eval=verbose_eval, params=params)\n\n        valid_pred = clf.predict(X_val, num_iteration=clf.best_iteration)\n        test_pred = clf.predict(X_test, num_iteration=clf.best_iteration)\n        fold_importance_df_fold = get_fea_importance(clf, list(X_tr.columns))\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df_fold], axis=0)\n        oof_train[valid_idx] = valid_pred\n        oof_test[:, i] = test_pred\n        i += 1\n        \n    feature_importance_df = feature_importance_df[\n            [\"feature\", \"split\",\"gain\"]\n    ].groupby(\"feature\",as_index=False).mean().sort_values(by=\"gain\", ascending=False)\n    oof_test = oof_test.mean(axis=1)\n    return  oof_train, oof_test, feature_importance_df","24e0e4d5":"lgb_params = {\n    'learning_rate': 0.005,\n    'eval_metric': 'l2',\n    'seed': 1337,\n    'bagging_fraction': 0.8,\n    'feature_fraction': 0.75,\n    'boosting': 'gbdt',\n    'max_depth':-1,\n    'num_leaves': 32,\n    \"objective\": \"regression\", \n}","c1c0539e":"oof_train_lgb, oof_test_lgb,feature_importance_df  = run_lgb(lgb_params, X_train_non_null, X_test_non_null)","533f1a40":"drop_f = list(feature_importance_df[feature_importance_df.gain == 0].feature)","a61bdc82":"X_train_non_null.drop(drop_f, inplace = True, axis = 1)\nX_test_non_null.drop(drop_f, inplace = True, axis = 1)","a0907b94":"print(drop_f)","bce14b3a":"oof_train, oof_test = run_xgb(xgb_params, X_train_non_null, X_test_non_null)","26d4d40a":"def plot_pred(pred):\n    sns.distplot(pred, kde=True, hist_kws={'range': [0, 5]})","e3ae82ab":"X_train_non_null['nn_pred'] = dnn_oof_train\nX_test_non_null['nn_pred'] = dnn_oof_test\nX_train_non_null['lgb_pred'] = oof_train_lgb\nX_test_non_null['lgb_pred'] = oof_test_lgb\nX_train_non_null['xgb_pred'] = oof_train\nX_test_non_null['xgb_pred'] = oof_test.mean(axis=1)\n\ndnn_oof_train = X_train_non_null['nn_pred'].values\ndnn_oof_test = X_test_non_null['nn_pred'].values\nlgb_oof_train = X_train_non_null['lgb_pred'].values\nlgb_oof_test = X_test_non_null['lgb_pred'].values","6235318b":"oof_train_ensemble = oof_train * 0.45 + dnn_oof_train * 0.1 + lgb_oof_train * 0.45\noof_test_ensemble = oof_test.mean(axis=1) * 0.45 + dnn_oof_test * 0.1 + lgb_oof_test * 0.45","333b29e1":"plot_pred(oof_train_ensemble)","812ed72d":"plot_pred(oof_test_ensemble)","4f76fc96":"optR = OptimizedRounder()\noptR.fit(oof_train_ensemble, X_train['AdoptionSpeed'].values)\ncoefficients = optR.coefficients()\nvalid_pred = optR.predict(oof_train_ensemble, coefficients)\nqwk = quadratic_weighted_kappa(X_train['AdoptionSpeed'].values, valid_pred)\nprint(\"QWK = \", qwk)\ntest_predictions = optR.predict(oof_test_ensemble, coefficients)\nCounter(test_predictions)\nplot_pred(test_predictions)\nsubmission = pd.DataFrame({'PetID': test['PetID'].values, 'AdoptionSpeed': test_predictions})\nsubmission.to_csv('submission.csv', index=False)","f37e05ba":"optR = OptimizedRounder()\noptR.fit(oof_train_lgb, X_train['AdoptionSpeed'].values)\ncoefficients = optR.coefficients()\nvalid_pred = optR.predict(oof_train_lgb, coefficients)\nqwk = quadratic_weighted_kappa(X_train['AdoptionSpeed'].values, valid_pred)\nprint(\"QWK LGB = \", qwk)","5649cd74":"optR = OptimizedRounder()\noptR.fit(dnn_oof_train, X_train['AdoptionSpeed'].values)\ncoefficients = optR.coefficients()\nvalid_pred = optR.predict(dnn_oof_train, coefficients)\nqwk = quadratic_weighted_kappa(X_train['AdoptionSpeed'].values, valid_pred)\nprint(\"QWK DNN= \", qwk)","6c46a09e":"optR = OptimizedRounder()\noptR.fit(oof_train, X_train['AdoptionSpeed'].values)\ncoefficients = optR.coefficients()\nvalid_pred = optR.predict(oof_train, coefficients)\nqwk = quadratic_weighted_kappa(X_train['AdoptionSpeed'].values, valid_pred)\nprint(\"QWK XGB= \", qwk)","d429a396":"coefficients_ = coefficients.copy()\ntrain_predictions = optR.predict(oof_train_ensemble, coefficients_).astype(np.int8)\nprint(f'train pred distribution: {Counter(train_predictions)}')\ntest_predictions = optR.predict(oof_test_ensemble, coefficients_).astype(np.int8)\nprint(f'test pred distribution: {Counter(test_predictions)}')","90494081":"Counter(train_predictions)","b6ac7a0c":"Counter(test_predictions)","ce8573bb":"oof_train_lgb_stacking, oof_test_lgb_stacking, feature_importance_df  = run_lgb(lgb_params, X_train_non_null, X_test_non_null)\nprint(feature_importance_df)\noptR = OptimizedRounder()\noptR.fit(oof_train_lgb_stacking, X_train['AdoptionSpeed'].values)\ncoefficients = optR.coefficients()\nvalid_pred = optR.predict(oof_train_lgb_stacking, coefficients)\nqwk = quadratic_weighted_kappa(X_train['AdoptionSpeed'].values, valid_pred)\nprint(\"QWK XGB= \", qwk)","9a0f45dc":"### Test","a3b24dc3":"## Image features","38b1ecb4":"## About metadata and sentiment","390939c2":"### Drop ID, name and rescuerID, hereis all the feature ","351d2b8d":"### group extracted features by PetID:","06235bcf":"### Merge image features","d1408307":"### OptimizeRounder from [OptimizedRounder() - Improved](https:\/\/www.kaggle.com\/naveenasaithambi\/optimizedrounder-improved)","4fe4f7c6":"### Add image_size features","c262fe3d":"## Extract features from json","460ce273":"### Train","e88a99dc":"## Train model","1df85e7b":"### TFIDF","7ee722eb":"### merge processed DFs with base train\/test DF:"}}