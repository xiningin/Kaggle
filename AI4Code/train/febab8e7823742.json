{"cell_type":{"65cfc822":"code","4d8f5726":"code","4d87c56a":"code","433188c3":"code","0f26fcab":"code","2ee70b97":"code","819cf1aa":"code","2adfb07c":"code","d61c135b":"code","28760efa":"code","41999c06":"code","2b4ade1b":"code","07586a6d":"code","0beb0565":"code","4bf78381":"code","6a8b2882":"code","60064e0d":"code","c60aef0e":"code","f9bc7651":"code","e1e233a6":"code","fb084d5c":"code","7eecab13":"code","f718f34b":"code","e298dbd2":"code","567a92f2":"code","91e2e50c":"code","75763ed7":"code","c0a8bb0b":"code","55a58bf7":"code","e4631ddd":"code","051e383e":"code","c3dd2b6f":"code","9eddee6c":"code","fb04a0c3":"code","13e86cc4":"code","89dc2d99":"code","8c28f044":"code","601a554e":"code","780af25f":"code","5a82e97b":"code","eebcefd7":"code","ea9ba286":"code","bdb28bcf":"code","be37c395":"code","6ea69d40":"code","40f231e2":"code","1df88070":"code","3f053cd9":"code","5584d9bc":"code","364c493b":"code","9ba72482":"code","a08d895f":"code","1c95eb78":"code","bbb14b5a":"code","4fe47386":"code","5d06cde4":"code","a593ca34":"code","49d1fda4":"code","982fc4af":"code","12f77614":"code","2c3a566b":"code","9c2fc81f":"code","db70e5f5":"code","811648ee":"code","939b0f40":"code","d9fc3250":"code","f1801288":"code","8ceb47ac":"code","f29011bd":"code","099495be":"code","06117d2c":"code","905fcaf5":"code","b7892aea":"code","cbeec38c":"code","63c1e8c0":"code","41ccd23c":"code","c34f9ffd":"code","6299117e":"code","6f80a634":"code","209e25f7":"code","fc14f09f":"code","3675d7fb":"code","902eb009":"code","e6613994":"code","1884b9d9":"code","4353c1b0":"code","a3a764d5":"code","0d7b5fdc":"code","2a4f71ce":"code","6baeb12d":"code","f8361c9d":"code","5f37cab8":"code","83168a8c":"markdown","f5b6a4b8":"markdown","120c2a52":"markdown","8155c6cb":"markdown","c012674a":"markdown","5f6037d1":"markdown","9e81e0dc":"markdown","6ad9d0f3":"markdown","832d919d":"markdown","d3c7bad5":"markdown","9077aa65":"markdown","7b80c34d":"markdown","386e62c3":"markdown","ecccab4b":"markdown","7234ae12":"markdown","39fd9d03":"markdown","e01f685f":"markdown","6195b91f":"markdown","85072394":"markdown","7e9bf403":"markdown","ad7e69ef":"markdown","5c2360c5":"markdown","4f793f45":"markdown","84f6ca3a":"markdown","e4bcebf0":"markdown","c013ae33":"markdown","f6edef75":"markdown","8aa8930d":"markdown","99de04df":"markdown"},"source":{"65cfc822":"#GENERAL\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\nimport time\n#PATH PROCESS\nimport os\nimport os.path\nfrom pathlib import Path\nimport glob\n#IMAGE PROCESS\nfrom PIL import Image\nfrom keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport cv2\nfrom keras.applications.vgg16 import preprocess_input, decode_predictions\nfrom keras.preprocessing import image\nfrom skimage.feature import hessian_matrix, hessian_matrix_eigvals\nfrom scipy.ndimage.filters import convolve\nfrom skimage import data, io, filters\nimport skimage\nfrom skimage.morphology import convex_hull_image, erosion\nfrom IPython import display\n#SCALER & TRANSFORMATION\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras import regularizers\nfrom sklearn.preprocessing import LabelEncoder\n#ACCURACY CONTROL\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n#OPTIMIZER\nfrom keras.optimizers import RMSprop,Adam,Optimizer,Optimizer, SGD\n#MODEL LAYERS\nfrom tensorflow.keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\\\n                        Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN,\\\nLSTM, GlobalAveragePooling2D, SeparableConv2D, ZeroPadding2D, Convolution2D, ZeroPadding2D,Reshape, Conv2DTranspose, LeakyReLU, GaussianNoise, GlobalMaxPooling2D\nfrom keras import models\nfrom keras import layers\nimport tensorflow as tf\nfrom keras.applications import VGG16,VGG19,inception_v3\nfrom keras import backend as K\nfrom keras.utils import plot_model\nfrom keras.datasets import mnist\nimport keras\nfrom keras.models import Model\n#IGNORING WARNINGS\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\",category=DeprecationWarning)\nfilterwarnings(\"ignore\", category=FutureWarning) \nfilterwarnings(\"ignore\", category=UserWarning)","4d8f5726":"M_2018_10 = Path(\"..\/input\/the-cloudcast-dataset\/2018M10\")","4d87c56a":"Coordinate_Path = \"..\/input\/the-cloudcast-dataset\/2018M10\/GEO.npz\"","433188c3":"Timestamps_Path = \"..\/input\/the-cloudcast-dataset\/2018M10\/TIMESTAMPS.npy\"","0f26fcab":"Single_Example_Image_Path = \"..\/input\/the-cloudcast-dataset\/2018M10\/0.npy\"","2ee70b97":"Coordinate_Load = np.load(Coordinate_Path)","819cf1aa":"print(Coordinate_Load.files)","2adfb07c":"print(type(Coordinate_Load))","d61c135b":"print(Coordinate_Load.fid)","28760efa":"print(\"FIRST 5 LATITUDE: \\n\",Coordinate_Load[\"lats\"][:5])\nprint(\"---\"*10)\nprint(\"FIRST 5 LONGITUDE: \\n\",Coordinate_Load[\"lons\"][:5])","41999c06":"print(type(Coordinate_Load[\"lats\"][:5]))\nprint(type(Coordinate_Load[\"lons\"][:5]))","2b4ade1b":"print(len(Coordinate_Load[\"lats\"][1]))\nprint(len(Coordinate_Load[\"lons\"][1]))","07586a6d":"Timestamps_Load = np.load(Timestamps_Path)","0beb0565":"print(type(Timestamps_Load))","4bf78381":"print(\"FIRST 5 TIMESTAMPS: \\n\",Timestamps_Load[:5])","6a8b2882":"Single_Example_Image_Load = np.load(Single_Example_Image_Path)","60064e0d":"print(type(Single_Example_Image_Load))","c60aef0e":"print(len(Single_Example_Image_Load))","f9bc7651":"figure = plt.figure(figsize=(7,7))\n\nplt.xlabel(Single_Example_Image_Load.shape)\nplt.ylabel(Single_Example_Image_Load.size)\nplt.title(\"EXAMPLE\")\nplt.imshow(Single_Example_Image_Load)","e1e233a6":"F\u0131rst_5_Image_List = []\n\nfor indexing in range(0,5):\n    F\u0131rst_5_Image_List.append(np.load(f\"..\/input\/the-cloudcast-dataset\/2018M10\/{indexing}.npy\"))","fb084d5c":"print(len(F\u0131rst_5_Image_List))","7eecab13":"figure = plt.figure(figsize=(7,7))\n\nplt.imshow(F\u0131rst_5_Image_List[4])\nplt.xlabel(Coordinate_Load[\"lats\"][4])\nplt.ylabel(Coordinate_Load[\"lons\"][4])","f718f34b":"figure = plt.figure(figsize=(7,7))\n\nplt.imshow(F\u0131rst_5_Image_List[1])\nplt.xlabel(Coordinate_Load[\"lats\"][1])\nplt.ylabel(Coordinate_Load[\"lons\"][1])","e298dbd2":"Image_List = []\nLats_List = []\nLons_List = []\n\nfor indexing in range(0,767):\n    Image_List.append(np.load(f\"..\/input\/the-cloudcast-dataset\/2018M10\/{indexing}.npy\"))\n    Lats_List.append(Coordinate_Load[\"lats\"][indexing])\n    Lons_List.append(Coordinate_Load[\"lons\"][indexing])","567a92f2":"print(np.shape(np.array(Image_List)))\nprint(np.shape(np.array(Lats_List)))\nprint(np.shape(np.array(Lons_List)))","91e2e50c":"Train_AE = np.array(Image_List,dtype=\"float32\")\nOutput_Lats = np.array(Lats_List,dtype=\"float32\") # also we will use it for prediction\nOutput_Lons = np.array(Lons_List,dtype=\"float32\") # also we will use it for prediction","75763ed7":"Scaler_Function = MinMaxScaler()\n\nOutput_Lats = Scaler_Function.fit_transform(Output_Lats)\nOutput_Lons = Scaler_Function.fit_transform(Output_Lons)","c0a8bb0b":"print(Train_AE.shape)\nprint(Output_Lats.shape)\nprint(Output_Lons.shape)","55a58bf7":"Train_AE = Train_AE \/ 255.","e4631ddd":"figure = plt.figure(figsize=(7,7))\n\nplt.imshow(Train_AE[1])\nplt.xlabel(Train_AE[1].shape)\nplt.ylabel(Train_AE[1].size)","051e383e":"Train_Prediction = Train_AE.reshape(Train_AE.shape[0],Train_AE.shape[1],Train_AE.shape[2],1)","c3dd2b6f":"Train_Prediction = Train_Prediction","9eddee6c":"print(Train_Prediction.shape)","fb04a0c3":"figure = plt.figure(figsize=(7,7))\n\nplt.imshow(Train_Prediction[1])\nplt.xlabel(Train_Prediction[1].shape)\nplt.ylabel(Train_Prediction[1].size)","13e86cc4":"S_Train_AE = Train_AE[0:-2]","89dc2d99":"print(len(Train_AE))\nprint(Train_AE.shape)","8c28f044":"print(len(S_Train_AE))\nprint(S_Train_AE.shape)","601a554e":"Testing_Image = Train_AE[765:]","780af25f":"print(len(Testing_Image))\nprint(Testing_Image.shape)","5a82e97b":"Early_Stopper = tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=3,mode=\"min\")\nCheckpoint_Model = tf.keras.callbacks.ModelCheckpoint(monitor=\"val_accuracy\",\n                                                      save_best_only=True,\n                                                      save_weights_only=True,\n                                                      filepath=\".\/modelcheck\")\nReduce_Model = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\",\n                                                   factor=0.1,\n                                                   patience=5) # if you want","eebcefd7":"Encoder = Sequential()\nEncoder.add(Flatten(input_shape=[S_Train_AE.shape[1],S_Train_AE.shape[2]]))\nEncoder.add(Dense(300,activation=\"relu\"))\nEncoder.add(Dense(200,activation=\"relu\"))\nEncoder.add(Dense(100,activation=\"relu\"))\nEncoder.add(Dense(50,activation=\"relu\"))\nEncoder.add(Dense(25,activation=\"relu\"))","ea9ba286":"Decoder = Sequential()\nDecoder.add(Dense(50,input_shape=[25],activation=\"relu\"))\nDecoder.add(Dense(100,activation=\"relu\"))\nDecoder.add(Dense(200,activation=\"relu\"))\nDecoder.add(Dense(300,activation=\"relu\"))\nDecoder.add(Dense(S_Train_AE.shape[1] * S_Train_AE.shape[2],activation=\"sigmoid\"))\nDecoder.add(Reshape([S_Train_AE.shape[1],S_Train_AE.shape[2]]))","bdb28bcf":"AutoEncoder = Sequential([Encoder,Decoder])","be37c395":"print(AutoEncoder.summary())","6ea69d40":"AutoEncoder.compile(loss=\"binary_crossentropy\",optimizer=RMSprop())","40f231e2":"AutoEncoder.fit(S_Train_AE,S_Train_AE,epochs=10,callbacks=[Early_Stopper,\n                                                                       Checkpoint_Model])\n\n# if you want, you can try by more epochs","1df88070":"Prediction_Images = AutoEncoder.predict(Testing_Image)","3f053cd9":"Count_Image = 0\n\nprint(\"ORIGINAL\")\nplt.imshow(Testing_Image[Count_Image])\nplt.show()\nprint(\"Auto_Encoder_Output\")\nplt.imshow(Prediction_Images[Count_Image])","5584d9bc":"Count_Image = 1\n\nprint(\"ORIGINAL\")\nplt.imshow(Testing_Image[Count_Image])\nplt.show()\nprint(\"Auto_Encoder_Output\")\nplt.imshow(Prediction_Images[Count_Image])","364c493b":"Early_Stopper = tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=3,mode=\"min\")\nCheckpoint_Model = tf.keras.callbacks.ModelCheckpoint(monitor=\"val_accuracy\",\n                                                      save_best_only=True,\n                                                      save_weights_only=True,\n                                                      filepath=\".\/modelcheck\")\nReduce_Model = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\",\n                                                   factor=0.1,\n                                                   patience=5) # if you want","9ba72482":"COMPILE_METRICS = [\"accuracy\"]\nCOMPILE_LATS_PREDICTION = \"mse\"\nCOMPILE_LONS_PREDICTION = \"mse\"\nCOMPILE_OPTIMIZER = \"rmsprop\"\nINPUT_SHAPE = (Train_Prediction.shape[1],Train_Prediction.shape[2],Train_Prediction.shape[3])\nEPOCH_PERIOD = 50\nBATCH_SIZE = 64","a08d895f":"Input_Layer = tf.keras.Input(shape=INPUT_SHAPE)\n#\nx = Conv2D(16,(3,3),activation=\"relu\",padding=\"same\",use_bias=True)(Input_Layer)\nx = BatchNormalization()(x)\nx = MaxPooling2D((2,2),strides=2)(x)\n#\nx = Conv2D(32,(3,3),activation=\"relu\",padding=\"same\",use_bias=True)(x)\nx = Dropout(0.3)(x)\nx = MaxPooling2D((2,2),strides=2)(x)\n#\nx = Conv2D(64,(3,3),activation=\"relu\",padding=\"same\",use_bias=True)(x)\nx = Dropout(0.3)(x)\nx = MaxPooling2D((2,2),strides=2)(x)\n#\nx = TimeDistributed(Flatten())(x)\nx = Bidirectional(LSTM(64,\n                                  return_sequences=True,\n                                  dropout=0.5,\n                                  recurrent_dropout=0.5))(x)\nx = Bidirectional(LSTM(64,\n                                  return_sequences=True,\n                                  dropout=0.5,\n                                  recurrent_dropout=0.5))(x)\n#\nx = Flatten()(x)\nx = Dense(256,activation=\"relu\")(x)\nx = Dropout(0.5)(x)\n#\nlats_prediction = Dense(1,name=\"LATS\")(x)\nlons_prediction = Dense(1,name=\"LONS\")(x)","1c95eb78":"Prediction_Model = Model(Input_Layer,[lats_prediction,lons_prediction])","bbb14b5a":"print(Prediction_Model.summary())","4fe47386":"plot_model(Prediction_Model, to_file='Model.png', show_shapes=True, show_layer_names=True)","5d06cde4":"Prediction_Model.compile(optimizer=COMPILE_OPTIMIZER,loss={\"LATS\":COMPILE_LATS_PREDICTION,\n                                                           \"LONS\":COMPILE_LONS_PREDICTION},\n                         loss_weights=[0.10,1.],metrics=COMPILE_METRICS)","a593ca34":"H\u0131story_Model = Prediction_Model.fit(Train_Prediction,{\"LATS\":Output_Lats,\n                                             \"LONS\":Output_Lons},\n                                  epochs=EPOCH_PERIOD,\n                                  callbacks=[Early_Stopper,Checkpoint_Model,Reduce_Model])","49d1fda4":"Model_Results = Prediction_Model.evaluate(Train_Prediction,[Output_Lats,Output_Lons])","982fc4af":"Test_Lats = Prediction_Model.predict(Train_Prediction[0:10])[0]","12f77614":"Test_Lons = Prediction_Model.predict(Train_Prediction[0:10])[1]","2c3a566b":"print(Test_Lats)","9c2fc81f":"print(Test_Lons)","db70e5f5":"Resized_IMG = []\nfor images in Train_Prediction:\n    \n    Picking_Images = cv2.resize(images,(180,180))\n    Resized_IMG.append(Picking_Images)","811648ee":"print(np.shape(np.array(Resized_IMG)))","939b0f40":"Gan_Train = np.array(Resized_IMG)\n\nGan_Train = Gan_Train.astype(\"float32\")\n\nGan_Train = Gan_Train.reshape(Gan_Train.shape[0],Gan_Train.shape[1],Gan_Train.shape[2],1)","d9fc3250":"print(Gan_Train.shape)","f1801288":"iterations = 60\nvector_noise_shape = 180\ncount_example = 9\nbatch_size = 3\ncount_buffer_time = 60000","8ceb47ac":"seed = tf.random.normal([count_example,vector_noise_shape])","f29011bd":"Train_Data = tf.data.Dataset.from_tensor_slices(Gan_Train).shuffle(count_buffer_time).batch(batch_size)","099495be":"print(Train_Data.element_spec)","06117d2c":"def Generator_Model():\n    \n    \n    Model = Sequential()\n    #\n    Model.add(Dense(90*90*128,use_bias=False,input_shape=(180,)))\n    Model.add(BatchNormalization())\n    Model.add(LeakyReLU())\n    #\n    Model.add(Reshape((90,90,128)))\n    #\n    Model.add(Conv2DTranspose(128,(3,3),padding=\"same\",use_bias=False))\n    Model.add(BatchNormalization())\n    Model.add(LeakyReLU())\n    \n    Model.add(Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', use_bias=False))\n    Model.add(BatchNormalization())\n    Model.add(LeakyReLU())\n    #\n    Model.add(Conv2DTranspose(1,(3,3),padding=\"same\",use_bias=False,activation=\"tanh\"))\n    \n    return Model","905fcaf5":"Generator = Generator_Model()","b7892aea":"print(Generator.summary())","cbeec38c":"def Discriminator_Model():\n    \n    model = Sequential()\n    \n    model.add(Conv2D(64,(3,3),padding=\"same\",input_shape=[180,180,1]))\n    model.add(Dropout(0.3))\n    model.add(LeakyReLU())\n    \n    \n    model.add(Conv2D(128,(3,3),padding=\"same\"))\n    model.add(Dropout(0.3))\n    model.add(LeakyReLU())\n    \n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n    \n    return model","63c1e8c0":"Discriminator = Discriminator_Model()","41ccd23c":"print(Discriminator.summary())","c34f9ffd":"Generator_Optimizer = tf.keras.optimizers.RMSprop(lr=0.0004,clipvalue=1.0,decay=1e-8)\nDiscriminator_Optimizer = tf.keras.optimizers.RMSprop(lr=0.0004,clipvalue=1.0,decay=1e-8)","6299117e":"Loss_Function = tf.keras.losses.BinaryCrossentropy(from_logits=True)","6f80a634":"def Discriminator_Loss(real_out,fake_out):\n    \n    real_loss = Loss_Function(tf.ones_like(real_out),real_out)\n    fake_loss = Loss_Function(tf.zeros_like(fake_out),fake_out)\n    total_loss = real_loss + fake_loss\n    \n    return total_loss\n\ndef Generator_Loss(fake_out):\n    \n    return Loss_Function(tf.ones_like(fake_out),fake_out)","209e25f7":"def display_and_save_images(model, epoch, test_input):\n    \n    predictions = model(test_input, training=False)\n    fig = plt.figure(figsize=(12, 12))\n    \n    for i in range(predictions.shape[0]):\n        plt.subplot(4, 4, i+1)\n        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5)\n        plt.axis('off')\n\n    plt.savefig('cloud_image{:04d}.png'.format(epoch))\n    plt.show()","fc14f09f":"def Train_Step(images):\n    \n    random_noise_vector = tf.random.normal([batch_size,vector_noise_shape])\n    \n    with tf.GradientTape() as Generator_Tape, tf.GradientTape() as Discriminator_Tape:\n        Generator_Fake_Images = Generator(random_noise_vector,training=False)\n        \n        real_out = Discriminator(images,training=True)\n        fake_out = Discriminator(Generator_Fake_Images,training=True)\n        \n        Generator_Loss_Out = Generator_Loss(fake_out)\n        Discriminator_Loss_Out = Discriminator_Loss(real_out,fake_out)\n        \n    Generator_Gradients = Generator_Tape.gradient(Generator_Loss_Out,Generator.trainable_variables)\n    Discriminator_Gradients = Discriminator_Tape.gradient(Discriminator_Loss_Out,Discriminator.trainable_variables)\n    \n    Generator_Optimizer.apply_gradients(zip(Generator_Gradients,Generator.trainable_variables))\n    Discriminator_Optimizer.apply_gradients(zip(Discriminator_Gradients,Discriminator.trainable_variables))","3675d7fb":"def Training(dataset,iterations):\n    \n    for epoch in range(iterations):\n        \n        start = time.time()\n        \n        for image_batch in dataset:\n            Train_Step(image_batch)\n            \n        display.clear_output(wait=True)\n        display_and_save_images(Generator,epoch+1,seed)\n    \n    display.clear_output(wait=True)\n    display_and_save_images(Generator,epoch,seed)","902eb009":"Training(Train_Data,iterations)","e6613994":"Predict_Generator_Noise = tf.random.normal(shape=[50,vector_noise_shape])","1884b9d9":"Generator_Predict = Generator(Predict_Generator_Noise)","4353c1b0":"figure, axes = plt.subplots(nrows=5,ncols=5,figsize=(10,10))\n\nfor i,ax in enumerate(axes.flat):\n    Prediction_Output = Generator_Predict[i]\n    ax.imshow(Prediction_Output,cmap=\"gray\")\n    ax.set_xlabel(Generator_Predict[i].shape)\nplt.tight_layout()\nplt.show()","a3a764d5":"figure, axes = plt.subplots(nrows=5,ncols=5,figsize=(10,10))\n\nfor i,ax in enumerate(axes.flat):\n    Prediction_Output = Generator_Predict[i]\n    ax.imshow(Prediction_Output,cmap=\"jet\")\n    ax.set_xlabel(Generator_Predict[i].shape)\nplt.tight_layout()\nplt.show()","0d7b5fdc":"figure = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.imshow(Generator_Predict[2],cmap=\"jet\")\nplt.show()","2a4f71ce":"figure = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.imshow(Generator_Predict[22],cmap=\"jet\")\nplt.show()","6baeb12d":"figure = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.imshow(Generator_Predict[32],cmap=\"jet\")\nplt.show()","f8361c9d":"figure = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.imshow(Generator_Predict[44],cmap=\"jet\")\nplt.show()","5f37cab8":"figure = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.imshow(Generator_Predict[48],cmap=\"jet\")\nplt.show()","83168a8c":"#### TIMESTAMPS PATH","f5b6a4b8":"#### TRANSFORMATION FOR PREDICTION","120c2a52":"#### ENCODER","8155c6cb":"# DATA PROCESS","c012674a":"# PREDICTION PROCESS","5f6037d1":"#### LATITUDE AND LONGITUDE","9e81e0dc":"#### TRAIN","6ad9d0f3":"#### STRUCTURE","832d919d":"#### TIMESTAMPS","d3c7bad5":"* Please do not focus on accuracy, there is an imbalance with the data. But this is the general logic and the way it works.","9077aa65":"#### PREDICTION","7b80c34d":"#### COORDINATE PATH","386e62c3":"#### SPLITTING FOR AUTO ENCODER","ecccab4b":"# PACKAGES AND LIBRARIES","7234ae12":"#### CALLBACKS","39fd9d03":"#### PARAMETERS","e01f685f":"#### EXPORTING","6195b91f":"#### TARGET PATH","85072394":"# AUTO ENCODER PROCESS","7e9bf403":"#### CALLBACKS","ad7e69ef":"# DC-GAN STEPS","5c2360c5":"#### TRANSFORMATION FOR AUTO ENCODER","4f793f45":"#### EXPORTING FIRST 5 IMAGES EXAMPLE","84f6ca3a":"#### DECODER","e4bcebf0":"#### SINGLE EXAMPLE IMAGE","c013ae33":"# HISTORY\n\n* The CloudCast dataset contains 70080 cloud-labeled satellite images with 10 different cloud types corresponding to multiple layers of the atmosphere. The raw satellite images come from a satellite constellation in geostationary orbit centred at zero degrees longitude and arrive in 15-minute intervals from the European Organisationfor Meteorological Satellites (EUMETSAT). The resolution of these images is 3712 x 3712 pixels for the full-disk of Earth, which implies that every pixel corresponds to a space of dimensions 3x3km. This is the highest possible resolution from European geostationary satellites when including infrared channels. Some pre- and post-processing of the raw satellite images are also being done by EUMETSAT before being exposed to the public, such as removing airplanes. We collect all the raw multispectral satellite images and annotate them individually on a pixel-level using a segmentation algorithm. The full dataset then has a spatial resolution of 928 x 1530 pixels recorded with 15-min intervals for the period 2017-2018, where each pixel represents an area of 3\u00d73 km. To enable standardized datasets for benchmarking computer vision methods, this includes a full-resolution gray-scaled dataset centered and projected dataset over Europe (728\u00d7728).\n\n#### Citation\n* If you use this dataset in your research or elsewhere, please cite\/reference the following paper:\n* CloudCast: A Satellite-Based Dataset and Baseline for Forecasting Clouds","f6edef75":"#### AUTO ENCODER","8aa8930d":"# PATH & LABEL & INFORMATION","99de04df":"#### SINGLE EXAMPLE IMAGE PATH"}}