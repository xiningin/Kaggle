{"cell_type":{"92a8000a":"code","425a9c4d":"code","65c8d443":"code","931d73ea":"code","7629693c":"code","47c66d63":"code","2f424cfa":"code","d8e2751b":"code","4e556455":"code","b74f5a87":"code","66f87076":"code","1664ba2c":"code","a5ac8836":"code","56b3824d":"code","1610509e":"code","b22ce97a":"code","8f67dabc":"code","4688999c":"code","a2de80d7":"code","0ba5cbd4":"code","c44119f5":"code","3feb6070":"code","f6628220":"code","fa0095d2":"code","5030b551":"code","ab612587":"code","09cbf2e2":"code","88cfbed5":"code","2f421585":"code","56ad70d4":"code","c5e9ecf9":"code","bb45b330":"code","80451a48":"code","1a086ffa":"code","63a694ab":"code","f63e6845":"code","492a4a48":"code","60507a67":"code","05bd16dd":"code","11eae0d4":"code","ee7d14e8":"code","19dcd261":"code","2910d5fb":"code","bdb4c5f3":"code","685ea072":"code","5669db3b":"code","8123fa21":"code","586685c9":"code","9d372cf0":"code","454846dd":"code","9af6c0db":"code","b6cf1aed":"code","89a97e1a":"code","7e27ae1b":"code","d7f1243b":"code","a7ab3640":"code","df433fdc":"code","e27518f3":"code","8ff41dbd":"code","2e138fc8":"code","7f80c883":"code","f4de493d":"code","101a1601":"code","e8544b1e":"code","ef834172":"code","17f5d521":"code","51561612":"code","fdc79245":"code","4fc803cc":"code","c43759f5":"code","174e76c7":"code","958ee3af":"code","b8a4aaac":"code","29a1105d":"code","7f760eea":"code","a7e217ac":"code","326d5ed0":"code","4ee391a9":"code","e7a28d11":"code","5c0556b0":"code","892c148e":"code","a5fd61c6":"code","1945de7c":"code","6c72660a":"code","0646363f":"code","30886209":"code","c5ad9231":"code","f10d58e7":"code","1ec47559":"code","327c52ea":"code","d410435c":"code","b905744c":"code","1391d78c":"code","3423a367":"code","c9f85cd0":"code","b905d753":"code","b346ab13":"code","f14ffe16":"code","b5d5f33d":"code","b36cfba0":"code","59fcc765":"code","f76bf7d7":"code","0639c396":"code","49fb6951":"code","10ea643e":"code","5d08996d":"code","b4a4d7c4":"code","66beb5ab":"code","c3ca37fe":"code","5f56afdf":"code","2685ef64":"code","2ab46ad9":"code","2bae8be2":"code","19346e3e":"code","4414627f":"code","d6ac063b":"code","c6934180":"code","34d02221":"code","d4b3d365":"code","f56937f3":"code","d6b3ba9e":"code","78f908f9":"code","b77219e6":"code","c0e5f21c":"code","ab2892d6":"code","5450b711":"code","11a90f72":"code","8712fc34":"code","e9934d1f":"code","f163ae89":"code","fb734b75":"code","448aaeba":"code","e3a9fee0":"code","0fc68d92":"code","bc6f49fa":"code","c05cbd6a":"code","35cebf53":"code","8c4baf04":"code","6b0b1979":"markdown","99adac5b":"markdown","7a569b19":"markdown","a9f2cacb":"markdown","77331aa3":"markdown","eaac5c81":"markdown","709b5635":"markdown","aafbab54":"markdown","4099884a":"markdown","49be0fed":"markdown","b37461a9":"markdown","168e976b":"markdown","a4968801":"markdown","beebb72e":"markdown","c4b49e54":"markdown","98e2926e":"markdown","47dc0988":"markdown","4a55e92b":"markdown","8b757e2f":"markdown","57f9ada1":"markdown","a33818aa":"markdown","89682621":"markdown","15e6810f":"markdown","65f2d830":"markdown"},"source":{"92a8000a":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.express as px \nimport pylab\nfrom scipy import stats\nfrom scipy.stats import zscore, probplot\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom tensorflow.keras import layers\nimport tensorflow as tf\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.python.keras.layers import Dense, Embedding,Flatten, Dense, LSTM, Dropout, Bidirectional, Conv1D, MaxPooling1D\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.layers import Dropout\nfrom tensorflow.keras.layers import Dense, BatchNormalization\nfrom tensorflow.keras.initializers import RandomNormal, Constant\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.python.keras.models import load_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nimport re\nimport nltk \nnltk.download(\"stopwords\")\nnltk.download(\"punkt\")\nfrom nltk.corpus import stopwords\nnltk.download('punkt')\nimport warnings\nwarnings.filterwarnings(\"ignore\")","425a9c4d":"train = pd.read_csv('..\/input\/iba-ml1-final-project\/train.csv')\ntest = pd.read_csv('..\/input\/iba-ml1-final-project\/test.csv')","65c8d443":"train.duplicated().sum()\ntest.duplicated().sum()","931d73ea":"train.describe()\n#from here we can see that the column \"Recommended\" infered as an integer(numeric) variable.","7629693c":"#in order to make sure let's get a types of variables in train and test data\nprint(train.dtypes) ","47c66d63":"print('We have {} training rows and {} test rows.'.format(train.shape[0], test.shape[0]))\nprint('We have {} training columns and {} test columns.'.format(train.shape[1], test.shape[1]))\nprint(train.shape)\nprint(test.shape)","2f424cfa":"null_df = pd.Series(pd.isnull(train).sum(), name = 'Null').to_frame()\ndisplay(null_df.loc[null_df['Null'] != 0], train.loc[train.isnull().any(axis = 1)])\n\nsns.heatmap(data = train.isnull(), yticklabels = False, cbar = False, cmap = 'magma')\nplt.title(label = 'Missing Values by Variable', size = 16)","d8e2751b":"null_variables = []\nfor x in null_df.loc[null_df['Null'] != 0].index:\n    null_variables.append(x)\nfor var in null_variables:\n    print('In the train data we have {:.2f} missing values for '.format(train[var].isnull().sum())+ '' + var + ' variable.')\n    print('Missing values form {:.2f}% of total records for'.format((train[var].isnull().sum() \/ train.shape[0]) * 100) + \n          ' ' + var + ' variable.')\n    print(\"===============================================================\")","4e556455":"null_df = pd.Series(pd.isnull(test).sum(), name = 'Null').to_frame()\ndisplay(null_df.loc[null_df['Null'] != 0], test.loc[test.isnull().any(axis = 1)])\n\nsns.heatmap(data = test.isnull(), yticklabels = False, cbar = False, cmap = 'magma')\nplt.title(label = 'Missing Values by Variable', size = 16)","b74f5a87":"null_variables = []\n\nfor x in null_df.loc[null_df['Null'] != 0].index:\n    null_variables.append(x)\n    \nfor var in null_variables:\n    print('In the test data we have {:.2f} missing values for '.format(test[var].isnull().sum())+ '' + var + ' variable.')\n    print('Missing values form {:.2f}% of total records for'.format((test[var].isnull().sum() \/ test.shape[0]) * 100) + \n          ' ' + var + ' variable.')\n    print(\"===============================================================\")","66f87076":"test[test['Department'].isnull()]","1664ba2c":"test[test['Review'].str.contains('leg warmers', case = False, na = False)]\n#we can get that the leg warmers belongs to Initmates, Intimate, Legwear","a5ac8836":"test[test['Review'].str.contains('socks', case = False, na = False)][:5]\n#socks also belong to the Initmates, Intimate, Legwear","56b3824d":"test[test['Review'].str.contains('warm and cozy', case = False, na = False)]\n# so from here we dont have exact choice for the nan values, I will assign Initmates, Initmate, Sleep to this nan ","1610509e":"test.fillna({'Division':'Initmates', 'Department': 'Intimate', 'Product_Category': 'Legwear'}, inplace=True)","b22ce97a":"imputer = SimpleImputer(strategy='most_frequent')\ntrain['Division']=imputer.fit_transform(train[['Division']])\ntrain['Department'] = imputer.fit_transform(train[['Department']])\ntrain['Product_Category'] = imputer.fit_transform(train[['Product_Category']])","8f67dabc":"print(test.isnull().sum())\nprint('-----------------------------')\nprint(train.isnull().sum())","4688999c":"train.columns","a2de80d7":"plt.figure()\nsns.distplot(a = train['Pos_Feedback_Cnt'], kde = True, rug = True, color = 'teal')\nplt.title(label = 'Normality Test for Positive Feedback Count', size = 16)\nplt.xlabel(xlabel = 'Positive Feedback Count', size = 16)\n\nplt.figure()\nsns.distplot(a = train['Age'], kde = True, rug = True, color = 'darkorange')\nplt.title(label = 'Normality Test for Age', size = 16)\nplt.xlabel(xlabel = 'Age', size = 16)","0ba5cbd4":"probplot(x = train['Pos_Feedback_Cnt'], plot = pylab)\nplt.title(label = 'QQ Plot', size = 16)\nplt.ylabel(ylabel = 'Ordered Values', size = 16)\nplt.xlabel(xlabel = 'Positive Feedback Count', size = 16)\npylab.show()\n\nprobplot(x = train['Age'], plot = pylab)\nplt.title(label = 'QQ Plot', size = 16)\nplt.ylabel(ylabel = 'Ordered Values', size = 16)\nplt.xlabel(xlabel = 'Age', size = 16)\npylab.show()\n#positive feedback count is not normally distirbuted but age is","c44119f5":"train['Product_Category'].value_counts().plot(kind = 'bar')","3feb6070":"train[['Division', 'Department', 'Product_Category']].value_counts().plot(kind = 'bar')","f6628220":"color = {\n    \"boxes\": \"DarkGreen\",\n    \"whiskers\": \"DarkOrange\",\n    \"medians\": \"DarkBlue\",\n    \"caps\": \"Gray\",\n}\n \ntrain.plot.box(figsize=(25,10),color=color, sym=\"grey\")","fa0095d2":"px.histogram(train, x = train['Rating'])","5030b551":"fig = px.histogram(train, x=\"Product_Category\" ,barmode=\"group\", color=\"Recommended\")\nfig.show()\n\n# mostly recommended and not recommended product category is clothe and knits","ab612587":"plt.figure(figsize=(10,10))\nsns.heatmap(train.corr(),linewidths=0.1,vmax=1.0, \n            square=True,  linecolor='white', annot=True, cmap='Blues')","09cbf2e2":"corr_w_target = train.corr().iloc[:-1, -1].sort_values(ascending=False)\nplt.figure(figsize=(9, 9))\nsns.barplot(x=corr_w_target.values, y=corr_w_target.index)\nplt.title('Correlation with target variable')\n#rating and recommended is highly correlated","88cfbed5":"fig = px.histogram(train['Recommended'], color = train['Recommended'], title = 'Proportion Target Class')\nfig.show()\n#class 1 is more than class 0","2f421585":"fig = px.pie(train, values='Rating', names='Rating', title='Ratings Proportion', hole = 0.6, color = 'Rating',\n             color_discrete_map={5:'darkblue',4:'blue',3:'royalblue',2:'cyan' , 1: 'lightcyan'})\nfig.show()\n#rating of 5 is more, it means customers who buy the products gives a rating of 5 to it.","56ad70d4":"px.scatter(train, x=\"Age\", y=\"Pos_Feedback_Cnt\", facet_col=\"Rating\",trendline=\"ols\",category_orders={\"Rating\": [1,2,3,4,5],'Recommended':[0,1]})","c5e9ecf9":"px.box(train, x=\"Age\", y=\"Division\",color = 'Recommended')","bb45b330":"general = train.loc[train['Division'] == 'General',:]\nratings = list(general['Rating'])\nprint(\"Average Rating of General division is:\",sum(ratings)\/len(ratings))","80451a48":"general = train.loc[train['Division'] == 'General']\nx_axis = general['Rating'].value_counts().index.tolist()\nrating_value = general['Rating'].value_counts().values.tolist()\n\nsns.barplot(x_axis, rating_value, alpha=0.8)\nplt.xlabel('General', fontsize=14)\nplt.ylabel('Number of that rating given', fontsize=14)\nplt.title(\"Rating for General division\", fontsize=16)\nplt.show()","1a086ffa":"general_petite = train.loc[train['Division'] == 'General Petite',:]\nratings = list(general['Rating'])\nprint(\"Average Rating of General Petite division is:\",sum(ratings)\/len(ratings))","63a694ab":"general_petite = train.loc[train['Division'] == 'General Petite']\nx_axis = general_petite['Rating'].value_counts().index.tolist()\nrating_value = general_petite['Rating'].value_counts().values.tolist()\n\nsns.barplot(x_axis, rating_value, alpha=0.8)\nplt.xlabel('General', fontsize=14)\nplt.ylabel('Number of that rating given', fontsize=14)\nplt.title(\"Rating for General division\", fontsize=16)\nplt.show()","f63e6845":"initmate = train.loc[train['Division'] == 'Initmates',:]\nratings = list(initmate['Rating'])\nprint(\"Average Rating of Initmate division is:\",sum(ratings)\/len(ratings))","492a4a48":"initmate = train.loc[train['Division'] == 'Initmates']\nx_axis = initmate['Rating'].value_counts().index.tolist()\nrating_value = initmate['Rating'].value_counts().values.tolist()\n\nsns.barplot(x_axis, rating_value, alpha=0.8)\nplt.xlabel('General', fontsize=14)\nplt.ylabel('Number of that rating given', fontsize=14)\nplt.title(\"Rating for Initmates division\", fontsize=16)\nplt.show()","60507a67":"totalreviews = list(train['Review'])\nlength = []\nfor i in range(0,len(totalreviews)):\n        totalreviews[i] = str(totalreviews[i])\n        a = len(totalreviews[i].split(' '))\n        length.append(a)\n\n    \nprint(\"On average a review has about:\", sum(length)\/len(length),\"words in them\")","05bd16dd":"dt = pd.DataFrame()\ndt['length'] =  length\ndt['ratings'] =  list(train['Rating'])\nfive_star = dt.loc[dt['ratings'] == 5,:]\nfive = sum(five_star['length'])\/len(five_star['length'])\nfour_star = dt.loc[dt['ratings'] == 4,:]\nfour = sum(four_star['length'])\/len(four_star['length'])\nthree_star = dt.loc[dt['ratings'] == 3,:]\nthree = sum(three_star['length'])\/len(three_star['length'])\nto_star = dt.loc[dt['ratings'] == 2,:]\nto = sum(to_star['length'])\/len(to_star['length'])\non_star = dt.loc[dt['ratings'] == 1,:]\non = sum(on_star['length'])\/len(on_star['length'])","11eae0d4":"colors = ['gold', 'orange','yellowgreen', 'lightcoral', 'lightskyblue']\ntop = ['one','two','three','four','five']\nvalue = [int(on), int(to),int(three),int(four),int(five)]\nsns.barplot(top, value, alpha=0.8)\nplt.xlabel('Rating of the product', fontsize=14)\nplt.ylabel('Average number of words in the review', fontsize=14)\nplt.title(\"Rating given vs Number of words used in review\", fontsize=16)\nplt.show()","ee7d14e8":"px.scatter(test, x=\"Age\", y=\"Pos_Feedback_Cnt\", trendline=\"ols\")","19dcd261":"plt.figure(figsize=(10,10))\nsns.heatmap(test.corr(),linewidths=0.1,vmax=1.0, \n            square=True,  linecolor='white', annot=True, cmap='Blues')","2910d5fb":"fig = px.histogram(test, x=\"Product_Category\" ,barmode=\"group\" )\nfig.show()","bdb4c5f3":"px.box(test, x=\"Age\", y=\"Division\")","685ea072":"plt.figure()\nsns.distplot(a = test['Pos_Feedback_Cnt'], kde = True, rug = True, color = 'teal')\nplt.title(label = 'Normality Test for Positive Feedback Count', size = 16)\nplt.xlabel(xlabel = 'Positive Feedback Count', size = 16)\n\nplt.figure()\nsns.distplot(a = test['Age'], kde = True, rug = True, color = 'darkorange')\nplt.title(label = 'Normality Test for Age', size = 16)\nplt.xlabel(xlabel = 'Age', size = 16)","5669db3b":"probplot(x = test['Pos_Feedback_Cnt'], plot = pylab)\nplt.title(label = 'QQ Plot', size = 16)\nplt.ylabel(ylabel = 'Ordered Values', size = 16)\nplt.xlabel(xlabel = 'Positive Feedback Count', size = 16)\npylab.show()\n\nprobplot(x = test['Age'], plot = pylab)\nplt.title(label = 'QQ Plot', size = 16)\nplt.ylabel(ylabel = 'Ordered Values', size = 16)\nplt.xlabel(xlabel = 'Age', size = 16)\npylab.show()","8123fa21":"len(train['Product_Category'].unique())","586685c9":"train['Product_Category'].unique()","9d372cf0":"len(test['Product_Category'].unique())","454846dd":"test['Product_Category'].unique()\n#in the test data we dont have a product category of chemises and casual bottoms, this can be a problem in our further evaluations and model. So we can remove \n#them from train data\n","9af6c0db":"train.drop(train[train['Product_Category'] == 'Chemises'].index, inplace = True)\ntrain.drop(train[train['Product_Category'] == 'Casual bottoms'].index, inplace = True)","b6cf1aed":"train[['Review']]=SimpleImputer(strategy='most_frequent').fit_transform(train[['Review']])\ntest[['Review']]=SimpleImputer(strategy='most_frequent').fit_transform(test[['Review']])\ntrain[['Review_Title']]=SimpleImputer(strategy='most_frequent').fit_transform(train[['Review_Title']])\ntest[['Review_Title']]=SimpleImputer(strategy='most_frequent').fit_transform(test[['Review_Title']])","89a97e1a":"#train['Review'] = train['Review'].astype('str')","7e27ae1b":"contractions = {\n    \"I'm\": \"I am\",\n    \"i'm\": \"i am\",\n    \"I've\": \"I have\",\n    \"I'll\": \"I will\",\n    \"I'll've\": \"I will have\",\n    \"I'd\": \"I would\",\n    \"I'd've\": \"I would have\",\n    \"Whatcha\": \"What are you\",\n    \"amn't\": \"am not\",\n    \"ain't\": \"are not\",\n    \"aren't\": \"are not\",\n    \"'cause\": \"because\",\n    \"can't\": \"can not\",\n    \"can't've\": \"can not have\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"couldn't've\": \"could not have\",\n    \"daren't\": \"dare not\",\n    \"daresn't\": \"dare not\",\n    \"dasn't\": \"dare not\",\n    \"didn't\": \"did not\",\n    \"didn\u2019t\": \"did not\",\n    \"don't\": \"do not\",\n    \"don\u2019t\": \"do not\",\n    \"doesn't\": \"does not\",\n    \"e'er\": \"ever\",\n    \"everyone's\": \"everyone is\",\n    \"finna\": \"fixing to\",\n    \"gimme\": \"give me\",\n    \"gon't\": \"go not\",\n    \"gonna\": \"going to\",\n    \"gotta\": \"got to\",\n    \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he've\": \"he have\",\n    \"he's\": \"he is\",\n    \"he'll\": \"he will\",\n    \"he'll've\": \"he will have\",\n    \"he'd\": \"he would\",\n    \"he'd've\": \"he would have\",\n    \"here's\": \"here is\",\n    \"how're\": \"how are\",\n    \"how'd\": \"how did\",\n    \"how'd'y\": \"how do you\",\n    \"how's\": \"how is\",\n    \"how'll\": \"how will\",\n    \"isn't\": \"is not\",\n    \"it's\": \"it is\",\n    \"'tis\": \"it is\",\n    \"'twas\": \"it was\",\n    \"it'll\": \"it will\",\n    \"it'll've\": \"it will have\",\n    \"it'd\": \"it would\",\n    \"it'd've\": \"it would have\",\n    \"kinda\": \"kind of\",\n    \"let's\": \"let us\",\n    \"luv\": \"love\",\n    \"ma'am\": \"madam\",\n    \"may've\": \"may have\",\n    \"mayn't\": \"may not\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\n    \"needn't've\": \"need not have\",\n    \"ne'er\": \"never\",\n    \"o'\": \"of\",\n    \"o'clock\": \"of the clock\",\n    \"ol'\": \"old\",\n    \"oughtn't\": \"ought not\",\n    \"oughtn't've\": \"ought not have\",\n    \"o'er\": \"over\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"shalln't\": \"shall not\",\n    \"shan't've\": \"shall not have\",\n    \"she's\": \"she is\",\n    \"she'll\": \"she will\",\n    \"she'd\": \"she would\",\n    \"she'd've\": \"she would have\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\n    \"so's\": \"so is\",\n    \"somebody's\": \"somebody is\",\n    \"someone's\": \"someone is\",\n    \"something's\": \"something is\",\n    \"sux\": \"sucks\",\n    \"that're\": \"that are\",\n    \"that's\": \"that is\",\n    \"that'll\": \"that will\",\n    \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\n    \"em\": \"them\",\n    \"there're\": \"there are\",\n    \"there's\": \"there is\",\n    \"there'll\": \"there will\",\n    \"there'd\": \"there would\",\n    \"there'd've\": \"there would have\",\n    \"these're\": \"these are\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"they'll\": \"they will\",\n    \"they'll've\": \"they will have\",\n    \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\n    \"this's\": \"this is\",\n    \"those're\": \"those are\",\n    \"to've\": \"to have\",\n    \"wanna\": \"want to\",\n    \"wasn't\": \"was not\",\n    \"we're\": \"we are\",\n    \"we've\": \"we have\",\n    \"we'll\": \"we will\",\n    \"we'll've\": \"we will have\",\n    \"we'd\": \"we would\",\n    \"we'd've\": \"we would have\",\n    \"weren't\": \"were not\",\n    \"what're\": \"what are\",\n    \"what'd\": \"what did\",\n    \"what've\": \"what have\",\n    \"what's\": \"what is\",\n    \"what'll\": \"what will\",\n    \"what'll've\": \"what will have\",\n    \"when've\": \"when have\",\n    \"when's\": \"when is\",\n    \"where're\": \"where are\",\n    \"where'd\": \"where did\",\n    \"where've\": \"where have\",\n    \"where's\": \"where is\",\n    \"which's\": \"which is\",\n    \"who're\": \"who are\",\n    \"who've\": \"who have\",\n    \"who's\": \"who is\",\n    \"who'll\": \"who will\",\n    \"who'll've\": \"who will have\",\n    \"who'd\": \"who would\",\n    \"who'd've\": \"who would have\",\n    \"why're\": \"why are\",\n    \"why'd\": \"why did\",\n    \"why've\": \"why have\",\n    \"why's\": \"why is\",\n    \"will've\": \"will have\",\n    \"won't\": \"will not\",\n    \"won't've\": \"will not have\",\n    \"would've\": \"would have\",\n    \"wouldn't\": \"would not\",\n    \"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\n    \"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\n    \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\",\n    \"you're\": \"you are\",\n    \"you've\": \"you have\",\n    \"you'll've\": \"you shall have\",\n    \"you'll\": \"you will\",\n    \"you'd\": \"you would\",\n    \"you'd've\": \"you would have\"\n }","d7f1243b":"def cont_2_ex(x):\n    if type(x) is str:\n        x = x.replace('\\\\', '')\n        for key in contractions:\n            value = contractions[key]\n            x = x.replace(key, value)\n        return x \n    else:\n        return x","a7ab3640":"x = \"\\n\\nthe 'shawl' portion is only attached at the very top of the shoulders,\\n\\nvery disappointed.\"\nprint(cont_2_ex(x))","df433fdc":"train['Review'] = train['Review'].apply(lambda x: cont_2_ex(x))","e27518f3":"test['Review'] = test['Review'].apply(lambda x: cont_2_ex(x))","8ff41dbd":"err1 = train['Review'].str.extractall(\"(&amp)\")\nerr2 = train['Review'].str.extractall(\"(\\xa0)\")","2e138fc8":"print('with &amp',len(err1[~err1.isna()]))\nprint('with (\\xa0)',len(err2[~err2.isna()]))","7f80c883":"train['Review'] = train['Review'].str.replace('(&amp)','')\ntrain['Review'] = train['Review'].str.replace('(\\xa0)','')","f4de493d":"err1 = train['Review'].str.extractall(\"(&amp)\")\nprint('with &amp',len(err1[~err1.isna()]))\nerr2 = train['Review'].str.extractall(\"(\\xa0)\")\nprint('with (\\xa0)',len(err2[~err2.isna()]))\ntrain.head()","101a1601":"test['Review'] = test['Review'].str.replace('(&amp)','')\nerr1 = test['Review'].str.extractall(\"(&amp)\")\nprint('with &amp',len(err1[~err1.isna()]))\ntest.head()","e8544b1e":"  # Removing digits\ntrain['Review'] = train['Review'].str.replace(r\"\\&\\#[0-9]+\\;\", ' ', regex = True)\ntrain['Review_Title'] = train['Review_Title'].str.replace(r\"\\&\\#[0-9]+\\;\", ' ', regex = True)\ntrain['Review'] = train['Review'].str.replace('\\d+', '')\ntrain['Review_Title'] = train['Review_Title'].str.replace('\\d+', '')\n\n    # remove all single characters\npattern = r'\\s+[a-zA-Z]\\s+'\ntrain['Review'] = train['Review'].str.replace(pat=pattern, repl=\" \", regex=True)\ntrain['Review_Title'] = train['Review_Title'].str.replace(pat=pattern, repl=\"\", regex=True)\n\n    # Remove single characters from the start\npattern = r'\\^[a-zA-Z]\\s+'\n\ntrain['Review'] = train['Review'].str.replace(pat=pattern, repl=\" \", regex=True)\ntrain['Review_Title'] = train['Review_Title'].str.replace(pat=pattern, repl=\"\", regex=True)\n\n    # Removing prefixed 'b'\npattern = r'^b\\s+'\n\ntrain['Review'] = train['Review'].str.replace(pat=pattern, repl=\" \", regex=True)\ntrain['Review_Title'] = train['Review_Title'].str.replace(pat=pattern, repl=\"\", regex=True)\n\n # Substituting multiple spaces with single space\npattern = r\"[\\s]+\"\n\ntrain['Review'] = train['Review'].str.replace(pat=pattern, repl=\" \", regex=True)\n#not implemented for Review Title because most of phrases here, have single space\n\n# Converting to a lower case\ntrain['Review'] = train['Review'].str.lower()\ntrain['Review_Title'] = train['Review_Title'].str.lower()\n\n# Word Net Lemmatizer\nlemmatizer = WordNetLemmatizer()\ntrain['Review'] = [lemmatizer.lemmatize(i) for i in train['Review']]\ntrain['Review_Title'] = [lemmatizer.lemmatize(i) for i in train['Review_Title']]\n\n# Stopwords\ntrain['Review'] = train['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in set(stopwords.words('english'))))\ntrain['Review_Title'] = train['Review_Title'].apply(lambda x: \" \".join(x for x in x.split() if x not in set(stopwords.words('english'))))\n","ef834172":"  # Removing digits\ntest['Review'] = test['Review'].str.replace(r\"\\&\\#[0-9]+\\;\", ' ', regex = True)\ntest['Review_Title'] = test['Review_Title'].str.replace(r\"\\&\\#[0-9]+\\;\", ' ', regex = True)\ntest['Review'] = test['Review'].str.replace('\\d+', '')\ntest['Review_Title'] = test['Review_Title'].str.replace('\\d+', '')\n\n\n    # remove all single characters\npattern = r'\\s+[a-zA-Z]\\s+'\n\ntest['Review'] = test['Review'].str.replace(pat=pattern, repl=\" \", regex=True)\n\n    # Remove single characters from the start\npattern = r'\\^[a-zA-Z]\\s+'\n\ntest['Review'] = test['Review'].str.replace(pat=pattern, repl=\" \", regex=True)\ntest['Review_Title'] = test['Review_Title'].str.replace(pat=pattern, repl=\"\", regex=True)\n\n    # Removing prefixed 'b'\npattern = r'^b\\s+'\n\ntest['Review'] = test['Review'].str.replace(pat=pattern, repl=\" \", regex=True)\ntest['Review_Title'] = test['Review_Title'].str.replace(pat=pattern, repl=\"\", regex=True)\n\n # Substituting multiple spaces with single space\npattern = r\"[\\s]+\"\n\ntest['Review'] = test['Review'].str.replace(pat=pattern, repl=\" \", regex=True)\n#not implemented for Review Title because most of phrases here, have single space\n\n# Converting to a lower case\ntest['Review'] = test['Review'].str.lower()\ntest['Review_Title'] = test['Review_Title'].str.lower()\n\n# Word Net Lemmatizer\nlemmatizer = WordNetLemmatizer()\ntest['Review'] = [lemmatizer.lemmatize(i) for i in test['Review']]\ntest['Review_Title'] = [lemmatizer.lemmatize(i) for i in test['Review_Title']]\n\n# Stopwords\ntest['Review'] = test['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in set(stopwords.words('english'))))\ntest['Review_Title'] = test['Review_Title'].apply(lambda x: \" \".join(x for x in x.split() if x not in set(stopwords.words('english'))))\n","17f5d521":"test.isnull().sum()","51561612":"import string\nstring.punctuation\ndef remove_punctuation(text):\n    no_punct=[words for words in text if words not in string.punctuation]\n    words_wo_punct=''.join(no_punct)\n    return words_wo_punct\ntrain['Review']=train['Review'].apply(lambda x: remove_punctuation(x))\ntrain['Review_Title']=train['Review_Title'].apply(lambda x: remove_punctuation(x))\ntrain.head()","fdc79245":"test['Review']=test['Review'].apply(lambda x: remove_punctuation(x))\ntest['Review_Title']=test['Review_Title'].apply(lambda x: remove_punctuation(x))\ntest.head()","4fc803cc":"full_text = '\\n'.join(train['Review'])","c43759f5":"import wordcloud","174e76c7":"plt.figure(figsize=(12, 8))\nwc = wordcloud.WordCloud(background_color='white',\n               width=1280, height=720).generate(full_text)\nplt.imshow(wc, interpolation='bilinear')\nplt.show()","958ee3af":"test","b8a4aaac":"df = train[['Review_Title', 'Review', 'Rating', 'Recommended']]\ndf2 = test[['Id', 'Review_Title', 'Review']]","29a1105d":"df['Merged_Review'] = df['Review_Title'] + ' ' + df['Review']\ndf = df.drop(labels=['Review','Review_Title'] , axis=1)\ndf.head()","7f760eea":"df2['Merged_Review'] = df2['Review_Title'] + ' ' + df2['Review']\ndf2 = df2.drop(labels=['Review','Review_Title'] , axis=1)\ndf2.head()","a7e217ac":"#train set tokenization\ntokenizer=Tokenizer(num_words = 10000, oov_token=\"'oov'\")\ntokenizer.fit_on_texts(df['Merged_Review'])\n#test set tokenization\ntokenizer2=Tokenizer(oov_token=\"'oov'\")\ntokenizer2.fit_on_texts(df2['Merged_Review'])","326d5ed0":"import os\nimport random\ndf[\"split\"] = df.apply(lambda x: \"train\" if random.randrange(0,100) > 10 else \"valid\", axis=1)","4ee391a9":"df[\"split\"].value_counts()","e7a28d11":"df[\"Rating\"].value_counts()","5c0556b0":"df_train = df[df[\"split\"] == \"train\"]\ndf_val = df[df[\"split\"] == \"valid\"]","892c148e":"maxlen = 200\ntrain_X = pad_sequences(tokenizer.texts_to_sequences(df_train['Merged_Review']), maxlen=maxlen)\nval_X = pad_sequences(tokenizer.texts_to_sequences(df_val['Merged_Review']), maxlen=maxlen)","a5fd61c6":"test_clean = pad_sequences(tokenizer2.texts_to_sequences(df2['Merged_Review']), maxlen=maxlen)","1945de7c":"import tensorflow as tf","6c72660a":"y_train = df_train[\"Rating\"]\ny_val = df_val[\"Rating\"]\ntrain_y_class = tf.keras.utils.to_categorical(df_train[\"Rating\"]-1, num_classes=5)\nval_y_class = tf.keras.utils.to_categorical(df_val[\"Rating\"]-1, num_classes=5)","0646363f":"max_words = 40000\nembedding_dim = 128\nmodel=Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\nmodel.add(Bidirectional(LSTM(32)))\nmodel.add(Dense(16, activation=\"relu\"))\nmodel.add(Dense(5, activation=\"softmax\"))\nmodel.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\nprint(model.summary())","30886209":"model.fit(train_X, train_y_class, epochs=4, validation_data=(val_X, val_y_class))","c5ad9231":"tf.keras.utils.plot_model(model)","f10d58e7":"pred = model.predict(test_clean)","1ec47559":"pred","327c52ea":"rating_pred=[]\nfor i in model.predict(test_clean):    \n    index = np.argmax(i)\n    rating_pred.append(index+1)","d410435c":"rating_pred[:10]","b905744c":"train_Y = df_train[\"Recommended\"]\nval_Y = df_val[\"Recommended\"]","1391d78c":"model=Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation=\"relu\"))\nmodel.add(Dropout(0.09))\nmodel.add(Dense(1, activation=\"sigmoid\"))\nmodel.compile(optimizer=\"Adam\", loss='binary_crossentropy', metrics=['accuracy'])","3423a367":"model.fit(train_X, train_Y, epochs=4, validation_data=(val_X, val_Y))","c9f85cd0":"pred = model.predict(test_clean)","b905d753":"recom_pred = pred.round(0)","b346ab13":"submission = df2","f14ffe16":"submission['Rating'] = rating_pred\nsubmission['Recommended'] = recom_pred","b5d5f33d":"submission[['Id', 'Rating', 'Recommended']].to_csv('Separate_submission.csv')","b36cfba0":"train","59fcc765":"X = train[['Age','Review','Review_Title','Pos_Feedback_Cnt','Division','Department','Product_Category']]\ny = train[['Rating','Recommended']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)","f76bf7d7":"cat_features = X_train[['Division', 'Department', 'Product_Category']]","0639c396":"for x in cat_features.columns:\n    print(x ,':', len(cat_features[x].unique()))\n#total number for unique categorical features is 27, we will use this in the next steps","49fb6951":"category = pd.concat([pd.get_dummies(cat_features)], axis=1).values","10ea643e":"category.shape","5d08996d":"categ_var = 27  \nnumeric_vars =2\nnum_words = 10000  \nclass_num = 5  \n\nBatchNormalization(momentum=0.95, epsilon=0.005,beta_initializer=RandomNormal(mean=0.0, stddev=0.05), gamma_initializer=Constant(value=0.9))\n\ntitle_inp = tf.keras.Input(shape=(None,), name=\"title\") \nbody_inp = tf.keras.Input(shape=(None,), name=\"body\") \ndepartment_inp = tf.keras.Input(shape=(categ_var,), name=\"department\")\nnum_inp = tf.keras.Input(shape=(numeric_vars,), name=\"num\")\n################Title features\ntitle_features = layers.Embedding(num_words, 64)(title_inp)\nBatchNormalization()\ntitle_features =  tf.keras.layers.LSTM(64)(title_features,)\nBatchNormalization()\ntitle_features=layers.Dropout(.3)(title_features)\ntitle_features=layers.Dense(16, activation='relu')(title_features)\nBatchNormalization()\n################Body features\nbody_features = layers.Embedding(num_words, 64)(body_inp)\nBatchNormalization()\nbody_features =  tf.keras.layers.LSTM(64)(body_features,)\nBatchNormalization()\nbody_features = layers.Dropout(.3)(body_features)\nbody_features = layers.Dense(16, activation='relu')(body_features)\nBatchNormalization()\n################Categorical features\ndep_features = layers.Dense(16, activation='relu')(department_inp)\nBatchNormalization()\n###############Numeric features\nnum_features = layers.Dense(8, activation='relu')(num_inp)\nBatchNormalization()","b4a4d7c4":"x = layers.concatenate([body_features,title_features,dep_features,num_features])\n\n#Predicting recommended\nrecommended_pred=layers.Dropout(.05)(x)\nrecommended_pred = layers.Dense(1,activation='sigmoid', name=\"recommended\")(recommended_pred)\n#Predicting rating\nrating_pred=layers.Dropout(.05)(x)\nrating_pred = layers.Dense(class_num,activation='softmax', name=\"rating\")(rating_pred)\n#Model\nmodel = tf.keras.Model(inputs=[body_inp,title_inp,department_inp,num_inp], outputs=[recommended_pred, rating_pred])","66beb5ab":"tf.keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)","c3ca37fe":"num_data = pd.concat([X_train['Age'], X_train['Pos_Feedback_Cnt']], axis = 1)","5f56afdf":"num_data = num_data.values","2685ef64":"title = X_train['Review_Title'].values\nbody = X_train['Review'].values\ntitle = np.asarray(title).astype('str')\nbody = np.asarray(body).astype('str')\n\nencoder = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=10000 ,ngrams=1)\nencoder.adapt(X_train['Review'].values)","2ab46ad9":"encoder.get_vocabulary()[50:60]","2bae8be2":"body_en=encoder(body).numpy()\ntitle_en=encoder(title).numpy()\nrecommended_tar = y_train['Recommended'].values\nrating_tar = pd.get_dummies(y_train['Rating']).values","19346e3e":"model.compile(\n    optimizer='Adam', #keras.optimizers.RMSprop(1e-3)\n    loss={\n        \"recommended\": tf.keras.losses.BinaryCrossentropy(),#from_logits=True),\n        \"rating\": tf.keras.losses.CategoricalCrossentropy()#from_logits=True),\n    },\n    metrics = ['accuracy']\n    #loss_weights=[0.2, 0.8],\n)","4414627f":"from tensorflow.keras.callbacks import EarlyStopping","d6ac063b":"custom_early_stopping = EarlyStopping(monitor='val_rating_accuracy',patience=3,min_delta=0.001,mode='auto', restore_best_weights = True)","c6934180":"# fit the model\nhist_train = model.fit({\"body\": body_en,\"title\":title_en, \"department\": category,\"num\":num_data},\n    {\"recommended\": recommended_tar, \"rating\": rating_tar},\n                               validation_split=0.2, \n                               epochs=15, batch_size = 28, callbacks=[custom_early_stopping])\n                               #callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_step_decay,verbose=1)]\n# plot model accuracy\n#plot_fig(1, history_step_decay)","34d02221":"plt.figure(figsize=(12, 6))\nplt.plot(hist_train.history['val_recommended_loss'], label='Validation Recomm Loss')\nplt.plot(hist_train.history['val_rating_loss'], label='Validation Rating Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","d4b3d365":"[recom_output, rating_output]=model.predict([body_en,title_en,category,num_data])","f56937f3":"recom_output=recom_output.round().astype('int64')\nrating_output=rating_output.argmax(axis=1)+1","d6b3ba9e":"category_xt = pd.concat([pd.get_dummies(X_test[['Division', 'Department', 'Product_Category']])], axis=1).values","78f908f9":"category_xt.shape","b77219e6":"num_data_xt = pd.concat([X_test['Age'], X_test['Pos_Feedback_Cnt']], axis = 1).values","c0e5f21c":"num_data_xt.shape","ab2892d6":"title_xt = X_test['Review_Title'].values.astype('str')\nbody_xt = X_test['Review'].values.astype('str')\n\nbody_en_xt=encoder(body_xt).numpy()\ntitle_en_xt=encoder(title_xt).numpy()\n#title_xt = np.asarray(title_xt).astype('str')\n#body_xt = np.asarray(body_xt).astype('str')","5450b711":"title_en_xt.shape","11a90f72":"[recom_output_xt, rating_output_xt]= model.predict([body_en_xt,title_en_xt,category_xt,num_data_xt])","8712fc34":"recom_output_xt=recom_output_xt.round().astype('int64')\nrating_output_xt=rating_output_xt.argmax(axis=1)+1","e9934d1f":"from collections import Counter\nfrom sklearn.metrics import accuracy_score\nprint('Training set performance for recommended :', accuracy_score(y_train['Recommended'].values, recom_output))\nprint('Training set performance for rating:', accuracy_score(y_train['Rating'].values, rating_output))\n\nprint()\nprint('Test set performance for recommended :', accuracy_score(y_test['Recommended'].values, recom_output_xt))\nprint('Test set performance for Rating :', accuracy_score(y_test['Rating'].values,rating_output_xt))","f163ae89":"test","fb734b75":"category_test = pd.concat([pd.get_dummies(test[['Division', 'Department', 'Product_Category']])], axis=1).values","448aaeba":"category_test.shape","e3a9fee0":"num_data_test = pd.concat([test['Age'], test['Pos_Feedback_Cnt']], axis = 1).values","0fc68d92":"num_data_xt.shape","bc6f49fa":"title_test = test['Review_Title'].values.astype('str')\nbody_test = test['Review'].values.astype('str')\nbody_en_test=encoder(body_test).numpy()\ntitle_en_test=encoder(title_test).numpy()","c05cbd6a":"[recom_output_final, rating_output_final]= model.predict([body_en_test,title_en_test,category_test,num_data_test])","35cebf53":"recom_output_final=recom_output_final.round().astype('int64')\nrating_output_final=rating_output_final.argmax(axis=1)+1","8c4baf04":"Id = test['Id']\npred_df = pd.DataFrame({'Rating' : rating_output_final, 'Recommended': recom_output_final})\npred_df.index = Id","6b0b1979":"#### Remove contractions","99adac5b":"#### Average Rating and visualization of them by division","7a569b19":"**Filling the places where we should feed our model**","a9f2cacb":"### Percentage of Missing values for test set","77331aa3":"## EDA for Test data","eaac5c81":"#### Recommended prediction","709b5635":"### Visual of missing values","aafbab54":"### X test data ","4099884a":"#### Visualizing the average number of words per rating\n> Five star ratings are shortest whereas 3 star ratings tend to be the longest","49be0fed":"#### Simple Bag of words model for **rating** prediction","b37461a9":"\nfilling missing values in test","168e976b":"*we cant drop the nan values that's why we need to impute them. For imputation I am looking for the other customer reviews with the key words in them. And find the division, department, prodcut category with the help of this technique.*","a4968801":"### Percentage of Missing values for train set","beebb72e":"> ### Functional API , RNN","c4b49e54":"### Visual of missing values for test set","98e2926e":"### Prediction on test data","47dc0988":"#### Count of average words in per review\n>  average count in each review is 57","4a55e92b":"#### Imputing nan values in Review and Review Title column","8b757e2f":"#### Remove text punctuation","57f9ada1":"> # Text preprocessing","a33818aa":"##### Let's check if our function works","89682621":"#### Test set review processing","15e6810f":">  # EDA","65f2d830":"#### Training set review processing"}}