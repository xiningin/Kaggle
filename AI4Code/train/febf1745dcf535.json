{"cell_type":{"e5639292":"code","1d13b626":"code","360f306b":"code","b3eb2c9c":"code","92758691":"code","d82a2111":"code","ae5fb166":"code","9ef5f83e":"code","3f03736c":"code","a9d7fc0a":"code","f3ac4fdd":"code","19396405":"code","0e886a4a":"code","6b20dbc0":"code","c67fb734":"code","b295f907":"code","0803e38f":"code","cceef4c7":"code","3d1c0ca5":"code","d0d35343":"code","437b7092":"code","36511f0d":"code","ab94b067":"code","57790e59":"code","7c9ce585":"code","f5f5c442":"code","43878ce8":"code","0f222b40":"code","78294d74":"code","893c7783":"code","d76d45c2":"code","c4419b5d":"code","9829ee22":"code","f0744a8b":"code","5d30f4f0":"code","b2c3a6f2":"code","a11793c6":"code","02061a95":"code","563770c0":"code","23cda7ba":"code","ac0ac06d":"code","9122b6f3":"code","4c1cc1dd":"code","4b6e4998":"code","45afbbb2":"code","0d5680c3":"code","e40f7f44":"code","7a667720":"markdown","ca7b8ceb":"markdown","68941c06":"markdown","7dd0ba0f":"markdown","90b2b822":"markdown","281780c5":"markdown","f56aacf3":"markdown","651e36be":"markdown","3cd5deae":"markdown","e2f97322":"markdown","ed569d30":"markdown","a4de2bbf":"markdown","87be81ba":"markdown","f1391e6c":"markdown"},"source":{"e5639292":"import numpy as np\nimport pandas as pd\n# Data visualisation & Preparation\nimport seaborn as sns \nimport matplotlib.pyplot as plt \n%matplotlib inline \nsns.set(color_codes=True)\nimport scikitplot as skplt\nfrom sklearn import preprocessing as prep\nfrom sklearn.utils import resample\nimport imblearn\nfrom imblearn.over_sampling import SMOTE\n\n# Logistic regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix as cm\nfrom sklearn.metrics import plot_confusion_matrix\n# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, make_scorer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\n# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import recall_score\n\n# Support Vector Classifications\nfrom sklearn.svm import SVC\n\n\n","1d13b626":"credit = pd.read_csv('..\/input\/default\/default.csv')\ncredit","360f306b":"credit.info()","b3eb2c9c":"credit.describe()","92758691":"# rename the target column\ncredit.rename(columns={\"default payment next month\": \"default\"}, inplace = True)","d82a2111":"# check unique values in different features\nprint(credit.EDUCATION.unique())\nprint(credit.MARRIAGE.unique())","ae5fb166":"# \u4e0a\u9762\u53ef\u4ee5\u770b\u89c1Education\u591a\u51fa\u6765\u51e0\u7c7b\n\n# \u628a\u6559\u80b2\u7a0b\u5ea64,5,6\uff0c0\u4e0d\u77e5\u9053\u662f\u5565\u7684\u7c7b\u522b\u5168\u5f52\u4e3aothers\nfil = (credit['EDUCATION'] == 5) | (credit['EDUCATION'] == 6) | (credit['EDUCATION'] == 0)\ncredit.loc[fil, 'EDUCATION'] = 4\ncredit['EDUCATION'].value_counts()","9ef5f83e":"# \u5a5a\u59fb\u72b6\u51b5\u540c\u6837\u7684\u64cd\u4f5c\nfil = (credit['MARRIAGE'] == 0)\ncredit.loc[fil, 'MARRIAGE'] = 3\ncredit['MARRIAGE'].value_counts()","3f03736c":"print(credit.EDUCATION.unique())\nprint(credit.MARRIAGE.unique())","a9d7fc0a":"# check data balance\ndef without_hue(plot, feature):\n    total = len(feature)\n    for p in ax.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_height()\/total)\n        x = p.get_x() + p.get_width() \/ 2 - 0.05\n        y = p.get_y() + p.get_height()\n        ax.annotate(percentage, (x, y), size = 12)\n    plt.show()\n\nplt.figure(figsize = (6,6))\nax = sns.countplot('default', data = credit)\nplt.xticks(size = 12)\nplt.xlabel('default', size = 12)\nplt.yticks(size = 12)\nplt.ylabel('count', size = 12)\n\nwithout_hue(ax, credit.default)","f3ac4fdd":"# Eduation\n# 1\uff1a\u7814\u7a76\u751f\u30012\uff1a\u672c\u79d1\u30013\uff1a\u9ad8\u4e2d\u30014\uff1a\u5176\u4ed6\n\npd.crosstab(credit.EDUCATION,credit.default).plot(kind='bar')\nplt.title('Default Frequency by Educational Level')\nplt.xlabel('EDUCATION')\nplt.ylabel('default')\nplt.show()","19396405":"# Marriage\n# 1: \u5df2\u5a5a\u30012\uff1a\u672a\u5a5a\u30013\uff1a\u5176\u4ed6\n\npd.crosstab(credit.MARRIAGE,credit.default).plot(kind='bar')\nplt.title('Default Frequency by Marriage Status')\nplt.xlabel('MARRIAGE')\nplt.ylabel('default')\nplt.show()","0e886a4a":"# SEX\n# 1\uff1a\u7537 \u30012\uff1a\u5973\n\npd.crosstab(credit.SEX,credit.default).plot(kind='bar')\nplt.title('Default Frequency by Gender')\nplt.xlabel('SEX')\nplt.ylabel('default')\nplt.show()","6b20dbc0":"plt.figure(figsize=(10,7), dpi= 80)\nplt.hist(credit['LIMIT_BAL'], bins=30, alpha = 0.5)\nplt.xlabel('LIMIT_BAL')\n\nplt.title(\"The Frequency Histogram of LIMIT_BAL\")\nplt.show()","c67fb734":"plt.figure(figsize=(10,7), dpi= 80)\nplt.hist(credit['AGE'], bins=30, facecolor='red', alpha = 0.5)\nplt.xlabel('AGE')\n\nplt.title(\"The Frequency Histogram of Age\")\nplt.show()","b295f907":"# Creating a new dataframe with categorical variables\nsubset = credit[['SEX', 'EDUCATION', 'MARRIAGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', \n               'PAY_5', 'PAY_6', 'default']]\n\nf, axes = plt.subplots(3, 3, figsize=(20, 15), facecolor='white')\nf.suptitle('Default Frequency of Important Features')\nax1 = sns.countplot(x=\"SEX\", hue=\"default\", data=subset, palette=\"coolwarm\", ax=axes[0,0])\nax2 = sns.countplot(x=\"EDUCATION\", hue=\"default\", data=subset, palette=\"coolwarm\",ax=axes[0,1])\nax3 = sns.countplot(x=\"MARRIAGE\", hue=\"default\", data=subset, palette=\"coolwarm\",ax=axes[0,2])\nax4 = sns.countplot(x=\"PAY_0\", hue=\"default\", data=subset, palette=\"coolwarm\", ax=axes[1,0])\nax5 = sns.countplot(x=\"PAY_2\", hue=\"default\", data=subset, palette=\"coolwarm\", ax=axes[1,1])\nax6 = sns.countplot(x=\"PAY_3\", hue=\"default\", data=subset, palette=\"coolwarm\", ax=axes[1,2])\nax7 = sns.countplot(x=\"PAY_4\", hue=\"default\", data=subset, palette=\"coolwarm\", ax=axes[2,0])\nax8 = sns.countplot(x=\"PAY_5\", hue=\"default\", data=subset, palette=\"coolwarm\", ax=axes[2,1])\nax9 = sns.countplot(x=\"PAY_6\", hue=\"default\", data=subset, palette=\"coolwarm\", ax=axes[2,2]);","0803e38f":"x1 = list(credit[credit['default'] == 1]['LIMIT_BAL'])\nx2 = list(credit[credit['default'] == 0]['LIMIT_BAL'])\n\nplt.figure(figsize=(12,4))\nsns.set_context('notebook', font_scale=1.2)\n#sns.set_color_codes(\"pastel\")\nplt.hist([x1, x2], bins = 40, normed=False, color=['firebrick', 'lightblue'])\nplt.xlim([0,600000])\nplt.legend(['Yes', 'No'], title = 'default', loc='upper right', facecolor='white')\nplt.xlabel('Limit Balance (NT dollar)')\nplt.ylabel('Frequency')\nplt.title('Default Histogram by Limit Balance', SIZE=15)\nplt.box(False)\nplt.show()","cceef4c7":"# \u6559\u80b2\u6c34\u5e73\n# generate binary values using get_dummies\ncredit_dum = pd.get_dummies(credit, columns=[\"EDUCATION\"], prefix=[\"Edu\"] )\n# merge with main df bridge_df on key values\ncredit_dum = credit.merge(credit_dum, how='outer')\ncredit_dum","3d1c0ca5":"# \u6027\u522b\n# generate binary values using get_dummies\ncredit_dum2 = pd.get_dummies(credit, columns=[\"SEX\"], prefix=[\"SEX\"] )\n# merge with main df bridge_df on key values\ncredit_dum2 = credit_dum.merge(credit_dum2, how='outer')\ncredit_dum2","d0d35343":"# \u5a5a\u59fb\n# generate binary values using get_dummies\ncredit_dum3 = pd.get_dummies(credit, columns=[\"MARRIAGE\"], prefix=[\"MARRIAGE\"] )\n# merge with main df bridge_df on key values\ncredit_dum3 = credit_dum2.merge(credit_dum3, how='outer')\ncredit_dum3","437b7092":"# Pay_0\n# generate binary values using get_dummies\ncredit_dum4 = pd.get_dummies(credit, columns=[\"PAY_0\"], prefix=[\"p0\"] )\n# merge with main df bridge_df on key values\ncredit_dum4 = credit_dum3.merge(credit_dum4, how='outer')\ncredit_dum4","36511f0d":"# Pay_2\n# generate binary values using get_dummies\ncredit_dum5 = pd.get_dummies(credit, columns=[\"PAY_2\"], prefix=[\"p2\"] )\n# merge with main df bridge_df on key values\ncredit_dum5 = credit_dum4.merge(credit_dum5, how='outer')\ncredit_dum5","ab94b067":"# Pay_3\n# generate binary values using get_dummies\ncredit_dum6 = pd.get_dummies(credit, columns=[\"PAY_3\"], prefix=[\"p3\"] )\n# merge with main df bridge_df on key values\ncredit_dum6 = credit_dum5.merge(credit_dum6, how='outer')\ncredit_dum6","57790e59":"# Pay_4\n# generate binary values using get_dummies\ncredit_dum7 = pd.get_dummies(credit, columns=[\"PAY_4\"], prefix=[\"p4\"] )\n# merge with main df bridge_df on key values\ncredit_dum7 = credit_dum6.merge(credit_dum7, how='outer')\ncredit_dum7","7c9ce585":"# Pay_5\n# generate binary values using get_dummies\ncredit_dum8 = pd.get_dummies(credit, columns=[\"PAY_5\"], prefix=[\"p5\"] )\n# merge with main df bridge_df on key values\ncredit_dum8 = credit_dum7.merge(credit_dum8, how='outer')\ncredit_dum8","f5f5c442":"# Pay_6\n# generate binary values using get_dummies\ncredit_dum9 = pd.get_dummies(credit, columns=[\"PAY_6\"], prefix=[\"p6\"] )\n# merge with main df bridge_df on key values\ncredit_dum9 = credit_dum8.merge(credit_dum9, how='outer')\ncredit_dum9","43878ce8":"credit_dum9 = credit_dum9.drop(['SEX','EDUCATION','MARRIAGE','PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6'],axis=1)\ncredit_dum9","0f222b40":"credit_dum9.info()","78294d74":"minmax_scale = prep.MinMaxScaler().fit(credit_dum9)\ncredit_minmax = minmax_scale.transform(credit_dum9)\ncredit_minmax = pd.DataFrame(credit_minmax, columns = list(credit_dum9))\ncredit_minmax","893c7783":"credit_minmax.hist(figsize=(20,20))\nplt.show()","d76d45c2":"X = credit_minmax.drop([\"default\"],axis=1)\ny = credit_minmax[\"default\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","c4419b5d":"# describes info about train and test set \nprint(\"Number transactions X_train dataset: \", X_train.shape) \nprint(\"Number transactions y_train dataset: \", y_train.shape) \nprint(\"Number transactions X_test dataset: \", X_test.shape) \nprint(\"Number transactions y_test dataset: \", y_test.shape) ","9829ee22":"# Run the logistic regression model on Imbalanced data\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nprediction = logreg.predict(X_test)\nprint(\"Accuracy:\")\na = accuracy_score(y_test,prediction)\nprint(a)\n\nprediction = dict()\nprediction['Logistic'] = logreg.predict(X_test)\nprint('f1 Score:' ,metrics.f1_score(y_test, prediction['Logistic']))\n\nconfusion_matrix = cm(y_test, prediction['Logistic'])\nprint(confusion_matrix)\n\nprint(classification_report(y_test, prediction['Logistic']))\n\nskplt.metrics.plot_confusion_matrix(y_test, prediction['Logistic'])\nplt.show()\nskplt.metrics.plot_confusion_matrix(y_test,prediction['Logistic'],normalize=True)\nplt.show()","f0744a8b":"# define training set\ndf_train = pd.concat([X_train, y_train],axis=1)\ndf_train","5d30f4f0":"# define test set\ndf_test = pd.concat([X_test, y_test],axis=1)\ndf_test","b2c3a6f2":"# summarize target variable distribution (training set)\ncount_class_0, count_class_1 = df_train.default.value_counts()\ncount_class_0, count_class_1","a11793c6":"# Separate majority and minority classes\ndf_majority = df_train[df_train.default==0]\ndf_minority = df_train[df_train.default==1]\n\ndf_minority_upsampled = df_minority.sample(count_class_0, replace=True)\ndf_upsampled = pd.concat([df_majority,df_minority_upsampled],axis=0)\n \nprint('Random Oversampling:')\nprint(df_upsampled.default.value_counts())\n \n\ndf_upsampled.default.value_counts().plot(kind='bar', title='Count (default)');","02061a95":"X_train_upsampled = df_upsampled.drop([\"default\"],axis=1)\ny_train_upsampled = df_upsampled[\"default\"]","563770c0":"# Run the logistic regression model\nlogreg = LogisticRegression()\nlogreg.fit(X_train_upsampled, y_train_upsampled)\ny_pred = logreg.predict(X_test)\nconfusion_matrix = cm(y_test, y_pred)\nprint(confusion_matrix)\n\nskplt.metrics.plot_confusion_matrix(y_test, y_pred)\nplt.show()\nskplt.metrics.plot_confusion_matrix(y_test,y_pred,normalize=True)\nplt.show()\n\nprint(classification_report(y_test, y_pred))","23cda7ba":"clf = RandomForestClassifier(n_jobs=1000,\n                            random_state=9,\n                            n_estimators=11,\n                            verbose=False)\nclf.fit(X_train_upsampled, y_train_upsampled)\nprediction['RandomForest'] = clf.predict(X_test)\nr = accuracy_score(prediction['RandomForest'],y_test)\nprint(r)\n\nconfusion_matrix = cm(y_test, prediction['RandomForest'])\nprint(confusion_matrix)\n\nskplt.metrics.plot_confusion_matrix(y_test, prediction['RandomForest'])\nplt.show()\nskplt.metrics.plot_confusion_matrix(y_test,prediction['RandomForest'],normalize=True)\nplt.show()\n\nprint(classification_report(y_test, prediction['RandomForest']))","ac0ac06d":"clf_gini = DecisionTreeClassifier(criterion = 'gini', random_state = 100,\n                                 max_depth = 3, min_samples_leaf = 5)\nclf_gini.fit(X_train_upsampled, y_train_upsampled)\n\nprediction['DecisionTree'] = clf_gini.predict(X_test)\nprint(\"The Classification Report of Decision Tree Model\")\nt = classification_report(y_test, prediction['DecisionTree'])\nprint(t)\n\nconfusion_matrix = cm(y_test, prediction['DecisionTree'])\nprint(confusion_matrix)\n\nskplt.metrics.plot_confusion_matrix(y_test, prediction['DecisionTree'])\nplt.show()\nskplt.metrics.plot_confusion_matrix(y_test,prediction['DecisionTree'],normalize=True)\nplt.show()","9122b6f3":"from sklearn import datasets,tree\nfrom sklearn import externals\nfrom sklearn.tree import export_graphviz\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nimport pydotplus\nimport pydot\n\ndot_data = StringIO()\nexport_graphviz(clf_gini, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True,feature_names = X.columns,class_names=['0','1'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n\nImage(graph.create_png())","4c1cc1dd":"knn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train_upsampled,y_train_upsampled)\npred = knn.predict(X_test)\nprint(cm(y_test,pred))\nprint(classification_report(y_test,pred))\nskplt.metrics.plot_confusion_matrix(y_test, pred)\nplt.show()\nskplt.metrics.plot_confusion_matrix(y_test,pred,normalize=True)\nplt.show()","4b6e4998":"random_seed = 12","45afbbb2":"outcome = []\nmodel_names = []\nmodels = [('LogReg', LogisticRegression()),  \n          ('DecTree', DecisionTreeClassifier()),\n          ('KNN', KNeighborsClassifier()),\n          ('RandomForest', RandomForestClassifier())]","0d5680c3":"from sklearn import model_selection\nfor model_name, model in models:\n    k_fold_validation = model_selection.KFold(n_splits=10, random_state=random_seed)\n    results = model_selection.cross_val_score(model, X_train_upsampled, y_train_upsampled, cv=k_fold_validation, scoring='accuracy')\n    outcome.append(results)\n    model_names.append(model_name)\n    output_message = \"%s| Mean=%f STD=%f\" % (model_name, results.mean(), results.std())\n    print(output_message)","e40f7f44":"fig = plt.figure(figsize=(8,5))\nfig.suptitle('Machine Learning Model Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(outcome)\nax.set_xticklabels(model_names)\nplt.show()","7a667720":"# EDA","ca7b8ceb":"plt.figure(figsize = (30,25))\nhm = credit_final.corr()\n\nsns.heatmap(hm, cmap = 'vlag', annot=True)\nplt.title('The Correlation Heatmap')\nplt.show()","68941c06":"# \u673a\u5668\u5b66\u4e60 Machine Learning\n\n\u5206\u7c7b\u95ee\u9898 Classification\n- \u903b\u8f91\u56de\u5f52 Logistic regression\n- \u51b3\u7b56\u6811 Decision tree\n- \u968f\u673a\u68ee\u6797 Random forest\n- \u652f\u6301\u5411\u91cf Support Vector Classifications\n- Stocastic Gradient Descend\n- K-Nearest Neighbours Classifiers\n- Gaussian Process Classification\n- Other ensemble\n- \u795e\u7ecf\u7f51\u7edc Neural Network Models\n- XGBoost","7dd0ba0f":"# \u6570\u636e\u975e\u5747\u8861\uff08Imbalanced Data\uff09\n\n![Imb%20Data.png](attachment:Imb%20Data.png)\n\u7b80\u5355\u8bf4\u4f60\u7684\u5206\u7c7b\u5668\u8bad\u7ec3\u96c6\u6570\u636e\u4e0d\u662f5\/5\u5f00\u6216\u80056\/4\u5f00\uff0c\u9884\u6d4b\u7ed3\u679c\u5c31\u5f88\u5bb9\u6613\u5177\u6709\u6b3a\u9a97\u6027\n\n\u6bd4\u5982\uff1a\u9009\u53d6\u7684\u6837\u672c98%\u5ba2\u6237\u90fd\u8fdd\u7ea6\uff0c\u90a3\u53ea\u8981\u9884\u6d4b\u5168\u731c\u8fdd\u7ea6\uff0c\u51c6\u786e\u7387\u5c31\u80fd\u8fbe\u523098%...\u8fd9\u663e\u7136\u4e0d\u79d1\u5b66...","90b2b822":"# Random Forest","281780c5":"# K-Nearest Neighbors","f56aacf3":"## One-Hot Encoding\n\nEach category value is converted into a new column and assigned a 1 or 0 (notation for true\/false) value to the column. ","651e36be":"# Model Comparison","3cd5deae":"# Oversampling on Training Set","e2f97322":"# Normalization: Applying MinMax Scaler","ed569d30":"# Logistic Regression\n","a4de2bbf":"# Splitting the new upsampled dataset","87be81ba":"# Decision Tree Classifier","f1391e6c":"# Dividing the data into train and test"}}