{"cell_type":{"52279a40":"code","ad4c8549":"code","838ea1ac":"code","4f1393d4":"code","1f0358ea":"code","43c090ac":"code","57ed9634":"code","f2e248fa":"code","2d71bbef":"code","52b4c9f6":"code","cc7abae9":"code","6e96e4ea":"code","769783b1":"code","6af47472":"code","93cfafa2":"code","0bf50709":"code","fdac4b42":"code","d4ee7e40":"code","b6734ecd":"code","f2c11ed0":"code","b1345b1b":"code","2538b04f":"code","265fc691":"code","1246caf2":"code","a100bdd2":"code","a094b781":"code","a847498f":"code","08f76535":"code","95de1e63":"code","044420c0":"code","71f53962":"code","18aff84c":"code","dbaf6fd6":"code","25707644":"markdown","31e63d51":"markdown","8a60ef59":"markdown","92fe075e":"markdown","a4fd1ebe":"markdown","50fd808b":"markdown","530a77cf":"markdown","08083a37":"markdown","0f8ea7f8":"markdown","020abbbf":"markdown","d6b37304":"markdown","0ebabe25":"markdown","30cfeb8a":"markdown","55a7178d":"markdown","eda1179a":"markdown","b80589d8":"markdown"},"source":{"52279a40":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom wordcloud import STOPWORDS\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nimport gc\nimport operator\nimport nltk\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow import keras\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.metrics import precision_score, recall_score, f1_score","ad4c8549":"train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","838ea1ac":"train.head()","4f1393d4":"test.head()","1f0358ea":"train.size","43c090ac":"test.size","57ed9634":"sns.countplot(train['target'])","f2e248fa":"sns.barplot(train['keyword'].isnull().values)\nprint(\"The number of null values in keyword are\", train['keyword'].isnull().sum())","2d71bbef":"sns.barplot(train['location'].isnull().values)\nprint(\"The number of null values in location are\", train['location'].isnull().sum())","52b4c9f6":"sns.barplot(test['keyword'].isnull().values)\nprint(\"The number of null values in keyword are\", test['keyword'].isnull().sum())","cc7abae9":"sns.barplot(test['location'].isnull().values)\nprint(\"The number of null values in keyword are\", test['location'].isnull().sum())","6e96e4ea":"for df in [train, test]:\n    for col in ['keyword', 'location']:\n        df[col] = df[col].fillna('Unknown')\ntrain.head()","769783b1":"train['keyword'].nunique()","6af47472":"ag = train.groupby('keyword').agg({'text':np.size, 'target':np.mean}).rename(columns={'text':'Count', 'target':'Disaster Probability'})\nag.sort_values('Disaster Probability', ascending=False).head(20)","93cfafa2":"ag = train.groupby('keyword').agg({'text':np.size, 'target':np.mean}).rename(columns={'text':'Count', 'target':'Disaster Probability'})\nag.sort_values('Disaster Probability', ascending=True).head(20)","0bf50709":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len1=train[train['target']==1]['text'].str.len()\nax1.hist(tweet_len1,color='pink')\nax1.set_title('disaster tweets')\ntweet_len2=train[train['target']==0]['text'].str.len()\nax2.hist(tweet_len2,color='blue')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets', size=20)\nplt.show()","fdac4b42":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nax1.hist(train['text'].apply(lambda x: len(str(x).split())), color='pink')\nax1.hist(test['text'].apply(lambda x: len(str(x).split())), color='green')\nax1.set_title('Word Count distribution in Training, Test Set')\nax2.hist(train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS])), color='pink')\nax2.hist(test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS])),color='green')\nax2.set_title('Stop words distribution in Training, Test Set')","d4ee7e40":"!pip install emot","b6734ecd":"from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n# Function for converting emojis into word\ndef convert_emojis(text):\n    for emot in UNICODE_EMO:\n        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n    return text\n\ntrain['text'] = train['text'].apply(lambda x: convert_emojis(x))\ntest['text'] = test['text'].apply(lambda x: convert_emojis(x))","f2c11ed0":"from nltk.corpus import stopwords\n# load stop words\nstop_word = stopwords.words('english')\n\ndef clean(text):\n    #     remove urls\n    text = re.sub(r'http\\S+', \" \", text)\n    #     remove mentions\n    text = re.sub(r'@\\w+',' ',text)\n    #     remove hastags\n    text = re.sub(r'#\\w+', ' ', text)\n    #     remove digits\n    text = re.sub(r'\\d+', ' ', text)\n    #     remove html tags\n    text = re.sub('r<.*?>',' ', text) \n    #     remove stop words \n    text = text.split()\n    text = \" \".join([word for word in text if not word in stop_word])\n        \n    return text","b1345b1b":"train['text'] = train['text'].apply(lambda x: clean(x))\ntest['text'] = test['text'].apply(lambda x: clean(x))","2538b04f":"train.head()","265fc691":"from tqdm import tqdm\ndef create_corpus(train):\n    corpus=[]\n    for tweet in tqdm(train['text']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus\n        ","1246caf2":"corpus=create_corpus(train)","a100bdd2":"embedding_dict={}\nwith open('..\/input\/glove6b\/glove.6B.100d.txt','r', encoding='utf8') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","a094b781":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","a847498f":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","08f76535":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec","95de1e63":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=1e-5)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","044420c0":"model.summary()","71f53962":"train1=tweet_pad[:train.shape[0]]","18aff84c":"X_train,X_test,y_train,y_test=train_test_split(train1,train['target'].values,test_size=0.2)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","dbaf6fd6":"history=model.fit(X_train,y_train,batch_size=4,epochs=15,validation_data=(X_test,y_test),verbose=2)","25707644":"## Filling the null values with \"Unknown\" for EDA purpose","31e63d51":"## Top keywords that suggest not being a disaster tweet","8a60ef59":"## URL, Mention etc. play no significant rule in the sentiment of tweet, so we will remove them ","92fe075e":"## Upvote if you found it helpful!","a4fd1ebe":"## So, around 8% of keyword values and 33% of Location values are null in train sample","50fd808b":"## Top keywords that suggest a disaster tweet","530a77cf":"## So, both training and test set have same ratio of missing values in keyword and location","08083a37":"## The character distribution is almost same for both","0f8ea7f8":"# EDA","020abbbf":"## Importing needed libraries","d6b37304":"## Word Embedding using Glove","0ebabe25":"## If you want to score even more, change all the tweets that has keywords wreckage, debris, derailment to 1 as we saw above in EDA that they have almost 100% chance of being disaster tweet. Similarly, check for Non Disaster tweets","30cfeb8a":"## Emojis convey a lot, so we will replace them with words","55a7178d":"## Basic LSTM model","eda1179a":"## There are more tweets with target 0 (No disaster) than of Target 1( Disaster)","b80589d8":"## Importing the dataset"}}