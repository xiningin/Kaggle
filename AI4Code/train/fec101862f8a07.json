{"cell_type":{"ae668ae2":"code","df27f881":"code","c0f1b028":"code","e7712974":"code","9617984e":"code","464d593e":"code","b1463332":"code","2f3411e6":"code","daf4ec2a":"code","0dbf478d":"code","98d2661f":"code","b0e8a55d":"code","d8a29782":"code","8c6dd7eb":"code","d3013545":"code","629d55ad":"code","02d6a4c2":"code","4c051680":"code","ecfb5409":"code","1f40459f":"code","a361249e":"code","e5376226":"code","2c4f52a2":"code","b99fe4b2":"code","dfd191e7":"code","1a6fb7a1":"code","be24c797":"code","c135ac16":"code","225c1266":"code","36e6dca8":"code","9747d5da":"markdown","7a9e4eed":"markdown","4a8eed7e":"markdown","d527771b":"markdown","1248da77":"markdown","54906aad":"markdown","3a49351c":"markdown","8a801628":"markdown","b3ab7986":"markdown","e098ec56":"markdown"},"source":{"ae668ae2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","df27f881":"df  = pd.read_csv('..\/input\/USA_Housing.csv')","c0f1b028":"df.info()","e7712974":"# So, we have 6 countinuos variable and one categorical variable. \n# Now, lets look at some of the data.\ndf.head()","9617984e":"df.describe()","464d593e":"# lets see if there is any null\/missing values in the datasets or not. It's important to remove or\n# replace all the missing values before moving further. \ndf.isna().sum()","b1463332":"# We don't have any missing values. So, we are good to go. \n# Now, let's understand the correlation between variable by plotting correlation plot.\ndf.corr()","2f3411e6":"plt.figure(figsize=(12,10))\nsns.heatmap(df.corr(), annot=True)","daf4ec2a":"# As we can see that price is more correlated to Avg. Income Area, House Age and Area Population than Number of Bedrooms and Rooms. Lets see these metrics in tabular format.\ndf.corr().Price.sort_values(ascending=False)","0dbf478d":"sns.pairplot(df)","98d2661f":"# As we can see here in the last line of graph that all the features seems to be in a linear relationship with price except Avg. Area Number of Bedroom.\n# We can also see this by plotting a separate graph\n\nplt.scatter(df.Price, df[['Avg. Area Income']])","b0e8a55d":"sns.distplot(df.Price)","d8a29782":"# We can see the price plot seems like a bell shaped curve and all the price is normally distributed.","8c6dd7eb":"df = df.drop(['Address'], axis=1)\ndf.head()","d3013545":"from sklearn import preprocessing\npre_process = preprocessing.StandardScaler()","629d55ad":"feature = df.drop(['Price'], axis = 1)\nlabel = df.Price\n\n# Now, we have feature and label for machine learning algorithms. Now, we can scale the data by using standard scaler.\n\nfeature = pre_process.fit_transform(feature)","02d6a4c2":"feature \n#this is how the scaled data looks like.","4c051680":"from sklearn.model_selection import train_test_split\nfeature_train, feature_test, label_train, label_test = train_test_split(feature, label.values, test_size = 0.2, random_state = 19)","ecfb5409":"from sklearn import linear_model\nlinear_regression = linear_model.LinearRegression()\nlinear_regression.fit(feature_train, label_train)","1f40459f":"from sklearn.metrics import r2_score, mean_squared_error\n\nscore = r2_score(linear_regression.predict(feature_train), label_train)\nerror = mean_squared_error(linear_regression.predict(feature_train), label_train)","a361249e":"score, error","e5376226":"linear_regression.coef_","2c4f52a2":"linear_regression.intercept_","b99fe4b2":"pd.DataFrame(linear_regression.coef_, index=df.columns[:-1], columns=['Values'])","dfd191e7":"# Applying this on test data.\nscore_test = r2_score(linear_regression.predict(feature_test), label_test)\nscore_test","1a6fb7a1":"ransac = linear_model.RANSACRegressor()\nransac.fit(feature_train, label_train)\n\n# Scoring the Ransac model\n\nransac_r2_score = r2_score(ransac.predict(feature_test), label_test)\nransac_r2_score","be24c797":"ridge_model = linear_model.Ridge()\nridge_model.fit(feature_train, label_train)\n\n# Scoring the Ridge Regression\n\nridge_r2_score = r2_score(ridge_model.predict(feature_test), label_test)\nridge_r2_score\n","c135ac16":"from sklearn import tree\ntree_model = tree.DecisionTreeRegressor()\ntree_model.fit(feature_train, label_train)\n\n# Scoring the Ridge Regression\n\ntree_r2_score = r2_score(tree_model.predict(feature_test), label_test)\ntree_r2_score","225c1266":"from sklearn.ensemble import RandomForestRegressor\nrandom_model = RandomForestRegressor()\nrandom_model.fit(feature_train, label_train)\n\n# Scoring the Ridge Regression\n\nrandom_r2_score = r2_score(tree_model.predict(feature_test), label_test)\nrandom_r2_score","36e6dca8":"data = [score_test, ransac_r2_score, ridge_r2_score, tree_r2_score,random_r2_score]\nindex = ['Linear Regression', 'Ransac Regression', 'Ridge Regression', 'Decision Tree Regressor', 'Random Forest Regressor']\npd.DataFrame(data, index=index, columns=['Scores']).sort_values(ascending = False, by=['Scores'])","9747d5da":"![](http:\/\/)**Divding the data into train\/test**","7a9e4eed":"Some Other Algorithms\n\n**RANSAC Regressor**","4a8eed7e":"![](http:\/\/)**Scale the data - Prepare the data to feed into the machine learning algorithm**","d527771b":"![](http:\/\/)**Testing Decision Tree Model**","1248da77":"**Understanding the data**","54906aad":"Before jumping into code directly its import to follow some basics. I have divided this kernel into 7 parts.\n\n* Understanding your data\n* Data Cleaning(if required)\n* Scaling the data into machine learning readbable format\n* Dividing the data into train\/test\n* Applying machine learning algorithm\n* Testing the effectiveness of the machine learning Algorithm\n* Trying different Algorithms","3a49351c":"**Applying machine learning algorithm**","8a801628":"**Random Forest Regressor**","b3ab7986":"Till now, we had a detailed look at the given data and fortunately we don't have any missing values. So, the data cleaning is not required for this data.\n\nWe will also be deleting the address data, as it does not seem that useful.'","e098ec56":"**Ridge Regression**"}}