{"cell_type":{"3266c60b":"code","d171836e":"code","97191b05":"code","41ac19a9":"code","dfc714c1":"code","06550d25":"code","b972b162":"code","38ac8e63":"code","39498d0e":"code","94adc6da":"code","9df9f15d":"code","7cff85fb":"code","e9bb984c":"code","24e9b5eb":"code","9b7003b6":"code","6fb8786b":"code","fada2bcb":"code","75a83ed2":"code","6157f46d":"code","8bf81938":"code","9f0126f2":"markdown"},"source":{"3266c60b":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\n\nfrom keras.preprocessing import image\nimport keras\n\nimport imgaug as ia\nfrom imgaug import augmenters as iaa","d171836e":"%env SM_FRAMEWORK=tf.keras\n!pip install segmentation_models\nimport segmentation_models as sm","97191b05":"test_cities = ['berlin', 'bielefeld', 'bonn', 'leverkusen', 'mainz', 'munich']\ntrain_cities = ['aachen', 'bochum', 'bremen', 'cologne', 'darmstadt', 'dusseldorf', 'erfurt', 'hamburg', 'hanover', 'jena', 'krefeld', 'monchengladbach', 'strasbourg', 'stuttgart', 'tubingen', 'ulm', 'weimar', 'zurich']\nval_cities = ['frankfurt', 'lindau', 'munster']","41ac19a9":"train_img_paths = []\ntrain_ann_paths = []\n\nfor cities in train_cities:\n\n    train_img_dir = \"..\/input\/city-scapes-images\/leftImg8bit\/train\/\" + cities\n    train_ann_dir = \"..\/input\/cityscapes-fine-annotations\/gtFine\/train\/\" + cities\n    \n    \n\n    train_img_paths = train_img_paths + sorted(\n        [\n            os.path.join(train_img_dir, fname)\n            for fname in os.listdir(train_img_dir)\n            if fname.endswith(\"_leftImg8bit.png\")\n        ]\n    )\n    train_ann_paths = train_ann_paths + sorted(\n        [\n            os.path.join(train_ann_dir, fname)\n            for fname in os.listdir(train_ann_dir)\n            if fname.endswith(\"_gtFine_labelIds.png\")\n        ]\n    )\n\nprint(\"Number of train images:\", len(train_img_paths))\n\nprint(\"Number of train annotations:\", len(train_ann_paths))\n\nval_img_paths = []\nval_ann_paths = []\n    \nfor cities in val_cities:\n    val_img_dir = \"..\/input\/city-scapes-images\/leftImg8bit\/val\/\" + cities\n    val_ann_dir = \"..\/input\/cityscapes-fine-annotations\/gtFine\/val\/\" + cities\n    \n\n\n    val_img_paths = val_img_paths + sorted(\n        [\n            os.path.join(val_img_dir, fname)\n            for fname in os.listdir(val_img_dir)\n            if fname.endswith(\"_leftImg8bit.png\")\n        ]\n    )\n    val_ann_paths = val_ann_paths + sorted(\n        [\n            os.path.join(val_ann_dir, fname)\n            for fname in os.listdir(val_ann_dir)\n            if fname.endswith(\"_gtFine_labelIds.png\")\n        ]\n    )\n\nprint(\"Number of val images:\", len(val_img_paths))\n\nprint(\"Number of val annotations:\", len(val_ann_paths))","dfc714c1":"img_size = (256, 256)\nnum_classes = 8\nbatch_size = 5\nimgaug_multiplier = 3\nepochs = 50","06550d25":"# Reducing 32 categories to only 8 categories\n\ncats = {\n 'void': [0, 1, 2, 3, 4, 5, 6],\n 'flat': [7, 8, 9, 10],\n 'construction': [11, 12, 13, 14, 15, 16],\n 'object': [17, 18, 19, 20],\n 'nature': [21, 22],\n 'sky': [23],\n 'human': [24, 25],\n 'vehicle': [26, 27, 28, 29, 30, 31, 32, 33, -1]}\n\ndef convertCats(x):\n    if x in cats['void']:\n        return 0\n    elif x in cats['flat']:\n        return 1\n    elif x in cats['construction']:\n        return 2\n    elif x in cats['object']:\n        return 3\n    elif x in cats['nature']:\n        return 4\n    elif x in cats['sky']:\n        return 5\n    elif x in cats['human']:\n        return 6\n    elif x in cats['vehicle']:\n        return 7\n    \nconvertCats_v = np.vectorize(convertCats)\n\ndef preprocessImg(img):\n    image_matrix = np.expand_dims(img, 2)\n    \n    converted_image = convertCats_v(image_matrix)\n    return converted_image\n","b972b162":"def generateRandomParams(seed):\n    np.random.seed(seed)\n    angle = np.random.randint(26)\n    positive = np.random.randint(2)\n    \n    if positive == 0:\n        angle = angle * -1\n        \n    crop = np.random.randint(3)\n    crop = crop \/ 10\n        \n    return angle, crop","38ac8e63":"class Image(keras.utils.Sequence):\n    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n\n    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.input_img_paths = input_img_paths\n        self.target_img_paths = target_img_paths\n\n    def __len__(self):\n        return len(self.target_img_paths) \/\/ self.batch_size\n\n    def __getitem__(self, idx):\n        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n        i = idx * self.batch_size\n        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n        \n        x = np.zeros((self.batch_size * imgaug_multiplier,) + self.img_size + (3,), dtype=\"float32\")        \n        for j, path in enumerate(batch_input_img_paths):\n            img = image.load_img(path, target_size=self.img_size)\n            x[j] = img\n        y = np.zeros((self.batch_size * imgaug_multiplier,) + self.img_size + (1,), dtype=\"uint8\")\n        \n        for j, path in enumerate(batch_target_img_paths):\n            _img = image.load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n            y[j] = preprocessImg(_img)\n            \n            # Image AUGMENTATION         \n        for mul in range(1, imgaug_multiplier):  \n            for i in range(0, self.batch_size):\n                \n                angle, crop = generateRandomParams(i * mul)\n                \n                seq = iaa.Sequential([                        \n                    iaa.Affine(rotate=(angle)),\n                    iaa.Crop(percent=(crop)),                    \n                ])\n                \n\n                image_aug = seq(image=x[i])\n                x[batch_size * mul + i] = image_aug\n\n                image_aug = seq(image=y[i])\n                y[batch_size * mul + i] = image_aug # END Image AUGMENTATION  \n            \n            \n        return x, y","39498d0e":"train_seq = Image(\n    batch_size, img_size, train_img_paths, train_ann_paths\n)\nval_seq = Image(\n    batch_size, img_size, val_img_paths, val_ann_paths\n)","94adc6da":"#Quick verif\nassert train_seq[0][0].shape == (batch_size * imgaug_multiplier, *img_size, 3)\nassert train_seq[0][1].shape == (batch_size * imgaug_multiplier, *img_size, 1)","9df9f15d":"BACKBONE = 'resnet101'\npreprocess_input = sm.get_preprocessing(BACKBONE)","7cff85fb":"model = sm.FPN(BACKBONE, encoder_weights='imagenet', classes=8 ,  input_shape=(*img_size, 3))\nmodel.compile(\n    'Adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=[sm.metrics.iou_score, 'sparse_categorical_accuracy'],\n)","e9bb984c":"callbacks = [\n    keras.callbacks.ModelCheckpoint(\"checkpoint.h5\", save_best_only=True),\n    keras.callbacks.EarlyStopping(patience=3, verbose=1)\n]\n\nhistory = model.fit(\n    train_seq,\n    steps_per_epoch=len(train_seq),\n    epochs=30,\n    callbacks=callbacks,\n    validation_data=val_seq,\n    validation_steps=len(val_seq),\n)","24e9b5eb":"model.save('model.h5')","9b7003b6":"if not os.path.exists('.\/model.h5'):\n    model = keras.models.load_model('..\/input\/model-gpu-v1\/model (5).h5', custom_objects={\n        'binary_crossentropy_plus_jaccard_loss': sm.losses.bce_jaccard_loss,\n        'iou_score' : sm.metrics.IOUScore\n        \n    })","6fb8786b":"X = np.empty((1, *img_size + (3,)))\nX[0,] = val_seq[0][0][0]","fada2bcb":"res = model.predict(X)","75a83ed2":"def combineMask(masks):\n    _output = np.empty((256,256) + (1,))\n    _x, _y = 0, 0\n    \n    for x in range(0, masks.shape[0]):\n        for y in range(0, masks.shape[1]):\n                   _target = masks[x][y]\n                   _output[x][y] = np.argmax(_target) \n    return _output\n","6157f46d":"testing = combineMask(res[0])","8bf81938":"keras.preprocessing.image.array_to_img(\n    testing, data_format=None, scale=True, dtype=None,\n    )","9f0126f2":"# Prepare data"}}