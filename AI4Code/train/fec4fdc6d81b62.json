{"cell_type":{"d9ba1c45":"code","6fe01549":"code","536ea31a":"code","b999147f":"code","793c3865":"code","f150a490":"code","61b6eb37":"code","c0adcb44":"code","f1771774":"code","b860ae32":"code","080aa67a":"code","65aba62e":"code","e187a16c":"code","919b2e3d":"code","e874b746":"code","59cf8070":"code","7a999b64":"code","c7e36276":"code","d0571e14":"code","fe520fa5":"code","658d0a38":"code","3c49fee7":"code","cf087cef":"code","af5d80c4":"code","eb547289":"code","bcf5b9f9":"code","e9207713":"code","d56ddbe5":"code","1afa83b1":"code","7b00bf3d":"code","3eccc5d7":"code","8074c7d1":"code","411d252f":"code","c8ed75c6":"code","22089d7e":"code","4033415b":"code","d37e27e8":"code","46d24f37":"code","3237ed52":"code","391128c6":"code","101f92df":"code","324a8048":"code","a5e1a437":"code","71d650ca":"code","45531679":"code","64bdb3ee":"code","a6d0eb7c":"code","1ebaaebc":"code","50932880":"code","4e7b7bec":"code","0c911d50":"code","a6b0bc77":"code","e25eb8fd":"code","b49e93f3":"code","1fb9670b":"code","c7ee8336":"code","dddace7d":"code","28f682dd":"code","fba2ecfb":"code","3d9027c2":"markdown","8b55d6a7":"markdown","3cdfd882":"markdown","788dda00":"markdown","600c0966":"markdown","4e949833":"markdown","10c8ab71":"markdown","c58b5b75":"markdown","082d171a":"markdown","a13c15de":"markdown","64966dcb":"markdown","84d9aed9":"markdown","9d7acc13":"markdown","1b754e81":"markdown","79e5b265":"markdown","f37d06a0":"markdown","352d3473":"markdown","be1eac45":"markdown","d230e57f":"markdown"},"source":{"d9ba1c45":"# importing libraries\nimport nltk\nprint(nltk.__version__)","6fe01549":"import numpy as np\nimport pandas as pd\nimport sys\nimport sklearn\nprint(\"numpy version: {}\".format(np.__version__))\nprint(\"pandas version: {}\".format(pd.__version__))\nprint(\"sklearn version: {}\".format(sklearn.__version__))\nprint(\"sys version: {}\".format(sys.version))\n","536ea31a":"!wget  https:\/\/archive.ics.uci.edu\/ml\/datasets\/sms+spam+collection\n\n        ","b999147f":"# I initially tried to use StringIO. StringIO has been deprecated in python3. instead we are using IO \nimport requests, zipfile, io\nr = requests.get('https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/00228\/smsspamcollection.zip')\n\nz = zipfile.ZipFile(io.BytesIO(r.content))","793c3865":"# gives the filelist in the zipfile object\nz.filelist","f150a490":"# extracts the file whose name is passed as an argument to the extract method\nz.extract('SMSSpamCollection')","61b6eb37":"# now lets go to the directory where our data file is extracted\n!chdir \/kaggle\/working","c0adcb44":"df = pd.read_table('\/kaggle\/working\/SMSSpamCollection',header=None)","f1771774":"\n#nowe lets read the file \n# df = pd.read_table('SMSSpamCollection',header= None, encoding = 'utf-8')\n# If we do not pass headee r = None, pandas will read from the second line.\nprint(type(df))\n\ndf.info()\nprint('______')\nprint(df.head())","b860ae32":"A=np.array([1,1,2,3,2,3,4,])\ndf9=pd.DataFrame(A)\ndf9[0].value_counts() # .value_counts() works on series and not on the dataframe","080aa67a":"#Lets check out the count of ham and spam\n\nclasses = df[0] # Point to be noted: we can  not use class, as class is a reserved key word in python\n\nclasses.value_counts()","65aba62e":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\n\nY = encoder.fit_transform(classes)\nprint(Y[:10])\n# Now we have converted the classes into binary argument. Lets check some of the labels","e187a16c":"# we can check details of any method by the command 'method?'. example below is for LabelEncoder\n#LabelEncoder?","919b2e3d":"text_msg = df[1]\ntext_msg[0:10]","e874b746":"\n\n# use regular expressions to replace email addresses, URLs, phone numbers, other numbers\n\n# Replace email addresses with 'email'\nprocessed = text_msg.str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$',\n                                 'emailaddress')\n\n# Replace URLs with 'webaddress'\nprocessed = processed.str.replace(r'^http\\:\/\/[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(\/\\S*)?$',\n                                  'webaddress')\n\n# Replace money symbols with 'moneysymb' (\u00a3 can by typed with ALT key + 156)\nprocessed = processed.str.replace(r'\u00a3|\\$', 'moneysymb')\n    \n# Replace 10 digit phone numbers (formats include paranthesis, spaces, no spaces, dashes) with 'phonenumber'\nprocessed = processed.str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$',\n                                  'phonenumbr')\n    \n# Replace numbers with 'numbr'\nprocessed = processed.str.replace(r'\\d+(\\.\\d+)?', 'numbr')\n\nprint(processed[8])\n","59cf8070":"\n\n# Remove punctuation\nprocessed = processed.str.replace(r'[^\\w\\d\\s]', ' ')\n\n# Replace whitespace between terms with a single space\nprocessed = processed.str.replace(r'\\s+', ' ')\n\n# Remove leading and trailing whitespace\nprocessed = processed.str.replace(r'^\\s+|\\s+?$', '')\n\n","7a999b64":"processed[2]","c7e36276":"processed = processed.str.lower()\nprint(processed)","d0571e14":"# Remove the stopwords\nfrom nltk.corpus import stopwords\n# print(stopwords)\nstopwords = set(stopwords.words('english'))\n\nprint(stopwords, end= ' ')\nprint(type(stopwords))","fe520fa5":"from nltk.tokenize import sent_tokenize,word_tokenize","658d0a38":"def filter(sent):\n    filtered = dict()\n    for i in processed:\n        words = word_tokenize(i)\n\n        for word in words:\n            rmv=[]\n            if word not in stopwords:\n                rmv.append(word)\n                filtered[i]= rmv\n                rmv=[]\n    return filtered\n    ","3c49fee7":"# filter(processed[1])","cf087cef":"processed_rm_stopwords = list(map(lambda x: ' '.join(term for term in x.split() if term not in stopwords),processed))\nprocessed_rm_s= processed.apply(lambda x: ' '.join(term for term in x.split() if term not in stopwords))","af5d80c4":"print(processed_rm_stopwords[0:5])\nprint(processed_rm_s[0:5])\nprint(f\"processed_rm_s: {type(processed_rm_s)}\")\nprint(f\"processed_rm_stopwords{type(processed_rm_stopwords)}\")\ndf_new=pd.DataFrame(processed_rm_stopwords)\nprint(type(df_new))\nprint(df_new[0:5])\n","eb547289":"ps = nltk.PorterStemmer()\nstemmed = list(map(lambda x: \" \".join(ps.stem(term) for term in x.split()), processed_rm_stopwords))\nprint(stemmed[0:5])\ndf_new_stemmed = pd.DataFrame(stemmed)\nprint(df_new_stemmed)","bcf5b9f9":"# Creating a bag of words\nall_words =[] # this will have all the words in our dataframe (processed)\n\nfor item in stemmed:\n    \n    words  = word_tokenize(item)\n    for word in words:\n        all_words.append(word)\n    \nall_words = nltk.FreqDist(all_words) # unique words\n","e9207713":"len(all_words)","d56ddbe5":"# Lets find out some common words\nprint(\"most common words {}\".format(all_words.most_common(10)))","1afa83b1":"\nall_words_common = all_words.most_common(1500)\n# all_words.keys()\nfeatures = all_words_common[0:1500]\nfeatures_list=[ key for  key, val in features]\nprint(features_list, end = ' ')","7b00bf3d":"# lets define feature finding function\ndef find_feature(message):\n    \n    words = word_tokenize(message)\n    \n    feature= dict()\n    \n    for word in features_list:\n        \n        feature[word]=word in words\n    return feature\n        \nfeatures = find_feature(stemmed[0])\n# print(features)\n# print(len(features))\n\n\nfor key, val in features.items():\n    if val == True:\n        print (key)\n    ","3eccc5d7":"print(stemmed[0])","8074c7d1":"messages = list(zip(stemmed,Y))\n# list(messages)","411d252f":"df_N = pd.DataFrame(messages)\ndf_N.head\nprint(type(df_N))","c8ed75c6":"# [key, label for (key, label) in df_N[0:5]]","22089d7e":"#define a seed \n\nseed = 1\nnp.random.seed =seed\nnp.random.shuffle(messages)\n# print(df_N[0:5])","4033415b":"featureset = [(find_feature(text),label) for (text, label) in messages]","d37e27e8":"print(type(featureset))\nprint(len(featureset))","46d24f37":"from sklearn.model_selection import train_test_split","3237ed52":"training, testing = train_test_split(featureset, test_size=0.2, random_state = seed)","391128c6":"print(len(training))\nprint(type(training))","101f92df":"print(\"length of training set: {}\".format(len(training)))\nprint(\"length of testing set: {}\".format(len(testing)))","324a8048":"training[0]","a5e1a437":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, precision_score, classification_report, confusion_matrix","71d650ca":"names = [\"K nearest neighbors\",\"decision tree\",\"random forest\",\"SVC\",\"Logistics regression\",\"SGD\",\"MultinomialNB\"]\nclassifiers = [KNeighborsClassifier(),\n               DecisionTreeClassifier(),\n               RandomForestClassifier(),\n               SVC(kernel = 'linear'),\n               LogisticRegression(),\n              SGDClassifier(max_iter=100),\n               MultinomialNB()]\nmodels = zip(names,classifiers)\nlist(models)\n\n","45531679":"\nfor (name, model) in models:\n    nltk_model = SklearnClassifier(model)\n    nltk_model.train(training)\n    accuracy = nltk.classify.accuracy(nltk_model, testing)*100\n    print(\"{} Accuracy: {}\".format(name, accuracy))","64bdb3ee":"# Define models to train\nfrom nltk.classify.scikitlearn import SklearnClassifier\nnames = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n         \"Naive Bayes\", \"SVM Linear\"]\n\nclassifiers = [\n    KNeighborsClassifier(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    LogisticRegression(),\n    SGDClassifier(max_iter = 100),\n    MultinomialNB(),\n    SVC(kernel = 'linear')\n]\nmodels = zip(names, classifiers)\nfrom nltk.classify.scikitlearn import SklearnClassifier\nfor name, model in models:\n    nltk_model = SklearnClassifier(model)\n    nltk_model.train(training)\n    accuracy = nltk.classify.accuracy(nltk_model, testing)*100\n    print(\"{} Accuracy: {}\".format(name, accuracy))","a6d0eb7c":"nl=SklearnClassifier(RandomForestClassifier())\nnl.train(training)","1ebaaebc":"accuracy_nl = nltk.classify.accuracy(nl, testing)*100\nprint(\"{} Accuracy: {}\".format('Randomforest', accuracy_nl))","50932880":"models = zip(names, classifiers)\n# list(models)\n# The zip object is very interesting. Once you execute the zip object and then you try to execute it again\n#we will get an error, if you try doing that you will find out why.","4e7b7bec":"classifiers","0c911d50":"# estimators= list(models)\n# # print(estimators)","a6b0bc77":"from sklearn.ensemble import VotingClassifier\nnltk_ensemble = SklearnClassifier(VotingClassifier(estimators = list(models), voting = 'hard', n_jobs = -1))\nnltk_ensemble.train(training)\n# accuracy = nltk.classifiy.accuracy(nltk_ensemble, test)*100\n# print(f\"the accuracy of the ensemble model is: {accuracy}\")","e25eb8fd":"nltk_ensemble","b49e93f3":"accuracy = nltk.classify.accuracy(nltk_ensemble, testing)*100\nprint(f\"the accuracy of the ensemble model is: {accuracy}\")","1fb9670b":"print(testing[0]) # as you can see testing is a tuple if feature_list and the label for the message","c7ee8336":"text_feature, label = zip(*testing)","dddace7d":"predictions = nltk_ensemble.classify_many(text_feature)","28f682dd":"print(classification_report(label, predictions))\n\n# pd.DataFrame(confusion_matrix(label, predictions), index=[['actual','actual'],['ham','spam']],\n#             columns = [['predicted','predicted'],['ham','spam']])","fba2ecfb":"pd.DataFrame(confusion_matrix(label, predictions), index=[['actual','actual'],['ham','spam']],\n            columns = [['predicted','predicted'],['ham','spam']])","3d9027c2":"## Data has a skewed distribution\nas you can see there are 4825 non spam messages and only 747 spam messages.\n\n## 2. Pre-processing the data\n\nLets encode the labels 'spam' and 'ham' using LabelEncoder. LabelEncoder Encodes target labels with value between 0 and n_classes-1. In this case we will get two classes 0 and 1 where ****0=normal, 1=spam","8b55d6a7":"As we can see that the best performaing model is the RandomForest Classisifer, Lets train the classifier once again. PLease note that we are not explicitely passing on the labels and the label data is part of the same training data set. This is the way the sklearn wrapper works. please go through the documentation for details.","3cdfd882":"# Now we can move on with generating features by tokenizing each sentence or each message","788dda00":"### Our feature will have words from these most common words. Lets use 1500 most common words as features.\n###  Now for every word in every msg we will have to create a vector of dimension 1500 (rows) with 1 in the word present and 0 elsewhere.\n### we can use hogher number of words to increase the accuracy, but that will increase our training time as well","600c0966":"## Change all the corpus to lower case","4e949833":"# After removing the stopwords lets do stemming using porter stemmer","10c8ab71":"## Scikit learn classifiers\n","c58b5b75":"## From this point onwards things are mostly strightforward like any other sklearn ml model\n### Preparing train and test data","082d171a":"The above and the below cell demonstrates that we can apply the map function to remove the stopwords in two ways. or by using map function in two ways.\n\nPersonally i like to use the map function as it has a wide variety of usage in python and knowing the map function better really helps to sharpen the overall skill in python","a13c15de":"## 1.Load the dataset\nDownloading the dataset from https:\/\/archive.ics.uci.edu\/ml\/datasets\/sms+spam+collection using wget command.\nThis is a zip file and hence we will use tools to extract the files from the zip file and then will use pandas.read_tables to read the file. \n\n\n","64966dcb":"## NOW ! since we have defined the features, the next thing is to find whether the feature is there or not in our processed messages. We need to define a function to find whether each word in the message is there in the feature list of not","84d9aed9":"### #Lets try ensemble method, as a spam clasifier has to have a higher accuracy and having an ensemble method m ay help us get higher accuracy. Lets try the voting  classifier \nEach of our classifier will have their individual vote for each email\/message and the highest vote wins. We need to have an odd number of classifier for this.  Note, nltk will see the ensembled model as one  model and it will not see the underlying 7 models of ours.\n### Hard voting and soft voting: \nHard voting is binary or yes or no. And soft voting takes the individual probabiities for each class and go with the average value","9d7acc13":"Hi Friends, This is my first NLP notebook. In this notebook,  I am  trying to classify spam and normal emails by training the model on a dataset from UCI.Will use multiple classifiers from Scikit-Learn and choose the one with highesst accuracy. We will also try to see if we can get a better performance using an ensemble model.\n\nlet's load the data set as a Pandas DataFrame. Furthermore, let's extract some useful information such as the column information and class distributions.\nThe data set we will be using comes from the UCI Machine Learning Repository. It contains over 5000 SMS labeled messages that have been collected for mobile phone spam research. It can be downloaded from the following URL:\n\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/sms+spam+collection","1b754e81":"Now, since we have our datafile. Lets create a dataframe using pandas\n\nThe **data file SMSSpamCollection is a text file** with first column as a label for spam or ham(non spam) and the second column there is the message.","79e5b265":"#### So, overall our model did a good job as desired. The important thing is to learn how to make NLTK and sklearn libraries work together. We can expect a better performance by using a larger feature list. Instead of using only 1500 words as in our case, we can use all the words, and hope to get a better performance.\n\nHI, If you have liked the notebook, I would humbly ask you to let me know by giving a vote or leave a comment. Your feedbacks and vote will encourage me to do more good work. Thanks.","f37d06a0":"We will now remove the most frequently words  which do not contribute in getting a better prediction. words such as \"the','on','is','there' etc do not help us and hence we will remove them.\n\n* we will import stopwords for english.\n \n* Then we will tokenize the sentences using word_tokenizer \n\n* Then write a for loop to find the non stop words and then store them in a dictionary variable names filtered\n","352d3473":"## we will use regular expresion to clean the data.Some transformations to be done on the data are-- \n### i) we want to replace any email as 'email'\n### ii) any url with 'url' \n### iii) phone numbers\n### iv ) currency symbols","be1eac45":"There are other methods to read a zip file directly. But, i decided to use a method which utilizes more components as this gives a chance to practice and use these libraries. Geting used to these libraries helps to enhance the skills of a beginner like me.\n\n* 1.We are usgin request module and storing the response in a variable r\n* 2.Reading the variable using zipfile module and storing the data in a variable z\n* 3.The zipfile object has a method filelist which gives out the file names\n* 4.Finally we will extract the datafile usging the extract method","d230e57f":"## the ensemble model has done almost similar to our best model which was RandomForest Usually, if none of our models are performing well, in that case ensemble model may come up with better and higher accuracy.IF the accuracy of the best model and the ensemble model is same, I would go with the best model as it will be computationally more efficient.Now, lets test some of the results"}}