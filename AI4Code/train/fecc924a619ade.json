{"cell_type":{"74f32aa0":"code","af042485":"code","f7a88400":"code","5eda4dee":"code","1a737737":"code","55832c32":"code","9d06a0ee":"code","f54ca63b":"code","b7f85d25":"code","922e903b":"code","311ae452":"code","4143251b":"code","176d60ec":"code","7843707d":"code","f28ce91e":"code","c060bbd4":"code","36df4959":"code","5ba701ff":"code","c0ac0cca":"code","43112807":"code","c27ea16e":"code","fd6be10e":"code","aa10ad24":"code","b0bbf19d":"code","043bd3a1":"code","c43ecdff":"code","d12e435c":"code","a3afa4a2":"code","185c0fc1":"code","bc8c8367":"code","0a2f5e9d":"code","c5188112":"code","e11a8acb":"code","dcc202c9":"code","0f077c7c":"code","51cb06d9":"code","f0e788db":"code","aee5796c":"code","06b417d0":"code","16fae38f":"code","2a3faaef":"code","086555b8":"code","a78e41f7":"code","d5e176fb":"code","30d5f1a5":"code","c638fda0":"code","2693c833":"code","6afe88f7":"code","92fdf7d3":"code","eebb4d83":"code","54211039":"code","371b69f2":"code","752fedcb":"code","cc92dd42":"markdown","36c26428":"markdown","20681345":"markdown","c6aae879":"markdown","976c626b":"markdown","e357f9c0":"markdown","fed2bbf9":"markdown","5894037a":"markdown","b3e780db":"markdown","07220f48":"markdown","b84be297":"markdown","9d9d13e6":"markdown","1803d00a":"markdown","fda111a9":"markdown","395185b2":"markdown","90d21505":"markdown","ca2b99cd":"markdown","40683e1d":"markdown","075d7350":"markdown","2ff6d9ba":"markdown","f2d71cbd":"markdown","0b6b5261":"markdown","ab97cff6":"markdown","7c880c6d":"markdown","e94f0ef0":"markdown","90f2e393":"markdown","ea37913e":"markdown","d230b72b":"markdown","4b87797d":"markdown","8094dced":"markdown","f0d9da15":"markdown","13c3f204":"markdown","d19f6d52":"markdown","f5ef0c8c":"markdown","3d59ede8":"markdown","5d94f941":"markdown","b36b1ed2":"markdown","75d71da0":"markdown","89a64fb2":"markdown","682af87f":"markdown","01186e04":"markdown","80345fdc":"markdown","ab6be1f0":"markdown","3a43001f":"markdown","931df41a":"markdown","f25b8269":"markdown","db07b30f":"markdown","ed6b1551":"markdown","2432df5e":"markdown"},"source":{"74f32aa0":"import warnings\nimport random\nimport os\nimport gc\n#import cudf","af042485":"import pandas            as pd\nimport numpy             as np\nimport matplotlib.pyplot as plt \nimport seaborn           as sns\nimport joblib            as jb","f7a88400":"from sklearn.model_selection import train_test_split, KFold, GridSearchCV\nfrom sklearn.preprocessing   import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler, QuantileTransformer\nfrom sklearn.impute          import SimpleImputer\nfrom sklearn                 import metrics","5eda4dee":"import xgboost               as xgb\nimport catboost              as ctb","1a737737":"def jupyter_setting():\n    \n    %matplotlib inline\n      \n    #os.environ[\"WANDB_SILENT\"] = \"true\" \n    #plt.style.use('bmh') \n    #plt.rcParams['figure.figsize'] = [20,15]\n    #plt.rcParams['font.size']      = 13\n     \n    pd.options.display.max_columns = None\n    #pd.set_option('display.expand_frame_repr', False)\n\n    warnings.filterwarnings(action='ignore')\n    warnings.simplefilter('ignore')\n    warnings.filterwarnings('ignore')\n    #warnings.filterwarnings(category=UserWarning)\n\n    warnings.filterwarnings('ignore', category=DeprecationWarning)\n    warnings.filterwarnings('ignore', category=FutureWarning)\n    warnings.filterwarnings('ignore', category=RuntimeWarning)\n    warnings.filterwarnings('ignore', category=UserWarning)\n    #warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n    pd.set_option('display.max_rows', 150)\n    pd.set_option('display.max_columns', 500)\n    pd.set_option('display.max_colwidth', None)\n\n    icecream = [\"#00008b\", \"#960018\",\"#008b00\", \"#00468b\", \"#8b4500\", \"#582c00\"]\n    #sns.palplot(sns.color_palette(icecream))\n    \n    return icecream\n\nicecream = jupyter_setting()\n\n# Colors\ndark_red = \"#b20710\"\nblack    = \"#221f1f\"\ngreen    = \"#009473\"\nmyred    = '#CD5C5C'\nmyblue   = '#6495ED'\nmygreen  = '#90EE90'\n\ncols= [myred, myblue,mygreen]","55832c32":"colors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","9d06a0ee":"def missing_zero_values_table(df):\n        mis_val         = df.isnull().sum()\n        mis_val_percent = round(df.isnull().mean().mul(100), 2)\n        mz_table        = pd.concat([mis_val, mis_val_percent], axis=1)\n        mz_table        = mz_table.rename(columns = {df.index.name:'col_name', \n                                                     0 : 'Valores ausentes', \n                                                     1 : '% de valores totais'})\n        \n        mz_table['Tipo de dados'] = df.dtypes\n        mz_table                  = mz_table[mz_table.iloc[:,1] != 0 ]. \\\n                                     sort_values('% de valores totais', ascending=False)\n        \n        msg = \"Seu dataframe selecionado tem {} colunas e {} \" + \\\n              \"linhas. \\nExistem {} colunas com valores ausentes.\"\n            \n        print (msg.format(df.shape[1], df.shape[0], mz_table.shape[0]))\n        \n        return mz_table.reset_index()","f54ca63b":"def describe(df):\n    var = df.columns\n\n    # Medidas de tend\u00eancia central, m\u00e9dia e mediana \n    ct1 = pd.DataFrame(df[var].apply(np.mean)).T\n    ct2 = pd.DataFrame(df[var].apply(np.median)).T\n\n    # Dispens\u00e3o - str, min , max range skew, kurtosis\n    d1 = pd.DataFrame(df[var].apply(np.std)).T\n    d2 = pd.DataFrame(df[var].apply(min)).T\n    d3 = pd.DataFrame(df[var].apply(max)).T\n    d4 = pd.DataFrame(df[var].apply(lambda x: x.max() - x.min())).T\n    d5 = pd.DataFrame(df[var].apply(lambda x: x.skew())).T\n    d6 = pd.DataFrame(df[var].apply(lambda x: x.kurtosis())).T\n    d7 = pd.DataFrame(df[var].apply(lambda x: (3 *( np.mean(x) - np.median(x)) \/ np.std(x) ))).T\n\n    # concatenete \n    m = pd.concat([d2, d3, d4, ct1, ct2, d1, d5, d6, d7]).T.reset_index()\n    m.columns = ['attrobutes', 'min', 'max', 'range', 'mean', 'median', 'std','skew', 'kurtosis','coef_as']\n    \n    return m","b7f85d25":"def graf_bar_churn(df, col, title, xlabel, ylabel, tol = 0):\n    \n    #ax    = df.groupby(['churn_cat'])['churn_cat'].count()\n    ax     = df    \n    colors = cols\n    \n    if tol == 0: \n        total  = sum(ax)\n        ax = (ax).plot(kind    ='bar',\n                   stacked = True,\n                    width = .5,\n                   rot     = 0,\n                   color   = colors)\n    else:\n        total  = tol     \n        \n        ax = (ax).plot(kind    ='bar',\n                       stacked = True,\n                       width = .5,\n                       rot     = 0,figsize = (10,6),\n                       color   = colors)\n\n    #ax.yaxis.set_major_formatter(mtick.PercentFormatter())\n    \n    #y_fmt = tick.FormatStrFormatter('%.0f') \n    #ax.yaxis.set_major_formatter(y_fmt)\n\n    title   = title + ' \\n'\n    xlabel  = '\\n ' + xlabel \n    ylabel  = ylabel + ' \\n'\n    \n    ax.set_title(title  , fontsize=22)\n    ax.set_xlabel(xlabel, fontsize=13)\n    ax.set_ylabel(ylabel, fontsize=13)    \n\n    min = [0,23000000]\n    #ax.set_ylim(min)\n    \n    for i in ax.patches:\n        # get_width pulls left or right; get_y pushes up or down\n        width, height = i.get_width(), i.get_height()\n        x, y = i.get_xy()        \n        \n        ax.annotate(str(round((i.get_height() * 100.0 \/ total), 1) )+'%', \n                    (i.get_x()+.3*width, \n                     i.get_y()+.5*height),\n                     color   = 'white',\n                     weight = 'bold',\n                     size   = 14)","922e903b":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","311ae452":"def reduce_memory_usage(df, verbose=True):\n    \n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    \n    for col in df.columns:\n        \n        col_type = df[col].dtypes\n        \n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n        \n    return df","4143251b":"def plot_roc_curve(fpr, tpr, label=None):\n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr, \"r-\", label=label)\n    ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.rcParams['font.size'] = 12\n    plt.title('ROC curve for TPS 09')\n    plt.xlabel('False Positive Rate (1 - Specificity)')\n    plt.ylabel('True Positive Rate (Sensitivity)')\n    plt.legend(loc=\"lower right\")\n    plt.grid(True)","176d60ec":"!mkdir Data\n!mkdir Data\/pkl\n!mkdir Data\/submission\n!mkdir model\n!mkdir model\/preds\n!mkdir model\/optuna\n\n!mkdir model\/preds\/test\n!mkdir model\/preds\/test\/n1\n!mkdir model\/preds\/test\/n2\n!mkdir model\/preds\/test\/n3\n\n!mkdir model\/preds\/train\n!mkdir model\/preds\/train\/n1\n!mkdir model\/preds\/train\/n2\n!mkdir model\/preds\/train\/n3\n!mkdir model\/preds\/param","7843707d":"path = '..\/input\/tabular-playground-series-oct-2021\/'\n#path = 'Data\/'","f28ce91e":"%%time\n\ndf1_train  = pd.read_csv(path + 'train.csv', index_col='id')\ndf1_test   = pd.read_csv(path + 'test.csv',index_col='id')\ndf_submission = pd.read_csv(path + 'sample_submission.csv')\n\ndf1_train.shape, df1_test.shape, df_submission.shape","c060bbd4":"print('TREINO')\nprint('Number of Rows: {}'.format(df1_train.shape[0]))\nprint('Number of Columns: {}'.format(df1_train.shape[1]), end='\\n\\n')\n\nprint('TESTE')\nprint('Number of Rows: {}'.format(df1_test.shape[0]))\nprint('Number of Columns: {}'.format(df1_test.shape[1]))","36df4959":"df1_train.info()","5ba701ff":"df1_test.info()","c0ac0cca":"print(f'{3*\"=\"} For Pandas {10*\"=\"}\\n{(df1_train.dtypes).value_counts()}')\nprint(f'\\n{3*\"=\"} For Datatable {7*\"=\"}\\n{(df1_test.dtypes).value_counts()}')","43112807":"feature_cat   = df1_test.select_dtypes(np.int64).columns.to_list()\nfeature_float = df1_test.select_dtypes(np.float64).columns.to_list()\n\ndf1_train[feature_cat].head()","c27ea16e":"df1_train[feature_float].head()","fd6be10e":"for col in feature_cat: \n    df1_train[col] = df1_train[col].astype(np.int8)\n    df1_test[col] = df1_test[col].astype(np.int8)\n    \nfor col in feature_float: \n    df1_train[col] = df1_train[col].astype(np.float32)\n    df1_test[col] = df1_test[col].astype(np.float32)","aa10ad24":"df1_train.info()","b0bbf19d":"df1_test.info()","043bd3a1":"missing = missing_zero_values_table(df1_train)\nmissing[:].style.background_gradient(cmap='Reds')","c43ecdff":"missing = missing_zero_values_table(df1_test)\nmissing[:].style.background_gradient(cmap='Reds')","d12e435c":"df_num = df1_train.select_dtypes(np.number)\ndf_cat = df1_train.select_dtypes(exclude=[np.number])\n\ndf_num.shape, df_cat.shape\n\nprint('Temos {} vari\u00e1vies num\u00e9ricas e {} categ\u00f3ricas.'.format(df_num.shape[1], df_cat.shape[1]))","a3afa4a2":"df1_train[feature_float].describe().style.background_gradient(cmap='YlOrRd')","185c0fc1":"df1_test[feature_float].describe().style.background_gradient(cmap='YlOrRd')","bc8c8367":"df1_train[feature_cat].columns","0a2f5e9d":"df = df1_train.corr().round(5)\n\n# M\u00e1scara para ocultar a parte superior direita do gr\u00e1fico, pois \u00e9 uma duplicata\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(16,16))\nax = sns.heatmap(df, annot=False, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"bold\", \"fontsize\":13})\n\nax.set_title(\"Mapa de calor de correla\u00e7\u00e3o das vari\u00e1vel\", fontsize=17)\n\nplt.setp(ax.get_xticklabels(), \n         rotation      = 90, \n         ha            = \"right\",\n         rotation_mode = \"anchor\", \n         weight        = \"normal\")\n\nplt.setp(ax.get_yticklabels(), \n         weight        = \"normal\",\n         rotation_mode = \"anchor\", \n         rotation      = 0, \n         ha            = \"right\");","c5188112":"fig, ax = plt.subplots(figsize=(5, 5))\n\npie = ax.pie([len(df1_train), len(df1_test)],\n             labels   = [\"Train dataset\", \"Test dataset\"],\n             colors   = [\"salmon\", \"teal\"],\n             textprops= {\"fontsize\": 15},\n             autopct  = '%1.1f%%')\n\nax.axis(\"equal\")\nax.set_title(\"Compara\u00e7\u00e3o de comprimento do conjunto de dados \\n\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","e11a8acb":"fig, ax = plt.subplots(figsize=(5, 5))\n\nplt.pie([len(feature_cat), len(feature_float)], \n        labels=['Categorical', 'Continuos'],\n        textprops={'fontsize': 13},\n        autopct='%1.1f%%')\n\n#ax.axis(\"equal\")\nax.set_title(\"Compara\u00e7\u00e3o vari\u00e1veis continuas\/categ\u00f3ricas \\n Dataset Treino\/Teste\", fontsize=18)\nfig.set_facecolor('white')\nplt.show()","dcc202c9":"L    = len(df1_train.columns[0:60])\nnrow = int(np.ceil(L\/6))\nncol = 6\n\nremove_last= (nrow * ncol) - L\n\nfig, ax = plt.subplots(nrow, ncol,figsize=(24, 30))\nfig.subplots_adjust(top=0.95)\ni = 1\n\nfor feature in df1_train.columns[0:60]:\n    \n    plt.subplot(nrow, ncol, i)\n    \n    ax = sns.kdeplot(df1_train[feature], shade=True, color='salmon',  alpha=0.5, label='train')\n    ax = sns.kdeplot(df1_test[feature], shade=True, color='teal',  alpha=0.5, label='test')\n    plt.xlabel(feature, fontsize=9)\n    plt.legend()\n    \n    i += 1\n    \nplt.suptitle('DistPlot: train & test data', fontsize=20)\nplt.show()","0f077c7c":"df_plot = ((df1_train - df1_train.min())\/(df1_train.max() - df1_train.min()))\nfig, ax = plt.subplots(4, 1, figsize = (25,25))\n\nfor i, (x) in enumerate([(1,30), (30,60), (60,90), (90,120)]): \n    sns.boxplot(data = df_plot.iloc[:, x[0]:x[1] ], ax = ax[i]);","51cb06d9":"df_plot = ((df1_test - df1_test.min())\/(df1_test.max() - df1_test.min()))\nfig, ax = plt.subplots(4, 1, figsize = (25,25))\n\nfor i, (x) in enumerate([(1,30), (30,60), (60,90), (90,120)]): \n    sns.boxplot(data = df_plot.iloc[:, x[0]:x[1] ], ax = ax[i]);","f0e788db":"plt.figure(figsize=(8, 6))\nax = sns.countplot(x=df1_train['target'], palette='viridis')\nax.set_title('Distribui\u00e7\u00e3o da vari\u00e1vel Target', fontsize=20, y=1.05)\n\nsns.despine(right=True)\nsns.despine(offset=10, trim=True)","aee5796c":"L    = len(df1_train.columns[0:60])\nnrow = int(np.ceil(L\/6))\nncol = 6\ni    = 1\nremove_last= (nrow * ncol) - L\n\nfig, ax = plt.subplots(nrow, ncol,figsize=(24, 30))\nfig.subplots_adjust(top=0.95)\n\nfor feature in df1_train.columns[0:60]:\n    \n    plt.subplot(nrow, ncol, i)\n    \n    ax = sns.kdeplot(df1_train[feature], \n                     shade    = True, \n                     palette  = 'viridis',  \n                     alpha    = 0.5, \n                     hue      = df1_train['target'], \n                     multiple = \"stack\")\n    \n    plt.xlabel(feature, fontsize=9)\n        \n    i += 1\n    \n    gc.collect()\n    \nplt.suptitle('DistPlot: Vari\u00e1vel de treino vs target', fontsize=20)\nplt.show()","06b417d0":"gc.collect()\n\nX      = df1_train.drop('target', axis=1)\ny      = df1_train['target']\nX_test = df1_test\ncols   = X.columns\n\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, \n                                                      test_size    = 0.2,\n                                                      shuffle      = True, \n                                                      stratify     = y,\n                                                      random_state = 0)\n\nX_train.shape, y_train.shape, X_valid.shape, y_valid.shape , X_test.shape","16fae38f":"X_test.head()","2a3faaef":"%%time \nseed   = 12359\nparams = {'random_state'  : seed,          \n          'predictor'     : 'gpu_predictor',\n          'tree_method'   : 'gpu_hist',\n          'eval_metric'   : 'auc'}\n\nmodel_baseline = xgb.XGBClassifier(**params)\n\nscalers = [None, StandardScaler(), RobustScaler(), MinMaxScaler(), \n           MaxAbsScaler(), QuantileTransformer(output_distribution='normal', random_state=0)]\n\nfor scaler in scalers: \n    \n    if scaler!=None:\n        X_train_s = scaler.fit_transform(X_train)\n        X_valid_s = scaler.fit_transform(X_valid)\n    else:\n        X_train_s = X_train\n        X_valid_s = X_valid\n                \n    model_baseline.fit(X_train_s, y_train, verbose = False)\n    y_hat = model_baseline.predict_proba(X_valid_s)[:, 1]\n    \n    fpr, tpr, thresholds = metrics.roc_curve(y_valid, y_hat)\n    \n    auc = metrics.auc(fpr, tpr)\n    print('Valida\u00e7ao AUC: {:2.5f} => {}'.format(auc, scaler))\n    \nprint()","086555b8":"path='Data\/'\n\ndef cross_val_model(model, scalers, name_file_submission, FOLDS=5, verbose=False, seed=59): \n    \n    mdl_train   = []\n    feature_imp = 0 \n    \n    for scaler in scalers: \n\n        df_submission.claim = 0\n        auc_best            = 0   \n        feature_imp_best    = 0       \n        auc                 = []\n        lloss               = []\n        f1                  = []\n        kfold               = KFold(n_splits=FOLDS, random_state=seed, shuffle=True)\n\n        if scaler!=None:\n            X_ts = scaler.fit_transform(X_test.copy())\n        else:\n            X_ts = X_test.copy()\n\n        print('='*80)\n        print('Scaler: {}'.format(scaler))\n        print('='*80)\n\n        for i, (train_idx, test_idx) in enumerate(kfold.split(X_train)):\n\n            i+=1\n\n            X_tr, y_tr = X_train.iloc[train_idx], y_train.iloc[train_idx]\n            X_vl, y_vl = X_train.iloc[test_idx], y_train.iloc[test_idx]\n\n            # Scaler\n            if scaler!=None:    \n                X_tr = scaler.fit_transform(X_tr)\n                X_vl = scaler.fit_transform(X_vl)                \n\n            eval_set     = [(X_tr,y_tr), (X_vl,y_vl)]            \n            n_estimators = model.get_params()['n_estimators'] \n\n            if n_estimators==100: \n                early_stopping_rounds = 20\n            else: \n                early_stopping_rounds = 200\n\n            model = model.fit(X_tr, y_tr, \n                              eval_set              = eval_set,\n                              early_stopping_rounds = early_stopping_rounds, \n                              verbose               = verbose,\n                             )\n            \n            best_ntree = model.best_ntree_limit\n            \n            y_hat_prob = model.predict_proba(X_vl, ntree_limit=best_ntree)[:, 1] #   \n            y_hat      = (y_hat_prob >.5).astype(int) \n\n            fpr, tpr, thresholds = metrics.roc_curve(y_vl, y_hat_prob)\n\n            log_loss_     = metrics.log_loss(y_vl, y_hat_prob)                \n            f1_score_     = metrics.f1_score(y_vl, y_hat)        \n            auc_          = metrics.auc(fpr, tpr)    \n\n            stop = ''\n            \n            if n_estimators > best_ntree: \n                stop = '*'\n                \n            msg = '[Fold {}] AUC: {:.5f} - F1: {:.5f} - L. LOSS: {:.5f} {}'\n            print(msg.format(i, auc_, f1_score_,log_loss_, stop))\n\n            # Getting mean feature importances (i.e. devided by number of splits)\n            feature_imp  += model.feature_importances_ \/ FOLDS\n            \n            df_submission['target'] += model.predict_proba(X_ts)[:, 1] \/ FOLDS\n\n            f1.append(f1_score_)\n            lloss.append(log_loss_)\n            auc.append(auc_)\n                        \n        auc_mean   = np.mean(auc)\n        auc_std    = np.std(auc)\n        lloss_mean = np.mean(lloss)\n        f1_mean    = np.mean(f1)\n        \n        if auc_mean > auc_best: \n            auc_best          = auc_mean\n            f1_best           = f1_mean\n            lloss_best        = lloss_mean\n            model_best        = model\n            feature_imp_best  = feature_imp\n            scaler_best       = scaler\n                                    \n        print('-'*80)\n        msg = '[Mean Fold] AUC: {:.5f}(Std:{:.5f}) - F1: {:.5f} - L. LOSS: {:.5f}'\n        print(msg.format(auc_mean,auc_std, f1_mean, lloss_mean))\n        print('='*80)\n        print('')\n\n        # Gerar o arquivo de submiss\u00e3o \n        name_file_sub = 'submission\/' + name_file_submission + '_' + str(scaler).lower()[:4] + '.csv'\n        df_submission.to_csv(path + name_file_sub, index = False)\n\n        gc.collect()\n        \n    print()\n    print('='*80)\n    print('Scaler Best: {}'.format(scaler_best))\n    print('AUC        : {:2.5f}'.format(auc_best))\n    print('F1-Score   : {:2.5f}'.format(f1_best))\n    print('L. Loss    : {:2.5f}'.format(lloss_best))\n    print('='*80)\n    print()\n            \n    gc.collect()  \n    \n    return model_best, feature_imp_best ","a78e41f7":"%%time\n\ngc.collect()\n\nseed    = 12359\nscalers = [None, StandardScaler(), RobustScaler(), MinMaxScaler(), \n           MaxAbsScaler(), QuantileTransformer(output_distribution='normal', random_state=0)]\n\nmodel_best, feat_imp_best = cross_val_model(xgb.XGBClassifier(**params), \n                                            scalers, \n                                            '001_xgb_baseline', \n                                            FOLDS = 5, \n                                            seed  = seed\n                                            )","d5e176fb":"%%time\n    \nseed   = 112359    \nparams = {'random_state'  : seed,         \n          'n_estimators'  : 1000,\n          'predictor'     : 'gpu_predictor',\n          'tree_method'   : 'gpu_hist',\n          'eval_metric'   : 'auc'}\n\nscalers = [None, \n           RobustScaler(), \n           QuantileTransformer(output_distribution='normal', random_state=0)]\n\nmodel_best, feat_imp_best  = cross_val_model(xgb.XGBClassifier(**params), \n                                             scalers, \n                                             '002_xgb_bl_n_estimators_1000', \n                                             FOLDS = 5, \n                                             seed  = seed\n                                            )","30d5f1a5":"model_best","c638fda0":"results     = model_best.evals_result()\nntree_limit = model_best.best_ntree_limit\n\nplt.figure(figsize=(7,5))\nplt.plot(results[\"validation_0\"][\"auc\"], label=\"Treinamento\")\nplt.plot(results[\"validation_1\"][\"auc\"], label=\"Valida\u00e7\u00e3o\")\n\n\nplt.axvline(ntree_limit, \n            color=\"gray\", \n            label=\"N. de \u00e1rvore ideal {}\".format(ntree_limit))\n\nplt.xlabel(\"N\u00famero de \u00e1rvores\")\nplt.ylabel(\"AUC\")\nplt.legend();","2693c833":"%%time\nthreshold =.5\n\ny_pred_prob = model_best.predict_proba(X_valid, ntree_limit=ntree_limit)[:, 1] \ny_pred      = (y_pred_prob > threshold).astype(int)\n\nf1_    = metrics.f1_score(y_valid, y_pred)\nauc_   = metrics.roc_auc_score(y_valid, y_pred)\nlloss_ = metrics.log_loss(y_valid, y_pred_prob) \n    \nprint('AUC     : {:2.5f}'.format(auc_))\nprint('AUC     : {:2.5f}'.format(auc))\nprint('F1-Score: {:2.5f}'.format(f1_))\nprint('L. Loss : {:2.5f}'.format(lloss_))\nprint()","6afe88f7":"metrics.plot_confusion_matrix(model_best, X_valid, y_valid, cmap='inferno')\nplt.title('Confusion matrix')\nplt.grid(False)\nplt.show()","92fdf7d3":"threshold    = .5\ny_pred_valid = (y_pred_prob > threshold).astype(int)\nf1_          = metrics.f1_score (y_valid, y_pred_valid)\nauc_         = metrics.roc_auc_score(y_valid, y_pred_prob)\n\nprint(metrics.classification_report(y_valid, y_pred_valid))\nprint('')\nprint('AUC     : {:2.5f}'.format(auc_))\nprint('F1-score: {:2.5f}'.format(f1_))\n","eebb4d83":"fpr, tpr, thresholds = metrics. roc_curve(y_valid, y_pred)\n\nplot_roc_curve(fpr, tpr, label=\"XGB\")\nplt.show()","54211039":"feature_imp_ = feat_imp_best","371b69f2":"df               = pd.DataFrame()\ndf[\"Feature\"]    = X.columns\ndf[\"Importance\"] = feature_imp_ \/ feature_imp_.sum()\n\ndf.sort_values(\"Importance\", axis=0, ascending=False, inplace=True)","752fedcb":"fig, ax = plt.subplots(figsize=(13, 70))\nbars    = ax.barh(df[\"Feature\"], \n                  df[\"Importance\"], \n                  height    = 0.4,\n                  color     = \"mediumorchid\", \n                  edgecolor = \"black\")\n\nax.set_title(\"Feature importances\", fontsize=30, pad=15)\nax.set_ylabel(\"Feature name\", fontsize=20, labelpad=15)\n#ax.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax.set_yticks(df[\"Feature\"])\nax.set_yticklabels(df[\"Feature\"], fontsize=13)\nax.tick_params(axis=\"x\", labelsize=15)\nax.grid(axis=\"x\")\n\n# Adicionando r\u00f3tulos na parte superior\nax2 = ax.secondary_xaxis('top')\n#ax2.set_xlabel(\"Feature importance\", fontsize=20, labelpad=13)\nax2.tick_params(axis=\"x\", labelsize=15)\nax.margins(0.05, 0.01)\n\n# Inverter a dire\u00e7\u00e3o do eixo y \nplt.gca().invert_yaxis()","cc92dd42":"<div class=\"alert alert-info\" role=\"alert\"> \n    \n`NOTA:` <br>\n\nN\u00e3o temos dados faltantes.\n    \n<\/div>","36c26428":"<div class=\"alert alert-info\" role=\"alert\"> \n \n`NOTA:` <br>\n    \nPodemos observar no gr\u00e1fico acima que n\u00e3o temos desbalanceamento nos dados. \n    \n<\/div>","20681345":"<div class=\"alert alert-info\" role=\"alert\"> \n \n`NOTA:` <br>\n    \nComo podemos ver, a correla\u00e7\u00e3o est\u00e1 entre ~-0,02 e 0, o que \u00e9 muito pequeno. Portanto, as vari\u00e1veis s\u00e3o fracamente correlacionados e negativas\n    \n<\/div>","c6aae879":"## 1.2. Fun\u00e7\u00f5es\nAqui centralizamos todas as fun\u00e7\u00f5es desenvolvidas durante o projeto para melhor organiza\u00e7\u00e3o do c\u00f3digo.","976c626b":"### 2.1.1. N\u00famero de Estimadores","e357f9c0":"<div class=\"alert alert-info\" role=\"alert\"> \n    \n`NOTA:` <br>\n   \n    \n<\/div>\n\n### 2.2.4. Feature Importances  ","fed2bbf9":"### 1.2.2. Distribui\u00e7\u00e3o","5894037a":"- Test","b3e780db":"# <div class=\"alert alert-success\">  2. Split Train\/Test <\/div> <br>","07220f48":"#### 1.2.2.4. Vari\u00e1veis preditoras  vs Target.","b84be297":"# <div class=\"alert alert-success\">  1.0. EDA  <\/div> ","9d9d13e6":"## 1.2. An\u00e1lise Gr\u00e1fica","1803d00a":"<div class=\"alert alert-info\" role=\"alert\"> \n    \n`NOTA:` <br>\n    \nObservando o resultado acima nos dados de valida\u00e7\u00e3o, o nosso modelo parece que est\u00e1 com um pequeno overfitting em rela\u00e7\u00e3o ao treinamento, mesmo assim \u00e9 um modelo bom, pois est\u00e1 generalizando pelo resultado na submiss\u00e3o na competi\u00e7\u00e3o.      \n    \n<\/div>","fda111a9":"<div class=\"alert alert-info\" role=\"alert\"> \n \n`NOTA:` <br>\n\n- Os conjuntos de treinamento e teste t\u00eam aproximadamente as mesmas distribui\u00e7\u00f5es em termos de vari\u00e1veis; <br>\n- Temos poucas vari\u00e1veis com distribui\u00e7\u00f5es normais; <br>\n- A maioria das vari\u00e1veis tem distribui\u00e7\u00f5es distorcidas. <br>\n\nPrecisamos pensar em como fazer tudo isso normalmente distribu\u00eddo se decidirmos usar modelos n\u00e3o baseados em \u00e1rvore. <br>\n\n> A verifica\u00e7\u00e3o da correla\u00e7\u00e3o n\u00e3o revelou rela\u00e7\u00f5es significativas entre as caracter\u00edsticas (a maioria estava entre -0.02  e 0).\n\n<\/div>","395185b2":"<div class=\"alert alert-info\" role=\"alert\"> \n \n`NOTA:` <br>\n    \nAcima observamos que temos muitos outliers em ambos conjunto de dados, no processamento vamos trart\u00e1-los.\n\n<\/div>","90d21505":"Uma redu\u00e7\u00e3o de 46.37% no dataset de treino e 43.91% no dataset de test. ","ca2b99cd":"##### 1.2.2.1.2. Data Test","40683e1d":"#### 1.1.6.1. Atributos Num\u00e9ricos","075d7350":"- Train","2ff6d9ba":"### 1.1.4. Idenficar Vari\u00e1veis Ausentes (NA)\nVamos verificar os valores ausentes em cada vari\u00e1vel conjunto de treinono e teste.","f2d71cbd":"<div class=\"alert alert-info\" role=\"alert\"> \n \n`NOTA:` <br>\n    \nCom scaler RobustScaler obtivemos uma AUC de 0.85144, como estamos fazer apenas uma valida\u00e7\u00e3o (treino\/valida\u00e7\u00e3o), neste caso a pontua\u00e7\u00e3o do score pode ser afetada por aleatoriedade dos dados, sendo assim, vamos fazer uma valida\u00e7\u00e3o cruzada para termos uma estimativa robusta.  <br>\n\nPara o treinamento do modelo foi criado a fun\u00e7\u00e3o abaixo que tem a finalidade de treinar um conjunto de scalers para um determinado modelo, durante o treinamento ser\u00e1 exibido os resultados e no final ser\u00e1 retornado o melhor modelo com as vari\u00e1veis e seus scores de import\u00e2ncia.\n    \n<\/div>","0b6b5261":"#### 1.2.2.1. Detec\u00e7\u00e3o de Outlier","ab97cff6":"#### 1.2.2.2. Propor\u00e7\u00e3o das vari\u00e1veis","7c880c6d":"<div class=\"alert alert-info\" role=\"alert\"> \n \n`NOTA:` <BR>\n    \nNo primeiro dataset, temos vari\u00e1veis categ\u00f3ricas que sofreram transforma\u00e7\u00f5es para dammy e no segundo dataset temos vari\u00e1veis quantitativas continuas, ent\u00e3o agora vamos transformar os tipos dessas vari\u00e1vieis para fazermos a redu\u00e7\u00e3o. \n    \n<\/div>","e94f0ef0":"# <div class=\"alert alert-success\">  2. Modelo Baseline XGB <\/div> <br>","90f2e393":"<div class=\"alert alert-info\" role=\"alert\"> \n \n`NOTA:` <br>\nNa submiss\u00e3o na competi\u00e7\u00e3o obtivemos uma AUC de `0.85407` com o QuantileTransformer em rela\u00e7\u00e3o ao treinamento do modelo, vamos fazer uma pequena an\u00e1lise do comportamento do modelo na pr\u00f3xima se\u00e7\u00e3o.\n   \n<\/div>","ea37913e":"### 2.1.2. Matriz de Confus\u00e3o","d230b72b":"#### 1.1.6.1. Atributos Categ\u00f3ricos","4b87797d":"<div class=\"alert alert-info\" role=\"alert\">\n\nNa valida\u00e7\u00e3o cruzada obtivemos uma `AUC` de `0.85093` com um desvio padr\u00e3o de `0.00088` sem fazer o scaler, vamos gerar dois arquivos com as melhores pontua\u00e7\u00e3o de AUC para submiss\u00e3o. <br>\n    \n`NOTA:` <br>\n   \nNa valida\u00e7\u00e3o cruzada que foi realizada, mostra que n\u00e3o fazer a normaliza\u00e7\u00e3o temos a melhor AUC, e isso \u00e9 confirmado na submiss\u00e3o da competi\u00e7\u00e3o, abaixo as duas submiss\u00f5es:   <br>\n    \n- AUC: 0.85383 => Sem normaliza\u00e7\u00e3o; <br> \n- AUC: 0.85246 => RobustScaler <br>\n    \nVamos treinar tr\u00eas modelos, sendo que vamos acrescentar o parametros `n_estimators` que indica o n\u00famero de arvores para o treinamento do modelo, pr\u00f3ximos notebooks vamos fazer ajuste em diversos parametros para ajusdar no score.\n    \n<\/div>","8094dced":"#### 1.2.2.3. Target\nA vari\u00e1vel alvo tem os valores 0 e 1, vamos verificar a distribui\u00e7\u00e3o da target.","f0d9da15":"### 2.1.3 Curva ROC","13c3f204":"<div class=\"alert alert-info\" role=\"alert\"> \n \n`NOTA:` <BR>\n    \n- O dataset de treiro tem 2.1 GB com 1000000 de registros e 286 columas; \n- O dataset de teste tem 1.1 GB com 500000 de registros e 285 columas\n    \nVamos fazer uma redu\u00e7\u00e3o desses dataset, primeiro vamos identificar os tipos de dados que temos nos datasets.\n\n<\/div>","d19f6d52":"### 1.2.1. Correla\u00e7\u00e3o\nVamos examinar a correla\u00e7\u00e3o entre as vari\u00e1veis.","f5ef0c8c":"<div class=\"alert alert-info\" role=\"alert\"> \n    \n`NOTA:` <br>\nAcima recuperamos as informa\u00e7\u00f5es de treinamento do nosso modelo, podemos observar que o n\u00famero de 1000 estimadores \u00e9 mais que suficiente para o treinamento do modelo, o ideal \u00e9 que fique em torno de 75 \u00e0 95 para esses dados e com a utiliza\u00e7\u00e3o dos parametros padr\u00f5es que devem ser ajustados para o `XGB`. <br>\n    \n    \nVamos agora utilizar o modelo treinado que foi retornado pela fun\u00e7\u00e3o e vamos fazer a previs\u00e3o para novos dados que o modelo n\u00e3o viu no treinamento, para termos uma ideia da generaliza\u00e7\u00e3o do modelo, lembrando que o modelo que foi treinado utiliza 1000 estimadores (arvores), sendo assim, vamos utilizar na previss\u00e3o 86 estimadores utilizando o parametro `ntree_limit` ao fazermos as previs\u00f5es.\n\n<\/div>","3d59ede8":"##### 1.2.2.1.1. Data Train ","5d94f941":"#### 1.2.2.1. Target\nVamos ver as ocorr\u00eancias de n\u00fameros individuais do conjunto de dados de treino.","b36b1ed2":"#### 1.2.2.1. Train \/ Test","75d71da0":"# Descri\u00e7\u00e3o de dados\n\nPara esta competi\u00e7\u00e3o, voc\u00ea vai prever se um cliente fez uma reclama\u00e7\u00e3o sobre uma ap\u00f3lice de seguro. A verdade fundamental claimtem valor bin\u00e1rio, mas uma previs\u00e3o pode ser qualquer n\u00famero de 0.0 para 1.0, representando a probabilidade de uma reclama\u00e7\u00e3o. Os recursos neste conjunto de dados foram tornados an\u00f4nimos e podem conter valores ausentes.\narquivos\n\n- `train.csv`: os dados de treinamento com o alvo claimcoluna\n- `test.csv`: o conjunto de teste; voc\u00ea estar\u00e1 prevendo o claimpara cada linha neste arquivo\n- `sample_submission.csv`:  um arquivo de envio de amostra no formato correto","89a64fb2":"<h1 div class='alert alert-success'><center> TPS-Oct: ponto de partida (EDA, linha de base XGB)<\/center><\/h1>\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/26480\/logos\/header.png?t=2021-04-09-00-57-05)","682af87f":"Nesta etapa, treinaremos nosso modelo **XGBClassifier** de linha de base simples, em rela\u00e7\u00e3o ao tratamento dos dados, vamos fazer apenas o scaler nesta etapa.","01186e04":"<div class=\"alert alert-info\" role=\"alert\"> \n    \n**`NOTA:`** <br>\n   \n    \n<\/div>","80345fdc":"## 1.3. Carregar Dados\nExistem 2 conjuntos de dados que s\u00e3o usados na an\u00e1lise, eles s\u00e3o o conjunto de dados de treinamento e de teste. O principal uso do conjunto de dados de treino \u00e9 treinar os modelos e us\u00e1-lo para prever o conjunto de dados de teste. \n\nEnquanto o arquivo de envio de amostra \u00e9 usado para informar os participantes sobre a inscri\u00e7\u00e3o prevista para a competi\u00e7\u00e3o. ","ab6be1f0":"## 2.1. An\u00e1lise do Modelo \n","3a43001f":"### 1.1.6. Estat\u00edstica Descritiva\nAbaixo est\u00e3o as estat\u00edsticas b\u00e1sicas para cada vari\u00e1vel que cont\u00e9m informa\u00e7\u00f5es sobre contagem, m\u00e9dia, desvio padr\u00e3o, m\u00ednimo, 1\u00ba quartil, mediana, 3\u00ba quartil e m\u00e1ximo.","931df41a":"## 1.1. Bibliotecas ","f25b8269":"### 1.1.2. Dimens\u00e3o do DataSet","db07b30f":"### 1.1.3. Tipo de Dados","ed6b1551":"# <div class=\"alert alert-success\">  1. IMPORTA\u00c7\u00d5ES <\/div> \n","2432df5e":"Temos dois tipos de dados nos datasets, sendo vamos criar duas vari\u00e1veis para podermos entender o conte\u00fado das informa\u00e7\u00f5es armazenadas. "}}