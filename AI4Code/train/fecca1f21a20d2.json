{"cell_type":{"42d1ba8f":"code","78e2e182":"code","b85c7526":"code","a2be4fea":"code","ed533419":"code","d7479d73":"code","c9fed2bc":"markdown","254212d6":"markdown","c2033769":"markdown","f5842fb3":"markdown","9ad55578":"markdown","0bc7ea2e":"markdown","39f0fcdd":"markdown","aac8a402":"markdown","db8a9ae5":"markdown","d76cf7ec":"markdown","1bdb67ed":"markdown","b5e18b5c":"markdown","3a44acc7":"markdown","17126a6b":"markdown","f0ff8dec":"markdown","334e3615":"markdown","2ccb255f":"markdown","bd495bda":"markdown","bb8b742d":"markdown","60d0eb72":"markdown","a7469393":"markdown","aaf82ab0":"markdown","bf02028e":"markdown","163fd445":"markdown","6676096b":"markdown"},"source":{"42d1ba8f":"# from imblearn.under_sampling import RandomUnderSampler\n\n# undersample_data = RandomUnderSampler(sampling_strategy=0.5)\n\n# X_under, y_under = undersample_data.fit_resample(X_train, y_train)","78e2e182":"# from imblearn.over_sampling import RandomOverSampler\n\n# oversample_data = RandomOverSampler(sampling_strategy=0.5)\n\n# X_over, y_over = oversample_data.fit_resample(X_train, y_train)","b85c7526":"# from imblearn.over_sampling import SMOTE\n\n# oversample_data = SMOTE(sampling_strategy=0.5)\n\n# X_smote, y_smote = oversample_data.fit_resample(X, y)","a2be4fea":"# over_sampler = RandomOverSampler(sampling_strategy=0.2)\n# under_sampler = RandomUnderSampler(sampling_strategy=0.5)\n\n# X_over, y_over = over_sampler.fit_resample(X_train, y_train)","ed533419":"# oversampled_data = SMOTE(sampling_strategy=0.5)\n\n# X_over, y_over = oversampled_data.fit_resample(X, y)\n","d7479d73":"# X_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.3)\n                                                                                                                  ","c9fed2bc":"<center>Consider training set with class A (50 records) and Class B (10 records)<center>","254212d6":"### Binary classification (Consider classes A and B)\n\n ### Example: \n- lets say training set contains 90% class 'A' data and 10% class 'B' data.\n- The result will ignore class 'B' as the training set seen much of class 'A' data compared to class 'B'.\n","c2033769":"* In above code, we are trying to do the oversampling for entire dataset and then splitting the train\/test data.\n* This is not the right way as it leads to 'Data Leakage'.\n* Split the train\/test data and then apply sampling techniques on train data set.","f5842fb3":"#### Sampling strategy = 0.5 means, it is the ratio of minority classess to majority classes","9ad55578":"# <center>Create samples only for train data set<center>","0bc7ea2e":"# Introduction","39f0fcdd":"* In undersampling, we try to reduce the sample size of Class A\n* Bring down the sample of Class A to either 20 or 10 as shown below, so that ratio of A:B is 2:1 or 1:1.. so that there wont be any overfit scenario.\n* This can be done using parameter 'sampling_strategy'","aac8a402":"**python code to perform oversampling**","db8a9ae5":"##   <center>Undersampling technique<center>\n\n","d76cf7ec":"* This technique is very similar to Oversampling, but it will not create duplicates like oversampling\n* New Samples are created by considering the features close to them","1bdb67ed":"<center>In this technique, we will be adding records to the minority class<center>","b5e18b5c":"### Multi classification (Lets consider three classes A, B and C)\n\n ### Example: \n- lets say training set contains 70% class 'A' data, 20% class 'B' data and 10% class 'C' data.\n- The result will ignore class 'B' and 'C' as the training set seen much of class 'A' data compared to class 'B' and class 'C'.","3a44acc7":"![3.PNG](attachment:3.PNG)","17126a6b":"### Now lets try to understand with some layman language and how to encounter them","f0ff8dec":"![imbalance.jpg](attachment:imbalance.jpg)","334e3615":"##   <center>SMOTE<center>\n####   <center>Synthetic Minority Over-sampling Technique<center>","2ccb255f":"![1.PNG](attachment:1.PNG)","bd495bda":"* Please note: Samples are created for minority classes using 'Random sampling with replacement'. \n* So there will be duplicates. In order to avoid the duplicates there is one technique called 'SMOTE'","bb8b742d":"**python code to perform undersampling**","60d0eb72":"### Combining oversampling and undersampling\n","a7469393":"##   <center>Oversampling technique<center>","aaf82ab0":"**There are many other methods through which we can nullify imbalance in the classification**\n","bf02028e":"![1.PNG](attachment:1.PNG)","163fd445":"![2.PNG](attachment:2.PNG)","6676096b":"* In oversampling, we try to increase the sample size of Class B\n* Make the sample size of Class B to either 40 or 50 as shown below.\n* This can be done using parameter 'sampling_strategy'"}}