{"cell_type":{"e24dfad8":"code","e09ff159":"code","51300307":"code","f60cbdd4":"code","b89d2739":"code","f8b883e9":"code","ddec841e":"code","5d3d04a5":"code","9274a295":"code","9ee99cc8":"code","260dd961":"code","7164045a":"code","5d7d61ae":"code","ba323442":"code","5802168f":"code","04740da6":"code","5ee93672":"code","f6a99c1b":"code","9b786c11":"code","dfcdbbc5":"code","04bf7fce":"code","da7f62e1":"code","c090a232":"code","19b39164":"code","1a710864":"code","5f2fae28":"code","e44d8fb4":"code","145fb366":"code","9a1b1576":"code","da4e8245":"code","cc2d3612":"code","ab03e8e0":"code","c35f3ed2":"code","840a29e2":"code","272ca30b":"code","dd2961d5":"code","8e0efe12":"code","109dd9c2":"code","ed1b3275":"code","0e085b49":"code","ae250b91":"code","62d8c463":"code","ed431108":"code","86728cf6":"code","31eb0338":"code","d491c611":"code","97b02a63":"code","eb525d3a":"code","fe2d9e01":"code","e8bb81c3":"code","268465d9":"code","0585d2aa":"code","4d62fcb2":"code","a25c4cf8":"code","9193d019":"code","86f2d0a1":"code","56bab66e":"code","59c9db43":"code","0ed0c3b8":"code","469fdc42":"code","c8596d46":"code","0e9597b9":"code","eb5eb74f":"code","764906b1":"code","6d9889f0":"code","15bd6cbb":"code","75f2c048":"code","f6af3f5d":"code","ea32e679":"code","55146deb":"code","308a6dd2":"code","5cbb95fa":"code","28757cbc":"code","9f2536a5":"code","94cecb10":"code","45661fa3":"code","881ab8bd":"code","888b5d41":"markdown","011eb6fb":"markdown","bd8f488b":"markdown","54e3729d":"markdown","bce88ebf":"markdown","9a868481":"markdown","21644cd6":"markdown","a905712f":"markdown","c63a76f7":"markdown","fa11bd99":"markdown","7436bfdb":"markdown","4a07cc1d":"markdown"},"source":{"e24dfad8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn.metrics as metrics\nfrom sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split","e09ff159":"# lets import data from the files \ntrain_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')\nsubmission_data = pd.read_csv('..\/input\/gender_submission.csv')","51300307":"# lets view the data \ntrain_data.head()","f60cbdd4":"# Objective of the problem is to classify among the passengers who survived.\n# Target variable considered here is Survived\ntrain_target = train_data.Survived\n#train_data.drop('Survived',axis=1,inplace=True)\n#Since passenger Id doesn't add any significant to Survival lets drop that variable as well\ntrain_data.drop('PassengerId',axis=1,inplace=True)","b89d2739":"train_data.head()","f8b883e9":"train_target.head()","ddec841e":"# lets see the basic statistics of data\ntrain_data.describe()","5d3d04a5":"train_data.describe(include=['O'])","9274a295":"# Lets check the correlation matrix\ntrain_data.corr()","9ee99cc8":"# lets Clean the data first\ntrain_data.info() # check the null in data","260dd961":"# from the above its clear Age,Cabin and Embarked are having null values\ntrain_data[train_data.Embarked.isnull()]","7164045a":"# these are the two instance where Embarked in null.\n# lets fill them with most frequent value.\ntrain_data[((train_data.Pclass==1)&(train_data.Fare <81)&(train_data.Fare >79)&(train_data.Sex =='female'))].Embarked.mode()","5d7d61ae":"# Lets fill the embarked null values with S\ntrain_data.Embarked.fillna('S',inplace=True)","ba323442":"combine = [train_data,test_data]","5802168f":"# lets convert Sex to binary categorical\nfor df in combine:\n    df['Sex'] = np.where(df.Sex=='male',1,0)","04740da6":"# lets now take care of Age and cabin\nfor dataset in combine:\n    # Filling all Nan with unknown cabin name.\n    dataset.Cabin.fillna('Unknown',inplace=True)\n    dataset['Cabin_lt'] = dataset.Cabin.apply(lambda x: str(x)[0])","5ee93672":"sns.countplot(train_data.Cabin_lt)\nplt.show()","f6a99c1b":"sns.countplot(test_data.Cabin_lt)\nplt.show()","9b786c11":"sns.barplot(x='Cabin_lt',y='Survived',data=train_data)\nplt.show()","dfcdbbc5":"# lets check how the class and Cabin_lt are related and replace it accordingly\ntrain_data.groupby(['Pclass','Survived','Sex']).Cabin_lt.value_counts()","04bf7fce":"# lets implement the above in order impute the cabin\ntrain_data['Cabin']= np.where(((train_data.Pclass == 1)&(train_data.Survived == 0)&(train_data.Sex == 1)&(train_data.Cabin_lt == 'U')),'C',\n                             np.where(((train_data.Pclass == 1)&(train_data.Survived == 1)&(train_data.Sex == 0)&(train_data.Cabin_lt == 'U')),'B',\n                             np.where(((train_data.Pclass == 1)&(train_data.Survived == 1)&(train_data.Sex == 1)&(train_data.Cabin_lt == 'U')),'C',\n                             np.where(((train_data.Pclass == 2)&(train_data.Survived == 0)&(train_data.Sex == 0)&(train_data.Cabin_lt == 'U')),'E',\n                             np.where(((train_data.Pclass == 2)&(train_data.Survived == 0)&(train_data.Sex == 1)&(train_data.Cabin_lt == 'U')),'D',\n                             np.where(((train_data.Pclass == 2)&(train_data.Survived == 1)&(train_data.Sex == 0)&(train_data.Cabin_lt == 'U')),'F',\n                             np.where(((train_data.Pclass == 2)&(train_data.Survived == 1)&(train_data.Sex == 1)&(train_data.Cabin_lt == 'U')),'F',\n                             np.where(((train_data.Pclass == 3)&(train_data.Survived == 0)&(train_data.Sex == 0)&(train_data.Cabin_lt == 'U')),'G',\n                             np.where(((train_data.Pclass == 3)&(train_data.Survived == 0)&(train_data.Sex == 1)&(train_data.Cabin_lt == 'U')),'F',\n                             np.where(((train_data.Pclass == 3)&(train_data.Survived == 1)&(train_data.Sex == 0)&(train_data.Cabin_lt == 'U')),'G',\n                             np.where(((train_data.Pclass == 3)&(train_data.Survived == 1)&(train_data.Sex == 1)&(train_data.Cabin_lt == 'U')),'E',\n                             np.where(((train_data.Pclass == 1)&(train_data.Survived == 0)&(train_data.Sex == 1)&(train_data.Cabin_lt == 'T')),'C',\n                             train_data.Cabin_lt))))))))))))","da7f62e1":"#Cross checking the update\ntrain_data.groupby(['Pclass','Survived','Sex']).Cabin.value_counts()","c090a232":"# lets check how the class and Cabin_lt are related and replace it accordingly\ntest_data.groupby(['Pclass','Sex']).Cabin_lt.value_counts()","19b39164":"test_data['Cabin'] = np.where(((test_data.Pclass == 1)&(test_data.Cabin_lt == 'U')),'C',\n                             np.where(((test_data.Pclass == 2)&(test_data.Sex == 0)&(test_data.Cabin_lt == 'U')),'F',\n                                     np.where(((test_data.Pclass == 2 )&(test_data.Sex == 1)&(test_data.Cabin_lt == 'U')),'D',\n                                             np.where(((test_data.Pclass == 3)&(test_data.Sex == 0)&(test_data.Cabin_lt =='U')),'G',\n                                                     np.where(((test_data.Pclass == 3)&(test_data.Sex == 1)&(test_data.Cabin_lt =='U')),'F',\n                                                             test_data.Cabin_lt)))))","1a710864":"test_data.groupby(['Pclass','Sex']).Cabin.value_counts()","5f2fae28":"# lets drop the Cabin_lt from train and test\nfor dataset in combine:\n    del dataset['Cabin_lt']","e44d8fb4":"for dataset in combine:\n    dataset['title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.',expand=False)\n    ","145fb366":"train_data.title.value_counts()","9a1b1576":"train_data[((train_data.Age.isnull())&(train_data.title == 'Dr'))] # since there is only 1 Dr with age NaN so lets put Dr into others","da4e8245":"for dataset in combine:# converting the titles into numericals\n    dataset['title'] = np.where((dataset.title == 'Mr'),1,\n                               np.where(dataset.title == 'Miss',2,\n                                       np.where(dataset.title == 'Mrs',3,\n                                               np.where(dataset.title == 'Master',4,5))))","cc2d3612":"train_data.title.value_counts()","ab03e8e0":"# there is one more missing value that tobe imputed that is Age. so lets derive as many as varibles and use them to predict Age\n# let us get one more variable Family size which can be derived from SibSp and Parch\nfor dataset in combine:\n    dataset['Family'] = dataset.SibSp+dataset.Parch+1\n    dataset['Family'] = np.where(dataset.Family == 1,'Single',\n                                np.where(dataset.Family == 2,'Couple',\n                                        np.where(dataset.Family <=4 ,'Nuclear','LargeF')))","c35f3ed2":"sns.barplot(x=train_data.Family,y=train_data.Survived,data=train_data)\nplt.show()# it shows Nuclear and Couples have survived in larger number","840a29e2":"sns.boxplot(y=train_data.Fare)","272ca30b":"train_data['Fare'] = train_data.Fare.astype('int32')\n","dd2961d5":"test_data[test_data.Fare.isnull()]","8e0efe12":"test_data.Fare.fillna(test_data[((test_data.Pclass == 3)&(test_data.Sex ==1)&(test_data.Embarked =='S')&(test_data.Family=='Single'))].Fare.mean(),inplace=True)","109dd9c2":"#lets check if the ticket is having any relation with survival\ntrain_data.groupby(['Pclass']).Fare.min()   #[train_data.Fare >= 500].count()#['Ticket','Fare','Pclass','Family','Parch','SibSp']]","ed1b3275":"test_data.groupby(['Pclass']).Fare.min()","0e085b49":"sns.distplot(train_data.Fare)\nplt.show()","ae250b91":"sns.distplot(test_data.Fare)\nplt.show()","62d8c463":"for dataset in combine:\n    dataset['Ticket_str'] = dataset.Ticket.apply(lambda x : str(x)[0])","ed431108":"train_data.groupby(['Pclass']).Ticket_str.value_counts()","86728cf6":"train_data.head()","31eb0338":"for dataset in combine:\n    dataset['Ticket_str'] = dataset['Ticket_str'].apply(lambda x: str(x))\n    dataset['Ticket_str'] = np.where((dataset['Ticket_str']).isin(['1', '2', '3', 'S', 'P', 'C', 'A']), dataset['Ticket_str'],\n                                np.where((dataset['Ticket_str']).isin(['W', '4', '7', '6', 'L', '5', '8']),\n                                        'Low_ticket', 'Other_ticket'))\n    dataset['Ticket_len'] = dataset['Ticket'].apply(lambda x: len(x))\n","d491c611":"# lets create dummies for all the variables avaiable\ndef dummy(data):\n    dataset = pd.concat([data,pd.get_dummies(data.Pclass,prefix='Pclass'),\n                     pd.get_dummies(data.Sex,prefix='Sex'),\n                     pd.get_dummies(data.Cabin,prefix='Cabin'),\n                    pd.get_dummies(data.Embarked,prefix='Embark'),\n                    pd.get_dummies(data.title,prefix='title'),\n                    pd.get_dummies(data.Family,prefix='Family'),\n                    pd.get_dummies(data.Ticket_str,prefix='ticket_str'),\n                    pd.get_dummies(data.Ticket_len,prefix='ticket_len')],axis=1)\n    dataset.drop(['Pclass','Name','Sex','SibSp','Parch','Ticket','Cabin','Embarked','title','Family','Ticket_str',\n             'Ticket_len'],axis=1,inplace=True)\n    return dataset","97b02a63":"train = dummy(train_data)\ntest = dummy(test_data)","eb525d3a":"train.drop(['Survived'],axis=1,inplace=True)\ntrain.columns","fe2d9e01":"test.drop(['PassengerId'],axis=1,inplace=True)\ntest.columns","e8bb81c3":"for dataset in [train_data,test_data]:\n    for i in list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index):\n        age_mean = dataset[\"Age\"][dataset['Family'] == dataset.iloc[i]['Family']].mean()\n        Age_mean = dataset[\"Age\"][((dataset['SibSp'] == dataset.iloc[i][\"SibSp\"]) & (dataset['Parch'] == dataset.iloc[i][\"Parch\"]) & (dataset['Pclass'] == dataset.iloc[i][\"Pclass\"])&(dataset['title'] == dataset.iloc[i][\"title\"]))].mean()\n        if not np.isnan(Age_mean) :\n            dataset['Age'].iloc[i] = Age_mean\n        else :\n            dataset['Age'].iloc[i] = age_mean","268465d9":"train['Age'] = train_data['Age']\ntest['Age'] = test_data['Age']","0585d2aa":"#trainx,testx,trainy,testy = train_test_split(train,train_target,test_size=0.2)","4d62fcb2":"#param = { \"criterion\" : [\"gini\", \"entropy\"],\n#         \"max_features\": [1,3,8,10,12],\n#         \"min_samples_leaf\" : [1, 5, 10,15],\n#         \"min_samples_split\" : [2, 4, 10, 12, 16,18,20],\n#         \"n_estimators\": [50, 100, 400, 700,800,900, 1000]\n#        }\n#rf = GridSearchCV(RandomForestClassifier(oob_score=True),param_grid=param,scoring='accuracy',cv=5,n_jobs=-1,verbose=True)","a25c4cf8":"#rf.fit(trainx,trainy)","9193d019":"#rf.best_estimator_","86f2d0a1":"rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n            max_depth=None, max_features=8, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=4,\n            min_weight_fraction_leaf=0.0, n_estimators=900, n_jobs=None,\n            oob_score=True, random_state=None, verbose=0, warm_start=False)\n\nrf.fit(train, train_target)\n#print(\"%.4f\" % rf.oob_score_)","56bab66e":"rf_train_pred = rf.predict(train)","59c9db43":"print(metrics.accuracy_score(train_target,rf_train_pred))","0ed0c3b8":"test_rf = rf.predict(test)","469fdc42":"rf_submission_testdata = pd.DataFrame({'PassengerId':test_data.PassengerId,'Survived':test_rf})","c8596d46":"#submission_testdata.to_csv('submit_learner.csv',index=False)","0e9597b9":"from xgboost.sklearn import XGBClassifier","eb5eb74f":"#param_xg = {'max_depth':[2,3,4,5,6,8,10],\n#        'learning_rate':[0.1,0.01,0.001,0.02],\n#        'n_estimators':[100,200,300,400,500,600,800,900,1000],\n#        'subsample':[0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n#        'reg_alpha':[0.0001,0.0002,0.00003,0.00004,0.00005,0.00006,0.00007]}","764906b1":"#xggs =  GridSearchCV(XGBClassifier(),param_grid=param_xg,cv=5,scoring='accuracy',n_jobs=-1,verbose=True)","6d9889f0":"#xggs.best_estimator_tor_","15bd6cbb":"#xggs.best_score_","75f2c048":"xggc = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=2, min_child_weight=1, missing=None, n_estimators=800,\n       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n       reg_alpha=0.0001, reg_lambda=1, scale_pos_weight=1, seed=None,\n       silent=True, subsample=0.9)","f6af3f5d":"xggc.fit(train,train_target)","ea32e679":"xg_train_pred = xggc.predict(train)","55146deb":"metrics.accuracy_score(train_target,xg_train_pred)","308a6dd2":"xg_test_pred = xggc.predict(test)","5cbb95fa":"print(metrics.classification_report(train_target,xg_train_pred))","28757cbc":"print(metrics.confusion_matrix(train_target,xg_train_pred))","9f2536a5":"sns.heatmap(metrics.confusion_matrix(train_target,xg_train_pred))","94cecb10":"xg_submission_testdata = pd.DataFrame({'PassengerId':test_data.PassengerId,'Survived':xg_test_pred})","45661fa3":"xg_submission_testdata.to_csv('submit.csv')","881ab8bd":"xg_submission_testdata","888b5d41":"- From  the above it is inferred that the starting number represent the pclass and other than 1,2,3, few letters represents\n- lower price and other represents random ","011eb6fb":"### Lets understand the data","bd8f488b":"- Now lets understand how this can be related to Survival of passenger","54e3729d":"- Name may not be significant but we could derive a variable from the name stating the title of the passenger by which \n- we can infer the status of passenger like mother single such wise\n\n- SibSp\/Parch : This variable states numb of siblings or spouse so we can derive the variable familysize\n- Fare : this effects the cabin and pclass variables and embarked which indirectly relates survival\n","bce88ebf":"- Pclass tell us passengers seat class and related to cabin. May be the passenger with higher class cabin\n- could survive as the cabin may be in safer place or safety boats are attached to this cabins. Lets check it pictorially","9a868481":"- in the above Mr Miss Mrs Master are more frequent\n- lets consider all other titles in to rare group","21644cd6":"Upvotes are most welcome","a905712f":"### Running with Xgboost Classifier ","c63a76f7":"- Pclass : Ticket Class\n- Name : name of the passenger\n- Sex : gender of the passenger\n- Age : Age of passenger\n- SibSp : Numb of siblings or spouse\n- Parch : parents\/children on board\n- Ticket : Ticket number\n- Fare : price of the ticket\n- Cabin : Location of the stay in ship\n- Embarked : point of boarding the ship","fa11bd99":"- now lets decide on what u should be.\n- if Pclass 1 Survival 0 Sex 1 then lets replace U and T with C(frequently Occurred) --- it defines people most of them in C cabin didn't survive\n- if Pclass 1 Survival 1 sex 0 then lets replace U with B(frequent and survived most of them in B)\n- if Pclass 1 Survival 1 sex 1 then lets replace U with C\n- if Pclass 2 Survival 0 sex 0 then lets replace U with E\n- if Pclass 2 Survival 0 sex 1 then lets replace U with D\n- if Pclass 2 Survival 1 sex 0 then lets replace U with F\n- if Pclass 2 Survival 1 sex 1 then lets replace U with F\n- if Pclass 3 Survival 0 sex 0 then lets replace U with G\n- if Pclass 3 Survival 0 sex 1 then lets replace U with F\n- if Pclass 3 Survival 1 sex 0 then lets replace U with G\n- if Pclass 3 Survival 1 sex 1 then lets replace U with E","7436bfdb":"- Seems T is a outlier in the Cabins lets replace it with most frequent one.\n- Passengers with cabins survival rate is more than the passengers without cabins ie., U\n- imputation of the cabins should be done more carfully based on the survival rate.","4a07cc1d":"- Since as above we dont have Survived variable here lets impute the cabin using Pcalss and sex in test data\n- Pclass 1 U = C\n- Pclass 2 Sex 0 U = F\n- PClass 2 sex 1 U = D\n- Pclass 3 sex 0 U = G\n- Pcass 3 sex 1 U = F"}}