{"cell_type":{"5feae9df":"code","7e963ca1":"code","84a0f954":"code","2e45dcf4":"code","2a2c6e12":"code","58da0e7c":"code","9208216c":"code","9d01392f":"code","8529afe4":"code","b59fb9a0":"code","ed5c074a":"code","d8efb1eb":"code","d6fc6d5b":"code","a136f735":"code","611c4dc4":"code","12b1c054":"code","ce2a037c":"code","3cf5f3de":"code","8fd00877":"code","b66911cf":"code","604b6ad7":"code","f0dbed28":"code","88bb79a5":"code","4569bc78":"code","da79cf30":"code","d8581b7b":"code","a476d946":"code","d7efc2e7":"code","55b7ad79":"code","ff558d71":"code","d48ff015":"code","9494dcf9":"code","a3073546":"code","ade564c0":"code","e710004c":"code","e5e1c27f":"code","64837c1c":"code","486abf47":"code","b0e5b3d4":"code","6fb54d9b":"code","69f3e551":"code","a5104a54":"code","c9961aca":"code","1902fb93":"code","86c42447":"code","bb6763ea":"code","4e09a07f":"code","d3610a40":"code","7e10446e":"code","3d4be698":"code","6fd25001":"code","446a518c":"code","2d3758b8":"code","71db2d80":"code","6694b93f":"code","14553f30":"code","b242677a":"code","ec20dbc8":"code","eca4958d":"code","5cf8795e":"code","c0ff6b01":"code","f057320a":"code","eba27ef5":"code","31fd38cb":"code","d164839f":"code","084ce225":"code","8bbb4a08":"code","50db260b":"code","c6e55f77":"code","0a0666ba":"code","008b79f0":"code","5b43c2d3":"code","948fc5cc":"code","05af9e4b":"code","c4d7431a":"code","d8bfdf50":"code","fadf4111":"code","537e3868":"code","951185ed":"code","2b30e62a":"code","42351484":"code","bbc99043":"code","8dde0c79":"code","afa7a8ad":"code","0d4bdfa1":"code","50000f34":"code","aab05b47":"code","2a8e39da":"code","3cea1efb":"code","a62008cf":"code","c7f10c48":"code","acd6348d":"code","948ab11f":"code","19bdc665":"code","02e27623":"code","e0da6625":"code","37969c07":"code","40c9fb5c":"code","bf3ec835":"code","54be1f69":"code","234289ea":"code","f916b8b7":"code","25eeb36c":"code","9b6f22f9":"code","e5394210":"code","30741b13":"code","5b024f42":"code","6e9908ac":"code","f556a8a9":"code","8ed5faea":"code","bcf1b167":"code","3fd84f99":"code","bea338d5":"code","46733e77":"code","974fe4fe":"code","2925a9da":"code","172a4d4d":"code","1bce9742":"code","ff09ebe0":"code","edab402c":"code","56f4469e":"code","6ce09794":"code","d30f3d7d":"code","4963fd8f":"code","01d72096":"code","b0c2d5d4":"code","bf3bd863":"markdown","fc17f9cd":"markdown","7e964281":"markdown","b57b4881":"markdown","f6eb3048":"markdown","997ce77d":"markdown","e3f1df07":"markdown","37b7fdaf":"markdown","4d1223fd":"markdown","8d2a02a3":"markdown","10064da4":"markdown","9600126a":"markdown","9e04247f":"markdown","b55501f7":"markdown","07dbde54":"markdown","af181963":"markdown","b3668479":"markdown","1bccd29d":"markdown","b987f537":"markdown","d0d0ff01":"markdown","b2a3ae97":"markdown","23f2d31e":"markdown","c6c46cab":"markdown","b7cf39f3":"markdown","343369ce":"markdown","334a3f9d":"markdown","fa8cd3c7":"markdown","4ab03e65":"markdown","42d7ab2b":"markdown","8d0295f3":"markdown","73e8c29c":"markdown","1f95bac6":"markdown","9a92f724":"markdown","e22952a2":"markdown","c20ccf58":"markdown","f527b917":"markdown","51679e4a":"markdown","48ec1d78":"markdown","7a64d16e":"markdown","de9f4621":"markdown","20d8064f":"markdown","8f0c863f":"markdown","4f7993ee":"markdown","41acdb94":"markdown","29b395a2":"markdown","34f4cb5f":"markdown","2e7d1019":"markdown","f344090e":"markdown","41384762":"markdown"},"source":{"5feae9df":"from datetime import datetime as dt \nimport numpy as np\nimport pandas as pd\nfrom numpy.random import default_rng\nfrom scipy.cluster import hierarchy\nimport matplotlib.pyplot as plt\nimport folium\n%matplotlib inline\nfrom folium import plugins\nimport datetime, calendar\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.statespace.tools import diff\nfrom statsmodels.tsa.stattools import acf\nfrom sklearn.compose import ColumnTransformer,make_column_selector\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder,OrdinalEncoder\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom sklearn.metrics import silhouette_score\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nimport seaborn as sns","7e963ca1":"pm = pd.read_csv('..\/input\/air-quality-dataset\/Air Quality Benchmark dataset.csv')\npm.head(5)","84a0f954":"# set list of category of all categorical features.\nTime_of_day_category=pm['Time of Day'].unique()\npeak_category=pm['Peak\/NoPeak'].unique()\nDay_category=pm['Day'].unique()\nWeekday_category=pm['Week Day'].unique()\nWeather_category=pm['Weather'].unique()\nlist_of_category_list=[Time_of_day_category,peak_category,Day_category,Weekday_category,Weather_category]\nprint(list_of_category_list)","2e45dcf4":"# set list of dictionaries for each categorical features\nTime_of_day_dict={}\npeak_dict={}\nDay_dict={}\nWeekday_dict={}\nWeather_dict={}\nlist_of_dict=[Time_of_day_dict,peak_dict,Day_dict,Weekday_dict,Weather_dict]\n\nfor j in range(len(list_of_dict)):\n    for i,items in enumerate(list_of_category_list[j]):\n        list_of_dict[j][i]=items\nlist_of_dict","2a2c6e12":"pm.replace(to_replace= r'^\\s*$', value=np.nan,regex=True, inplace=True ) \n#replace any unit value that only contains \" \" or space, with 0.\npm.isnull().any() \n#check whether each column contains a missing value","58da0e7c":"# after finding the longtitude and latitude values for each sensors, we store them into a dictionary.\nplace_dict = {\n    'iGude':[50.100216, 8.693827],\n    'Rothschildallee':[50.127313, 8.696384],\n    'FeinstaubFFM':[50.128178, 8.691848],\n    'Frankfurt_Riederwald':[50.128679, 8.732761],\n    'Medienzentrum Frankfurt':[50.113110, 8.685966],\n    'FFM_Westend_Sued':[50.115127, 8.658714],\n    'ioki':[50.117625, 8.671350],\n    'Ginnheim_Dust_Light_Temp':[50.143277, 8.647729],\n    'Alt Bornheim Feinstaub':[50.130067, 8.710821],\n    'Bernem':[50.124220, 8.709344],\n    'MousonSense':[50.147694, 8.695171]\n}\n\n# we also drop those rows with sensors that cannot identify its location.\ndrop_rows = ['nordsand', 's4', 'Luftdaten.info [6703181]']\nfor i in drop_rows:\n    pm = pm.drop(pm[pm['boxName'] == i].index)","9208216c":"# Convert each text value in categorical feature to numerical value\ncolumns=['Time of Day','Peak\/NoPeak','Day','Week Day','Weather','label']\n\nfor columns in columns:\n    u = pm[columns].unique()\n    \n    def conver(x):\n        return np.argwhere(u==x)[0,0]\n    \n    pm[columns] = pm[columns].map(conver)\n\n# using a loop to append latitude and longtitude for each observation at the end of dataset.\nfor k, v in place_dict.items():\n    pm.loc[pm['boxName'] == k, 'latitude'] = v[0]\n    pm.loc[pm['boxName'] == k, 'longitude'] = v[1]","9d01392f":"# convert the type of Time_stamp(string) into Timestamp, \n#so that we can straightly use days for that time stamp in the following step.\npm['Time_stamp'] = pd.to_datetime(pm['Time_stamp'])\ntype(pm.iloc[0]['Time_stamp'])\n\n# Split dataframe by boxName\nclassification=list(pm['boxName'].unique())\nfor i in classification:\n    pm1=pm[pm['boxName'].isin([i])]\n    exec(\"group%s=pm1\"%classification.index(i))\n\n# Process multiple DataFrame data in batches\nname=[]\ndata=[]\nfor j in range(0,len(classification)):\n    dfName='group'+str(j)\n    dfData=eval(dfName)\n    name.append(dfName)\n    data.append(dfData)\ndata[3].head() # show the splited dataframe where the boxName is Frankfurt_Riederwold.","8529afe4":"# Average by timestamp: in order to integrate all the data of one day into one value, we take the average for the numerical features\nb=[]\nfor k in range(0,len(classification)):\n    a=data[k].groupby(pd.Grouper(key='Time_stamp', freq='1D')).aggregate(np.mean)\n    a['boxName']=classification[k]\n    columns_total=['boxName','PM 2.5','temp','pressure','humidity','wind_speed', 'latitude', 'longitude']\n    a=pd.DataFrame(a,columns=columns_total)\n    b.append(a)\n\n# Mode by timestamp: in order to integrate all the data of one day into one value, we take the mode for the categorical features.\nd=[]\nfor l in range(0,len(classification)):\n    c=data[l].groupby(pd.Grouper(key='Time_stamp', freq='1D')).aggregate(lambda x: np.max(pd.Series.mode(x)))\n    c['boxName']=classification[l]\n    columns_total=['boxName','Time of Day','Peak\/NoPeak','Day','Week Day','Weather','label']\n    c=pd.DataFrame(c,columns=columns_total)\n    d.append(c)\n\n# Merge the dataframes\nfor m in range(1,len(classification)):\n    b[m]=pd.concat([b[m-1],b[m]],axis=0)\n    d[m]=pd.concat([d[m-1],d[m]],axis=0)\ntotal1=b[10]\ntotal2=d[10]\n\ntotal1 = total1.reset_index().dropna()\ntotal2 = total2.reset_index().dropna()\ntotal2.head()","b59fb9a0":"total2.loc[:,'Time of Day':'label'] = total2.loc[:,'Time of Day':'label'].astype(int)\ntotal2.head()","ed5c074a":"total =pd.merge(total1,total2,on=['Time_stamp','boxName'])\ntotal.head()","d8efb1eb":"total.groupby('boxName').describe()","d6fc6d5b":"total.boxName.unique()","a136f735":"print(total.boxName.unique()[0:2],\n      total.boxName.unique()[2:5],\n      total.boxName.unique()[5:8],\n      total.boxName.unique()[8:11])","611c4dc4":"print(total[total.boxName=='Rothschildallee'][-1:].index,\n      total[total.boxName=='Frankfurt_Riederwald'][-1:].index,\n      total[total.boxName=='FFM_Westend_Sued'][-1:].index,\n      total[total.boxName=='Alt Bornheim Feinstaub'][-1:].index)","12b1c054":"total_vis1=total[0:437]\ntotal_vis2=total[437:1701]\ntotal_vis3=total[1701:2243]\ntotal_vis4=total[2243:2455]\ntotal[total.boxName=='Medienzentrum Frankfurt']","ce2a037c":"fig, ax = plt.subplots(2, 2, figsize=(12,8))\nsns.lineplot(x=\"Time_stamp\", y=\"PM 2.5\",\n             hue=\"boxName\", sizes=0.25, data=total_vis1,ax=ax[0][0])\nsns.lineplot(x=\"Time_stamp\", y=\"PM 2.5\",\n             hue=\"boxName\", sizes=0.25, data=total_vis2,ax=ax[0][1])\nsns.lineplot(x=\"Time_stamp\", y=\"PM 2.5\",\n             hue=\"boxName\", sizes=0.25, data=total_vis3,ax=ax[1][0])\nsns.lineplot(x=\"Time_stamp\", y=\"PM 2.5\",\n             hue=\"boxName\", sizes=0.25, data=total_vis4,ax=ax[1][1])\nfig.tight_layout()","3cf5f3de":"fig, ax = plt.subplots(2, 2, figsize=(12,8))\nsns.lineplot(x=\"Time_stamp\", y=\"humidity\",\n             hue=\"boxName\", sizes=0.25, data=total_vis1,ax=ax[0][0])\nsns.lineplot(x=\"Time_stamp\", y=\"humidity\",\n             hue=\"boxName\", sizes=0.25, data=total_vis2,ax=ax[0][1])\nsns.lineplot(x=\"Time_stamp\", y=\"humidity\",\n             hue=\"boxName\", sizes=0.25, data=total_vis3,ax=ax[1][0])\nsns.lineplot(x=\"Time_stamp\", y=\"humidity\",\n             hue=\"boxName\", sizes=0.25, data=total_vis4,ax=ax[1][1])\nfig.tight_layout()","8fd00877":"plt.figure(figsize=(10,6))\nsns.lineplot(x=\"Time_stamp\", y=\"temp\",\n             hue=\"boxName\", data=total)","b66911cf":"plt.figure(figsize=(10,6))\nsns.lineplot(x=\"Time_stamp\", y=\"pressure\",\n             hue=\"boxName\", data=total)","604b6ad7":"plt.figure(figsize=(10,6))\nsns.lineplot(x=\"Time_stamp\", y=\"wind_speed\",\n             hue=\"boxName\", data=total)","f0dbed28":"san_map = folium.Map(location = [50.128679, 8.732761], zoom_start = 12)\n\n# instantiate a mark cluster object for the incidents in the dataframe\nincidents = plugins.MarkerCluster().add_to(san_map)\n\n# loop through the dataframe and add each data point to the mark cluster\nfor lat, lng, label, in zip(total.latitude, total.longitude, total['PM 2.5']):\n    folium.Marker(\n        location=[lat, lng],\n        icon=None,\n        popup=label,\n    ).add_to(incidents)\n\n# add incidents to map\nsan_map.add_child(incidents)","88bb79a5":"total.to_csv('processed_data.csv',index=False)","4569bc78":"place_name = list(place_dict.keys())\ndf_place = pd.DataFrame.from_dict(place_dict, orient='index',columns=['latitude', 'longitude'])\ndf_place","da79cf30":"pm_mean = pm[pm.boxName.isin(place_name)][['boxName','PM 2.5','latitude','longitude']].groupby('boxName').mean()\npm_mean","d8581b7b":"colour = ['red', 'blue', 'green', 'purple', 'orange', 'darkred',\n            'beige', 'darkblue', 'pink', 'cadetblue',\n         'darkpurple', 'white', 'lightblue', 'lightgreen',\n         'gray', 'black', 'lightgray']","a476d946":"ger_map = folium.Map(location=[50.105, 8.69], zoom_start=12, max_zoom=30)\nfor i in range(0, len(pm_mean)):\n    folium.Circle(location = [pm_mean.iloc[i, 1], pm_mean.iloc[i, 2]], \n                  radius = pm_mean.iloc[i, 0] * 100, \n                  popup = str(pm_mean.index[i]),\n                 color = colour[i],\n                 fill = True).add_to(ger_map)\nger_map","d7efc2e7":"from sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LinearRegression \nfrom sklearn import metrics\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import model_selection\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom math import sqrt\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\nfrom sklearn.model_selection import cross_val_score","55b7ad79":"X=total.loc[:,('temp','pressure','humidity','wind_speed')]\ny=total.loc[:,'PM 2.5']\nX_train,X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state=42)\nprint ('X_train.shape={}\\ny_train.shape ={}\\nX_test.shape={}\\ny_test.shape={}'.format(X_train.shape,y_train.shape,X_test.shape,y_test.shape))","ff558d71":"# Define a function that compares the CV perfromance of a set of predetrmined models \ndef cv_comparison(models, X, y, cv):\n    # Initiate a DataFrame for the averages and a list for all measures\n    cv_accuracies = pd.DataFrame()\n    maes = []\n    mses = []\n    r2s = []\n    accs = []\n    for model in models:\n        mae = -np.round(cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv), 4)\n        maes.append(mae)\n        mae_avg = round(mae.mean(), 4)\n        mse = -np.round(cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv), 4)\n        mses.append(mse)\n        mse_avg = round(mse.mean(), 4)\n        r2 = np.round(cross_val_score(model, X, y, scoring='r2', cv=cv), 4)\n        r2s.append(r2)\n        r2_avg = round(r2.mean(), 4)\n        acc = np.round((100 - (100 * (mae * len(X))) \/ sum(y)), 4)\n        accs.append(acc)\n        acc_avg = round(acc.mean(), 4)\n        cv_accuracies[str(model)] = [mae_avg, mse_avg, r2_avg, acc_avg]\n    cv_accuracies.index = ['Mean Absolute Error', 'Mean Squared Error', 'R^2', 'Accuracy']\n    return cv_accuracies, maes, mses, r2s, accs\n","d48ff015":"# Create the models to be tested\nmlr_reg = LinearRegression()\nrf_reg = RandomForestRegressor(random_state=42)\nxgb_reg = xgb_regressor = xgb.XGBRegressor(random_state=42)\n\n# Put the models in a list to be used for Cross-Validation\nmodels = [mlr_reg, rf_reg, xgb_reg]\n\n# Run the Cross-Validation comparison with the models used in this analysis\ncomp, maes, mses, r2s, accs = cv_comparison(models, X_train, y_train, 4)\n\ncomp.columns = ['Linear Regression', 'Random Forest', 'XGBoosting']\ncomp","9494dcf9":"# Number of trees in Random Forest\nrf_n_estimators = [int(x) for x in np.linspace(20, 1000, 20)]\n\n# Add the default as a possible value\n#rf_max_depth.append(None)\n\n# Number of features to consider at every split\nrf_max_features = ['auto', 'sqrt', 'log2']\n\n# Criterion to split on\nrf_criterion = ['mse']\n\n# Minimum number of samples required to split a node\nrf_min_samples_split = [2]\n\n# Minimum decrease in impurity required for split to happen\nrf_min_impurity_decrease = [0.0, 0.05, 0.1]\n\n# Method of selecting samples for training each tree\nrf_bootstrap = [True]\n\n# Create the grid\nrf_grid = {'n_estimators': rf_n_estimators,\n           'max_features': rf_max_features,\n           'criterion': rf_criterion,\n           'min_samples_split': rf_min_samples_split,\n           'min_impurity_decrease': rf_min_impurity_decrease,\n           'bootstrap': rf_bootstrap}\n","a3073546":"# Number of trees to be used\nxgb_n_estimators = [int(x) for x in np.linspace(100, 1000, 20)]\n\n# Maximum number of levels in tree\nxgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n\n# Minimum number of instaces needed in each node\nxgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n\n# Tree construction algorithm used in XGBoost\nxgb_tree_method = ['auto', 'exact', 'approx', 'hist', 'gpu_hist']\n\n# Learning rate\nxgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n\n# Minimum loss reduction required to make further partition\nxgb_gamma = [int(x) for x in np.linspace(0, 0.5, 6)]\n\n# Learning objective used\nxgb_objective = ['reg:squarederror', 'reg:squaredlogerror']\n\n# Create the grid\nxgb_grid = {'n_estimators': xgb_n_estimators}\n\n# Create the model to be tuned\nxgb_base = xgb.XGBRegressor()\n\n# Create the random search Random Forest\nxgb_random = GridSearchCV(estimator = xgb_base, param_grid = xgb_grid, cv = 4, n_jobs = 2)\n\n# Fit the random search model\nxgb_random.fit(X_train, y_train)\n\n# Get the optimal parameters\nxgb_random.best_params_","ade564c0":"# Create the final Multiple Linear Regression\nmlr_final = LinearRegression()\n\n\n# Create the final Random Forest\nrf_final = RandomForestRegressor(n_estimators = 432,\n                                 min_samples_split = 2,\n                                 min_impurity_decrease = 0.0,\n                                 max_features = 'sqrt',\n                                 criterion = 'mse',\n                                 bootstrap = True)\n\n\n# Create the fnal Extreme Gradient Booster\nxgb_final = xgb.XGBRegressor(n_estimators = 147)","e710004c":"# Train the models using 80% of the original data\nmlr_final.fit(X_train, y_train)\nrf_final.fit(X_train, y_train)\nxgb_final.fit(X_train, y_train)","e5e1c27f":"# Call the comparison function with the three final models\ncomp2, maes, mses, r2s, accs = cv_comparison([mlr_final, rf_final, xgb_final], X_train, y_train, 4)\ncomp2.columns = ['Linear Regression', 'Random Forest', 'XGBoosting']\ncomp2","64837c1c":"plt.figure()\nresult = mlr_final.predict(X_test)\nplt.plot(np.arange(len(result)), y_test,'g-',label='true value')\nplt.plot(np.arange(len(result)),result,'r-',label='predict value')\nplt.legend()\nplt.title('Linear Regression')\nplt.show()","486abf47":"plt.figure()\nresult = rf_final.predict(X_test)\nplt.plot(np.arange(len(result)), y_test,'g-',label='true value')\nplt.plot(np.arange(len(result)),result,'r-',label='predict value')\nplt.legend()   \nplt.title('Random Forest Regression')\nplt.show()","b0e5b3d4":"plt.figure()\nresult = xgb_final.predict(X_test)\nplt.plot(np.arange(len(result)), y_test,'g-',label='true value')\nplt.plot(np.arange(len(result)),result,'r-',label='predict value')\nplt.legend()   \nplt.title('Xgboosting Regression')\nplt.show()","6fb54d9b":"total.head()","69f3e551":"daily_average_PM =  pd.DataFrame(total.groupby('Time_stamp')['PM 2.5'].mean())\ndaily_average_PM.head()","a5104a54":"daily_average_PM.plot(title='Air Quality of Frankfurt', legend=False, figsize=(18,5))\nplt.xlabel('Day-Year'); plt.ylabel('PM 2.5');","c9961aca":"total.head()","1902fb93":"for i in range(0,len(total.index)):\n    if total.loc[i,'PM 2.5']<=10:\n        total.loc[i,'PM 2.5 level']= 'AQG'\n    elif total.loc[i,'PM 2.5']<=15:\n        total.loc[i,'PM 2.5 level']= 'IT-3'\n    elif total.loc[i,'PM 2.5']<=25:\n        total.loc[i,'PM 2.5 level']= 'IT-2'\n    elif total.loc[i,'PM 2.5']<=35:\n        total.loc[i,'PM 2.5 level']= 'IT-1'\n    else:\n        total.loc[i,'PM 2.5 level']= 'exceed standard'\ntotal.head()","86c42447":"total['PM 2.5 level'].value_counts()","bb6763ea":"plt.figure(figsize=(5,5))\nlabels = [u'AQG', u'IT-1', u'IT-2', u'IT-3',u'exceed standard']\nsizes = [1765, 59, 261, 341, 29]\ncolors = ['red', 'yellow', 'blue', 'green','orange']\nexplode = (0.05, 0, 0, 0,0)\n\npatches, l_text, p_text = plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n                                       labeldistance=1.1, autopct='%2.0f%%', shadow=False,\n                                       startangle=90, pctdistance=0.6)\nplt.axis('equal')\nplt.legend(loc='upper left', bbox_to_anchor=(-0.1, 1))\nplt.grid()\nplt.show()","4e09a07f":"# define X as the features we use in classification model, y is the target.\nX1=total[['temp','pressure','humidity','wind_speed','Time of Day','Peak\/NoPeak','Day','Week Day','Weather']]\ny=total['PM 2.5 level']\nX1","d3610a40":"# transform all the categorical features back into 'object' type\nX1['Time of Day'].replace(Time_of_day_dict, inplace=True)\nX1['Peak\/NoPeak'].replace(peak_dict,inplace=True)\nX1['Day'].replace(Day_dict,inplace=True)\nX1['Week Day'].replace(Weekday_dict,inplace=True)\nX1['Weather'].replace(Weather_dict,inplace=True)\nX1","7e10446e":"# define X2 as the features we use in classification model, y is the target.\nX2=total[['temp','pressure','humidity','wind_speed','Day','Week Day','Weather']]\ny=total['PM 2.5 level']\nX2.head()","3d4be698":"# transform all the categorical features back into 'object' type\nX2['Day'].replace(Day_dict,inplace=True)\nX2['Week Day'].replace(Weekday_dict,inplace=True)\nX2['Weather'].replace(Weather_dict,inplace=True)\nX2","6fd25001":"#split training and test data, and make the proportion of y in training set and test set are the same.\nX_train,X_test,y_train,y_test = train_test_split(X2,y,test_size=0.3,random_state=42,stratify=y)\ny_train.value_counts(normalize=True)","446a518c":"y_test.value_counts(normalize=True)","2d3758b8":"# by fitting X_train, we use a pipeline to scale numerical features and one hot encode categorical features in X_train and X_test\nct = ColumnTransformer([\n    ('scale',StandardScaler(),\n    make_column_selector(dtype_include=np.number)),\n    ('onehot',OneHotEncoder(drop='first'),    # for onehot encoding, we drop each categorical features first column.\n    make_column_selector(dtype_include=object))\n])","71db2d80":"ct.fit(X_train)\nX_ttrain=ct.transform(X_train)\nX_ttest=ct.transform(X_test)\nX_train.iloc[:3]","6694b93f":"X_ttrain[:3]","14553f30":"from keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Dropout\nfrom keras.utils.np_utils import to_categorical","b242677a":"# change target (y_train,y_test) into onehot code.\ny_train_encode=pd.get_dummies(y_train)\ny_test_encode=pd.get_dummies(y_test)\ny_test_encode","ec20dbc8":"n_cols=X_ttrain.shape[1] #find number of node in input layer\nmodel=Sequential()\nmodel.add(Dense(100,activation='relu',input_shape=(n_cols,)))\nmodel.add(Dense(100,activation='relu'))\nmodel.add(Dense(100,activation='relu'))\nmodel.add(Dense(5,activation='softmax'))\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['categorical_accuracy'])","eca4958d":"early_stopping_monitor=EarlyStopping(patience=2)        #set a early stop to prevent overfitting\nrecord=model.fit(X_ttrain,y_train_encode,validation_split=0.2,epochs=50,callbacks=[early_stopping_monitor])   #20% of data would be used for validation","5cf8795e":"# Plot accuracy change vs validation accuracy change based on epochs\nplt.plot(record.epoch, record.history.get('categorical_accuracy'),color='orange')\nplt.plot(record.epoch, record.history.get('val_categorical_accuracy'),color='blue')","c0ff6b01":"# output the test prediction results\ny_test_pred = model.predict(X_ttest)\n\ny_test_pred[:10]","f057320a":"# convert probabilities to labels\ny_test_pred=np.argmax(y_test_pred, axis=1)\n\nlabels=['AQG', 'IT-3', 'IT-2', 'IT-1', 'exceed standard']\ny_test_pred_dnn=[]\nfor item in y_test_pred:\n    y_test_pred_dnn.append(labels[item])\n    \nnp.array(y_test_pred_dnn)[:10]","eba27ef5":"model.summary()","31fd38cb":"_,dnn_accuracy=model.evaluate(X_ttest,y_test_encode)","d164839f":"print('Accuracy on test data of deep neural network is ',dnn_accuracy)","084ce225":"from sklearn.metrics import confusion_matrix\n\nC_dnn=confusion_matrix(\n    np.array(y_test),   # array, Gound true (correct) target values\n    np.array(y_test_pred_dnn),  # array, Estimated targets as returned by a classifier\n    labels=['AQG', 'IT-3', 'IT-2', 'IT-1', 'exceed standard'],  # array, List of labels to index the matrix.\n    sample_weight=None  # array-like of shape = [n_samples], Optional sample weights\n)\n\ndef plot_confusion_matrix(cm, labels_name, title):\n    cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]    # normalization\n    plt.imshow(cm, interpolation='nearest')    \n    plt.title(title)    \n    plt.colorbar()\n    num_local = np.array(range(len(labels_name)))    \n    plt.xticks(num_local, labels_name, rotation=90)    \n    plt.yticks(num_local, labels_name)    \n    plt.ylabel('True label')    \n    plt.xlabel('Predicted label')","8bbb4a08":"col_name=pd.MultiIndex.from_product([['Predicted label'], labels])\nrow_name=pd.MultiIndex.from_product([['True label'], labels])\npd.DataFrame(C_dnn,columns=col_name,index=row_name)","50db260b":"plot_confusion_matrix(C_dnn, labels, \"DNN_pred Confusion Matrix\")\n\nplt.show()","c6e55f77":"from sklearn.metrics import classification_report  \nprint('classification_report:')  \nprint (classification_report(y_test, y_test_pred_dnn))","0a0666ba":"k_range = list(range(1,20))\nweight_options = ['uniform','distance'] # Uniform (equal weight), distance (weight and distance are inversely proportional, the closer it is, the stronger the influence)\nalgorithm_options = ['auto','ball_tree','kd_tree','brute'] # Different kinds of formations of trees\nparam_grid = dict(n_neighbors = k_range,weights = weight_options,algorithm=algorithm_options)\n\nknn=KNeighborsClassifier()\nknn_cv=GridSearchCV(knn,param_grid,scoring='accuracy',cv=5)\nknn_cv.fit(X_ttrain,np.array(y_train).ravel())\nprint(knn_cv.best_params_) # find best hyperparameter n_neighbor.\nprint(knn_cv.best_score_)  # find the score of the best hyperparameter.","008b79f0":"knn_accuracy=knn_cv.score(X_ttest,y_test)\nprint('Accuracy on test data of knn is ',knn_accuracy)","5b43c2d3":"y_test_pred_knn=knn_cv.predict(X_ttest)\ny_test_pred_knn[:10]","948fc5cc":"C_knn=confusion_matrix(\n    np.array(y_test),   \n    np.array(y_test_pred_knn),  \n    sample_weight=None  \n)","05af9e4b":"pd.DataFrame(C_knn,columns=col_name,index=row_name)","c4d7431a":"plot_confusion_matrix(C_knn, labels, \"KNN_pred Confusion Matrix\")\nplt.show()","d8bfdf50":"print('classification_report:')  \nprint (classification_report(y_test, y_test_pred_knn))","fadf4111":"# transform y_train, y_test into dataframe\ny_train=pd.DataFrame(y_train)\ny_test=pd.DataFrame(y_test)\ny_train.head()","537e3868":"# using ordinal encoder to transform y_train, y_test\noe=OrdinalEncoder()\noe.fit(y_train)\ny_train_ordinal=oe.transform(y_train)\ny_test_ordinal=oe.transform(y_test)\ny_train_ordinal[0:5]","951185ed":"clf = tree.DecisionTreeClassifier()\nclf = clf.fit(X_ttrain,y_train_ordinal)\nimport graphviz \ndot_data = tree.export_graphviz(clf, out_file=None,\n                     filled=True, rounded=True,  \n                     special_characters=True) \ngraph = graphviz.Source(dot_data)  \ngraph ","2b30e62a":"clf_accuracy=clf.score(X_ttest,y_test_ordinal)\nprint('Accuracy on test data of decision tree is ',clf_accuracy)","42351484":"y_test_pred_clf=clf.predict(X_ttest)\n\nC_clf=confusion_matrix(\n    np.array(y_test_ordinal),   \n    np.array(y_test_pred_clf),  \n    sample_weight=None  \n)\npd.DataFrame(C_clf,columns=col_name,index=row_name)","bbc99043":"plot_confusion_matrix(C_clf, labels, \"Decision Tree Confusion Matrix\")\nplt.show()","8dde0c79":"print('classification_report:')  \nprint (classification_report(y_test_ordinal, y_test_pred_clf))","afa7a8ad":"rfc = RandomForestClassifier()\ntuned_parameters = [{'min_samples_leaf':[1,2,3,4,5], 'n_estimators':[50,100,150,200]}]\nrfc_cv = GridSearchCV(estimator=rfc,param_grid=tuned_parameters, cv=5, n_jobs=1)\nrfc_cv.fit(X_ttrain,np.array(y_train).ravel())\n\nprint(rfc_cv.best_params_) #find the best parameters\nprint(rfc_cv.best_score_)  #find the score of best parameters\n# Obviously, the smaller the min_samples_leaf and the bigger the n_estimators, the better the result.","0d4bdfa1":"rfc_accuracy=rfc_cv.score(X_ttest,y_test)\nprint('Accuracy on test data of random forest is ',rfc_accuracy)","50000f34":"y_test_pred_rfc=rfc_cv.predict(X_ttest)\n\nC_rfc=confusion_matrix(\n    np.array(y_test),   \n    np.array(y_test_pred_rfc),  \n    sample_weight=None  \n)\npd.DataFrame(C_rfc,columns=col_name,index=row_name)","aab05b47":"plot_confusion_matrix(C_rfc, labels, \"Random Forest Confusion Matrix\")\nplt.show()","2a8e39da":"print('classification_report:')  \nprint (classification_report(y_test, y_test_pred_rfc))","3cea1efb":"param_grid={'C':np.arange(0.1,10,0.1)}\nmodel1=svm.SVC(kernel='linear')\nmodel1_cv=GridSearchCV(model1,param_grid,scoring='accuracy',cv=5)\nmodel1_cv.fit(X_ttrain,y_train)","a62008cf":"print(model1_cv.best_params_) # find best hyperparameter.\nprint(model1_cv.best_score_)  # find the score of the best hyperparameter.","c7f10c48":"y_test_pred_svm1=model1_cv.predict(X_ttest)\n# find the accuracy on test data\nsvm1_accuracy=model1_cv.score(X_ttest,y_test)\nprint('svm with linear kernel model accuracy on test data is',svm1_accuracy)","acd6348d":"C_svm1=confusion_matrix(\n    np.array(y_test),   \n    np.array(y_test_pred_svm1),  \n    sample_weight=None  \n)\npd.DataFrame(C_svm1,columns=col_name,index=row_name)","948ab11f":"plot_confusion_matrix(C_svm1, labels, \"SVM(kernel=linear)  Confusion Matrix\")\nplt.show()","19bdc665":"print('classification_report:')  \nprint (classification_report(y_test, y_test_pred_svm1))","02e27623":"param_grid={'C':np.arange(0.1,10,0.1)}\nmodel2=svm.LinearSVC(max_iter=10000)\nmodel2_cv=GridSearchCV(model2,param_grid,scoring='accuracy',cv=5)\nmodel2_cv.fit(X_ttrain,y_train)","e0da6625":"print(model2_cv.best_params_) # find best hyperparameter.\nprint(model2_cv.best_score_)  # find the score of the best hyperparameter.","37969c07":"y_test_pred_svm2=model2_cv.predict(X_ttest)\n# find the accuracy on test data\nsvm2_accuracy=model2_cv.score(X_ttest,y_test)\nprint('svm.SVC linear model accuracy on test data is',svm2_accuracy)","40c9fb5c":"C_svm2=confusion_matrix(\n    np.array(y_test),   \n    np.array(y_test_pred_svm2),  \n    sample_weight=None  \n)\npd.DataFrame(C_svm2,columns=col_name,index=row_name)","bf3ec835":"plot_confusion_matrix(C_svm2, labels, \"SVM.LinearSVC Confusion Matrix\")\nplt.show()","54be1f69":"print('classification_report:')  \nprint (classification_report(y_test, y_test_pred_svm2))","234289ea":"param_grid={'C':np.arange(0.1,10,0.1)}\nmodel3=svm.SVC(kernel='rbf',gamma='scale')\nmodel3_cv=GridSearchCV(model3,param_grid,scoring='accuracy',cv=3)\nmodel3_cv.fit(X_ttrain,y_train)","f916b8b7":"print(model3_cv.best_params_) # find best hyperparameter.\nprint(model3_cv.best_score_)  # find the score of the best hyperparameter.","25eeb36c":"y_test_pred_svm3=model3_cv.predict(X_ttest)\n# find the accuracy on test data\nsvm3_accuracy=model3_cv.score(X_ttest,y_test)\nprint('svm with rbf kernel model when C=9.7 accuracy on test data is',svm3_accuracy)","9b6f22f9":"C_svm3=confusion_matrix(\n    np.array(y_test),   \n    np.array(y_test_pred_svm3),  \n    sample_weight=None  \n)\npd.DataFrame(C_svm3,columns=col_name,index=row_name)","e5394210":"plot_confusion_matrix(C_svm3, labels, \"SVM.SVC(kernel='rbf') Confusion Matrix\")\nplt.show()","30741b13":"print('classification_report:')  \nprint (classification_report(y_test, y_test_pred_svm3))","5b024f42":"param_grid={'C':np.arange(0.1,10,0.1),'degree':np.arange(2,5)}\nmodel4=svm.SVC(kernel='poly',gamma='scale')\nmodel4_cv=GridSearchCV(model4,param_grid,scoring='accuracy',cv=3)\nmodel4_cv.fit(X_ttrain,y_train)","6e9908ac":"print(model4_cv.best_params_) # find best hyperparameter.\nprint(model4_cv.best_score_)  # find the score of the best hyperparameter.","f556a8a9":"y_test_pred_svm4=model4_cv.predict(X_ttest)\n# find the accuracy on test data\nsvm4_accuracy=model4_cv.score(X_ttest,y_test)\nprint('Accuracy on test data of SVM.SVC with kernel=poly when C=3, degree=4 is ',svm4_accuracy)","8ed5faea":"C_svm4=confusion_matrix(\n    np.array(y_test),   \n    np.array(y_test_pred_svm4),  \n    sample_weight=None  \n)\npd.DataFrame(C_svm4,columns=col_name,index=row_name)","bcf1b167":"plot_confusion_matrix(C_svm4, labels, \"SVM.SVC(kernel='poly') Confusion Matrix\")\nplt.show()","3fd84f99":"print('classification_report:')  \nprint (classification_report(y_test, y_test_pred_svm4))","bea338d5":"# we set hyperparameter n_cluster = 5\nfrom sklearn.cluster import KMeans\nmodel6= KMeans(n_clusters=5)\nmodel6.fit(X_ttrain)\nkmeans_labels=model6.predict(X_ttrain)","46733e77":"grouping1=pd.DataFrame({'kmeans_labels':kmeans_labels,'true lables':pd.Series(y_train['PM 2.5 level'])})\ngrouping1","974fe4fe":"ct1=pd.crosstab(grouping1['kmeans_labels'],grouping1['true lables'])\nct1","2925a9da":"sc_scores = []\nclusters = range(2,11)\nfor i in clusters:  \n    model=KMeans(n_clusters=i)\n    model.fit(X_ttrain)\n    labels = model.predict(X_ttrain).ravel()\n    sc_scores.append(silhouette_score(X_ttrain, labels))\nplt.plot(clusters, sc_scores)\nplt.xlabel('n_clusters')","172a4d4d":"from sklearn.decomposition import PCA\nimport plotly.express as px\n\npca = PCA(n_components=3, random_state=42)\nX2 = pd.DataFrame(pca.fit_transform(X_ttrain))\nSCATTER_SIZE=700\nX2['target']=y_train.values\nfig = px.scatter_3d(\n    X2, \n    x=0, \n    y=1,\n    z=2, \n    color='target', \n    title='3d scatter for PCA',\n    width=SCATTER_SIZE,\n    height=SCATTER_SIZE\n)\n\nfig.show()","1bce9742":"print(pca.explained_variance_ratio_)","ff09ebe0":"pca = PCA(n_components=2) \nX_train_pca = pca.fit_transform(X_ttrain) \nX_test_pca = pca.transform(X_ttest)\nX_train_pca_dataframe = pd.DataFrame(X_train_pca)\ny_train_pca_dataframe = pd.DataFrame(y_train)\nX_train_pca_dataframe.index = y_train_pca_dataframe.index\n\nfinalDf = pd.concat([X_train_pca_dataframe, y_train_pca_dataframe], axis = 1)\nfinalDf.columns = ['Principal Component 1', 'Principal Component 2', 'target']\nfinalDf","edab402c":"fig = px.scatter(\n    finalDf, \n    x='Principal Component 1', \n    y='Principal Component 2',\n    color=\"target\", \n    title='2d scatter for PCA',\n    width=SCATTER_SIZE,\n    height=SCATTER_SIZE\n)\n\nfig.show()","56f4469e":"train = pd.DataFrame(X_ttrain).copy()\ntrain['cluster'] = kmeans_labels\ntrain","6ce09794":"pca = PCA(n_components=5)\nnew_pca = pd.DataFrame(pca.fit_transform(X_ttrain))\nd = new_pca[train['cluster'] == 0]\nplt.plot(d[0], d[1], 'r.')\nd = new_pca[train['cluster'] == 1]\nplt.plot(d[0], d[1], 'g.')\nd = new_pca[train['cluster'] == 2]\nplt.plot(d[0], d[1], 'b.')\nd = new_pca[train['cluster'] == 3]\nplt.plot(d[0], d[1], 'c.')\nd = new_pca[train['cluster'] == 4]\nplt.plot(d[0], d[1], 'm.')","d30f3d7d":"lm1 = linkage(X_ttrain, method='ward')\nplt.figure(figsize=(12,4))\ndendrogram(lm1, p=5,truncate_mode='level');","4963fd8f":"# cut the tree by 5 clusters\nout=hierarchy.cut_tree(lm1,n_clusters=5).ravel()\ngrouping2=pd.DataFrame({'hierachical_labels':out,'target':pd.Series(y_train['PM 2.5 level'])})\ngrouping2","01d72096":"ct2=pd.crosstab(grouping2['hierachical_labels'],grouping2['target'])\nct2","b0c2d5d4":"sc_scores = []\nclusters = range(2,11)\nfor i in clusters:  \n    labels = hierarchy.cut_tree(lm1, n_clusters=i).ravel()\n    sc_scores.append(silhouette_score(X_ttrain, labels))\nplt.plot(clusters, sc_scores)","bf3bd863":"<a id='ts'><\/a>\n## 3.2 Regression with Time Series\n<a href=#top>(back to top)<\/a>\n### Data visualization\nIn order to apply regression with time series data, we need to plot with historical data to visualize the trend.From the trend we can determine what kind of regression is suitable to implement.","fc17f9cd":"<a id='intro'><\/a>\n# 1. Introduction\n<a href=#top>(back to top)<\/a>\n\n<a id='back'><\/a>\n## 1.1 Background\nWith the rapid development of economy and the continuous improvement of industrialization and urbanization, air pollution is becoming more and more serious. The pollution of PM2.5 is more serious, and haze weather occurs frequently, which causes serious adverse effects on the atmospheric environment and human health, which has aroused widespread concern in the world. \n\n<a id='object'><\/a>\n## 1.2 Objective\nBased on the monitoring data of Frankfurt area, the PM2.5 concentration in the air of the area was studied in detail. The monitoring data were preprocessed and a variety of models were established. The correlation between PM2.5 concentration and other environmental factors was obtained.","7e964281":"From sihouette score we can see in Kmeans model, when number of clusters is 3 or 4 it will achieve the highest score, and the score of number of cluster = 5 is also acceptable.","b57b4881":"# Air Quality from Frankfurt<a id='top'><\/a>\n## Contents <a id='top'><\/a>\n1. <a href=#intro>Introduction<\/a>\n    1. <a href=#back>Background<\/a>\n    1. <a href=#object>Objective<\/a>\n1. <a href=#data>Data exploration<\/a>\n1. <a href=#model>Modeling<\/a>\n    1. <a href=#regression>Simple Regression<\/a>\n    1. <a href=#ts>Regression with Time Series<\/a>\n    1. <a href=#cm>Classification Model<\/a>\n    1. <a href=#clustering>Clustering Model<\/a>\n    1. <a href=#tsa>Time Series analysis<\/a>\n1. <a href=#conclusion>Conclusion<\/a>\n1. <a href=#ref>Links<\/a>","f6eb3048":"### Hyperparameter Tuning","997ce77d":"### 2.KNN model\n#### Hyperparameter tuning","e3f1df07":"#### evaluate model with sihouette score ","37b7fdaf":"#### 2.Using useful features for modeling ","4d1223fd":"#### Modeling","8d2a02a3":"#### Modeling","10064da4":"#### evaluate model with sihouette score ","9600126a":"#### Modeling","9e04247f":"### 3. Decision Tree\n#### Preprocessing data","b55501f7":"confusion matrix and classification report","07dbde54":"#### Modeling","af181963":"# 2.PCA","b3668479":"#### Model 2: SVM.LinearSVC\n#### hyperparameter tuning","1bccd29d":"<a id='model'><\/a>\n# 3. Modeling\n<a href=#top>(back to top)<\/a>\n\n<a id='regression'><\/a>\n## 3.1 Simple Regression\n<a href=#top>(back to top)<\/a>","b987f537":"### 4.Random Forest Model\n\n#### Modeling","d0d0ff01":"### 5. Support Vector Machine (SVM)\n#### Model 1: SVM.SVC(kernel='linear')\n#### hyperparameter tuning","b2a3ae97":"confusion matrix and classification report","23f2d31e":"#### 4.Making pipeline (scaling and onehot encoding)","c6c46cab":"<a id='data'><\/a>\n# 2. Data exploration\n<a href=#top>(back to top)<\/a>\n\nFor this project the dataset is downloaded from Kaggle.com, a website which is known as a machine learning and data analytics competition platform. It contains 14 columns in total, and there are more than 1200000 observations in this dataset.\n\nAttribute information:\n1. Time_stamp: timing records with internal of 3 min from 2018\/12\/31 to 2020\/2\/28\n2. boxName: categorical variables with 14 different sensors:'iGude','Rothschildallee' etc.\n3. PM 2.5: the value of PM 2.5 for a certain time collected by certain sensor.\n4. temp: the temperature for a certain time at the place of certain sensor.\n5. pressure: the pressure for a certain time at the place of certain sensor.\n6. humidity: the humidity for a certain time at the place of certain sensor.\n7. wind_speed: the speed of wind for a certain time at the place of certain sensor.\n8. Time of Day: the time period of a day for the observations. 4 categories: morining, afternoon, evening, night.\n9. Peak\/NoPeak: determine whether it is a peak time of the day or not.\n10. Day: determine what day it is. Monday-Sunday.\n11. Week Day: determine whether it is a workday or a weekend.\n12. Weather: determine what the weather is for a certain observation\n13. Weather Description: describe what the weather is in detail.\n14. label: normal or abnormal.\n\n## 2.1 Loading data","b7cf39f3":"#### 1.Using all features for modeling","343369ce":"#### Modeling","334a3f9d":"confusion matrix and classification report","fa8cd3c7":"The result of gridsearch has been calculated, the code is commented to avoid getting stuck","4ab03e65":"### Modeling \n### 1.DNN model","42d7ab2b":"#### Model 3: SVM.SVC(kernel='rbf')\n#### hyperparameter tuning","8d0295f3":"#### Crosstable labels vs actual target","73e8c29c":"## show the distribution of k-means clusters after using PCA","1f95bac6":"## 2.2 Preprocessing data\n### Check for missing value","9a92f724":"#### Modeling","e22952a2":"#### Modeling","c20ccf58":"#### Aggregate timing records\nClose scrutiny of the case reveals that there are too many observations among the dataset, and the interval of each observation are so small, so we try to make dataset size smaller. We aggregate timing records with interval of 3 min into interval of one day for each sensor, so some of the columns (PM 2.5 etc)collected by one sensor within one day will be taken average to represent the data of this day by certain sensor, and the rest of columns (weather etc) collected by one sensor within one day will be taken mode to represent the data of this day.","f527b917":"## 2.3 Map Visualization\n### Distribution of number of observations for each sensor on the map","51679e4a":"confusion matrix and classification report","48ec1d78":"#### 3.Slipting training set and test set","7a64d16e":"confusion matrix and classification report","de9f4621":"From  above, we can see sihouette score reach a high point when number of clusters is 2 for hierarchical model.","20d8064f":"<a id='clustering'><\/a>\n## 3.4 Clustering Model\n<a href=#top>(back to top)<\/a>\n\n### 1. K-means Clustering Model\n\nFor unsupervised learning, we can also use K-means Clustering model to cluster those given features into certain groups. We also apply sihouette score to evaluate the number of cluster.\n\n#### Model Construction","8f0c863f":"confusion matrix and classification report","4f7993ee":"#### visualization of mean value of PM2.5 observed by each sensor","41acdb94":"#### Model 4: SVM.SVC(kernel='poly')\n#### hyperparameter tuning","29b395a2":"### 3. Hierarchical Clustering model\nApart from Kmeans clustering, we also apply hierachical clustering to cluster the dataset without target. We also apply sihouette score to evaluate the number of clusters.\n\n#### Model Construction","34f4cb5f":"<a id='cm'><\/a>\n## 3.3 Classification Model\n<a href=#top>(back to top)<\/a>\n### Data preprocessing\nIn order to apply classification model in this dataset, we first need to change our target PM 2.5 which is continous, into categorical. From World Health Organization (WHO) we find out the air quality guidelines and interim targets for PM 2.5:\n1. Interim target-1(IT-1): PM 2.5 = 35 ug\/m^3 It is associated with about a 15% gigher long-term mortality risk relative to AQG level.\n2. Interim target-2(IT-2): PM 2.5 = 25 ug\/m^3 In addition to other health benefits, this lebel lower the risk of premature mortality bu approximately 6% [2-11%] relative to the IT-1 level.\n3. Interim target-3(IT-3): PM 2.5 = 15 ug\/m^3 In addition to other health benefits, this level reduced the mortality risk by approximately 6% [2-11%] relative to the -IT-2 level.\n4. Air quality guideline (AQG): PM 2.5 = 10 ug\/m^3 These are the lowest levels at which total, cardiopulmonary and lung cancer mortality have been shown to increase with more than 95% confidence in response to long-term exposure to PM2.5","2e7d1019":"confusion matrix and classification report","f344090e":"#### Data visualization","41384762":"From above we can see there is no null value in the dataset, so we can use this dataset straightly.\n\n#### Add longitude and latitude columns\nWe try to add longitude and latitude columns at the end of dataset for each observation, so that we can do map vasualization for those sensors in different locations."}}