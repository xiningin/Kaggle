{"cell_type":{"4306306d":"code","84348452":"code","024f94c8":"code","30d7059f":"code","34cd4cf7":"code","d8bd29ce":"code","ea1bf264":"code","93ba8cb7":"code","05d8f781":"code","087d0ddc":"code","9fa2ff92":"code","5a8bf438":"code","79dabc55":"code","898d58a6":"code","8e033762":"code","7d529e28":"code","7c39867d":"code","8b45b848":"code","520ca7ec":"code","41e6b526":"code","b430e7e9":"code","23939918":"code","a3728102":"code","6a9eaddc":"code","5767d9c9":"code","814bd8ed":"code","46e1f4c8":"code","2c01a4f3":"code","1463a843":"code","fa3ed787":"code","cb6cf39f":"code","1e095aa9":"code","9a16de32":"code","c19d0504":"markdown","520650b8":"markdown","8ee0c06c":"markdown","cd7147da":"markdown","66d0fb9e":"markdown","a4d520db":"markdown","dc11d5d6":"markdown","b17f697d":"markdown","3c5bd256":"markdown","40f831a4":"markdown","87d25258":"markdown","2df31d8e":"markdown","e1e4bb61":"markdown","14224003":"markdown","d7812b02":"markdown","73e137a2":"markdown","bffd1781":"markdown","792ccc44":"markdown","d846c147":"markdown"},"source":{"4306306d":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\nimport math\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","84348452":"!apt-get install p7zip\n!p7zip -d -f -k \/kaggle\/input\/mercari-price-suggestion-challenge\/train.tsv.7z","024f94c8":"def rmsle(y, y_pred):\n    assert len(y) == len(y_pred)\n    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n    return (sum(to_sum) * (1.0\/len(y))) ** 0.5\n#Source: https:\/\/www.kaggle.com\/marknagelberg\/rmsle-function","30d7059f":"#S\u1eed d\u1ee5ng pandas \u0111\u1ecdc file d\u1eef li\u1ec7u \ntrain = pd.read_table(\"train.tsv\", sep=\"\\t\")\ntest = pd.read_csv(\"..\/input\/mercari-price-suggestion-challenge\/test_stg2.tsv.zip\" , sep='\\t')\nprint(train.shape)\nprint(test.shape)","34cd4cf7":"train.shape\ntrain.head(5)","d8bd29ce":"train.info()\n","ea1bf264":"train.isnull().sum()","93ba8cb7":"def handle_missing(dataset):\n    #x\u1eed l\u00fd d\u1eef li\u1ec7u tr\u1ed1ng\n    dataset.category_name.fillna(value=\"missing\", inplace=True)\n    dataset.brand_name.fillna(value=\"missing\", inplace=True)\n    dataset.item_description.fillna(value=\"missing\", inplace=True)\n    return (dataset)","05d8f781":"train = handle_missing(train)\ntest = handle_missing(test)","087d0ddc":"train.price.describe()","9fa2ff92":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(14,8))\nax.hist(train.price,bins = 30, range = [0,200],label=\"Price\")\nax.set_xlabel('Price',fontsize=15)\nax.set_ylabel('No of items', fontsize=15)\nplt.show()","5a8bf438":"train[\"logPrice\"] = np.log(train[\"price\"]+1)","79dabc55":"from scipy.stats import norm\nimport seaborn as sns\n\n\nfig, axes = plt.subplots(1, 3, figsize=(10,4))\n\naxes[0].set_title('Price')\nsns.distplot(train.price, ax=axes[0], kde=False)\naxes[0].grid()\n\naxes[1].set_title('Price < 75')\nsns.distplot(train.price[train.price<75], ax=axes[1], kde=False)\naxes[1].grid()\n\naxes[2].set_title('log(Price + 1)')\nsns.distplot(train[\"logPrice\"], ax=axes[2], fit=norm, kde=False)\naxes[2].set_xticks(range(0,9))\naxes[2].grid()","898d58a6":"def plot_distribution_and_violin(variable):\n    fig, axes = plt.subplots(2,1,figsize=(5,6), sharex=True)\n    axes[0].set_title(variable)\n    sns.countplot(x=variable, data=train, palette=\"ch:.25\", color=\"c\", ax=axes[0])\n    sns.violinplot(x=variable, y='logPrice', palette=\"ch:.25\", data=train, ax=axes[1])\n    fig.tight_layout()\n\nplot_distribution_and_violin('shipping')\nprint('The shipping prices of products are either \u20181\u2019(buyer charged) or \u20180\u2019(seller charged)')","8e033762":"train['category_name'].value_counts()[:10]\n","7d529e28":"def transform_category_name(category_name):\n    try:\n        main, sub1, sub2= category_name.split('\/')\n        return main, sub1, sub2\n    except:\n        return \"none\", \"none\", \"none\"\n\ntrain['category_main'], train['subcat_1'], train['subcat_2'] = zip(*train['category_name'].apply(transform_category_name))\ntest['category_main'], test['subcat_1'], test['subcat_2'] = zip(*test['category_name'].apply(transform_category_name))\ntrain.head(10)","7c39867d":"print(\"There are %d unique main-categories.\" % train['category_main'].nunique())\nprint(\"There are %d unique first sub-categories.\" % train['subcat_1'].nunique())\nprint(\"There are %d unique second sub-categories.\" % train['subcat_2'].nunique())\n","8b45b848":"import plotly\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot\n\n\nx = train['category_main'].value_counts().index.values.astype('str')\ny = train['category_main'].value_counts().values\npct = [(\"%.2f\"%(v*100))+\"%\"for v in (y\/len(train))]\n\ntrace1 = go.Bar(x=x, y=y, text=pct)\nlayout = dict(title= 'Number of Items by Main Category',\n              yaxis = dict(title='Count'),\n              xaxis = dict(title='Category'))\nfig=dict(data=[trace1], layout=layout)\niplot(fig)","520ca7ec":"x = train['subcat_1'].value_counts().index.values.astype('str')[:15]\ny = train['subcat_1'].value_counts().values[:15]\npct = [(\"%.2f\"%(v*100))+\"%\"for v in (y\/len(train))][:15]\n\ntrace1 = go.Bar(x=x, y=y, text=pct)\nlayout = dict(title= 'Number of Items by Main Category',\n              yaxis = dict(title='Count'),\n              xaxis = dict(title='subcat_1'))\nfig=dict(data=[trace1], layout=layout)\niplot(fig)\n","41e6b526":"x = train['subcat_2'].value_counts().index.values.astype('str')[:15]\ny = train['subcat_2'].value_counts().values[:15]\npct = [(\"%.2f\"%(v*100))+\"%\"for v in (y\/len(train))][:15]\n\ntrace1 = go.Bar(x=x, y=y, text=pct)\nlayout = dict(title= 'Number of Items by Main Category',\n              yaxis = dict(title='Count'),\n              xaxis = dict(title='subcat_2'))\nfig=dict(data=[trace1], layout=layout)\niplot(fig)\n","b430e7e9":"print(\"There are %d unique brand names in the training dataset.\" % train['brand_name'].nunique())","23939918":"x = train['brand_name'].value_counts().index.values.astype('str')[:15]\ny = train['brand_name'].value_counts().values[:15]\npct = [(\"%.2f\"%(v*100))+\"%\"for v in (y\/len(train))][:15]\n\ntrace1 = go.Bar(x=x, y=y, text=pct)\nlayout = dict(title= 'Number of Items by Main Category',\n              yaxis = dict(title='Count'),\n              xaxis = dict(title='brand_name'))\nfig=dict(data=[trace1], layout=layout)\niplot(fig)","a3728102":"from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nprint(\"Handling categorical variables...\")\nle = LabelEncoder()\ntrain.category_main = le.fit_transform(train.category_main)\ntrain.subcat_1 = le.fit_transform(train.subcat_1)\ntrain.subcat_2 = le.fit_transform(train.subcat_2)\ntrain.brand_name = le.fit_transform(train.brand_name)\n\ntest.category_main = le.fit_transform(test.category_main)\ntest.subcat_1 = le.fit_transform(test.subcat_1)\ntest.subcat_2 = le.fit_transform(test.subcat_2)\ntest.brand_name = le.fit_transform(test.brand_name)\ndel le","6a9eaddc":"print(\"Text to seq process...\")\nfrom keras.preprocessing.text import Tokenizer\nraw_text = np.hstack([train.item_description.str.lower(), train.name.str.lower()])\n\nprint(\"...\")\ntok_raw = Tokenizer()\ntok_raw.fit_on_texts(raw_text)\nprint(\"...\")\n#chuy\u1ec3n v\u0103n b\u1ea3n th\u00e0nh chu\u1ed7i\ntrain[\"seq_item_description\"] = tok_raw.texts_to_sequences(train.item_description.str.lower())\ntest[\"seq_item_description\"] = tok_raw.texts_to_sequences(test.item_description.str.lower())\ntrain[\"seq_name\"] = tok_raw.texts_to_sequences(train.name.str.lower())\ntest[\"seq_name\"] = tok_raw.texts_to_sequences(test.name.str.lower())\ntrain.shape\ntrain.head(5)","5767d9c9":"max_name_seq = np.max([np.max(train.seq_name.apply(lambda x: len(x))), np.max(test.seq_name.apply(lambda x: len(x)))])\nmax_seq_item_description = np.max([np.max(train.seq_item_description.apply(lambda x: len(x)))\n                                   , np.max(test.seq_item_description.apply(lambda x: len(x)))])\nprint(\"max name seq \"+str(max_name_seq))\nprint(\"max item desc seq \"+str(max_seq_item_description))\n\nMAX_NAME_SEQ = 10\nMAX_ITEM_DESC_SEQ = 75\nMAX_TEXT = np.max([np.max(train.seq_name.max())\n                   , np.max(test.seq_name.max())\n                  , np.max(train.seq_item_description.max())\n                  , np.max(test.seq_item_description.max())])+2\nMAX_CATEGORY = np.max([train.category_main.max(), test.category_main.max(), train.subcat_1.max(), test.subcat_1.max(),train.subcat_2.max(), test.subcat_2.max()])+1\nMAX_BRAND = np.max([train.brand_name.max(), test.brand_name.max()])+1\nMAX_CONDITION = np.max([train.item_condition_id.max(), test.item_condition_id.max()])+1\n","814bd8ed":"train[\"target\"] = np.log(train.price+1)\ntarget_scaler = MinMaxScaler(feature_range=(-1, 1))\ntrain[\"target\"] = target_scaler.fit_transform(train.target.values.reshape(-1,1))\npd.DataFrame(train.target).hist()\nfrom sklearn.model_selection import train_test_split\ndtrain, dvalid = train_test_split(train, random_state=123, train_size=0.70)\nprint(dtrain.shape)\nprint(dvalid.shape)","46e1f4c8":"\nfrom keras.preprocessing.sequence import pad_sequences\n\ndef get_keras_data(dataset):\n    X = {\n        'name': pad_sequences(dataset.seq_name, maxlen=MAX_NAME_SEQ)\n        ,'item_desc': pad_sequences(dataset.seq_item_description, maxlen=MAX_ITEM_DESC_SEQ)\n        ,'brand_name': np.array(dataset.brand_name)\n        ,'category_main': np.array(dataset.category_main)\n        ,'subcat_1': np.array(dataset.subcat_1)\n        ,'subcat_2': np.array(dataset.subcat_2)\n        ,'item_condition': np.array(dataset.item_condition_id)\n        ,'num_vars': np.array(dataset[[\"shipping\"]])\n    }\n    return X\n\nX_train = get_keras_data(dtrain)\nX_valid = get_keras_data(dvalid)\nX_test = get_keras_data(test)","2c01a4f3":"from keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GRU, Embedding, Flatten, BatchNormalization, Conv1D, GlobalMaxPooling1D\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom keras import backend as K\n\ndef get_model():  \n    \n    #Inputs\n    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n    brand_name = Input(shape=[1], name=\"brand_name\")\n    category_main = Input(shape=[1], name=\"category_main\")\n    subcat_1 = Input(shape=[1], name=\"subcat_1\")\n    subcat_2 = Input(shape=[1], name=\"subcat_2\")\n    item_condition = Input(shape=[1], name=\"item_condition\")\n    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n    \n    #Embeddings layers\n    emb_name = Embedding(MAX_TEXT, 50)(name)  \n    emb_item_desc = Embedding(MAX_TEXT, 50)(item_desc)\n    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n    emb_category_main = Embedding(MAX_CATEGORY, 10)(category_main)\n    emb_subcat_1 = Embedding(MAX_CATEGORY, 10)(subcat_1)\n    emb_subcat_2 = Embedding(MAX_CATEGORY, 10)(subcat_2)\n    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n    \n    convs1 = []\n    convs2 = []\n    \n    for filter_length in [1,2]:\n        cnn_layer1 = Conv1D(filters=50, kernel_size=filter_length, padding='same', activation='relu', strides=1) (emb_name)\n        cnn_layer2 = Conv1D(filters=50, kernel_size=filter_length, padding='same', activation='relu', strides=1) (emb_item_desc)\n        \n        maxpool1 = GlobalMaxPooling1D() (cnn_layer1)\n        maxpool2 = GlobalMaxPooling1D() (cnn_layer2)\n        \n        convs1.append(maxpool1)\n        convs2.append(maxpool2)\n\n    convs1 = concatenate(convs1)\n    convs2 = concatenate(convs2)\n    \n      #rnn layer\n    \n    rnn_layer1 = GRU(16) (emb_item_desc)  #GRU, s\u1eed d\u1ee5ng cho d\u1eef li\u1ec7u text chuy\u1ec3n sang chu\u1ed7i (s\u1ed1)\n    rnn_layer2 = GRU(8) (emb_name)  # d\u1eef li\u1ec7u cho l\u1edbp ti\u1ebfp theo\n    \n    #Li\u00ean k\u1ebft c\u00e1c l\u1edbp c\u1ee7a m\u1ea1ng NN\n    \n    main_l = concatenate([\n        Flatten() (emb_brand_name)\n        , Flatten() (emb_category_main)\n        , Flatten() (emb_subcat_1)\n        , Flatten() (emb_subcat_2)\n        , Flatten() (emb_item_condition)\n        , convs1 \n        , convs2\n        , rnn_layer1\n        , rnn_layer2\n        , num_vars\n    ])\n    \n    main_l = Dropout(0.25)(Dense(128, activation='relu') (main_l))\n    main_l = Dropout(0.1)(Dense(64, activation='relu') (main_l)) \n    \n    output = Dense(1, activation='linear') (main_l)\n\n    model = Model([name, item_desc, brand_name\n                   , category_main, subcat_1, subcat_2, item_condition, num_vars], output)\n    model.compile(loss='mse', optimizer='adam')\n    \n    return model","1463a843":"BATCH_SIZE = 20000\nepochs = 5\nmodel = get_model()\nmodel.summary()","fa3ed787":"model.fit(X_train, dtrain.target, epochs=epochs, batch_size=BATCH_SIZE\n          , validation_data=(X_valid, dvalid.target)\n          , verbose=1)","cb6cf39f":"import math\n\nval_preds = model.predict(X_valid)\nval_preds = target_scaler.inverse_transform(val_preds)\nval_preds = np.exp(val_preds)+1\n\n#absolute_error & squared_log_error\ny_true = np.array(dvalid.price.values)\ny_pred = val_preds[:,0]\nv_rmsle = rmsle(y_true, y_pred)\nprint(\" RMSLE error on dev test: \"+str(v_rmsle))","1e095aa9":"preds = model.predict(X_test, batch_size=BATCH_SIZE)\npreds = target_scaler.inverse_transform(preds)\npreds = np.expm1(preds)\n\nsubmission = test[[\"test_id\"]]\nsubmission[\"price\"] = preds","9a16de32":"submission.shape\nsubmission.to_csv(\"submission.csv\", index=False)","c19d0504":"S\u1ebd kh\u00f3 kh\u0103n h\u01a1n khi ph\u00e2n t\u00edch m\u1ee5c description v\u00ec \u0111\u00f3 l\u00e0 d\u1eef li\u1ec7u phi c\u1ea5u tr\u00fac. Nh\u00ecn s\u01a1 qua v\u1ec1 d\u1eef li\u1ec7u c\u00f3 th\u1ec3 nh\u1eadn th\u1ea5y, khi description c\u00e0ng d\u00e0i th\u00ec m\u1eb7t h\u00e0ng c\u00f3 xu h\u01b0\u1edbng gi\u00e1 cao h\u01a1n. Ch\u00fang ta s\u1ebd lo\u1ea1i b\u1ecf t\u1ea5t c\u1ea3 c\u00e1c d\u1ea5u c\u00e2u, lo\u1ea1i b\u1ecf m\u1ed9t s\u1ed1 t\u1eeb ng\u1eafn (stop words) trong ti\u1ebfng Anh (nh\u01b0 \"a\", \"the\", v.v.) v\u00e0 b\u1ea5t k\u1ef3 t\u1eeb n\u00e0o kh\u00e1c c\u00f3 \u0111\u1ed9 d\u00e0i nh\u1ecf h\u01a1n 3","520650b8":"V\u00ec c\u00e1c s\u1ea3n ph\u1ea9m 'women' xu\u1ea5t hi\u1ec7n v\u1edbi s\u1ed1 l\u01b0\u1ee3ng l\u1edbn trong Danh m\u1ee5c 'main_category', n\u00ean c\u00e1c danh m\u1ee5c xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t trong Danh m\u1ee5c 'subcat_1' ph\u00f9 h\u1ee3p v\u1edbi danh c\u00e1c danh m\u1ee5c nhi\u1ec1u nh\u1ea5t \u1edf m\u1ee5c 'main_category' ('Athletic Apparel','Makeup','Tops&Blueses')","8ee0c06c":"\u0110a ph\u1ea7n c\u00e1c m\u1eb7t h\u00e0ng kh\u00f4ng c\u00f3 th\u01b0\u01a1ng hi\u1ec7u, nhi\u1ec1u th\u1ee9 2 v\u00e0 3 l\u1ea7n l\u01b0\u1ee3t l\u00e0 \"PINK\" v\u00e0 \"Nike","cd7147da":"# Ph\u00e2n t\u00edch s\u1ed1 li\u1ec7u <\/br>\n**Price** <\/br>\nD\u01b0\u1edbi \u0111\u00e2y l\u00e0 s\u1ef1 ph\u00e2n b\u1ed1 c\u00e1c m\u1ee9c gi\u00e1 \u0111\u01b0\u1ee3c th\u1ed1ng k\u00ea","66d0fb9e":"T\u1eeb s\u1ed1 li\u1ec7u th\u1ed1ng k\u00ea, c\u00f3 th\u1ec3 n\u00f3i, Women c\u00f3 s\u1ed1 l\u01b0\u1ee3ng m\u1eb7t h\u00e0ng t\u1ed1i \u0111a. <\/br>\nT\u00ean danh m\u1ee5c \u0111\u01b0\u1ee3c li\u1ec7t k\u00ea b\u1eb1ng d\u1ea5u ph\u00e2n c\u00e1ch \u2018\/\u2019 cho bi\u1ebft main category, sub-category 1 v\u00e0 sub-category 2 c\u1ee7a s\u1ea3n ph\u1ea9m. Do \u0111\u00f3, \u0111\u1ec3 hi\u1ec3u r\u00f5 h\u01a1n v\u1ec1 t\u1eebng s\u1ea3n ph\u1ea9m, ta s\u1ebd th\u1ef1c hi\u1ec7n k\u1ef9 thu\u1eadt chia t\u00ean danh m\u1ee5c th\u00e0nh 3 c\u1ed9t kh\u00e1c nhau, c\u1ee5 th\u1ec3 l\u00e0 \"category_main\", \"subcat_1\" v\u00e0 \"subcat_2\".","a4d520db":"Ta nh\u1eadn th\u1ea5y s\u1ef1 ph\u00e2n ph\u1ed1i v\u1ec1 gi\u00e1 b\u1ecb l\u1ec7ch r\u1ea5t nhi\u1ec1u <\/br> \nL\u1ed7i n\u00e0y l\u00e0 L\u1ed7i l\u00f4garit b\u00ecnh ph\u01b0\u01a1ng g\u1ed1c (RMSLE).Do \u0111\u00f3, \u00e1p d\u1ee5ng ph\u00e9p bi\u1ebfn \u0111\u1ed5i logarit cho bi\u1ebfn m\u1ee5c ti\u00eau gi\u00e1, \u0111\u1ec3 l\u00e0m cho gi\u1ea3 \u0111\u1ecbnh n\u00e0y c\u00f3 s\u1eb5n cho vi\u1ec7c \u0111\u00e0o t\u1ea1o m\u00f4 h\u00ecnh.","dc11d5d6":"**M\u1ee5c ti\u00eau** <\/br>\nD\u1ef1 \u0111o\u00e1n gi\u00e1 c\u1ee7a m\u1ed9t m\u1eb7t h\u00e0ng v\u1edbi t\u00ecnh tr\u1ea1ng, m\u00f4 t\u1ea3 v\u00e0 c\u00e1c t\u00ednh n\u0103ng li\u00ean quan kh\u00e1c. Gi\u1ea3m thi\u1ec3u s\u1ef1 kh\u00e1c bi\u1ec7t gi\u1eefa gi\u00e1 d\u1ef1 \u0111o\u00e1n v\u00e0 gi\u00e1 th\u1ef1c t\u1ebf (RMSLE) <\/br>\n**\u0110\u00e1nh gi\u00e1 t\u1ed5ng quan:** <\/br>\n\u0110\u00e2y l\u00e0 b\u00e0i to\u00e1n h\u1ed3i quy tuy\u1ebfn t\u00ednh. D\u1eef li\u1ec7u \u0111\u01b0\u1ee3c cung c\u1ea5p \u1edf d\u1ea1ng text v\u00e0 lable, n\u00ean c\u1ea7n chuy\u1ec3n c\u00e1c \u0111\u1eb7c tr\u01b0ng v\u1ec1 d\u1ea1ng s\u1ed1 th\u1ef1c ho\u1eb7c vector \u0111\u1ec3 x\u1eed l\u00fd tuy\u1ebfn t\u00ednh","b17f697d":"# Y\u00caU C\u1ea6U\nS\u1eed d\u1ee5ng c\u00e1c th\u00f4ng s\u1ed1 v\u1ec1 danh m\u1ee5c s\u1ea3n ph\u1ea9m, th\u01b0\u01a1ng hi\u1ec7u, t\u00ecnh tr\u1ea1ng m\u1eb7t h\u00e0ng,... \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n gi\u00e1 c\u1ee7a c\u00e1c m\u1eb7t h\u00e0ng \u0111\u01b0\u1ee3c b\u00e1n","3c5bd256":"\u1ede \u0111\u00e2y, ch\u00fang ta c\u00f3 th\u1ec3 th\u1ea5y r\u1eb1ng \u0111\u1ed1i v\u1edbi c\u00e1c m\u1eb7t h\u00e0ng c\u00f3 gi\u00e1 th\u1ea5p h\u01a1n, ng\u01b0\u1eddi mua th\u01b0\u1eddng ph\u1ea3i tr\u1ea3 ph\u00ed v\u1eadn chuy\u1ec3n v\u00ec l\u00fd do l\u1ee3i nhu\u1eadn. Ngo\u00e0i ra, khi gi\u00e1 t\u0103ng l\u00ean, ch\u00fang ta c\u00f3 th\u1ec3 th\u1ea5y r\u1eb1ng ng\u01b0\u1eddi b\u00e1n \u0111\u00e3 l\u1ef1a ch\u1ecdn thanh to\u00e1n ph\u00ed v\u1eadn chuy\u1ec3n \u0111\u1ec3 k\u00edch th\u00edch ti\u00eau d\u00f9ng. Xu h\u01b0\u1edbng n\u00e0y th\u01b0\u1eddng \u0111\u01b0\u1ee3c quan s\u00e1t th\u1ea5y khi mua s\u1ea3n ph\u1ea9m tr\u1ef1c tuy\u1ebfn v\u1edbi gi\u00e1 tr\u1ecb th\u1ea5p h\u01a1n m\u1ed9t ng\u01b0\u1ee1ng nh\u1ea5t \u0111\u1ecbnh \u0111\u1ec3 \u0111\u01b0\u1ee3c giao h\u00e0ng mi\u1ec5n ph\u00ed.","40f831a4":"# \u0110i\u1ec1u l\u00e0m \u0111\u01b0\u1ee3c trong l\u1ea7n test n\u00e0y (version4):\nNh\u1eadn th\u1ea5y loss v\u1eabn c\u00f2n \u1edf t\u1ec9 s\u1ed1 cao -> t\u0103ng th\u1eddi gian hu\u1ea5n luy\u1ec7n\nt\u0103ng epoch(5->20) Nh\u1eadn th\u1ea5y \u0111\u01b0\u1ee3c loss gi\u1ea3m \u0111i t\u01b0\u01a1ng \u0111\u1ed1i nhi\u1ec1u (0.0159->0.0112) \nK\u1ebft qu\u1ea3: Gi\u1ea3m \u0111\u01b0\u1ee3c Error c\u1ee7a h\u00e0m RMSLE t\u1eeb 0.506 xu\u1ed1ng 0.488","87d25258":"T\u1eeb bi\u1ec3u \u0111\u1ed3 tr\u00ean, c\u00f3 th\u1ec3 n\u00f3i r\u1eb1ng c\u00e1c s\u1ea3n ph\u1ea9m d\u00e0nh cho 'Women' c\u00f3 s\u1ed1 l\u01b0\u1ee3ng l\u1edbn nh\u1ea5t. ti\u1ebfp theo l\u00e0 c\u00e1c s\u1ea3n ph\u1ea9m 'beauty'. Danh m\u1ee5c chung l\u1edbn th\u1ee9 3 l\u00e0 'Kids'.\n","2df31d8e":"Ki\u1ec3m tra c\u00e1c gi\u00e1 tr\u1ecb null trong t\u1eadp d\u1eef li\u1ec7u: <\/br> Ng\u01b0\u1eddi ta ph\u1ea3i ki\u1ec3m tra c\u00e1c gi\u00e1 tr\u1ecb b\u1ecb thi\u1ebfu trong t\u1eadp d\u1eef li\u1ec7u tr\u01b0\u1edbc khi s\u1eed d\u1ee5ng b\u1ea5t k\u1ef3 m\u00f4 h\u00ecnh h\u1ecdc m\u00e1y n\u00e0o. <\/br> Sau \u0111\u00f3 ta s\u1ebd ph\u1ea3i \"l\u1ea5p\" c\u00e1c gi\u00e1 tr\u1ecb tr\u1ed1ng c\u1ee7a ch\u00fang","e1e4bb61":"B\u01b0\u1edbc 1: Ph\u00e2n t\u00edch d\u1eef li\u1ec7u kh\u00e1m ph\u00e1 B\u01b0\u1edbc \u0111\u1ea7u ti\u00ean \u0111\u1ec3 gi\u1ea3i quy\u1ebft b\u1ea5t k\u1ef3 nghi\u00ean c\u1ee9u \u0111i\u1ec3n h\u00ecnh n\u00e0o trong khoa h\u1ecdc d\u1eef li\u1ec7u l\u00e0 xem x\u00e9t v\u00e0 ph\u00e2n t\u00edch d\u1eef li\u1ec7u b\u1ea1n c\u00f3 m\u1ed9t c\u00e1ch ch\u00ednh x\u00e1c. N\u00f3 gi\u00fap cung c\u1ea5p nh\u1eefng hi\u1ec3u bi\u1ebft c\u00f3 gi\u00e1 tr\u1ecb v\u1ec1 m\u00f4 h\u00ecnh v\u00e0 th\u00f4ng tin m\u00e0 n\u00f3 ph\u1ea3i truy\u1ec1n t\u1ea3i. <\/br> \u0110\u1ec3 t\u1ea3i d\u1eef li\u1ec7u, ch\u00fang ta ch\u1ec9 c\u1ea7n t\u1ec7p train.tsv. Ch\u00fang t\u00f4i s\u1ebd t\u1ea3i n\u00f3 v\u00e0o khung d\u1eef li\u1ec7u g\u1ea5u tr\u00fac.","14224003":"Qua so s\u00e1nh 3 bi\u1ec3u \u0111\u1ed3 ta c\u00f3 th\u1ec3 th\u1ea5y d\u1eef li\u1ec7u khi chuy\u1ec3n \u0111\u1ed5i logarit c\u00f3 s\u1ef1 \u1ed5n \u0111\u1ecbnh h\u01a1n (chu\u1ea9n h\u01a1n)\n","d7812b02":"Ch\u00fang ta c\u00f3 th\u1ec3 th\u1ea5y r\u1eb1ng c\u00e1c c\u1ed9t \u2018category_name\u2019, \u2018brand_name\u2019 v\u00e0 \u2018item_description\u2019 c\u00f3 gi\u00e1 tr\u1ecb r\u1ed7ng. C\u00f3 c\u00e1c k\u1ef9 thu\u1eadt kh\u00e1c nhau \u0111\u1ec3 x\u1eed l\u00fd c\u00e1c gi\u00e1 tr\u1ecb b\u1ecb thi\u1ebfu nh\u01b0 lo\u1ea1i b\u1ecf c\u00e1c h\u00e0ng c\u00f3 gi\u00e1 tr\u1ecb b\u1ecb thi\u1ebfu, lo\u1ea1i b\u1ecf \u0111\u1ed1i t\u01b0\u1ee3ng \u0111\u1ecba l\u00fd c\u00f3 t\u1ef7 l\u1ec7 gi\u00e1 tr\u1ecb b\u1ecb thi\u1ebfu cao ho\u1eb7c l\u1ea5p \u0111\u1ea7y c\u00e1c c\u1ed9t c\u00f3 gi\u00e1 tr\u1ecb b\u1ecb thi\u1ebfu b\u1eb1ng m\u1ed9t s\u1ed1 gi\u00e1 tr\u1ecb kh\u00e1c. \u1ede \u0111\u00e2y, ch\u00fang ta \u0111\u00e3 ch\u1ecdn \u0111\u1ec3 l\u1ea5p \u0111\u1ea7y c\u00e1c gi\u00e1 tr\u1ecb c\u00f2n thi\u1ebfu n\u00e0y trong d\u1eef li\u1ec7u b\u1eb1ng m\u1ed9t s\u1ed1 gi\u00e1 tr\u1ecb kh\u00e1c.","73e137a2":"**H\u1ea1n ch\u1ebf:** Code c\u00f2n l\u1edbn, chi\u1ebfm dung l\u01b0\u1ee3ng nhi\u1ec1u, x\u1eed l\u00fd ch\u1eadm (m\u1ed7i epoch ch\u1ea1y trong kho\u1ea3ng 280s)\n\nCh\u01b0a kh\u1eafc ph\u1ee5c \u0111\u01b0\u1ee3c: Ch\u01b0a t\u00ecm \u0111\u01b0\u1ee3c model t\u1ed1t h\u01a1n \u0111\u1ec3 thay th\u1ebf (score v\u1eabn gi\u1eef \u1edf m\u1ee9c ~ 0.6) (best: 0.3888)","bffd1781":"# Mercari Price Suggestion Challenge:\n> D\u1ef1 \u0111o\u00e1n gi\u00e1 ti\u1ec1n c\u1ee7a m\u1ed9t lo\u1ea1i m\u1eb7t h\u00e0ng d\u1ef1a v\u00e0o c\u00e1c th\u00f4ng s\u1ed1 k\u0129 thu\u1eadt, m\u00f4 t\u1ea3 c\u1ee7a ng\u01b0\u1eddi b\u00e1n, h\u00ecnh th\u1ee9c b\u00e1n...\n\nB\u00e1o c\u00e1o v\u1ea5n \u0111\u1ec1 C\u00f3 th\u1ec3 kh\u00f3 bi\u1ebft th\u1ee9 g\u00ec \u0111\u00f3 th\u1ef1c s\u1ef1 \u0111\u00e1ng gi\u00e1 bao nhi\u00eau. Chi ti\u1ebft nh\u1ecf c\u00f3 th\u1ec3 c\u00f3 ngh\u0129a l\u00e0 s\u1ef1 kh\u00e1c bi\u1ec7t l\u1edbn v\u1ec1 gi\u00e1 c\u1ea3. V\u00ed d\u1ee5, m\u1ed9t trong nh\u1eefng chi\u1ebfc \u00e1o len c\u00f3 gi\u00e1 335 \u0111\u00f4 la v\u00e0 chi\u1ebfc c\u00f2n l\u1ea1i c\u00f3 gi\u00e1 9,99 \u0111\u00f4 la th\u00ec \u0111i\u1ec1u g\u00ec quy\u1ebft \u0111\u1ecbnh v\u1ec1 gi\u00e1 c\u1ee7a s\u1ea3n ph\u1ea9m <\/br>\n\nVi\u1ec7c \u0111\u1ecbnh gi\u00e1 s\u1ea3n ph\u1ea9m th\u1eadm ch\u00ed c\u00f2n kh\u00f3 h\u01a1n tr\u00ean quy m\u00f4 l\u1edbn, ch\u1ec9 c\u1ea7n xem x\u00e9t c\u00f3 bao nhi\u00eau s\u1ea3n ph\u1ea9m \u0111\u01b0\u1ee3c b\u00e1n tr\u1ef1c tuy\u1ebfn. Qu\u1ea7n \u00e1o c\u00f3 xu h\u01b0\u1edbng \u0111\u1ecbnh gi\u00e1 theo m\u00f9a m\u1ea1nh m\u1ebd v\u00e0 b\u1ecb \u1ea3nh h\u01b0\u1edfng nhi\u1ec1u b\u1edfi th\u01b0\u01a1ng hi\u1ec7u, trong khi \u0111\u1ed3 \u0111i\u1ec7n t\u1eed c\u00f3 gi\u00e1 dao \u0111\u1ed9ng d\u1ef1a tr\u00ean th\u00f4ng s\u1ed1 k\u1ef9 thu\u1eadt c\u1ee7a s\u1ea3n ph\u1ea9m. <\/br>\n\nMercari mu\u1ed1n \u0111\u01b0a ra \u0111\u1ec1 xu\u1ea5t v\u1ec1 gi\u00e1 cho ng\u01b0\u1eddi b\u00e1n, nh\u01b0ng \u0111i\u1ec1u n\u00e0y r\u1ea5t kh\u00f3 v\u00ec ng\u01b0\u1eddi b\u00e1n c\u1ee7a h\u1ecd \u0111\u01b0\u1ee3c ph\u00e9p \u0111\u01b0a b\u1ea5t k\u1ef3 th\u1ee9 g\u00ec ho\u1eb7c b\u1ea5t k\u1ef3 g\u00f3i n\u00e0o l\u00ean th\u1ecb tr\u01b0\u1eddng c\u1ee7a Mercari.","792ccc44":"c\u00f3 m\u1ed9t t\u1eadp d\u1eef li\u1ec7u g\u1ed3m 1482535 s\u1ea3n ph\u1ea9m c\u00f9ng v\u1edbi c\u00e1c th\u00f4ng s\u1ed1 c\u1ee7a ch\u00fang. 5 h\u00e0ng \u0111\u1ea7u ti\u00ean xu\u1ea5t hi\u1ec7n nh\u01b0 h\u00ecnh tr\u00ean. <\/br> xem th\u00eam m\u1ed9t s\u1ed1 th\u00f4ng tin v\u1ec1 d\u1eef li\u1ec7u \u1edf d\u01b0\u1edbi","d846c147":"V\u00ec v\u1eady, t\u1eeb b\u1ea3ng m\u00f4 t\u1ea3, ch\u00fang ta c\u00f3 th\u1ec3 k\u1ebft lu\u1eadn r\u1eb1ng: <\/br> 25% s\u1ea3n ph\u1ea9m c\u00f3 gi\u00e1 d\u01b0\u1edbi 10 $, 50% s\u1ea3n ph\u1ea9m c\u00f3 gi\u00e1 d\u01b0\u1edbi 17 $ v\u00e0 75% s\u1ea3n ph\u1ea9m c\u00f3 gi\u00e1 d\u01b0\u1edbi 29 $ "}}