{"cell_type":{"7021b152":"code","84fe5a84":"code","53bd984a":"code","ed541b5f":"code","3abf4974":"code","9455209a":"code","1e1995ce":"code","9541b52f":"code","cdb2e8bf":"code","dcccfd5f":"code","f8bf4f01":"code","d8ea426e":"code","e15cb41b":"code","352c859e":"code","ab448404":"code","9b76532d":"code","ab8d577b":"code","b9123093":"code","ef51e053":"code","536f320b":"code","c46e0c4d":"code","71aa3150":"code","ba99fdfa":"code","19d7986f":"code","57b43cc9":"code","dd34cfd3":"code","2c4df1d2":"code","c2151c01":"code","bcea13c1":"code","990f3d3f":"code","2bd376f1":"code","b77034db":"code","d5e0a299":"code","0bbd4b13":"code","debfa9f7":"code","b3706359":"code","7448d56e":"code","aeca833e":"code","a839e1ce":"code","e147b37a":"code","56304a32":"code","1173a16d":"code","c348e080":"code","c7f70434":"code","88a13c45":"code","e0c8226e":"code","cc9a700a":"code","2d5b0953":"code","d37ef1ef":"code","d897bd89":"code","6ae419cf":"code","6cf51b66":"code","2bfee099":"code","1c56b809":"code","69d73c9a":"code","f44df14e":"code","4209f250":"code","667cc3d6":"code","903f05c0":"code","1a936a3d":"code","e9cecbb6":"code","109c3891":"code","f8d560b1":"code","08759966":"code","fb8b45b6":"code","a20627f4":"code","62e94a1f":"code","6e0f0ec8":"code","6b78a9c6":"code","0a00b38e":"code","31be914c":"code","f204b9ac":"code","93566fe3":"code","4af6a01f":"code","ac551da8":"code","56e08bf8":"code","98912649":"code","291b7ebd":"markdown","0e1c319b":"markdown","89010d67":"markdown","55b47022":"markdown","443bf5d1":"markdown","683f4f37":"markdown","13c0ef01":"markdown","14890594":"markdown","f83020db":"markdown","e9543e12":"markdown","da5e31bb":"markdown","8f34a516":"markdown","4b9a838d":"markdown","72fbd08b":"markdown","fbca8f3a":"markdown","8f1252e2":"markdown","4915fb40":"markdown","4e63bc55":"markdown","fb793fce":"markdown","1e7dd9a0":"markdown","fa06c36b":"markdown","87b48677":"markdown","4ae0fcf3":"markdown","d89ea4b5":"markdown","3801626c":"markdown","89bf583a":"markdown","97c7f615":"markdown","4ba1b78b":"markdown","c46afce0":"markdown","1768f8bb":"markdown","1ef62c83":"markdown","116fd9e7":"markdown","01c8b6d3":"markdown","0abf3d4f":"markdown","5f2387a3":"markdown","d73dfc13":"markdown","18675721":"markdown","70e913b8":"markdown","c3283df8":"markdown","f6b9d05d":"markdown","450b9446":"markdown","1f55d774":"markdown","97949fc6":"markdown","d42ef4c2":"markdown","a1c5fcb5":"markdown","c63e5741":"markdown","537d3ddb":"markdown","5d610a8c":"markdown"},"source":{"7021b152":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nimport matplotlib.pyplot as plt\n%matplotlib inline","84fe5a84":"data_filepath = \"\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\"\nraw_data = pd.read_csv(data_filepath)","53bd984a":"raw_data.head()","ed541b5f":"raw_data = raw_data.sample(len(raw_data), random_state=42)\nraw_data = raw_data.reset_index(drop=True)","3abf4974":"raw_data.describe()","9455209a":"raw_data.info()","1e1995ce":"plt.figure(figsize=(8, 6));\n\nsns.violinplot(x=\"DEATH_EVENT\", y=\"ejection_fraction\", data=raw_data, split=True, inner=\"quart\", linewidth=1, \n               palette={0: \"orangered\", 1: \"lightcoral\"})\nsns.despine(offset=10, trim=True)\nplt.xlabel(\"Died\");\nplt.ylabel(\"Ejection Fraction\");","9541b52f":"fig, axes = plt.subplots(3, 2, figsize=(16, 16));\nfig.suptitle('Distribution of features for different DEATH_COUNT');\n\n_=sns.violinplot(x=\"DEATH_EVENT\", y=\"ejection_fraction\", data=raw_data, split=True, inner=\"quart\", linewidth=1, \n               palette={0: \"orangered\", 1: \"lightcoral\"}, ax=axes[0, 0]);\nplt.setp(axes[0, 0], ylabel=\"Ejection Fraction\");\n\n_=sns.violinplot(x=\"DEATH_EVENT\", y=\"age\", data=raw_data, split=True, inner=\"quart\", linewidth=1, \n               palette={0: \"orangered\", 1: \"lightcoral\"}, ax=axes[0, 1]);\nplt.setp(axes[0, 1], ylabel=\"Age\");\n\n_=sns.violinplot(x=\"DEATH_EVENT\", y=\"creatinine_phosphokinase\", data=raw_data, split=True, inner=\"quart\", linewidth=1, \n               palette={0: \"orangered\", 1: \"lightcoral\"}, ax=axes[1, 0]);\nplt.setp(axes[1, 0], ylabel=\"Creatinine Phosphokinase\");\n\n_=sns.violinplot(x=\"DEATH_EVENT\", y=\"platelets\", data=raw_data, split=True, inner=\"quart\", linewidth=1, \n               palette={0: \"orangered\", 1: \"lightcoral\"}, ax=axes[1, 1]);\nplt.setp(axes[1, 1], ylabel=\"Platelets\");\n\n_=sns.violinplot(x=\"DEATH_EVENT\", y=\"serum_creatinine\", data=raw_data, split=True, inner=\"quart\", linewidth=1, \n               palette={0: \"orangered\", 1: \"lightcoral\"}, ax=axes[2, 0]);\nplt.setp(axes[2, 0], ylabel=\"Serum Creatinine\");\n\n_=sns.violinplot(x=\"DEATH_EVENT\", y=\"serum_sodium\", data=raw_data, split=True, inner=\"quart\", linewidth=1, \n               palette={0: \"orangered\", 1: \"lightcoral\"}, ax=axes[2, 1]);\nplt.setp(axes[2, 1], ylabel=\"Serum Sodium\");\n\nsns.despine(offset=10, trim=True);\nplt.setp(axes, xlabel=\"Died\");","cdb2e8bf":"import matplotlib.patches as mpatches","dcccfd5f":"def get_cat_percent_matrix(feature, mode):\n    \"\"\"\n    'mode' can be 'individual', 'in_column' or 'column'\n    \"\"\"\n    total_amount = len(raw_data)\n    \n    feature_died_amount = sum((raw_data[feature]==1) & (raw_data[\"DEATH_EVENT\"]==1));\n    not_feature_died_amount = sum((raw_data[feature]==0) & (raw_data[\"DEATH_EVENT\"]==1));\n    feature_lived_amount = sum((raw_data[feature]==1) & (raw_data[\"DEATH_EVENT\"]==0));\n    not_feature_lived_amount = sum((raw_data[feature]==0) & (raw_data[\"DEATH_EVENT\"]==0));\n    \n    feature_amount = feature_died_amount+feature_lived_amount\n    not_feature_amount = not_feature_died_amount+not_feature_lived_amount\n    if mode==\"individual\":\n        feature_died_percent = str(round(feature_died_amount*100\/(total_amount), 2))+\"%\";\n        not_feature_died_percent = str(round(not_feature_died_amount*100\/(total_amount), 2))+\"%\";\n        feature_lived_percent = str(round(feature_lived_amount*100\/(total_amount), 2))+\"%\";\n        not_feature_lived_percent = str(round(not_feature_lived_amount*100\/(total_amount), 2))+\"%\";\n        return feature_died_percent, not_feature_died_percent, feature_lived_percent, not_feature_lived_percent\n    elif mode==\"in_column\":\n        feature_died_percent = str(round(feature_died_amount*100\/(feature_amount), 2))+\"%\";\n        not_feature_died_percent = str(round(not_feature_died_amount*100\/(not_feature_amount), 2))+\"%\";\n        feature_lived_percent = str(round(feature_lived_amount*100\/(feature_amount), 2))+\"%\";\n        not_feature_lived_percent = str(round(not_feature_lived_amount*100\/(not_feature_amount), 2))+\"%\";\n        return feature_died_percent, not_feature_died_percent, feature_lived_percent, not_feature_lived_percent\n    elif mode==\"column\":\n        feature_percent = str(round((feature_amount)*100\/(total_amount), 2))+\"%\";\n        not_feature_percent = str(round((not_feature_amount)*100\/(total_amount), 2))+\"%\";\n        return feature_percent, not_feature_percent\n\ndef make_count_plot(x, ax, x_label):\n    _=sns.histplot(binwidth=0.5, x=x, hue=\"DEATH_EVENT\", data=raw_data, palette={0: \"firebrick\", 1: \"maroon\", 2: \"darkturquoise\", 3:\"teal\"}, ax=ax, stat=\"count\", multiple=\"stack\")\n    sns.despine(left=True, ax=ax)\n    ax.set_xticks([0, 1]);\n    ax.set_xlabel(x_label);\n    \n    feature_died_amount = sum((raw_data[x]==1) & (raw_data[\"DEATH_EVENT\"]==1));\n    not_feature_died_amount = sum((raw_data[x]==0) & (raw_data[\"DEATH_EVENT\"]==1));\n    feature_lived_amount = sum((raw_data[x]==1) & (raw_data[\"DEATH_EVENT\"]==0));\n    not_feature_lived_amount = sum((raw_data[x]==0) & (raw_data[\"DEATH_EVENT\"]==0));\n    feature_amount = feature_died_amount+feature_lived_amount\n    not_feature_amount = not_feature_died_amount+not_feature_lived_amount\n    \n    feature_died_percent, not_feature_died_percent, feature_lived_percent, not_feature_lived_percent = get_cat_percent_matrix(x, \"in_column\")\n    ax.text(0.2, not_feature_died_amount\/2, not_feature_died_percent, style='normal', fontsize=12, fontweight=\"bold\");\n    ax.text(0.7, feature_died_amount\/2, feature_died_percent, style='normal', fontsize=12, fontweight=\"bold\");\n    ax.text(0.7, (feature_lived_amount\/2)+feature_died_amount, feature_lived_percent, style='normal', fontsize=12, fontweight=\"bold\");\n    ax.text(0.2, (not_feature_lived_amount\/2)+not_feature_died_amount, not_feature_lived_percent, style='normal', fontsize=12, fontweight=\"bold\");\n    feature_died_percent, not_feature_died_percent, feature_lived_percent, not_feature_lived_percent = get_cat_percent_matrix(x, \"individual\")\n    ax.text(0.2, (not_feature_died_amount\/2)-9, not_feature_died_percent, style='normal', fontsize=12, color=\"white\");\n    ax.text(0.7, (feature_died_amount\/2)-9, feature_died_percent, style='normal', fontsize=12, color=\"white\");\n    ax.text(0.7, ((feature_lived_amount\/2)+feature_died_amount)-9, feature_lived_percent, style='normal', fontsize=12, color=\"white\");\n    ax.text(0.2, ((not_feature_lived_amount\/2)+not_feature_died_amount)-9, not_feature_lived_percent, style='normal', fontsize=12, color=\"white\");\n    feature_percent, not_feature_percent = get_cat_percent_matrix(x, \"column\")\n    ax.text(0.2, not_feature_amount+2, not_feature_percent, style='normal', fontsize=12);\n    ax.text(0.7, feature_amount+2, feature_percent, style='normal', fontsize=12);\n\n    survived_patch = mpatches.Patch(color='firebrick', label='Survived')\n    died_patch = mpatches.Patch(color='maroon', label='Died')\n    ax.legend(handles=[survived_patch, died_patch]);","f8bf4f01":"fig, axes = plt.subplots(3, 2, figsize=(16, 16));\n\nmake_count_plot(\"anaemia\", axes[0, 0], \"Anaemia\")\nmake_count_plot(\"diabetes\", axes[0, 1], \"Diabetes\")\nmake_count_plot(\"high_blood_pressure\", axes[1, 0], \"High Blood Pressure\")\nmake_count_plot(\"sex\", axes[1, 1], \"Sex\")\nmake_count_plot(\"smoking\", axes[2, 0], \"Smoking\")\n\nsns.countplot(x=\"DEATH_EVENT\", data=raw_data, palette={0: \"firebrick\", 1: \"maroon\"}, ax=axes[2, 1]);\naxes[2, 1].text(-0.1, len(raw_data[raw_data[\"DEATH_EVENT\"]==0])+2, str(round(len(raw_data[raw_data[\"DEATH_EVENT\"]==0])*100\/len(raw_data), 2))+\"%\", style='normal', fontsize=12);\naxes[2, 1].text(0.9, len(raw_data[raw_data[\"DEATH_EVENT\"]==1])+2, str(round(len(raw_data[raw_data[\"DEATH_EVENT\"]==1])*100\/len(raw_data), 2))+\"%\", style='normal', fontsize=12);\nsns.despine(left=True, right=True, top=True, ax=axes[2, 1]);\naxes[2, 1].set_xlabel(\"Died\");","d8ea426e":"fig, axes = plt.subplots(3, 2, figsize=(16, 16));\n\nsns.histplot(data=raw_data,x=\"ejection_fraction\",ax=axes[0, 0],hue=\"DEATH_EVENT\",multiple=\"stack\",bins=10,kde=True,palette={0: \"orangered\", 1: \"lightcoral\"});\nsns.despine(left=True, right=True, top=True, ax=axes[0, 0]);\naxes[0, 0].set_xlabel(\"Ejection Fraction\");\nsns.histplot(data=raw_data,x=\"age\",ax=axes[0, 1],hue=\"DEATH_EVENT\",multiple=\"stack\",bins=10,kde=True,palette={0: \"orangered\", 1: \"lightcoral\"});\nsns.despine(left=True, right=True, top=True, ax=axes[0, 1]);\naxes[0, 1].set_xlabel(\"Age\");\nsns.histplot(data=raw_data,x=\"creatinine_phosphokinase\",ax=axes[1, 0],hue=\"DEATH_EVENT\",multiple=\"stack\",bins=10,kde=True,palette={0: \"orangered\", 1: \"lightcoral\"}, log_scale=True);\nsns.despine(left=True, right=True, top=True, ax=axes[1, 0]);\naxes[1, 0].set_xlabel(\"Creatinine Phosphokinase\");\nsns.histplot(data=raw_data,x=\"platelets\",ax=axes[1, 1],hue=\"DEATH_EVENT\",multiple=\"stack\",bins=10,kde=True,palette={0: \"orangered\", 1: \"lightcoral\"});\nsns.despine(left=True, right=True, top=True, ax=axes[1, 1]);\naxes[1, 1].set_xlabel(\"Platelets\");\nsns.histplot(data=raw_data,x=\"serum_creatinine\",ax=axes[2, 0],hue=\"DEATH_EVENT\",multiple=\"stack\",bins=10,kde=True,palette={0: \"orangered\", 1: \"lightcoral\"}, log_scale=True);\nsns.despine(left=True, right=True, top=True, ax=axes[2, 0]);\naxes[2, 0].set_xlabel(\"Serum Creatinine\");\nsns.histplot(data=raw_data,x=\"serum_sodium\",ax=axes[2, 1],hue=\"DEATH_EVENT\",multiple=\"stack\",bins=10,kde=True,palette={0: \"orangered\", 1: \"lightcoral\"});\nsns.despine(left=True, right=True, top=True, ax=axes[2, 1]);\naxes[2, 1].set_xlabel(\"Serum Sodium\");","e15cb41b":"engineered_data = raw_data.copy()","352c859e":"engineered_data[\"heart_function1_ratio\"] = engineered_data[\"creatinine_phosphokinase\"] * (engineered_data[\"ejection_fraction\"]\/100)\nengineered_data[\"heart_function2_ratio\"] = engineered_data[\"platelets\"] * (engineered_data[\"ejection_fraction\"]\/100)\nengineered_data[\"blood_function1_ratio\"] = engineered_data[\"serum_sodium\"] \/ engineered_data[\"creatinine_phosphokinase\"]\nengineered_data[\"general_healthiness\"] = engineered_data[\"age\"] * engineered_data[\"serum_creatinine\"]\nengineered_data[\"heart_healthiness\"] = engineered_data[\"ejection_fraction\"] \/ (engineered_data[\"general_healthiness\"])\nengineered_data[\"auto1\"] = engineered_data[\"age\"] \/ (engineered_data[\"ejection_fraction\"]*engineered_data[\"serum_sodium\"])\nengineered_data[\"auto2\"] = (engineered_data[\"age\"]*engineered_data[\"serum_sodium\"]) \/ engineered_data[\"ejection_fraction\"]\nengineered_data[\"auto3\"] = (engineered_data[\"platelets\"]*engineered_data[\"serum_creatinine\"])\/engineered_data[\"ejection_fraction\"]","ab448404":"# Reordering the columns so that DEATH_EVENT is last\nnum_df = engineered_data.copy()\ncols = num_df.columns.tolist()\ncols = cols[:12] + cols[13:] + [cols[12]]\nnum_df = num_df[cols]","9b76532d":"all_corrs = abs(num_df.corr())","ab8d577b":"death_corrs = dict(sorted(all_corrs[\"DEATH_EVENT\"].drop(\"DEATH_EVENT\").to_dict().items(), key=lambda x: x[1], reverse=True))\nfor feature in death_corrs.keys():\n    print(feature+\"'s correlation with 'DEATH_EVENT': \"+str(death_corrs[feature]))","b9123093":"all_engineered_corrs = abs(pd.concat([engineered_data.drop(raw_data.columns, axis=1), engineered_data[\"DEATH_EVENT\"]], axis=1).corr())\nall_raw_corrs = abs(raw_data.corr())","ef51e053":"plt.figure(figsize=(16, 6));\nsns.heatmap(all_engineered_corrs, linewidths=0, vmin=0, vmax=1, annot=True);","536f320b":"plt.figure(figsize=(16, 6));\nsns.heatmap(all_raw_corrs, linewidths=0, vmin=0, vmax=1, annot=True);","c46e0c4d":"print(\"Time of the average survivor: \"+str(raw_data.loc[raw_data['DEATH_EVENT']==0]['time'].median()))\nprint(\"Time of the average deceased: \"+str(raw_data.loc[raw_data['DEATH_EVENT']==1]['time'].median()))","71aa3150":"plt.figure(figsize=(8, 6));\n\nsns.violinplot(x=\"DEATH_EVENT\", y=\"time\", data=raw_data, split=True, inner=\"quart\", linewidth=1, \n               palette={0: \"orangered\", 1: \"lightcoral\"})\nsns.despine(offset=10, trim=True)\nplt.xlabel(\"Died\");\nplt.ylabel(\"Time\");","ba99fdfa":"sns.histplot(data=raw_data, x=\"time\", bins=15, hue=\"DEATH_EVENT\", multiple=\"stack\", kde=True, \n             palette={0: \"orangered\", 1: \"lightcoral\"});","19d7986f":"print(\"Correlation of time with DEATH_EVENT: \"+str(abs(raw_data[[\"time\", \"DEATH_EVENT\"]].corr().to_numpy()[0, 1])))","57b43cc9":"engineered_data = engineered_data.drop(\"time\", axis=1)","dd34cfd3":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nimport warnings","2c4df1d2":"class ScaleData(BaseEstimator, TransformerMixin):\n    def __init__(self, quantile=0.95, ignore_missing=False):\n        self.quantile = quantile\n        self.ignore_missing = ignore_missing\n    def fit(self, X, y=None):\n        self.max_features = abs(X).quantile(self.quantile)\n        return self\n    def transform(self, X):\n        if str(type(X)) != \"<class 'pandas.core.frame.DataFrame'>\":\n            raise Exception(\"'X' must be a pd.DataFrame\")\n        try:\n            self.max_features\n        except AttributeError:\n            raise Exception(\"fit method wasn't called.\")\n        X_scaled = pd.DataFrame()\n        if not self.ignore_missing:\n            missing_cols = []\n            for col in self.max_features.keys():\n                if col not in list(X.columns):\n                    missing_cols.append(col)\n            if missing_cols != []:\n                raise Exception(f\"{len(missing_cols)} columns missing: {missing_cols}\")\n        for key in list(self.max_features.keys()):\n            try:\n                X_scaled = pd.concat([X_scaled, X[key]\/self.max_features[key]], axis=1)\n            except Exception:\n                pass\n        X_scaled.columns = list(self.max_features.keys())\n        if list(X_scaled.columns) != list(X.columns):\n            warnings.warn(f\"{len(X.columns)-len(X_scaled.columns)} column(s) were removed, as they weren't recognized: {np.setdiff1d(X.columns, X_scaled.columns)}\", Warning)\n        return X_scaled\n    \ndef model_metrics(make_model_func, X, y, num_iters=200, silence=False):\n    accuracies = []\n    precisions = []\n    recalls = []\n    for i in range(num_iters):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n        \n        scale_data = ScaleData()\n        X_train = scale_data.fit_transform(X_train)\n        X_test = scale_data.transform(X_test)\n        \n        model = make_model_func(X_train, y_train)\n        accuracies.append(model.score(X_test, y_test))\n        y_pred = model.predict(X_test)\n        precisions.append(precision_score(y_test, y_pred, zero_division=0))\n        recalls.append(recall_score(y_test, y_pred, zero_division=0))\n        \n        if (i%10 == 0) and not silence:\n            print(f\"\\r|{''.join('\u2588' for _ in range(round( (i\/num_iters)*20 )))}\"+\\\n                  f\"{''.join(' ' for _ in range(20-round( (i\/num_iters)*20 )))}|\"+\\\n                  f\" - {round( (i\/num_iters)*100, 1 )}%\"+\\\n                  f\" - Running Accuracy: {round( np.array(accuracies).mean()*100, 3 )}%\"+\\\n                  f\" - Running Precision: {round( np.array(precisions).mean()*100, 3 )}%\"+\\\n                  f\" - Running Recall: {round( np.array(recalls).mean()*100, 3 )}%            \", end=\"\")\n    \n    if not silence:\n        print(f\"\\rFinished! - Average Accuracy: {round( np.array(accuracies).mean()*100, 3 )}%\"+\\\n              f\" - Average Precision: {round( np.array(precisions).mean()*100, 3 )}%\"+\\\n              f\" - Average Recall: {round( np.array(recalls).mean()*100, 3 )}%                                     \", end=\"\")\n    return accuracies, precisions, recalls\n\n# Extra misc function that's never actually used in the notebook\nfrom sklearn.model_selection import GridSearchCV\ndef model_tuner(model, param_grid, X, y, num_iters=200, silence=False):\n    best_params = []\n    for i in range(num_iters):\n        scale_data = ScaleData()\n        X = scale_data.fit_transform(X)\n        \n        search = GridSearchCV(model(), param_grid)\n        search.fit(X, y)\n        best_params.append(str(search.best_params_))\n        \n        if (i%10 == 0) and not silence:\n            print(f\"\\r|{''.join('\u2588' for _ in range(round( (i\/num_iters)*20 )))}\"+\\\n                  f\"{''.join(' ' for _ in range(20-round( (i\/num_iters)*20 )))}|\"+\\\n                  f\" - {round( (i\/num_iters)*100, 1 )}%\"+\\\n                  f\" - Best Params: {pd.Series(best_params).value_counts().keys()[0]}            \", end=\"\")\n    \n    if not silence:\n        print(f\"\\rFinished! - Best Params: {pd.Series(best_params).value_counts().keys()[0]}\"+\\\n              f\"                                     \", end=\"\")\n    return best_params\n\ndef select_features(X, y):\n    def make_svc(X, y):\n        svc = SVC()\n        svc.fit(X, y)\n        return svc\n    print(\"\\rStarting...                             \", end=\"\")\n    _, precisions, recalls = model_metrics(make_svc, X, y, num_iters=2000, silence=True)\n    feature_combination_scores = {tuple(X.columns):(2*np.array(precisions).mean()*np.array(recalls).mean())\/(np.array(precisions).mean()+np.array(recalls).mean())}\n    total_rounds = sum([i+2 for i in range(len(X.columns)-1)])\n    num_columns = len(X.columns)\n    i = 0\n    while len(X.columns) > 1:\n        j = 0\n        feature_costs = {}\n        for feature in X.columns:\n            print(f\"\\r|{''.join(['\u2588' for _ in range( round((i*len(X.columns)+j)*20\/total_rounds) )])}{''.join([' ' for _ in range( 20-round((i*len(X.columns)+j)*20\/total_rounds) )])}| - {round((i*len(X.columns)+j)*100\/total_rounds, 2)}%\"+\\\n                  f\" - Round {i+1}\/{num_columns} - { round(j*100\/len(X.columns), 2) }%                               \", end=\"\")\n            _, precisions, recalls = model_metrics(make_svc, X.drop(feature, axis=1), y, num_iters=2000, silence=True)\n            feature_costs[feature] = (2*np.array(precisions).mean()*np.array(recalls).mean())\/\\\n                                       (np.array(precisions).mean()+np.array(recalls).mean())\n            j += 1\n        X = X.drop(pd.Series(feature_costs).idxmax(), axis=1)\n        feature_combination_scores[tuple(X.columns)] = feature_costs[pd.Series(feature_costs).idxmax()]\n        i += 1\n    print(\"\\rFinished!                                                                      \")\n    return list(max(feature_combination_scores, key=feature_combination_scores.get))","c2151c01":"from sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import precision_score, recall_score","bcea13c1":"# Previously found to be the best set of features\nselected_features = [\"creatinine_phosphokinase\", \"sex\", \"heart_function1_ratio\", \"general_healthiness\", \"heart_healthiness\", \"auto1\", \"auto2\"]","990f3d3f":"### IF YOU WANT TO RUN THE FEATURE SELECTION ALGORITHM; UNCOMMENT AND RUN THE FOLLOWING LINE:\n# selected_features = select_features(engineered_data.drop(\"DEATH_EVENT\",axis=1), engineered_data[\"DEATH_EVENT\"])\n### BE WARNED, IT TAKES A LONG TIME","2bd376f1":"print(selected_features)","b77034db":"plt.figure(figsize=(16, 6));\nsns.heatmap(engineered_data[selected_features+[\"DEATH_EVENT\"]].corr(), linewidths=0, vmin=0, vmax=1, annot=True);","d5e0a299":"from sklearn.model_selection import StratifiedShuffleSplit","0bbd4b13":"engineered_data = engineered_data.reset_index(drop=True)\nsplit = StratifiedShuffleSplit(n_splits=100, test_size=0.2, random_state=42) \nfor train_index, test_index in split.split(engineered_data[selected_features+[\"DEATH_EVENT\"]], engineered_data[\"DEATH_EVENT\"]):\n    strat_train_set = engineered_data[selected_features+[\"DEATH_EVENT\"]].loc[train_index]\n    strat_test_set = engineered_data[selected_features+[\"DEATH_EVENT\"]].loc[test_index]","debfa9f7":"start_bold = \"\\033[1m\"\nend_bold = \"\\033[0m\"","b3706359":"print(f\"{start_bold}Original proportions of DEATH_EVENT:{end_bold}\")\nprint(f'Percent that lived: {round(sum(engineered_data[\"DEATH_EVENT\"] == 0)*100 \/ len(engineered_data), 3)}% - Percent that died: {round(sum((engineered_data[\"DEATH_EVENT\"] == 1)*100 \/ len(engineered_data)), 3)}%')\nprint(\"------------------------------------\")\nprint(f\"{start_bold}Train\/test set proportions:{end_bold}\")\nprint(f'Train - Percent that lived: {round(sum(strat_train_set[\"DEATH_EVENT\"] == 0)*100 \/ len(strat_train_set), 3)}% - Percent that died: {round(sum((strat_train_set[\"DEATH_EVENT\"] == 1)*100 \/ len(strat_train_set)), 3)}%')\nprint(f'Test  - Percent that lived: {round(sum(strat_test_set[\"DEATH_EVENT\"] == 0)*100 \/ len(strat_test_set), 3)}% - Percent that died: {round(sum((strat_test_set[\"DEATH_EVENT\"] == 1)*100 \/ len(strat_test_set)), 3)}%')","7448d56e":"X_train, y_train = strat_train_set.drop(\"DEATH_EVENT\", axis=1), strat_train_set[\"DEATH_EVENT\"]\nX_test, y_test = strat_test_set.drop(\"DEATH_EVENT\", axis=1), strat_test_set[\"DEATH_EVENT\"]","aeca833e":"X_train = X_train.reset_index(drop=True)\ny_train = y_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\ny_test = y_test.reset_index(drop=True)","a839e1ce":"scale_data = ScaleData()","e147b37a":"X_train = scale_data.fit_transform(X_train)\nX_test = scale_data.transform(X_test) # Make sure not to call the `fit` method on the test set!","56304a32":"X_train.head()","1173a16d":"from sklearn.model_selection import train_test_split","c348e080":"def make_knn(X_train, y_train):\n    # Fine-tunes `n_neighbours` of KNN, changing the search space until it finds the optimal value\n    prev_best = None\n    neighbours_search, neighbours_prev_best = [6, 7, 8], None\n    while True:\n        knn_search = GridSearchCV(KNeighborsClassifier(), [{\n            \"n_neighbors\": neighbours_search\n        }], cv=5, scoring=\"accuracy\", return_train_score=True)\n        knn_search.fit(X_train, y_train)\n\n        if knn_search.best_params_[\"n_neighbors\"] == min(neighbours_search):\n            if (knn_search.best_params_[\"n_neighbors\"] == neighbours_prev_best) or (knn_search.best_params_[\"n_neighbors\"] == 1):\n                break\n            else:\n                neighbours_search = list(range( min(neighbours_search)-len(neighbours_search)+1, min(neighbours_search)+1 ))\n                neighbours_search = [num for num in neighbours_search if num >= 1] # Filter illegal values\n        elif knn_search.best_params_[\"n_neighbors\"] == max(neighbours_search):\n            if (knn_search.best_params_[\"n_neighbors\"] == neighbours_prev_best) or (knn_search.best_params_[\"n_neighbors\"]==int(0.7*len(X_train))):\n                break\n            else:\n                neighbours_search = list(range( max(neighbours_search), max(neighbours_search)+len(neighbours_search) ))\n                neighbours_search = [num for num in neighbours_search if num <= int(0.7*len(X_train))] # Filter illegal values\n        else:\n            break\n\n        neighbours_prev_best = knn_search.best_params_[\"n_neighbors\"]\n    return knn_search.best_estimator_","c7f70434":"knn_accuracies, knn_precisions, knn_recalls = model_metrics(make_knn, engineered_data[selected_features], \n                                                            engineered_data[\"DEATH_EVENT\"], num_iters=2000)","88a13c45":"from sklearn.linear_model import LogisticRegression","e0c8226e":"def make_log_reg(X_train, y_train):\n    # Fine-tunes various aspects of logistic regression\n    log_reg = LogisticRegression(C=1, penalty=\"elasticnet\", l1_ratio=0.5, solver=\"saga\", max_iter=1000)\n    log_reg.fit(X_train, y_train)\n    return log_reg","cc9a700a":"log_accuracies, log_precisions, log_recalls = model_metrics(make_log_reg, engineered_data[selected_features], \n                                                            engineered_data[\"DEATH_EVENT\"], num_iters=2000)","2d5b0953":"from sklearn.svm import SVC","d37ef1ef":"def make_svc(X_train, y_train):\n    # Returns a trained SVC model\n    svc = SVC()\n    svc.fit(X_train, y_train)\n    return svc","d897bd89":"svc_accuracies, svc_precisions, svc_recalls = model_metrics(make_svc, engineered_data[selected_features], \n                                                            engineered_data[\"DEATH_EVENT\"], num_iters=2000)","6ae419cf":"model = SVC(probability=True);\nmodel.fit(X_train, y_train);","6cf51b66":"import shap","2bfee099":"med = X_train.median().values.reshape((1, X_train.shape[1]))","1c56b809":"from sklearn.ensemble import BaggingClassifier","69d73c9a":"svc = BaggingClassifier(base_estimator=SVC(probability=True), n_estimators=100)\nsvc.fit(X_train, np.ravel(y_train.to_numpy()))\nsvc_f = lambda x: svc.predict_proba(x)[:,1]\nsvc_explainer = shap.Explainer(svc_f, med)\nsvc_shap_values = svc_explainer(X_test)","f44df14e":"import ipywidgets as widgets\nfrom IPython.display import clear_output","4209f250":"output = widgets.Output()\n\ndef explain_prediction(change):\n    with output:\n        clear_output(wait=True)\n        plt.show(shap.plots.waterfall(svc_shap_values[change[\"new\"]]))\n        print(\"DEATH_EVENT: \"+str(y_test.iloc[change[\"new\"]]))\n\nindex_chooser = widgets.BoundedIntText(\n    value=0,\n    min=0,\n    max=len(svc_shap_values),\n    step=1,\n    description='Index:',\n    disabled=False\n)\nindex_chooser.observe(explain_prediction, names=\"value\")","667cc3d6":"display(index_chooser, output)\nexplain_prediction({\"new\": 0})","903f05c0":"import math","1a936a3d":"num_rows, num_cols = round(np.ceil(len(X_train.columns)\/2)), 2\nfig, axes = plt.subplots(num_rows, num_cols, figsize=(16, 12))\n\ni = 0\nfor col in X_train.columns:\n    if i%2 == 0:\n        shap.plots.scatter(svc_shap_values[:, col], hist=False, ax=axes[round(np.floor(i\/2)), 0])\n    else:\n        shap.plots.scatter(svc_shap_values[:, col], hist=False, ax=axes[round(np.floor(i\/2)), 1])\n    clear_output(wait=True)\n    i += 1","e9cecbb6":"shap.plots.beeswarm(svc_shap_values, plot_size=(16, 6))","109c3891":"from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, roc_curve","f8d560b1":"svc_f1s = (2*np.array(svc_precisions)*np.array(svc_recalls))\/(np.array(svc_precisions)+np.array(svc_recalls))","08759966":"print(f\"{start_bold}SVC Metrics{end_bold}\\n\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\")\nprint(f\"Accuracy:  {round(np.array(svc_accuracies).mean()*100, 3)}% - Deviation: {round(np.array(svc_accuracies).std()*100, 3)}%\")\nprint(f\"Precision: {round(np.array(svc_precisions).mean()*100, 3)}% - Deviation: {round(np.array(svc_precisions).std()*100, 3)}%\")\nprint(f\"Recall:    {round(np.array(svc_recalls).mean()*100, 3)}% - Deviation: {round(np.array(svc_recalls).std()*100, 3)}%\")\nprint(f\"F1:        {round(np.array(svc_f1s).mean()*100, 3)}% - Deviation: {round(np.array(svc_f1s).std()*100, 3)}%\")","fb8b45b6":"y_pred_proba = model.predict_proba(X_test)[:, 1]","a20627f4":"def svc_metrics_for_threshold(X, y, threshold, num_iters=200, silence=False):\n    accuracies = []\n    precisions = []\n    recalls = []\n    tprs = []\n    fprs = []\n    for i in range(num_iters):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n        \n        scale_data = ScaleData()\n        X_train = scale_data.fit_transform(X_train)\n        X_test = scale_data.transform(X_test)\n        \n        model = SVC(probability=True)\n        model.fit(X_train, y_train)\n        y_pred = (model.predict_proba(X_test)[:, 1] > threshold).astype(np.int64)\n        \n        TP, FP, TN, FN = sum((y_pred == 1) & (y_test == 1)), sum((y_pred == 1) & (y_test == 0)), sum((y_pred == 0) & (y_test == 0)), sum((y_pred == 0) & (y_test == 1))\n        \n        accuracies.append(sum(y_pred == y_test)\/len(y_pred))\n        precisions.append(precision_score(y_test, y_pred, zero_division=0))\n        recalls.append(recall_score(y_test, y_pred, zero_division=0))\n        tprs.append(TP \/ (TP + FN))\n        fprs.append(FP \/ (FP + TN))\n        \n        if (i%10 == 0) and not silence:\n            print(f\"\\r|{''.join('\u2588' for _ in range(round( (i\/num_iters)*20 )))}\"+\\\n                  f\"{''.join(' ' for _ in range(20-round( (i\/num_iters)*20 )))}|\"+\\\n                  f\" - {round( (i\/num_iters)*100, 1 )}%\"+\\\n                  f\" - Running Accuracy: {round( np.array(accuracies).mean()*100, 3 )}%\"+\\\n                  f\" - Running Precision: {round( np.array(precisions).mean()*100, 3 )}%\"+\\\n                  f\" - Running Recall: {round( np.array(recalls).mean()*100, 3 )}%            \", end=\"\")\n    \n    if not silence:\n        print(f\"\\rFinished! - Average Accuracy: {round( np.array(accuracies).mean()*100, 3 )}%\"+\\\n              f\" - Average Precision: {round( np.array(precisions).mean()*100, 3 )}%\"+\\\n              f\" - Average Recall: {round( np.array(recalls).mean()*100, 3 )}%                                     \", end=\"\")\n    return accuracies, precisions, recalls, tprs, fprs","62e94a1f":"_, _, thresholds = precision_recall_curve(y_test, y_pred_proba)\naccuracies, precisions, recalls = [], [], []\nfor thresh in thresholds:\n    accs, precs, recs, _, _ = svc_metrics_for_threshold(engineered_data[selected_features], engineered_data[\"DEATH_EVENT\"], thresh, num_iters=500, silence=True)\n    accuracies.append(np.array(accs).mean())\n    precisions.append(np.array(precs).mean())\n    recalls.append(np.array(recs).mean())","6e0f0ec8":"_, _, roc_thresholds = roc_curve(y_test, y_pred_proba)\ntprs, fprs = [], []\nfor thresh in roc_thresholds:\n    _, _, _, tps, fps = svc_metrics_for_threshold(engineered_data[selected_features], engineered_data[\"DEATH_EVENT\"], thresh, num_iters=500, silence=True)\n    tprs.append(np.array(tps).mean())\n    fprs.append(np.array(fps).mean())","6b78a9c6":"def plot_precisions_recalls_vs_thresholds(precisions, recalls, thresholds):\n    fig= plt.figure(figsize=(10,5))\n    axes= fig.add_axes([0.1,0.1,0.8,0.8])\n    \n    axes.plot(thresholds, precisions[:-1], \"b-\", label=\"Precision\")\n    axes.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    \n    plt.title(\"Precision & Recall vs. Threshold\")\n    plt.xlabel(\"Threshold\")\n    plt.legend()\n    plt.grid(True)\n    \ndef plot_precision_vs_recall(precisions, recalls):\n    fig= plt.figure(figsize=(10,5))\n    axes= fig.add_axes([0.1,0.1,0.8,0.8])\n    \n    axes.plot(precisions, recalls, \"b-\")\n    \n    plt.title(\"Precision vs. Recall\")\n    plt.xlabel(\"Precision\")\n    plt.ylabel(\"Recall\")\n    plt.grid(True)\n\ndef plot_roc_curve(fpr, tpr):\n    fig= plt.figure(figsize=(10,5))\n    axes= fig.add_axes([0.1,0.1,0.8,0.8])\n    \n    axes.plot(fpr, tpr, linewidth=2)\n    axes.plot([0,1], [0,1], \"k--\")\n    \n    plt.title(\"FPR vs. TPR (a.k.a. ROC curve)\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.grid(True)","0a00b38e":"plot_precisions_recalls_vs_thresholds(precisions, recalls, thresholds[:-1])","31be914c":"plot_precision_vs_recall(precisions, recalls)","f204b9ac":"plot_roc_curve(fprs, tprs)","93566fe3":"def model_metrics_with_subsample(make_model_func, X, y, num_iters=200, subsample=None, silence=False):\n    accuracies = []\n    precisions = []\n    recalls = []\n    for i in range(num_iters):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=60)\n        if subsample != None:\n            inds = np.random.randint(0, len(X_train), (subsample,))\n            X_train, y_train = X_train.iloc[inds], y_train.iloc[inds]\n        \n        scale_data = ScaleData()\n        X_train = scale_data.fit_transform(X_train)\n        X_test = scale_data.transform(X_test)\n        \n        model = make_model_func(X_train, y_train)\n        accuracies.append(model.score(X_test, y_test))\n        y_pred = model.predict(X_test)\n        precisions.append(precision_score(y_test, y_pred, zero_division=0))\n        recalls.append(recall_score(y_test, y_pred, zero_division=0))\n        \n        if (i%10 == 0) and not silence:\n            print(f\"\\r|{''.join('\u2588' for _ in range(round( (i\/num_iters)*20 )))}\"+\\\n                  f\"{''.join(' ' for _ in range(20-round( (i\/num_iters)*20 )))}|\"+\\\n                  f\" - {round( (i\/num_iters)*100, 1 )}%\"+\\\n                  f\" - Running Accuracy: {round( np.array(accuracies).mean()*100, 3 )}%\"+\\\n                  f\" - Running Precision: {round( np.array(precisions).mean()*100, 3 )}%\"+\\\n                  f\" - Running Recall: {round( np.array(recalls).mean()*100, 3 )}%            \", end=\"\")\n    \n    if not silence:\n        print(\"\\r\", end=\"\")\n    return accuracies, precisions, recalls","4af6a01f":"import warnings\nwarnings.filterwarnings(\"ignore\")","ac551da8":"# scores_for_n = {}\n# for num_points in range(20, 240, 10):\n#     scores_for_n[num_points], _, _ = model_metrics_with_subsample(make_knn, engineered_data[selected_features], \n#                                                                   engineered_data[\"DEATH_EVENT\"], \n#                                                                   subsample=num_points, num_iters=2000, silence=False)\n#     print(\"Finished \"+str(num_points)+\" points. Got a mean accuracy of \"+str(round(np.array(scores_for_n[num_points]).mean()*100, 3))+\\\n#           \"% With a standard deviation of: \"+str(round( np.array(scores_for_n[num_points]).std()*100 ,3))+\\\n#           \"%                                                                                                                            \")","56e08bf8":"# data_points = []\n# accuracies = []\n# for key in scores_for_n.keys():\n#     data_points.append(int(key))\n#     accuracies.append(np.array(scores_for_n[key]).mean())","98912649":"# plt.figure(figsize=(12, 8))\n# plt.plot(data_points, accuracies, alpha=0.25)\n# plt.scatter(data_points, accuracies)\n# plt.xlabel(\"Dataset Size\")\n# plt.ylabel(\"Mean Accuracy\")\n# plt.grid(True)","291b7ebd":"## Error Analysis","0e1c319b":"# Graphing the Data\n***","89010d67":"From this we can see that the only features that really make change in **DEATH_EVENT** are `anaemia` and `high_blood_pressure`. Also, that (around) two thirds of patiens lived.","55b47022":"Looks good! The stratification seems to have worked.","443bf5d1":"<h4><strong><i>Our goal is to predict the probability of death from heart failure as early warning for patients in hospitals with CVD.<\/i><\/strong><\/h4>","683f4f37":"Now, let's put aside some of the data for testing models later on. I'll use a stratified train-test split, as the dataset size is small and I wan't to make sure `DEATH_EVENT`'s classes have the same proportion in each split.","13c0ef01":"## Making New Features","14890594":"We see it is very correlated, those who perished did so quickly, and those who survived had to wait for a long recovery.<br>\nBut should we use it?<br>\n<br>\nTo answer this, we need to think about how the final model will be used in production (yes, I'm aware the final model will probably never *actually* make it to production, but I think we should still think like it will nonetheless).<br>\nSo, how will the model be used? The limit is your imagination! However, I will pretend it'll be used as [vaguely] described in the \n<a href=\"https:\/\/www.kaggle.com\/andrewmvd\/heart-failure-clinical-data\" target=\"_blank\">dataset's description<\/a>, which is using it for early detection. That would mean this model will be used in a hospital setting, in order to give early warning for patients with CVD for heart failure. Now we know how it's going to be used, we can know (or at least guess) what features will be possible to provide.<br>\nSince it'll be used as early warning, we won't know how long a patient is *going to* stay in contact (which is what `time` is), and thus we can't provide `time`... Well, we can't provide `time` *exactly*, but we could in that situation instead provide the time in contact *so far*. Doing this will essentially bias the model to think all new patients will die, but will get less and less confident as said patients keeps surviving. This would make it a pretty terrible predictor of death probability, but instead it'll give a good \"how much attention you should give a patient\". However, the way it does it (give new patients more attention) is already pretty obvious to hospitals and we probably don't need to skew a model's predictions to say the same thing.<br>\nIn conclusion, for how I want the model to be used, the use of `time` will only skew the results in ways we don't want. Thus, I will *not* use the `time` variable in this notebook.","f83020db":"## Histograms of the Numerical Data","e9543e12":"Here's a little side-question; \"*How much would more data improve the accuracy of our models?*\". It's a suprisingly tough question to answer, I might not have even done it right in the following code, but I tried my best:","da5e31bb":"# Train-test Split","8f34a516":"## Violin Plots of the Numerical Data","4b9a838d":"Now, we just need to preprocess the data to make it better for machine learning models. There's not much we actually have to do (the data came pretty prepared already), I'll just be scaling to to the range `[0. - 1.]`.","72fbd08b":"As we can see, the features don't affect the output much, but they have outliers where they do. Also, the features seem to only ever *raise* the predicted probability.","fbca8f3a":"# Data Preprocessing","8f1252e2":"# The End","4915fb40":"# Testing Different Models","4e63bc55":"Now, we need to perform feature selection (remove the useless features for prediction).<br>\nHere, I've decided to do a small variaton of the [Sequential Feature Selector](https:\/\/scikit-learn.org\/stable\/auto_examples\/release_highlights\/plot_release_highlights_0_24_0.html). Essentially, it starts with the dataset with all the features, and then gets the (average)* test score of a fine-tuned KNN on that dataset, but with one of the features removed. It does this for all the features, and then properly removes the feature it got the highest score without (if it managed to get a high score without a feature, that means it didn't *need* that feature to get a good score). Then, just rince and repeat, (properly) removing features until there's none left, at which point, we find the point in the process where the (average)* score on the test set was the highest, returning that as the best selection of features it found.<br>\n<br>\n\\* - When I say \"*average*\" test score, I mean I go from the raw data to the fine-tuned KNN scored on the test set multiple times, eventually finishing all the runs and returning the average test score. I do this because the dataset size is very small, and means the test set size is always very small, that means it'll rarely ever be representative of the population, and thus, not a good way to score a model... To resolve this, I just get the average over many runs, each time doing the train-test split, data preperation, defining the model, training it and scoring it.","fb793fce":"# How Much Would More Data Help?","1e7dd9a0":"# Feature Engineering","fa06c36b":"Since the SVC performed the best, we'll use the SVC...","87b48677":"## Explaining Decisions with SHAP","4ae0fcf3":"Certaintly not too great... But, as to be expected. Next up is logistic regression:","d89ea4b5":"Here's something you might not have seen before, it's called SHAP (SHapley Additive exPlanations), it essentially can explain why a model made it's decision.","3801626c":"***","89bf583a":"Since the dataset size is small, scoring from a test set is always very random. So, I've automated the process, from raw data to fine-tuned model so that I can get an average of the test set results.<br>\nFirst let's try the KNN:","97c7f615":"Looking into our engineered features, they seem to have more clearly pronounced the relations of the features with **DEATH_EVENT** in the correlation metric, which means it should also be easier for machine learning models to use, which is great.<br>\nLooking into the raw features, most of them seem relatively independant from each other, which is good (except that it also applies to **DEATH_EVENT**). For the few correlations that do exist; `time` looks very correlated with **DEATH_EVENT**, which is good (we'll look into it next), and `sex` is very correlated with `smoking`. Other than that, there's some small non-sensical correlations (e.g. between `time` and `high_blood_pressure`).<br> \n","4ba1b78b":"# Making the Final Model","c46afce0":"We can see that the data seems to have ordered `DEATH_EVENT`, let's just quickly shuffle the dataset.","1768f8bb":"# Feature Selection","1ef62c83":"Okay! Let's check the proportions:","116fd9e7":"As we can see, the SVC performed best.<br>\n**Note** - I've also tried Decision Trees, Random Forests and XGBoosting, but they weren't very successful, they are just very parametric and complex, and after spending all that time feature engineering and selecting, I don't thing we'd need a very complex model in the first place.","01c8b6d3":"## Count Plots of the Categorical Data","0abf3d4f":"# Introduction to the Data","5f2387a3":"## Feature Correlations","d73dfc13":"## Looking at time","18675721":"The graph above shows the density of points at different values of `ejection_fraction` for those who died and those who didn't, we call it the *distribution*.<br>\n<br>\nWe can see that for those who survived the large majority had an `ejection_fraction` of around 40, but there is also another peak at about 60 (which is interesting, wonder if anyone can explain it). But then for those who didn't survive can see that the distributuion of `ejection_fraction` is much more spread out, but the median is lower.","70e913b8":"This is how different values for different features affected the model's output:","c3283df8":"Finally, let's do the SVM:","f6b9d05d":"A general conclusion to make from all of this is that it seems the people that survived had much less variation in their characteristics than for those who didn't, in other words, only specific people were able to survive, but almost anyone could've died (note though that two thirds of patients lived, which means that a lot of people were this \"specific person\" that could survive, in these aspects, at least...).","450b9446":"And a more compact version of above:","1f55d774":"Note that most of these new (engineered) features were made by hand, but the features `auto 1` - `3` were made by a search algorithm I made. I haven't shown it in a notebook yet, but if you want to see it, it's in <a href=\"https:\/\/stackoverflow.com\/questions\/65448806\/deep-feature-synthesis-depth-for-transformation-primitives-featuretools\/65470819#65470819\">this<\/a> StackOverflow post I made.","97949fc6":"**UNCOMMENT THE FOLLOWING CELLS IF YOU WISH TO RUN THE ALGORITHM, PLEASE NOTE THAT THEY TAKE A LONG TIME TO RUN.**","d42ef4c2":"Dataset from [here](https:\/\/www.kaggle.com\/andrewmvd\/heart-failure-clinical-data)<br><br>\nThis is a dataset of patients with Cardiovascular diseases (CVDs), tracking different conditions (below), and if the patient died from heart failure.\n<br>\n* **age** - *Age of the patient (years)*\n* **anaemia** - *Decrease of red blood cells or hemoglobin (boolean)*\n* **creatinine_phosphokinase** - *Level of the CPK enzyme in the blood (mcg\/L)*\n* **diabetes** - *If the patient has diabetes (boolean)*\n* **ejection_fraction** - *Percentage of blood leaving the heart at each contraction (percentage)*\n* **high_blood_pressure** - *If the patient has hypertension (boolean)*\n* **platelets** - *Platelets in the blood (kiloplatelets\/mL)*\n* **serum_creatinine** - *Level of serum creatinine in the blood (mg\/dL)*\n* **serum_sodium** - *Level of serum sodium in the blood (mEq\/L)*\n* **sex** - *Woman or man (binary)*\n* **smoking** - *If the patient smokes (boolean)*\n<br><br>\n* **time** - *This variable captures the time at which DEATH_EVENT happened in days. For example; if the patient died, then it tells how many days it took to happen, if the patient survives, it tells how long recovery took. We could, in theory, use time as another feature to predict DEATH_EVENT, more on this later*\n* **DEATH_EVENT** - *If the patient died from heart failure*","a1c5fcb5":"Now's the time to talk about `time`. This variable appears to be the time which the patient already sees the doctor for treatment. And while it was said that this feature is meant to be a target variable, as others have said, this is open to interpretation... So let's see how it relates to **DEATH_EVENT**:","c63e5741":"<img src=\"https:\/\/imgur.com\/gK5BGK3.png\">","537d3ddb":"Note in this next table of histograms, that `creatinine_phosphokinase` and `serum_creatinine` are on a logarithmic scale.","5d610a8c":"We now have a better idea of how our model will do in the wild, and it's not great... The features have very low correlations with `DEATH_EVENT`, so we can't have high hopes to begin with. Things like `ejection_fraction` and `serum_creatinine` don't have very good predictive power, and there really isn't that much data. Although, that's what makes this dataset a good challenge."}}