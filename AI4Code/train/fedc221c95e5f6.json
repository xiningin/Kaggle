{"cell_type":{"c586f2d1":"code","689c7f11":"code","be084b06":"code","d2bd1135":"code","567aedb7":"code","a1cf552c":"code","70c47b91":"code","ccca50d8":"code","58cd6ab5":"code","5cb2a863":"code","67385679":"code","161f6810":"code","1b2efd34":"code","1936ceed":"code","1f62374b":"code","207ccc4e":"code","efc9b2d6":"code","609e61ce":"code","9b7ffa6b":"code","f2b9ba12":"code","3e616ab6":"code","fa5485be":"code","36057602":"code","13cde445":"code","cb816ff0":"code","57d6374f":"code","1c7caace":"code","d500146f":"markdown","18803f90":"markdown","090da225":"markdown","5f06005c":"markdown","eed4665e":"markdown","e4964ae2":"markdown","0c69a5ef":"markdown","cab4e316":"markdown","cef57850":"markdown","9efb0c72":"markdown","eadc244d":"markdown","7f60832c":"markdown","8ae8dba9":"markdown","c864cdf9":"markdown","043ebfbf":"markdown","48cc4d31":"markdown","2b39feee":"markdown","89a5a6cd":"markdown"},"source":{"c586f2d1":"!git clone https:\/\/github.com\/facebookresearch\/detr.git   #cloning github repo of detr to import its unique loss","689c7f11":"import os\nimport numpy as np \nimport pandas as pd \nfrom datetime import datetime\nimport time\nimport random\nfrom tqdm import tqdm\npd.options.mode.chained_assignment = None\nfrom pandas.io.json import json_normalize\nimport ast \n\n#Torch\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nimport torchvision\n#sklearn\nfrom sklearn.model_selection import StratifiedKFold,train_test_split\n#CV\nimport cv2\n\n################# DETR FUCNTIONS FOR LOSS######################## \nimport sys\nsys.path.append('.\/detr\/')\n\nfrom detr.models.matcher import HungarianMatcher\nfrom detr.models.detr import SetCriterion\n#################################################################\n\n#Albumenatations\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n#Glob\nfrom glob import glob","be084b06":"df_train = pd.read_csv(\"\/kaggle\/input\/tensorflow-great-barrier-reef\/train.csv\")\ndf_train['image_path'] = \"\/kaggle\/input\/tensorflow-great-barrier-reef\/train_images\/video_\" + df_train['video_id'].astype(str) + '\/' + df_train['video_frame'].astype(str) + '.jpg'\ndf_train=df_train.loc[df_train[\"annotations\"].astype(str) != \"[]\"]\ndf_train['annotations'] = df_train['annotations'].apply(ast.literal_eval)\ndf_train.reset_index(inplace=True)\ndf_train.head()","d2bd1135":"df_train[df_train['annotations'].apply(len) >=5 ]\n#df_train.iloc[3902].annotations","567aedb7":"len_df = len(df_train)\nrow_df = pd.DataFrame()\nannot_df = pd.DataFrame()\nfor i in range(len_df):\n    row = df_train.iloc[i]\n    for annot in row['annotations']:\n        row_annot = pd.json_normalize(annot)\n        row_attr = row[['video_id','sequence','video_frame','sequence_frame','image_id','image_path']].to_frame().transpose()\n        row_df = row_df.append(row_attr, ignore_index=True)\n        annot_df = annot_df.append(row_annot, ignore_index=True)","a1cf552c":"df_train = pd.concat([row_df, annot_df], axis=1)","70c47b91":"df_train.sample(5)","ccca50d8":"df_train.drop(df_train[df_train[['x','width']].sum(axis=1) > 1280].index, inplace=True)","58cd6ab5":"df_train.drop(df_train[df_train[['y','height']].sum(axis=1) > 720].index, inplace=True)","5cb2a863":"n_folds = 5\nseed = 42\nnum_classes = 2\nnum_queries = 20\nnull_class_coef = 0.2\nBATCH_SIZE = 8 # 16\nLR = 1e-4   #2e-5\nEPOCHS = 1","67385679":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","161f6810":"seed_everything(seed)","1b2efd34":"skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)","1936ceed":"df_folds = df_train[['image_path']].copy()\ndf_folds.loc[:, 'bbox_count'] = 1\ndf_folds = df_folds.groupby('image_path').count()\ndf_folds.loc[:, 'video_id'] = df_train[['image_path', 'video_id']].groupby('image_path').min()['video_id']\ndf_folds.loc[:, 'stratify_group'] = np.char.add(\n    df_folds['video_id'].values.astype(str),\n    df_folds['bbox_count'].apply(lambda x: f'_{x \/\/ 15}').values.astype(str)\n)\ndf_folds.loc[:, 'fold'] = 0\nfor fold_number, (train_index, val_index) in enumerate(skf.split(X=df_folds.index, y=df_folds['stratify_group'])):\n    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number\ndf_folds.head()","1f62374b":"df_folds[\"bbox_count\"].max()","207ccc4e":"def get_train_transforms():\n    return A.Compose([A.OneOf([\n                    A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, val_shift_limit=0.2, p=0.9),\n                      A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.9)],p=0.9),\n                      \n                      A.ToGray(p=0.01),\n                      \n                      A.HorizontalFlip(p=0.5),\n                      \n                      A.VerticalFlip(p=0.5),\n                      \n                      #A.Resize(height=512, width=512, p=1),\n                      \n                      #A.Cutout(num_holes=8, max_h_size=32, max_w_size=32, fill_value=0, p=0.5),\n                      \n                      ToTensorV2(p=1.0)],\n                      \n                      p=1.0,\n                     \n                      bbox_params=A.BboxParams(format='coco',min_area=0, min_visibility=0,label_fields=['labels']),\n                      )\n                      \n\ndef get_valid_transforms():\n    return A.Compose([ToTensorV2(p=1.0)], \n                      p=1.0, \n                      bbox_params=A.BboxParams(format='coco',min_area=0, min_visibility=0,label_fields=['labels']),\n                      )\n\n#A.Resize(height=512, width=512, p=1.0),","efc9b2d6":"\nclass StarfishDataset(Dataset):\n    def __init__(self,image_ids,dataframe,transforms=None):\n        self.image_ids = image_ids\n        self.df = dataframe\n        self.transforms = transforms\n          \n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n    \n    def __getitem__(self,index):\n        image_id = self.image_ids[index]\n        records = self.df[self.df['image_path'] == image_id]\n        \n        image = cv2.imread(f'{image_id}', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        \n        # DETR takes in data in coco format \n        boxes = records[['x', 'y', 'width', 'height']].values\n        #Area of bb\n        area = boxes[:,2]*boxes[:,3]\n        area = torch.as_tensor(area, dtype=torch.float32)\n        \n        # AS pointed out by PRVI It works better if the main class is labelled as zero\n        labels =  np.zeros(len(boxes), dtype=np.int32)\n        \n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': boxes,\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            boxes = sample['bboxes']\n            labels = sample['labels']\n                    \n        \n                                \n        #Normalizing BBOXES\n        \n        _,h,w = image.shape\n        boxes = A.augmentations.bbox_utils.normalize_bboxes(sample['bboxes'],rows=h,cols=w)\n        target = {}\n        target['boxes'] = torch.as_tensor(boxes,dtype=torch.float32)\n        target['labels'] = torch.as_tensor(labels,dtype=torch.long)\n        target['image_id'] = torch.tensor([index])\n        target['area'] = area\n        \n        return image, target, image_id","609e61ce":"class DETRModel(nn.Module):\n    def __init__(self,num_classes,num_queries):\n        super(DETRModel,self).__init__()\n        self.num_classes = num_classes\n        self.num_queries = num_queries\n        \n        self.model = torch.hub.load('facebookresearch\/detr', 'detr_resnet50', pretrained=True)\n        self.in_features = self.model.class_embed.in_features\n        \n        self.model.class_embed = nn.Linear(in_features=self.in_features,out_features=self.num_classes)\n        self.model.num_queries = self.num_queries\n        \n    def forward(self,images):\n        return self.model(images)","9b7ffa6b":"'''\ncode taken from github repo detr , 'code present in engine.py'\n'''\n\nmatcher = HungarianMatcher()\n\nweight_dict = weight_dict = {'loss_ce': 0.5, 'loss_bbox': 0.5 , 'loss_giou': 0.5}\n\nlosses = ['labels', 'boxes', 'cardinality']","f2b9ba12":"def train_fn(data_loader,model,criterion,optimizer,device,scheduler,epoch):\n    model.train()\n    criterion.train()\n    \n    summary_loss = AverageMeter()\n    \n    tk0 = tqdm(data_loader, total=len(data_loader))\n    \n    for step, (images, targets, image_ids) in enumerate(tk0):\n        \n        \n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n        output = model(images)\n        \n        loss_dict = criterion(output, targets)\n        weight_dict = criterion.weight_dict\n                \n        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n        \n        optimizer.zero_grad()\n\n        losses.backward()\n        optimizer.step()\n        if scheduler is not None:\n            scheduler.step()\n        \n        summary_loss.update(losses.item(),BATCH_SIZE)\n        tk0.set_postfix(loss=summary_loss.avg)\n                \n\n    return summary_loss","3e616ab6":"def eval_fn(data_loader, model,criterion, device):\n    model.eval()\n    criterion.eval()\n    summary_loss = AverageMeter()\n    \n    with torch.no_grad():\n        \n        tk0 = tqdm(data_loader, total=len(data_loader))\n        for step, (images, targets, image_ids) in enumerate(tk0):\n            \n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            output = model(images)\n        \n            loss_dict = criterion(output, targets)\n            weight_dict = criterion.weight_dict\n            \n    \n        \n            losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n            \n            summary_loss.update(losses.item(),BATCH_SIZE)\n            tk0.set_postfix(loss=summary_loss.avg)\n        \n    \n    return summary_loss","fa5485be":"def collate_fn(batch):\n    return tuple(zip(*batch))","36057602":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","13cde445":"def run(fold):\n    \n    df_extrain = df_folds[df_folds['fold'] != fold]\n    df_valid = df_folds[df_folds['fold'] == fold]\n    \n    train_dataset = StarfishDataset(\n    image_ids=df_extrain.index.values,\n    dataframe=df_train,\n    transforms=get_train_transforms()\n    )\n\n    valid_dataset = StarfishDataset(\n    image_ids=df_valid.index.values,\n    dataframe=df_train,\n    transforms=get_valid_transforms()\n    )\n    \n    train_data_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=2, #4\n    collate_fn=collate_fn\n    )\n\n    valid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=2, #4\n    collate_fn=collate_fn\n    )\n    \n    device = torch.device('cuda')\n    model = DETRModel(num_classes=num_classes,num_queries=num_queries)\n    model = model.to(device)\n    criterion = SetCriterion(num_classes-1, matcher, weight_dict, eos_coef = null_class_coef, losses=losses)\n    criterion = criterion.to(device)\n    \n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n    \n    best_loss = 10**5\n    for epoch in range(EPOCHS):\n        train_loss = train_fn(train_data_loader, model,criterion, optimizer,device,scheduler=None,epoch=epoch)\n        valid_loss = eval_fn(valid_data_loader, model,criterion, device)\n                \n        print('|EPOCH {}| TRAIN_LOSS {}| VALID_LOSS {}|'.format(epoch+1,train_loss.avg,valid_loss.avg))\n        \n        if valid_loss.avg < best_loss:\n            best_loss = valid_loss.avg\n            print('Best model found for Fold {} in Epoch {}........Saving Model'.format(fold,epoch+1))\n            torch.save(model.state_dict(), f'detr_best_{fold}.pth')","cb816ff0":"run(fold=0)\n#run(df_to_train)","57d6374f":"def view_sample(df_valid,model,device):\n    '''\n    Code taken from Peter's Kernel \n    https:\/\/www.kaggle.com\/pestipeti\/pytorch-starter-fasterrcnn-train\n    '''\n    valid_dataset = StarfishDataset(image_ids=df_valid.index.values,\n                                 dataframe=df_train,\n                                 transforms=get_valid_transforms()\n                                )\n     \n    valid_data_loader = DataLoader(\n                                    valid_dataset,\n                                    batch_size=BATCH_SIZE,\n                                    shuffle=True,\n                                   num_workers=2,\n                                   collate_fn=collate_fn)\n    \n    images, targets, image_ids = next(iter(valid_data_loader))\n    \n\n    _,h,w = images[0].shape # for de normalizing images\n    \n    images = list(img.to(device) for img in images)\n    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n    \n    boxes = targets[0]['boxes'].cpu().numpy()\n    boxes = [np.array(box).astype(np.int32) for box in A.augmentations.bbox_utils.denormalize_bboxes(boxes,h,w)]\n    sample = images[0].permute(1,2,0).cpu().numpy()\n    \n    model.eval()\n    model.to(device)\n    cpu_device = torch.device(\"cpu\")\n    \n    with torch.no_grad():\n        outputs = model(images)\n        \n    outputs = [{k: v.to(cpu_device) for k, v in outputs.items()}]\n    \n    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\n    for box in boxes:\n        cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2]+box[0], box[3]+box[1]),\n                  (220, 0, 0), 3)\n\n    oboxes = outputs[0]['pred_boxes'][0].detach().cpu().numpy()\n    oboxes = [np.array(box).astype(np.int32) for box in A.augmentations.bbox_utils.denormalize_bboxes(oboxes,h,w)]\n    prob   = outputs[0]['pred_logits'][0].softmax(1).detach().cpu().numpy()[:,0]\n    for box,p in zip(oboxes,prob):\n        if p >0.2:\n            print(p)\n            color = (0,0,220) #if p>0.5 else (0,0,0)\n            cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2]+box[0], box[3]+box[1]),\n                  color, 3)\n    \n    ax.set_axis_off()\n    ax.imshow(sample)","1c7caace":"model = DETRModel(num_classes=num_classes,num_queries=10)\nmodel.load_state_dict(torch.load(\".\/detr_best_0.pth\"))\nview_sample(df_folds[df_folds['fold'] == 0],model=model,device=torch.device('cuda'))","d500146f":"Import libraries\/modules","18803f90":"# - - - - - ","090da225":"## View on sample","5f06005c":"# Model","eed4665e":"## Separate on folds","e4964ae2":"This notebook was adaptaded from [https:\/\/www.kaggle.com\/tanulsingh077\/end-to-end-object-detection-with-transformers-detr].","0c69a5ef":"# Training Function","cab4e316":"# Eval Function","cef57850":"# This is my first attempt on a competition\n## If my understanding is correct we have to train an object detector on underwater images (video) to detect starfish, let's try to use DeTr (Facebook).","9efb0c72":"## To maintain the needed format (based on MR_KNOWNOTHING notebook), we will extract the bounding boxes from each image and separate them.","eadc244d":"## Run on the 0 fold and save the best model.","7f60832c":"## Some images have BB that overpass the image size (720,1280), we will drop them to avoid future problems.","8ae8dba9":"## Define Transformations to perform on the images.","c864cdf9":"Firstly, clone the DeTr Repo.","043ebfbf":"# Engine","48cc4d31":"# Run","2b39feee":"## Load the dataset, the annotations column contain the bounding boxes for each image as a string, we will transform then to literal and only keep the ones with BB.","89a5a6cd":"## Let's define the basic configuration for this model."}}