{"cell_type":{"d54bfa3c":"code","e9202dfd":"code","dace165c":"code","3a4e983e":"code","d65d3cfc":"code","21505f18":"code","b815900f":"code","2e3f2b40":"code","44bf74a7":"code","8f322a83":"code","22b45d48":"code","e60a2165":"code","2441b1e5":"code","00a71647":"code","3b8d8dca":"code","74ddb492":"code","988931f9":"code","04cfd9e2":"code","37a8251e":"code","4f127a07":"code","3125a35f":"code","46b29e9e":"code","a5325f4c":"code","978f8efd":"code","75e9f6cc":"code","76440663":"code","92a88389":"code","7e74c1d3":"code","462d3873":"code","6722cbf3":"code","e30cadf9":"code","a9f610a1":"code","c558640a":"code","b49ba163":"code","3a604568":"code","ace49abc":"code","f1d00688":"code","1b04de9f":"code","8d8c231c":"code","1da6eda2":"code","80edd89d":"code","f2aedfcb":"markdown","f437168f":"markdown","87bb1860":"markdown","a7375b6b":"markdown","e383610f":"markdown","2c60fd23":"markdown","98c0eaba":"markdown","3baa11fa":"markdown","10ded305":"markdown","01638a50":"markdown","3f74e3e2":"markdown","ae725391":"markdown","2aec98ee":"markdown","dabd7c67":"markdown","1d52cb2d":"markdown","48a1072f":"markdown","915e9b24":"markdown","1e0567f0":"markdown","89faaabe":"markdown"},"source":{"d54bfa3c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e9202dfd":"# \u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u30a4\u30f3\u30dd\u30fc\u30c8\nimport numpy as np\nimport pandas as pd\nimport copy\nfrom functools import partial\nfrom pandas import DataFrame\nimport xgboost as xgb\nimport lightgbm as lgb\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\nfrom sklearn.metrics import mean_squared_log_error\n\nimport optuna","dace165c":"# Read in the dataset as a dataframe\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain.shape, test.shape","3a4e983e":"#train.columns","d65d3cfc":"#train['SalePrice']","21505f18":"X_train = np.array(pd.read_csv('..\/input\/houseprice-ppdata\/train_x.csv'))\nX_train = X_train[:, 1:]\ny_train = np.array(pd.read_csv('..\/input\/houseprice-ppdata\/train_y.csv'))\ny_train = y_train[:, 1]\nXX_TEST = np.array(pd.read_csv('..\/input\/houseprice-ppdata\/test_x.csv'))\nXX_TEST = XX_TEST[:, 1:]\n#X_test = np.array(pd.read_csv('..\/input\/houseprice-data\/test_x.csv'))\n#y_test = pd.read_csv('..\/input\/')\n#print(X_train.shape, y_train.shape, XX_TEST.shape)\n#print(X_test.shape)#, test_y.shape\n#print(np.any(np.isnan(XX_TEST) == True))\n\nstd1 = X_train.std(axis=0)\n#print(x_std.shape)\n#print(x_std == 0)\nX_train = X_train[:, std1 != 0]\nXX_TEST = XX_TEST[:, std1 != 0]\n\nmean1 = X_train.mean(axis=0)\nstd1 = X_train.std(axis=0)\n\nprint(X_train.shape, XX_TEST.shape)","b815900f":"print(np.any(X_train.std(axis=0) == 0))\n#X_train = X_train[:, X_train.std(axis=0) != 0]\nprint(X_train.shape)","2e3f2b40":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, train_size= 0.90, random_state=2020)\n\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","44bf74a7":"def objective_xgb(X_train, X_test, y_train, y_test, trial):\n    # \u8abf\u6574\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\n    params = {\n        'objective': 'reg:squarederror',\n        'max_depth': trial.suggest_int('max_depth', 1, 9),\n        'n_estimators': trial.suggest_int('n_estimators', 4000, 7000),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1.0),\n        'gamma': trial.suggest_uniform('gamma', 0.0, 1.0),\n        'min_child_weight': trial.suggest_uniform('min_child_weight', 0.0, 1.0),\n        'subsample': trial.suggest_uniform('subsample', 0.0, 1.0),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.0, 1.0)\n    }\n\n    model = xgb.XGBRegressor(**params)\n    model.fit(X_train, y_train)\n\n    pred = model.predict(X_test)\n\n    mse = mean_squared_error(y_test, pred)\n\n    return mse","8f322a83":"#from sklearn.metrics import mean_squared_error\n#\n#optuna.logging.enable_default_handler()\n#\n#obj_xgb = partial(objective_xgb, X_train, X_test, y_train, y_test)\n#study_xgb = optuna.create_study()\n#study_xgb.optimize(obj_xgb, n_trials=30)\n#\n#print(study_xgb.best_params)\n#print(study_xgb.best_value)\n#print(study_xgb.best_trial)","22b45d48":"mod_xgb_opt = xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.7245610853936947,\n             gamma=0.005388388541926817, gpu_id=-1, importance_type='gain',\n             interaction_constraints='', learning_rate=0.004021246542657597,\n             max_delta_step=0, max_depth=3, min_child_weight=0.5256214626878104,\n             monotone_constraints='()', n_estimators=5880,\n             n_jobs=0, num_parallel_tree=1, random_state=0, reg_alpha=0,\n             reg_lambda=1, scale_pos_weight=1, subsample=0.793174165882878,\n             tree_method='exact', validate_parameters=1, verbosity=None)\nmod_xgb_opt.fit(X_train, y_train)\n\n#base_score=0.5, booster='gbtree', colsample_bylevel=1,\n#colsample_bynode=1, colsample_bytree=0.29722105286028205,\n#gamma=0.08457565501465564, gpu_id=-1, importance_type='gain',\n#interaction_constraints='', learning_rate=0.047402831711455,\n#max_delta_step=0, max_depth=4, min_child_weight=0.8001042591730565,\n#monotone_constraints='()', n_estimators=5892,\n#n_jobs=0, num_parallel_tree=1, random_state=0, reg_alpha=0,\n#reg_lambda=1, scale_pos_weight=1, subsample=0.5482630222376768,\n#tree_method='exact', validate_parameters=1, verbosity=None\n\n#base_score=0.5, booster='gbtree', colsample_bylevel=1,\n#colsample_bynode=1, colsample_bytree=0.5136285905056824,\n#gamma=0.004629477748644764, gpu_id=-1, importance_type='gain',\n#interaction_constraints='', learning_rate=0.004416638307734541,\n#max_delta_step=0, max_depth=6, min_child_weight=0.4425455160536608,\n#monotone_constraints='()', n_estimators=4957,\n#n_jobs=0, num_parallel_tree=1, random_state=0, reg_alpha=0,\n#reg_lambda=1, scale_pos_weight=1, subsample=0.7499784566776143,\n#tree_method='exact', validate_parameters=1, verbosity=None\n\n#base_score=0.5, booster='gbtree', colsample_bylevel=1,\n#colsample_bynode=1, colsample_bytree=0.44524950644666017,\n#gamma=0.0032081606208964435, gpu_id=-1, importance_type='gain',\n#interaction_constraints='', learning_rate=0.005010651862121085,\n#max_delta_step=0, max_depth=4, min_child_weight=0.0765048915097559,\n#monotone_constraints='()', n_estimators=5322,\n#n_jobs=0, num_parallel_tree=1, random_state=0, reg_alpha=0,\n#reg_lambda=1, scale_pos_weight=1, subsample=0.3442011404087598,\n#tree_method='exact', validate_parameters=1, verbosity=None\n\n#max_depth=4, n_estimators=5892, learning_rate=0.047402831711455, gamma=0.08457565501465564, \n#min_child_weight=0.3962344822299318, subsample=0.7758320645906966, colsample_bytree=0.029327940052785217\n\n#base_score=0.5, booster='gbtree', colsample_bylevel=1,\n#colsample_bynode=1, colsample_bytree=0.7245610853936947,\n#gamma=0.005388388541926817, gpu_id=-1, importance_type='gain',\n#interaction_constraints='', learning_rate=0.004021246542657597,\n#max_delta_step=0, max_depth=3, min_child_weight=0.5256214626878104,\n#monotone_constraints='()', n_estimators=5880,\n#n_jobs=0, num_parallel_tree=1, random_state=0, reg_alpha=0,\n#reg_lambda=1, scale_pos_weight=1, subsample=0.793174165882878,\n#tree_method='exact', validate_parameters=1, verbosity=None","e60a2165":"from sklearn.metrics import mean_squared_log_error\n\n# XGB(Optuna)\ny_train_pred_xgb_opt = mod_xgb_opt.predict(X_train)\ny_test_pred_xgb_opt = mod_xgb_opt.predict(X_test)\n# RMSLE\nfrom sklearn.metrics import mean_squared_error\nprint('RMSLE train : %.6f, test : %.6f' % (np.sqrt(mean_squared_log_error(np.exp(y_train)-1, np.exp(y_train_pred_xgb_opt)-1)), np.sqrt(mean_squared_log_error(np.exp(y_test)-1, np.exp(y_test_pred_xgb_opt)-1))) )\n# R^2\n#from sklearn.metrics import r2_score\n#print('R^2 train : %.3f, test : %.3f' % (r2_score(y_train, y_train_pred_xgb_opt), r2_score(y_test, y_test_pred_xgb_opt)) )","2441b1e5":"def objective_lgbm(X_train, X_test, y_train, y_test, trial):\n    # \u8abf\u6574\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\n    params = {\n        'objective':'regression',\n        'metric':'mse',\n        'num_leaves': trial.suggest_int('num_leaves', 2, 64),\n        'max_bin': trial.suggest_int('max_bin', 128, 512),\n        'n_estimators': trial.suggest_int('n_estimators', 4000, 7000),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.0, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 0, 10),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.0, 1.0),\n        'min_sum_hessian_in_leaf': trial.suggest_loguniform('min_sum_hessian_in_leaf', 1e-3, 10.0)\n    }\n\n    model = lgb.LGBMRegressor(**params)\n    model.fit(X_train, y_train)\n\n    pred = model.predict(X_test)\n\n    mse = mean_squared_error(y_test, pred)\n\n    return mse","00a71647":"#obj_lgbm = partial(objective_lgbm, X_train, X_test, y_train, y_test)\n#study_lgbm = optuna.create_study()\n#study_lgbm.optimize(obj_lgbm, n_trials=30)\n#\n#print(study_lgbm.best_params)\n#print(study_lgbm.best_value)\n#print(study_lgbm.best_trial)","3b8d8dca":"mod_lgb_opt = lgb.LGBMRegressor(bagging_fraction=0.9724444503168079, bagging_freq=2,\n              feature_fraction=0.572706177440646,\n              learning_rate=0.007117847154213432, max_bin=199,\n              min_sum_hessian_in_leaf=4.837478916656753, n_estimators=6157,\n              num_leaves=5)\nmod_lgb_opt.fit(X_train, y_train)\n\n#bagging_fraction=0.23186809353398913, bagging_freq=0,\n#feature_fraction=0.3882521385240972,\n#learning_rate=0.015608129872614208, max_bin=229,\n#min_sum_hessian_in_leaf=0.09577191082816064, n_estimators=6533,\n#num_leaves=33\n\n#bagging_fraction=0.9345030598158235, bagging_freq=0,\n#feature_fraction=0.2515772305023268,\n#learning_rate=0.03291987593733417, max_bin=511,\n#min_sum_hessian_in_leaf=0.013431283148747171, n_estimators=5821,\n#num_leaves=37\n\n#bagging_fraction=0.8820056676099275, bagging_freq=2,\n#feature_fraction=0.22378538461962236,\n#learning_rate=0.0025191289752261523, max_bin=335,\n#min_sum_hessian_in_leaf=9.152313758972483, n_estimators=4293,\n#num_leaves=57\n\n#num_leaves=51, max_bin=361, n_estimators=5834, learning_rate=0.0010198989526885994, \n#bagging_fraction=0.2915421547464846, bagging_freq=8, feature_fraction=0.2876060362044632, \n#min_sum_hessian_in_leaf=0.031550101194560747\n\n#bagging_fraction=0.9724444503168079, bagging_freq=2,\n#feature_fraction=0.572706177440646,\n#learning_rate=0.007117847154213432, max_bin=199,\n#min_sum_hessian_in_leaf=4.837478916656753, n_estimators=6157,\n#num_leaves=5","74ddb492":"# LGB(Optuna)\ny_train_pred_lgbm_opt = mod_lgb_opt.predict(X_train)\ny_test_pred_lgbm_opt = mod_lgb_opt.predict(X_test)\n# RMSLE\nfrom sklearn.metrics import mean_squared_error\nprint('RMSLE train : %.6f, test : %.6f' % (np.sqrt(mean_squared_log_error(np.exp(y_train)-1, np.exp(y_train_pred_lgbm_opt)-1)), np.sqrt(mean_squared_log_error(np.exp(y_test)-1, np.exp(y_test_pred_lgbm_opt)-1))) )\n# R^2\n#from sklearn.metrics import r2_score\n#print('R^2 train : %.3f, test : %.6f' % (r2_score(y_train, y_train_pred_lgbm_opt), r2_score(y_test, y_test_pred_lgbm_opt)) )","988931f9":"# NN\u524d\u51e6\u7406\n# \uff08\u5165\u529b\uff09\u30c7\u30fc\u30bf\u3092\u6a19\u6e96\u5316(\u5e73\u57470\u3001\u5206\u65631\u306b\u5909\u63db)\u3059\u308b\nX_train_nn = copy.deepcopy(X_train)\nX_test_nn = copy.deepcopy(X_test)\ny_train_nn = copy.deepcopy(y_train)\ny_test_nn = copy.deepcopy(y_test)\n\n#mean1 = X_train_nn.mean(axis=0)\n#std1 = X_train_nn.std(axis=0)\nX_train_nn = (X_train_nn - mean1) \/ std1\nX_test_nn = (X_test_nn - mean1) \/ std1\n\nmean2 = y_train_nn.mean(axis=0)\nstd2 = y_train_nn.std(axis=0)\ny_train_nn = (y_train_nn - mean2) \/ std2\ny_test_nn = (y_test_nn - mean2) \/ std2\n\nprint(np.any(std1 == 0))\nprint(np.any(std2 == 0))\n#print(std1.shape)","04cfd9e2":"###########################################\n#np.nan_to_num(X_train_nn, copy=False)\n#np.nan_to_num(X_test_nn, copy=False)\n###########################################\n\nprint('X_train_nn:', X_train_nn.shape ,np.any(np.isnan(X_train_nn) == True))\nprint('X_test_nn: ', X_test_nn.shape ,np.any(np.isnan(X_test_nn) == True))\nprint('y_train_nn:', y_train_nn.shape ,np.any(np.isnan(y_train_nn) == True))\nprint('y_test_nn: ', y_test_nn.shape ,np.any(np.isnan(y_test_nn) == True))","37a8251e":"np.random.seed(seed=0)\n\nos.environ['PYTHONHASHSEED'] = '0'\n# random.seed(0)\n\n#tf.config.intra_op_parallelism_threads=1\n#tf.config.inter_op_parallelism_threads=1\n\ntf.random.set_seed(0)","4f127a07":"# NN\nmodel_nn = Sequential()\n\nmodel_nn.add(Dense(256, activation='relu', input_shape=(X_train_nn.shape[1],)))\nmodel_nn.add(Dense(512))\nmodel_nn.add(Activation('relu'))\n#model_nn.add(Dropout(0.1))\nmodel_nn.add(Dense(512, activation='relu'))\nmodel_nn.add(Dense(512))\nmodel_nn.add(Activation('relu'))\nmodel_nn.add(Dense(512, activation='relu'))\nmodel_nn.add(Dense(1))\noptimizer = Adam(lr=1e-3)\nmodel_nn.compile(loss=\"mean_squared_error\", optimizer=optimizer)\nmodel_nn.summary()","3125a35f":"model_nn.fit(X_train_nn, y_train_nn, epochs=100, batch_size=16)","46b29e9e":"# NN\ny_train_pred_nn = np.squeeze(model_nn.predict(X_train_nn) * std2 + mean2)\ny_test_pred_nn = np.squeeze(model_nn.predict(X_test_nn) * std2 + mean2)\n\n###########################################\n#not_nan = ~np.isnan(y_test_pred_nn)\n###########################################\n\n#print(np.any(np.isnan(X_test_nn) == True))\nprint(np.any(np.isnan(y_test_pred_nn) == True))\n#print(not_nan)\n# RMSLE\nfrom sklearn.metrics import mean_squared_error\nprint('RMSLE train : %.6f, test : %.6f' % (np.sqrt(mean_squared_log_error(np.exp(y_train)-1, np.exp(y_train_pred_nn)-1)), np.sqrt(mean_squared_log_error(np.exp(y_test)-1, np.exp(y_test_pred_nn)-1))) )\n#RMSLE train : 0.044242, test : 0.137520\n#RMSLE train : 0.032471, test : 0.136924\n# R^2\n#from sklearn.metrics import r2_score\n#print('R^2 train : %.3f, test : %.3f' % (r2_score(y_train, y_train_pred_nn), r2_score(y_test, y_test_pred_nn)) )","a5325f4c":"from sklearn.linear_model import Lasso\n\nregLasso = Lasso(alpha=0.0007)\nregLasso.fit(X_train, y_train)","978f8efd":"# LGB(Optuna)\ny_train_pred_lasso = regLasso.predict(X_train)\ny_test_pred_lasso = regLasso.predict(X_test)\n# RMSLE\nfrom sklearn.metrics import mean_squared_error\nprint('RMSLE train : %.6f, test : %.6f' % (np.sqrt(mean_squared_log_error(np.exp(y_train)-1, np.exp(y_train_pred_lasso)-1)), np.sqrt(mean_squared_log_error(np.exp(y_test)-1, np.exp(y_test_pred_lasso)-1))) )","75e9f6cc":"# \u6bd4\u7387\u3092\u56fa\u5b9a\ndef blended_predictions(X, X_nn):\n    return (2.27101366e-02 * mod_xgb_opt.predict(X)) + (5.76237370e-01 * mod_lgb_opt.predict(X)) \\\n            + (1.90522937e-19 * np.squeeze(model_nn.predict(X_nn) * std2 + mean2)) + (4.02316541e-01 * regLasso.predict(X))\n\ndef blended_predictions2(X, X_nn):\n    return (0.678351173 * mod_lgb_opt.predict(X)) + (0.322875938 * regLasso.predict(X))\n\n# \u6bd4\u7387\u8abf\u6574\u7528\ndef blended_predictions_tune111(X, X_nn, r1, r2, r3):#, r4):\n    return (r1 * mod_xgb_opt.predict(X)) + (r2 * mod_lgb_opt.predict(X)) + (r3 * np.squeeze(model_nn.predict(X_nn) * std2 + mean2))# + (r4 * regLasso.predict(X))","76440663":"#print(X_test_nn[0:1,:].shape)\n#print(np.any(X_test_nn[55:63,:] > 50))\n#print(X_test_nn[63:64,272:])\n#print(np.squeeze(model_nn.predict(X_test_nn[63:64,:]) * std2 + mean2))","92a88389":"#from scipy.optimize import minimize\n#\n## \u76ee\u7684\u95a2\u6570\n#def blended_predictions_tune(r):\n#    pred = (r[0] * mod_xgb_opt.predict(X_test)) + (r[1] * mod_lgb_opt.predict(X_test)) \\\n#            + (r[2] * np.squeeze(model_nn.predict(X_test_nn) * std2 + mean2)) + (r[3] * regLasso.predict(X_test))\n#    return mean_squared_error(y_test, pred)\n#\n## \u5236\u7d04\u6761\u4ef6\u5f0f\u304c\u975e\u8ca0\u3068\u306a\u308b\u3088\u3046\u306b\u3059\u308b\n#cons = (\n#    {'type': 'ineq', 'fun': lambda x: -(x[0]+x[1]+x[2]+x[3]-1.2)},\n#    \n#    {'type': 'ineq', 'fun': lambda x: x[0]},\n#    {'type': 'ineq', 'fun': lambda x: -(x[0] - 1)},\n#    \n#    {'type': 'ineq', 'fun': lambda x: x[1]},\n#    {'type': 'ineq', 'fun': lambda x: -(x[1] - 1)},\n#    \n#    {'type': 'ineq', 'fun': lambda x: x[2]},\n#    {'type': 'ineq', 'fun': lambda x: -(x[2] - 1)},\n#    \n#    {'type': 'ineq', 'fun': lambda x: x[3]},\n#    {'type': 'ineq', 'fun': lambda x: -(x[3] - 1)}\n#)\n#ratio = [0.1,0.1,0.1,0.1] # \u521d\u671f\u5024\n#\n#result = minimize(blended_predictions_tune, x0=ratio, constraints=cons, method=\"SLSQP\")\n#print(result)","7e74c1d3":"#from tqdm import tqdm\n#\n## \u30d9\u30b9\u30c8\u306a\u30d6\u30ec\u30f3\u30c9\u6bd4\u7387\u3092\u6c7a\u5b9a\n#best_mse = 100\n#best_R2  = 0\n#for ratio1 in tqdm(np.round(np.arange(0.01, 1, 0.01), 2)):\n#    for ratio2 in np.round(np.arange(0.01, 1, 0.01), 2):\n#        for ratio3 in np.round(np.arange(0.01, 1, 0.01), 2):\n#            for ratio4 in np.round(np.arange(0.01, 1, 0.01), 2):\n#                if ratio1 + ratio2 + ratio3 + ratio4 == 1:\n#                    #y_train_pred_blended = blended_predictions_tune111(X_train, X_train_nn, ratio1, ratio2, ratio3)\n#                    y_test_pred_blended = blended_predictions_tune111(X_test, X_test_nn, ratio1, ratio2, ratio3, ratio4)\n#                    # MSE\n#                    #from sklearn.metrics import mean_squared_error\n#                    #print('MSE train : %.3f, test : %.3f' % (mean_squared_error(y_train, y_train_pred_blended), mean_squared_error(y_test, y_test_pred_blended)) )\n#                    #print('y_test:', y_test.shape ,np.any(np.isnan(y_test) == True))\n#                    #print('y_train_pred_blended: ', y_train_pred_blended.shape ,np.any(np.isnan(y_train_pred_blended) == True))\n#                    #print('y_test_pred_blended: ', y_test_pred_blended.shape ,np.any(np.isnan(y_test_pred_blended) == True))\n#                    #print(y_test_pred_blended)\n#                    mse_ = mean_squared_error(y_test, y_test_pred_blended)\n#                    if best_mse > mse_:\n#                        best_mse = mse_\n#                        best_mse_ratio1 = ratio1\n#                        best_mse_ratio2 = ratio2\n#                        best_mse_ratio3 = ratio3\n#                        best_mse_ratio4 = ratio4\n#                        #print(ratio1, ratio2, ratio3)\n#                    # R^2\n#                    #from sklearn.metrics import r2_score\n#                    #print('R^2 train : %.3f, test : %.3f' % (r2_score(y_train, y_train_pred_blended), r2_score(y_test, y_test_pred_blended)) )\n#                    #R2_ = r2_score(y_test, y_test_pred_blended)\n#                    #if best_R2 < R2_:\n#                    #    best_R2 = R2_\n#                    #    best_R2_ratio1 = ratio1\n#                    #    best_R2_ratio2 = ratio2\n#                    #    best_R2_ratio3 = ratio3\n#\n#print(\"MSLE\")\n#print(best_mse)\n#print(best_mse_ratio1 , \" : \" , best_mse_ratio2 , \" : \" , best_mse_ratio3)\n#print(\"---------\")\n#print(\"R2\")\n#print(best_R2)\n#print(best_R2_ratio1 , \" : \" , best_R2_ratio2 , \" : \" , best_R2_ratio3)","462d3873":"#y_train_pred_blended = blended_predictions_tune111(X_train, X_train_nn, best_mse_ratio1, best_mse_ratio2, best_mse_ratio3, best_mse_ratio4)\n#y_test_pred_blended = blended_predictions_tune111(X_test, X_test_nn, best_mse_ratio1, best_mse_ratio2, best_mse_ratio3, best_mse_ratio4)\ny_train_pred_blended = blended_predictions(X_train, X_train_nn)\ny_test_pred_blended = blended_predictions(X_test, X_test_nn)\n# RMSLE\nfrom sklearn.metrics import mean_squared_error\nprint('RMSLE train : %.6f, test : %.6f' % (np.sqrt(mean_squared_log_error(np.exp(y_train)-1, np.exp(y_train_pred_blended)-1)), np.sqrt(mean_squared_log_error(np.exp(y_test)-1, np.exp(y_test_pred_blended)-1))) )\n# R^2\n#from sklearn.metrics import r2_score\n#print('R^2 train : %.6f, test : %.6f' % (r2_score(y_train, y_train_pred_blended), r2_score(y_test, y_test_pred_blended)) )","6722cbf3":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.figure(figsize = (10, 7))\n#plt.scatter(, , c = 'black', marker = 'o', s = 15, alpha = 0.5, label = 'Training data')\nplt.scatter(y_test_pred_blended, y_test, c = 'blue', marker = 's', s = 15, alpha = 0.7, label = 'Test data')\nplt.xlabel('Predicted log values')\nplt.ylabel('Real log values')\nyx = np.arange(11, 13, 0.01)\nplt.plot(yx, yx)\n#plt.legend(loc = 'upper left')\n#plt.hlines(y = 0, xmin = -10, xmax = 50, lw = 2, color = 'red')\n#plt.xlim([-10, 50])\nplt.show()","e30cadf9":"# NN\u524d\u51e6\u7406\n# \uff08\u5165\u529b\uff09\u30c7\u30fc\u30bf\u3092\u6a19\u6e96\u5316(\u5e73\u57470\u3001\u5206\u65631\u306b\u5909\u63db)\u3059\u308b\nXX_TEST_nn = copy.deepcopy(XX_TEST)\nXX_TEST_nn = (XX_TEST_nn - mean1) \/ std1","a9f610a1":"#print(std1)\n#print(np.any(XX_TEST[:, 57] == 1))\nprint(np.any(np.isnan(XX_TEST_nn) == True))\n#print(XX_TEST_nn[0])","c558640a":"# \u4e88\u6e2c\nTEST_pred = blended_predictions(XX_TEST, XX_TEST_nn)\nprint(TEST_pred)","b49ba163":"TEST_pred_raw = np.exp(TEST_pred)-1\nprint(TEST_pred_raw)","3a604568":"# \u4e88\u6e2c\nTEST_pred_2 = blended_predictions2(XX_TEST, XX_TEST_nn)\nprint(TEST_pred_2)","ace49abc":"TEST_pred_raw2 = np.exp(TEST_pred_2)-1\nprint(TEST_pred_raw2)","f1d00688":"submission= pd.DataFrame()\nsubmission['id'] = test['Id']\nsubmission['SalePrice'] = TEST_pred_raw\n\nsubmission2= pd.DataFrame()\nsubmission2['id'] = test['Id']\nsubmission2['SalePrice'] = TEST_pred_raw2","1b04de9f":"submission.head()","8d8c231c":"submission.to_csv('submission.csv', index=False)","1da6eda2":"submission2.head()","80edd89d":"submission2.to_csv('submission2.csv', index=False)","f2aedfcb":"### -\u7d50\u679c\u3092\u8868\u793a","f437168f":"## \u4e88\u6e2c\u7d50\u679c\u3092\u30d6\u30ec\u30f3\u30c9","87bb1860":"### -\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u8abf\u6574","a7375b6b":"### -\u7d50\u679c\u3092\u8868\u793a","e383610f":"### -\u7d50\u679c\u3092\u8868\u793a","2c60fd23":"## XGB","98c0eaba":"### -\u6700\u9069\u306a\u6bd4\u7387\u3092\u63a2\u7d22","3baa11fa":"## LGBM","10ded305":"### -\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u8abf\u6574","01638a50":"## NN","3f74e3e2":"### -\u30e2\u30c7\u30eb","ae725391":"### -\u6700\u9069\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u5b66\u7fd2","2aec98ee":"### -\u6700\u7d42\u7d50\u679c","dabd7c67":"### -\u5b66\u7fd2","1d52cb2d":"## \u63d0\u51fa\u7528","48a1072f":"### -\u524d\u51e6\u7406\uff08\u6a19\u6e96\u5316\uff09","915e9b24":"### -\u6700\u9069\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u5b66\u7fd2","1e0567f0":"## \u8aa4\u5dee\u306e\u53ef\u8996\u5316","89faaabe":"## Lasso"}}