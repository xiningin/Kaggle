{"cell_type":{"1522fe13":"code","51aad28f":"code","6fed0106":"code","6717f711":"code","00dbe214":"code","ea68a2fe":"code","1fbf2ec4":"code","e08f409a":"code","b07dad6f":"code","8f47edcf":"code","80794478":"code","b06277bd":"code","21a83fcf":"code","7bb2f96a":"code","33073489":"code","2ef39b29":"code","13f269ca":"code","02562952":"code","b3734739":"code","378b65f1":"code","11c73016":"code","ef6f3f3c":"code","aa509072":"code","7d7a2f04":"code","1333dac7":"code","36f73934":"code","d602b090":"code","7bd5bf7d":"code","24a29d38":"code","6f9b4144":"code","e284539a":"code","3e826f5d":"code","dd6044ab":"code","292bc659":"code","5afa26d0":"code","62ca4091":"code","8605901d":"code","94746646":"code","90f51c1a":"code","a458c1d0":"code","9045066e":"markdown","8be29185":"markdown","cc60fef0":"markdown","4037e9d2":"markdown","f4a9e0fc":"markdown","7e11e92a":"markdown","546f17e6":"markdown","63e30f05":"markdown","bb1a87f9":"markdown","c71d1869":"markdown","09743a89":"markdown","84d4c132":"markdown","e856ec62":"markdown","a91f80eb":"markdown","f34704b7":"markdown","414707c2":"markdown","b55638ad":"markdown","75dd0fcd":"markdown","a65cb820":"markdown","7e64529a":"markdown"},"source":{"1522fe13":"import pandas as pd\nimport numpy as np\n\nimport dask\nimport dask.dataframe as dd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom itertools import chain","51aad28f":"path_append = '\/kaggle\/input\/riiid-test-answer-prediction\/'\ntrain_data = pd.read_csv(path_append + 'train.csv', nrows=1000)","6fed0106":"train_data.info()","6717f711":"train_data.head()","00dbe214":"# Load data using dask\n\ntrain_data_dd = dd.read_csv(path_append + \"train.csv\", low_memory=False) # Lazy evaluation - doesn't actually load until .compute() is called","ea68a2fe":"# Calling dask.compute() with multiple objects allows for shared computation steps, e.g., file loading, and reduces overall runtime\nnum_content_ids, num_task_ids, num_content_types = dask.compute(train_data_dd['content_id'].nunique(),\\\n                                                               train_data_dd['task_container_id'].nunique(),\\\n                                                               train_data_dd['content_type_id'].nunique())\nprint(\"Unique content IDs: {}\".format(num_content_ids))\nprint(\"Unique task container IDs: {}\".format(num_task_ids))\nprint(\"Number of content types: {}\".format(num_content_types))","1fbf2ec4":"# How many users in total?\n\nprint(\"Total number of users: {}\".format(train_data_dd['user_id'].nunique().compute()))","e08f409a":"# What proportion of all answers are correct?\n\nprint(\"Overall answer correctness rate: {:.2f}%\".format(train_data_dd[train_data_dd['content_type_id']==0]['answered_correctly'].mean().compute() * 100))","b07dad6f":"# Aggregating at the content (question) level\ncontent_df = (train_data_dd.query(\"content_type_id==0\")\n              .groupby('content_id')\n              .agg({'user_id': 'count',\n                    'answered_correctly': 'mean'})\n              .compute())","8f47edcf":"questions_df = pd.read_csv(path_append + 'questions.csv')\nquestions_df.info()","80794478":"questions_df.head()","b06277bd":"# How many unique 'parts'?\nprint(\"# unique parts: {}\".format(questions_df['part'].nunique()))","21a83fcf":"# Merge the questions dataframe with the aggregated content dataframe \ncontent_df = (content_df.reset_index().rename(columns={\"content_id\": \"question_id\"})\n              .merge(questions_df, how='left', on=['question_id']))\ncontent_df['num_answered_correctly'] = (content_df['user_id'] * content_df['answered_correctly']).astype(int)","7bb2f96a":"content_df.head()","33073489":"plt.plot(content_df['user_id'].sort_values(ascending=False).values)\nplt.yscale(\"log\")\nplt.title(\"Questions vs number of attempts\")\nplt.xlabel(\"Questions\")\nplt.ylabel(\"Number of attempts\")\nplt.show()","2ef39b29":"content_df[content_df['user_id']>100]['answered_correctly'].hist(bins=100)\nplt.title(\"Distribution of answer correctness rate by questions\")\nplt.show()","13f269ca":"train_data_dd['prior_question_elapsed_time'].compute().hist(bins=100)\nplt.title(\"Distribution of time elapsed on previous question\")\nplt.show()","02562952":"prior_question_qcut = train_data_dd['prior_question_elapsed_time'].map_partitions(pd.qcut, 10, labels=False,\\\n                                                                                   meta=train_data_dd['prior_question_elapsed_time'])\npqet_df = (train_data_dd.groupby(prior_question_qcut).agg({'answered_correctly': 'mean'}).compute()\n           .reset_index().rename(columns={'prior_question_elapsed_time': 'prior_question_elapsed_time_decile'}))\nsns.barplot(x='prior_question_elapsed_time_decile', y='answered_correctly', data=pqet_df, hue=None)\nplt.show()","b3734739":"train_data_dd.groupby('prior_question_had_explanation').agg({'user_id': 'count', 'answered_correctly': 'mean'}).compute()","378b65f1":"part_agg = content_df.groupby('part', as_index=False).agg({'user_id': 'sum', 'num_answered_correctly': 'sum'})\npart_agg['prop_correct'] = part_agg['num_answered_correctly'] \/ part_agg['user_id']\npart_agg","11c73016":"sns.barplot(x='part', y='prop_correct', data=part_agg, hue=None)\nplt.show()","ef6f3f3c":"# Convert the 'tags' string to a list\ncontent_df['tags'].fillna('', inplace=True)\ncontent_df['tags_list'] = content_df['tags'].apply(lambda x: [int(t) for t in x.split()])","aa509072":"tags_df = content_df.apply(lambda x: [(t, x['user_id'], x['num_answered_correctly']) for t in x['tags_list']],axis=1).values\ntags_df = chain.from_iterable(tags_df)\ntags_df = pd.DataFrame(tags_df, columns=['tag', 'num_questions', 'num_answered_correctly'])\ntags_df = tags_df.groupby('tag', as_index=False).sum()\ntags_df['prop_correct'] = tags_df['num_answered_correctly'] \/ tags_df['num_questions']\ntags_df","7d7a2f04":"tags_df['prop_correct'].hist(bins=20)\nplt.title(\"Distribution of correctness rate by tag\")\nplt.show()","1333dac7":"user_df = (train_data_dd.query(\"content_type_id==0\")\n           .groupby('user_id')\n           .agg({'user_answer': 'count', 'answered_correctly': 'mean', 'timestamp': 'max'})\n           .rename(columns={'user_answer': 'num_questions_answered', 'timestamp': 'total_time_spent'})).compute()","36f73934":"user_df['total_time_spent_mins'] = user_df['total_time_spent'] \/ 60000.0","d602b090":"user_df.head()","7bd5bf7d":"user_df[user_df['num_questions_answered'] < 2000]['num_questions_answered'].hist(bins=100)\nplt.title(\"Distribution of # questions answered by users\")\nplt.show()","24a29d38":"user_df['total_time_spent_mins'].hist(bins=100)\nplt.title(\"Distribution of # questions answered by users\")\nplt.show()","6f9b4144":"# Closer look at users with millions of minutes (long-time users)\n\noutlier_user_ids = train_data_dd[train_data_dd['timestamp'] > 1e6 * 60000][['user_id']].drop_duplicates()\noutlier_user_df = outlier_user_ids.merge(train_data_dd, how='inner', on='user_id').compute()","e284539a":"outlier_user_df","3e826f5d":"sample_user = outlier_user_df[outlier_user_df['user_id']==np.random.choice(outlier_user_df['user_id'].unique())].copy()","dd6044ab":"sample_user['timestamp_mins'] = sample_user['timestamp'] \/ 60000\nsample_user['timestamp_hrs'] = sample_user['timestamp_mins'] \/ 60","292bc659":"plt.plot(sample_user['timestamp_hrs'].values)\nplt.show()","5afa26d0":"sample_user.head(50)","62ca4091":"user_df[user_df['num_questions_answered']>10]['answered_correctly'].hist(bins=50)\nplt.title(\"Distribution of answer correctness by users\")\nplt.show()","8605901d":"lectures_df = pd.read_csv(path_append + 'lectures.csv')\nlectures_df.info()","94746646":"lectures_df.head()","90f51c1a":"lectures_df['type_of'].value_counts()","a458c1d0":"lectures_df['part'].value_counts()","9045066e":"### Distribution of user ability - are some users correct more often than others?","8be29185":"## Analysis of users","cc60fef0":"## Analysis of questions","4037e9d2":"### Distribution of number of questions answered by each user","f4a9e0fc":"## Data loading\nLoad a sample of the data using pandas","7e11e92a":"# That's all, folks!","546f17e6":"### Distribution of total time spent","63e30f05":"Calling `dask.compute()` with multiple objects allows for shared computation steps, e.g., file loading, and reduces overall runtime","bb1a87f9":"To do: does viewing a lecture affect performance on the next set of questions? (feeling too lazy to do it now.....)","c71d1869":"# Exploratory Data Analysis using Dask\n\nHi Kagglers, this is the first notebook I'm sharing on Kaggle. I've used dask dataframes to keep the memory usage low. This notebook is just for EDA. Let me know if it helped you in any way or if you have suggestions for better code, presentation, etc. Thanks!","09743a89":"### Are all questions attempted equally often?","84d4c132":"## Analysis of lectures","e856ec62":"### How well does time elapsed on previous question predict answer correctness?","a91f80eb":"### Does the section of the test (column: 'part') predict answer correctness?","f34704b7":"### Are some questions harder than others?","414707c2":"## Data loading using dask","b55638ad":"### Analysis of tags","75dd0fcd":"### Distribution of time elapsed on previous question","a65cb820":"### Load questions data","7e64529a":"### Prior question had explanation - does this predict answer correctness?"}}