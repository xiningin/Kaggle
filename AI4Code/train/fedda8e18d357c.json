{"cell_type":{"16e93c7c":"code","2f440c73":"code","c2756047":"code","87b7e90a":"code","9ddff37c":"code","414e5c2e":"code","470e3207":"code","eca3dd70":"code","9cc44e56":"code","675790c0":"code","6b3dadf0":"code","ada3c342":"code","bb72efd2":"code","9dc22fa8":"code","6cdf3992":"code","fa955d53":"code","7cadc140":"code","3d909ae0":"code","79ef80bb":"code","c3b3cab0":"code","2b82a9af":"code","b4f5e97f":"code","cb9f1d0e":"code","ea5b50fe":"code","ebb71211":"code","5c5e0016":"code","c74ed18c":"code","a602e200":"code","77ced47c":"code","e0eafff8":"code","bda448d4":"code","e3929588":"code","977b1b6f":"code","ff7bd1e8":"code","d173659f":"code","5ec70b19":"code","9ddc7533":"code","3663719b":"code","7d00ff1a":"markdown","4459fcaf":"markdown","7f28b400":"markdown","9ced6b72":"markdown","17e1190d":"markdown"},"source":{"16e93c7c":"# import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures, RobustScaler, LabelEncoder\nfrom sklearn.model_selection import GridSearchCV, KFold, cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.decomposition import TruncatedSVD, PCA\nfrom sklearn.metrics import r2_score\nimport xgboost as xgb\n%matplotlib inline","2f440c73":"df_train = pd.read_csv('..\/input\/train.csv',index_col = 'Id')\ndf_test = pd.read_csv('..\/input\/test.csv',index_col = 'Id')\nconserve_test_ids = df_train.shape[0]\ndf = pd.concat((df_train, df_test))","c2756047":"# Show dataframe attributes\ndf.shape","87b7e90a":"df.info()","9ddff37c":"# Examination of numerical variables (correlation to target)\nnum_features = [feature for feature in df.columns if df[feature].dtype!= 'object']\n\ncols = 4\nrows = 10\n\nfig, ax = plt.subplots(rows,cols,figsize=(rows*3,cols*10))\ni=0\n\nfor row in range(rows):\n    for col in range(cols):\n        if i<len(num_features):\n            sns.regplot(x=num_features[i],y='SalePrice',data=df,order=1,ax=ax[row][col])\n            i+=1\n            \nplt.tight_layout()\nplt.show()","414e5c2e":"# Examination of non-numerical features (correlation matrix)\n\nsns.heatmap(df.corr())","470e3207":"# Examination of numerical features (distributions)\ncols = 4\nrows = 10\n\nfig, ax = plt.subplots(rows,cols,figsize=(rows*3,cols*10))\ni=0\n\nfor row in range(rows):\n    for col in range(cols):\n        if i<len(num_features):\n            sns.distplot(df[num_features[i]].dropna(),ax = ax[row][col]).set_title('p-value: {}'.format(stats.shapiro(df[num_features[i]].dropna())[1]))\n            i+=1\n            \nplt.tight_layout()\nplt.show()","eca3dd70":"# Examination of non-numerical features\nnon_numerical_features = [feature for feature in df.columns if feature not in num_features]\n\ncols = 3\nrows = 15\n\nfig,ax = plt.subplots(rows,cols,figsize=(cols*5,rows*3))\ni = 0\n\nfor row in range(rows):\n    for col in range(cols):\n        if i<len(non_numerical_features):\n            cat_means = df.groupby(non_numerical_features[i])[[non_numerical_features[i],'SalePrice']].mean()\n            target_cats = cat_means.index.tolist()\n            target_means = list(cat_means.iloc[:,0])\n            ordered_list = sorted(zip(target_means,target_cats),key=lambda x: x[0])\n            order = np.array([y for x,y in ordered_list])\n            sns.boxplot(x=non_numerical_features[i],y='SalePrice',data=df,order=order,ax=ax[row][col])\n            i+=1\n            \nplt.tight_layout()\n\nplt.show()","9cc44e56":"# Examination of missing data\nmissing_report = pd.DataFrame(df.isnull().sum().sort_values(ascending=False),columns=['missing_values'])\nmissing_report['missing_values_rel'] = missing_report['missing_values']\/df.shape[0]\n        \nfor index,value in missing_report.iterrows():\n    if value[0]==0:\n        missing_report.drop(index,inplace=True)\n        \nmissing_report","675790c0":"## Imputation of missing values\n# PoolQC: NA means no pool, therefore None is imputed as its own category.\ndf['PoolQC'] = df['PoolQC'].fillna('None')\n\n# MiscFeature: NA means, that there is no specific additional feature. Therefore None is imputed as its own category.\ndf['MiscFeature'] = df['MiscFeature'].fillna('None')\n\n# Alley: NA means no alley access\ndf['Alley'] = df['Alley'].fillna('None')\n\n# Fence: NA means no fence\ndf['Fence'] = df['Fence'].fillna('None')\n\n# FireplaceQu: NA means no fireplace\ndf['FireplaceQu'] = df['FireplaceQu'].fillna('None')\n\n# GarageType: NA means no garage (garage features are treated accordingly)\ndf['GarageType'] = df['GarageType'].fillna('None')\ndf['GarageCond'] = df['GarageCond'].fillna('None')\ndf['GarageFinish'] = df['GarageFinish'].fillna('None')\ndf['GarageQual'] = df['GarageQual'].fillna('None')\ndf['GarageYrBlt'] = df['GarageYrBlt'].fillna(0)\n\n# BsmtFinType2: NA means no basement (related features are treated accordingly)\ndf['BsmtFinType2'] = df['BsmtFinType2'].fillna('None')\ndf['BsmtExposure'] = df['BsmtExposure'].fillna('None')\ndf['BsmtQual'] = df['BsmtQual'].fillna('None')\ndf['BsmtCond'] = df['BsmtCond'].fillna('None')\ndf['BsmtFinType1'] = df['BsmtFinType1'].fillna('None')\n\n# MasVnrArea: Object can have no veneer area\ndf['MasVnrArea'] = df['MasVnrArea'].fillna(0)\n\n# MasVnrType: See above\ndf['MasVnrType'] = df['MasVnrType'].fillna('None')\n\n# Electrical: As the most frequent value by far is SBrkr, and only one single value is affected, SBkrkr can be imputed.\ndf['Electrical'] = df['Electrical'].fillna('SBrkr')\n\n# MSZoning: Assign Mode of data set to missing values\ndf['MSZoning'] = df['MSZoning'].fillna(df['MSZoning'].mode()[0])\nassert df['MSZoning'].isnull().sum()==0\n\n# Utilities: The overwhelming majority of train observations are 'AllPub', therefore, that value is imputed\ndf['Utilities'] = df['Utilities'].fillna('AllPub')\n\n# Functional: Similar to utilities but less pronounced \u2192 under the assumption, that there is no systematic reason for a certain value to\n# be missing, the mode is the best option again\ndf['Functional'] = df['Functional'].fillna(df['Functional'].mode()[0])\n\n# Exterior2nd: See 'Functional'\ndf['Exterior2nd'] = df['Exterior2nd'].fillna(df['Exterior2nd'].mode()[0])\n\n# SaleType: See 'Functional'\ndf['SaleType'] = df['SaleType'].fillna(df['SaleType'].mode()[0])\n\n# Exterior1st: See 'Functional'\ndf['Exterior1st'] = df['Exterior1st'].fillna(df['Exterior1st'].mode()[0])\n\n# KitchenQual: See 'Functional'\ndf['KitchenQual'] = df['KitchenQual'].fillna(df['KitchenQual'].mode()[0])\n\n# GarageCars: Similar to other Garage related features\ndf['GarageCars'] = df['GarageCars'].fillna(0)\n\n# GarageArea: Similar to other Garage related features\ndf['GarageArea'] = df['GarageArea'].fillna(0)\n\n# Bsmt Features: Similar consideration as in Training set (np.nan = no Basement so np.nan \u2192 None)\ndf['TotalBsmtSF'] = df['TotalBsmtSF'].fillna(0)\ndf['BsmtFinSF1'] = df['BsmtFinSF1'].fillna(0)\ndf['BsmtFinSF2'] = df['BsmtFinSF2'].fillna(0)\ndf['BsmtUnfSF'] = df['BsmtUnfSF'].fillna(0)\ndf['BsmtHalfBath'] = df['BsmtHalfBath'].fillna(0)\ndf['BsmtFullBath'] = df['BsmtFullBath'].fillna(0)\n\n# LotFrontage: Missing values must be imputed; as there is no other obvious way, the value can be imputed by linear regression\ntsvd = TruncatedSVD(n_components=10)\nimputation_pipeline = Pipeline([('encoder',OneHotEncoder(handle_unknown='ignore')),\n                               ('tsvd',tsvd),\n                               ('linreg',LinearRegression())])\n\nX_imp_train = df.loc[df['LotFrontage'].notnull()].drop(['SalePrice','LotFrontage'],axis=1)\ny_imp_train = df.loc[:,'LotFrontage'].dropna()\nX_imp = df.loc[df['LotFrontage'].isnull()].drop(['SalePrice','LotFrontage'],axis=1)\n\nimputation_pipeline.fit(X_imp_train,y_imp_train)\nimp_values = imputation_pipeline.predict(X_imp)\n\nindex_list = df.loc[df['LotFrontage'].isnull()].index.tolist()\nimp_Series = pd.Series(imp_values,index=index_list)\ndf['LotFrontage'] = df['LotFrontage'].fillna(imp_Series)\n\nassert df.drop('SalePrice',axis=1).isnull().sum().sum() == 0","6b3dadf0":"#Add information: Houses with same YearRemodAdd and YearBuilt have no Remod Added\n\ndf.loc[df['YearRemodAdd']==df['YearBuilt'],'YearRemodAdd'] = 'None'","ada3c342":"# Converting numerical features to objects: Nominal features\nto_be_converted = ['MoSold','YrSold','YearBuilt','YearRemodAdd','GarageYrBlt','MSSubClass']\n\nfor feature in to_be_converted:\n    df[feature].astype(object)","bb72efd2":"# Convert numerical features to objects\/categories: Ordinal Features\nto_be_converted_ordinal = ['MSZoning','Alley','MasVnrType','ExterQual','FireplaceQu','BsmtQual','BsmtCond','HeatingQC','KitchenQual',\n                           'GarageQual','GarageCond','PoolQC','Utilities','BldgType','OverallQual','OverallCond','ExterCond','BsmtExposure',\n                           'BsmtFinType1','BsmtFinType2','Functional','GarageFinish','PavedDrive','Fence']\n\nfor feature in to_be_converted_ordinal:\n    le = LabelEncoder()\n    encoded_feature = le.fit_transform(df[feature])\n    df[feature] = encoded_feature","9dc22fa8":"# Shift numerical features' distribution towards normal distribution by running a boxcox transformation over all remaining numerical features\n# For easier interpretability, use simple log transformation for target variable \n\nnum_features = [feature for feature in df.columns if df[feature].dtype != 'object']\nfor feature in num_features:\n    if feature != 'SalePrice':\n        transform_data = df[feature].loc[df[feature]>0]\n        lm = 0.15\n        transform_result = stats.boxcox(transform_data,lm)\n        transform_impose = np.empty_like(df[feature])\n        transform_impose[df[feature] > 0] = transform_result\n        transform_impose[df[feature]<= 0] = df[feature].loc[df[feature]<=0]\n        df[feature] = transform_impose\n    else:\n        df[feature] = df[feature].transform(lambda x: np.log(x))","6cdf3992":"# Get dummies from dataset\ndf_processed = pd.get_dummies(df)","fa955d53":"# Data Wrangling is concluded\ntrain_processed = df_processed.loc[df_processed['SalePrice'].notnull()]\ntest_processed = df_processed.loc[df_processed['SalePrice'].isnull()]\nassert train_processed.isnull().sum().sum() == 0\nassert test_processed.isnull().sum().sum() == test_processed.shape[0]\nassert train_processed.shape[1] == test_processed.shape[1]\nassert test_processed.shape[0] == df_test.shape[0]\ntrain_processed.to_csv('train_processed.csv',index_label='Id')\ntest_processed.drop('SalePrice',axis=1).to_csv('test_processed.csv',index_label='Id')","7cadc140":"# Implementing RMSLE function\ndef rmsle(labels,predictions,exponential_transformation=True):\n    if exponential_transformation:\n        labels = np.exp(labels)\n        predictions = np.exp(predictions)\n    root_mean_squared_error = np.mean((np.log(labels)-np.log(predictions))**2)**0.5\n\n    return root_mean_squared_error","3d909ae0":"# Load processed training data\ntrain_data = pd.read_csv('train_processed.csv',index_col='Id')","79ef80bb":"# Prepare the data\nX = train_data.drop('SalePrice',axis=1)\ny_train = np.array(train_data.loc[:,'SalePrice'])","c3b3cab0":"# Data preparation\npreparation_pipeline = make_pipeline(StandardScaler())\nprint(X.shape)\nX_train = preparation_pipeline.fit_transform(X)","2b82a9af":"# Prepare KFold validation function\ndef rmsle_cv(model):\n    kf = KFold(3,shuffle=True,random_state=42).get_n_splits(X_train)\n    rmse = np.sqrt(-cross_val_score(model,X_train,y_train,scoring='neg_mean_squared_error',cv=kf))\n    return rmse","b4f5e97f":"# Define model test function\n# TODO: Implement GridSearchCV or RandomizedGridSearchCV\ndef model_testing(model):\n    rmsle_train_score = rmsle_cv(model)\n    model.fit(X_train,y_train)\n    \n    return rmsle_train_score","cb9f1d0e":"# Train Base Models\nlin_reg = LinearRegression()\nrand_for = RandomForestRegressor()\nsup_vec_reg = SVR()\nada_boost = AdaBoostRegressor()\ngrad_boost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                       max_depth=4, max_features='sqrt',\n                                       min_samples_leaf=15, min_samples_split=10, \n                                       loss='huber', random_state =5)\nxgboost_reg = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                               learning_rate=0.05, max_depth=3, \n                               min_child_weight=1.7817, n_estimators=2200,\n                               reg_alpha=0.4640, reg_lambda=0.8571,\n                               subsample=0.5213, silent=1,\n                               random_state =7, nthread = -1)\n\nmodels= [lin_reg,rand_for,sup_vec_reg,ada_boost,grad_boost,xgboost_reg]\nnames = ['LR','RF','SVR','ABR','GBR','XGB-Test']\n\ntrain_results = list(map(model_testing,models))","ea5b50fe":"# Training Results\nrmsle_train_scores = [train_results[i] for i in range(len(train_results))]","ebb71211":"# Plotting Training Results\ntrain_results_df = pd.DataFrame(rmsle_train_scores,columns=['CV'+str(i+1) for i in range(rmsle_train_scores[0].shape[0])])\ntrain_results_df['Model'] = names\ntrain_results_df1 = train_results_df.melt(id_vars='Model',var_name='Validation',value_name='RMSLE-Score')\nsns.barplot(x='Model',y='RMSLE-Score',data=train_results_df1,hue='Validation').set_title('Training RMSLEs')\nplt.ylim(0,0.5)\n\nplt.show()\nprint(train_results_df)","5c5e0016":"# Train a UL model on the data (for testing purposes of the meta model)\n# Model: Gaussian Mixture Model, testing for optimal k\nfrom sklearn.mixture import GaussianMixture","c74ed18c":"# Define Testing function\ndef gmm_test(data,max_components):\n    total_score = []\n    test_components = []\n    for i in range(max_components):\n        model = GaussianMixture(n_components=i+1)\n        labels = model.fit_predict(data)\n        score = model.aic(data)\n        total_score.append(score)\n        test_components.append(i+1)\n    sns.lineplot(x=test_components,y=total_score)\n    plt.xticks(list(range(1,31)))\n    plt.show()","a602e200":"# Testing for the optimal model\ngmm_test(X_train,30)","77ced47c":"# It appears, that the optimal model, according to the AIC, is one with n_components = 22\ngm_model = GaussianMixture(n_components = 22)\nlabel_predictions = gm_model.fit_predict(X_train)\nif gm_model not in models:\n    models.append(gm_model)","e0eafff8":"# Combine predictions and labels\nlin_reg_pred = lin_reg.predict(X_train)\nrand_for_pred = rand_for.predict(X_train)\nsup_vec_reg_pred = sup_vec_reg.predict(X_train)\nada_boost_pred = ada_boost.predict(X_train)\ngrad_boost_pred = grad_boost.predict(X_train)\nxgboost_reg_pred = xgboost_reg.predict(X_train)\n\nmeta_data_train = np.column_stack((lin_reg_pred,rand_for_pred,sup_vec_reg_pred,ada_boost_pred,grad_boost_pred,xgboost_reg_pred,label_predictions))","bda448d4":"# Train Meta model (keras DLN)\nfrom keras.models import Sequential\nfrom keras.layers import Dropout, Dense, LeakyReLU\n\nmeta_model_dnn = Sequential()\nmeta_model_dnn.add(Dense(256,kernel_initializer='normal',input_dim=meta_data_train.shape[1]))\nmeta_model_dnn.add(Dropout(rate=0.2))\nmeta_model_dnn.add(LeakyReLU())\n\nfor i in range(100):\n    meta_model_dnn.add(Dense(128,kernel_initializer='normal'))\n    meta_model_dnn.add(Dropout(rate=0.2))\n    meta_model_dnn.add(LeakyReLU())\n\nmeta_model_dnn.add(Dense(1,kernel_initializer='normal',activation='linear'))\n\nmeta_model_dnn.compile(optimizer='SGD',loss='mean_squared_error',metrics=['mean_squared_error'])\n\nmeta_model_dnn.summary()","e3929588":"# Train Meta Model\nmeta_model_dnn.fit(meta_data_train,y_train,epochs=500,batch_size=32,shuffle=True,validation_split=0.1,verbose=1)","977b1b6f":"# Evaluate Meta Model\nyhat = meta_model_dnn.predict(meta_data_train)\nmeta_training_score = rmsle(y_train,yhat,exponential_transformation=True)\nprint('Meta-Model Training Score: {}'.format(meta_training_score))","ff7bd1e8":"# As the simple Keras model did not perform as hoped, let's use a simple XGBRegressor model\nxgb_meta = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                            learning_rate=0.05, max_depth=3, \n                            min_child_weight=1.7817, n_estimators=2200,\n                            reg_alpha=0.4640, reg_lambda=0.8571,\n                            subsample=0.5213, silent=1,\n                            random_state =7, nthread = -1)\n\nxgb_meta.fit(meta_data_train,y_train)\ntest_prediction = xgb_meta.predict(meta_data_train)\n\nprint('XGBR Meta Model Score: {}'.format(rmsle(y_train,test_prediction,exponential_transformation=True)))","d173659f":"# Although the final Meta model returned the best accuracy, let's try and submit the results from all models!\nsubmission_df = pd.read_csv('test_processed.csv',index_col='Id')\nsubmission_index = submission_df.index.to_list()","5ec70b19":"# Prepare test_data\nsubmission_pipeline = make_pipeline(StandardScaler())\nX_test = submission_pipeline.fit_transform(submission_df)","9ddc7533":"# Create Predictions for submission\ndef create_submissions(models,meta_model,test_data):\n    meta_data = np.empty((test_data.shape[0],len(models)))\n    result_dict = {}\n    for i,model in enumerate(models):\n        yhat = np.exp(model.predict(test_data))\n        yhat_meta = model.predict(test_data)\n        meta_data[:,i] = yhat_meta\n        result_dict[type(model).__name__] = yhat\n    else:\n        meta_predictions = np.exp(meta_model.predict(meta_data))\n        result_dict['MetaModel'] = meta_predictions\n    \n    for key,value in result_dict.items():\n        if key != 'GaussianMixture':\n            export = pd.DataFrame(value,columns=['SalePrice'])\n            export.index = submission_index\n            export.index.name='Id'            \n            export.to_csv(key+'.csv')","3663719b":"# Export Predictions\nassert X_train.shape[1] == X_test.shape[1] and X_test.shape[0] == df_test.shape[0]\n\nmeta_data = create_submissions(models,xgb_meta,X_test)","7d00ff1a":"The previous evaluation has shown that:\n- There are some numerical features, that should be encoded as non-numerical features.\n- Most features are far from normally distributed.Those distributions will have to be shifted.\n- Missing values have to be imputed.\n- There is some multicollinearity in amongst the features in the dataset.\n\n### Feature Engineering","4459fcaf":"## Modelling","7f28b400":"### Basic Exploration and EDA","9ced6b72":"### Submission","17e1190d":"# Kaggle Competition: Advanced Regression Techniques\nhttps:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\n\nThis is my first attempt at a Kaggle Kernel. I am basically a beginner and this Kernel is surely nothing special, but I am still happy to have completed it. I hope you will find it interesting and if you have any question, suggestions or criticisms, I would be happy to hear it down in the comments! \n\n## Outline\n1. Basic Exploration and EDA\n2. Feature Engineering\n5. Modelling\n6. Submission"}}