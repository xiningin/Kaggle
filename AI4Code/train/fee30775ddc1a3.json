{"cell_type":{"f0853229":"code","2fe4de74":"code","dca35a7e":"code","5c367615":"code","75b57b62":"code","6924f227":"code","dc304980":"code","d385e053":"code","a58f606f":"code","ccafafd5":"code","ceaa78f7":"code","46518ff0":"code","b7fc82e6":"code","0db11f76":"code","d2fa42f6":"code","aced9d10":"code","90bb668c":"code","8acde3e8":"code","f69fe4ba":"code","1833a2c7":"code","665a7758":"code","9d2156df":"code","e18cef4c":"code","76e4b25c":"code","e41b1fb5":"code","59941fbb":"code","9f2e3684":"code","f139c03a":"code","4354787c":"code","e43e8c89":"code","080da142":"code","52d300c7":"code","dc94fc4d":"code","249165e6":"code","7d785770":"code","613e5073":"code","08a40d83":"code","c83e0858":"code","74fd6cbc":"code","02cb2d45":"code","787b6615":"code","664b2ddb":"code","c9edf4cd":"code","fa38093f":"markdown","9fa12dec":"markdown","1056faa0":"markdown","d5569525":"markdown","bb1e28bf":"markdown","5472fe18":"markdown","b7c178cb":"markdown"},"source":{"f0853229":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2fe4de74":"df3 = pd.read_csv('..\/input\/eol-trait-bank\/trait_bank\/pages.csv', encoding='ISO-8859-2')\npd.set_option('display.max_columns', None)\ndf3.head()","dca35a7e":"nRowsRead = 1000 # specify 'None' if want to read whole file\ndf1 = pd.read_csv('..\/input\/eol-trait-bank\/trait_bank\/inferred.csv', delimiter=',', encoding = \"utf8\", nrows = nRowsRead)\ndf1.dataframeName = 'inferred.csv'\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')\ndf1.head()","5c367615":"nRowsRead = 1000 # specify 'None' if want to read whole file\ndf = pd.read_csv('..\/input\/eol-trait-bank\/trait_bank\/metadata.csv', delimiter=',', encoding = \"ISO-8859-2\", nrows = nRowsRead)\ndf.dataframeName = 'metadata.csv'\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')\ndf.head()","75b57b62":"nRowsRead = 1000 # specify 'None' if want to read whole file\ndf2 = pd.read_csv('..\/input\/eol-trait-bank\/trait_bank\/traits.csv', delimiter=',', encoding = \"ISO-8859-2\", nrows = nRowsRead)\ndf2.dataframeName = 'traits.csv'\nnRow, nCol = df2.shape\nprint(f'There are {nRow} rows and {nCol} columns')\ndf2.head()","6924f227":"nRowsRead = 1000 # specify 'None' if want to read whole file\ndf4 = pd.read_csv('..\/input\/eol-trait-bank\/trait_bank\/terms.csv', delimiter=',', encoding = \"ISO-8859-2\", nrows = nRowsRead)\ndf4.dataframeName = 'terms.csv'\nnRow, nCol = df4.shape\nprint(f'There are {nRow} rows and {nCol} columns')\ndf4.head()","dc304980":"df.isnull().sum()","d385e053":"!pip install selenium","a58f606f":"!pip install beautifulsoup4","ccafafd5":"!pip install scrapy","ceaa78f7":"!pip install praw","46518ff0":"!pip install newspaper3k","b7fc82e6":"# importing basic libraries\nimport datetime as dt\n\n\n# importing libraries for scraping\n#from twitterscraper import query_tweets\nfrom urllib.request import urlopen\nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup\nfrom newspaper import Article\nimport scrapy\nimport praw","0db11f76":"#Third column, predicate, 2nd Row\n\ndf.iloc[1,2]","d2fa42f6":"#Seventh column, literal, 2nd Row\n\ndf.iloc[1,6]","aced9d10":"#A new article from TOI \nurl = \"http:\/\/purl.org\/dc\/terms\/bibliographicCitation\"","90bb668c":"#For different language newspaper refer above table \ntoi_article = Article(url, language=\"en\") # en for English ","8acde3e8":"import nltk\nnltk.download('punkt')","f69fe4ba":"#To download the article \ntoi_article.download() \n  \n#To parse the article \ntoi_article.parse() \n  \n#To perform natural language processing ie..nlp \ntoi_article.nlp()","1833a2c7":"#To extract title \nprint(\"Article's Title:\") \nprint(toi_article.title) \nprint(\"n\") \n  \n#To extract text \nprint(\"Article's Text:\") \nprint(toi_article.text) \nprint(\"n\") \n  \n#To extract summary \nprint(\"Article's Summary:\") \nprint(toi_article.summary) \nprint(\"n\") \n  \n#To extract keywords \nprint(\"Article's Keywords:\") \nprint(toi_article.keywords)","665a7758":"html = \"\"\"<i>Vampyrodes caraccioli<\/i>\"\"\"","9d2156df":"soup = BeautifulSoup(html, 'html.parser')","e18cef4c":"type(soup)","76e4b25c":"soup","e41b1fb5":"print(soup.prettify())","59941fbb":"# getting url of the scrapping dataset\nurl = 'http:\/\/purl.org\/dc\/terms\/bibliographicCitation'","9f2e3684":"df5 = pd.read_html(url,header=0)","f139c03a":"df5","4354787c":"len(df5)","e43e8c89":"# selecting that one table\ndf2020_metadata_terms = df5[0]","080da142":"# load the dataset\ndf2020_metadata_terms","52d300c7":"df2020_metadata_terms.info()","dc94fc4d":"#We don't have * to be removed. Save that snippet for another time.\n\n# remove '*' from HS column\n#df2020_metadata_terms['Title:'] = df2020_metadata_terms['Title:'].map(lambda x : x.rstrip('*'))","249165e6":"#In Avg column their are '-' in place of NaN value.So we will replace '-' to NaN and then will convert the dtype of the column.\n\n#df2020_metadata_terms['Title:'].replace({'-':'NaN'},inplace=True)\n\n#We don't have '-' Skip that snippet.","7d785770":"# changing dtype\n#convert_dict = {'Title:': int , 'DCMI Metadata Terms' : float }\n\n#df2020_metadata_terms = df2020_metadata_terms.astype(convert_dict)","613e5073":"# specify the url\n#url2 = 'http:\/\/eol.org\/schema\/terms\/extant'\n\n# read html page\n#df_2 = pd.read_html(url2,header=0)\n#df_2","08a40d83":"# specify the url\nurl2 = 'http:\/\/rs.tdwg.org\/dwc\/terms\/locality'\n\n# read html page\ndf_2 = pd.read_html(url2,header=0)\ndf_2","c83e0858":"len(df_2)","74fd6cbc":"# we have 338 table (you can see it by clicking on the url)\n# selecting that 3rd table\ndf2020_term = df_2[2]\n\n# load the dataset\ndf2020_term","02cb2d45":"# some basic info\ndf2020_term.info()","787b6615":"#df2020_term.drop(['Term Name dwc:acceptedNameUsageID.1'],axis=1,inplace=True)\n#Save for next time","664b2ddb":"# after dropping the column\ndf2020_term.head()","c9edf4cd":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Thank you Diksha Bhati for the script, @dikshabhati2002' )","fa38093f":"![](http:\/\/www.mammalogy.org\/uploads\/imagecache\/library_image\/library\/362.jpg)mammalogy.org","9fa12dec":"#Let's quickly check how our dataset looks like with a function .info() which tells us null values , dtype and shape of the dataset.","1056faa0":"#HTTP Error 404: Not Found","d5569525":"#Code by Diksha Bhati https:\/\/www.kaggle.com\/dikshabhati2002\/web-scrapping-few-lines-of-code","bb1e28bf":"![](https:\/\/miro.medium.com\/max\/1024\/1*K9i1YpWCBNlDEJVpuU31Jg.png)medium.com","5472fe18":"#So we have 12 tables in our dataset also on http:\/\/purl.org\/dc\/terms\/bibliographicCitation .So now we are going to select that 1st table and load it.","b7c178cb":"#Another URL\/Dataset"}}