{"cell_type":{"1bf2110f":"code","9fe1581f":"code","bdd21d7c":"code","a97e682c":"code","b0572926":"code","8a142e0f":"code","447dc09a":"code","cdb74705":"code","70084af4":"code","7fe2a615":"code","37965c0b":"code","610aca4d":"code","0ce3a717":"code","83138baf":"code","da2f10c1":"code","19660b64":"code","621d5f97":"code","7da19c6b":"code","67403b1e":"code","c7dd42d1":"code","57fbc854":"code","796ed4da":"code","5326d98f":"code","fc880189":"code","7a023829":"code","9b54e4af":"code","d930008e":"code","850f874c":"code","179794c7":"code","fdafdf2a":"code","af5efcb4":"code","6e47d505":"code","980efdc8":"markdown","ea77edc5":"markdown","b505447b":"markdown","25bf83cc":"markdown","ba933af0":"markdown","a8a35f80":"markdown","0b9a0992":"markdown","d833ce7b":"markdown","4319d221":"markdown","d599daae":"markdown","1941f881":"markdown","2e6b3d24":"markdown","4f635b89":"markdown"},"source":{"1bf2110f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px","9fe1581f":"train = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/train.csv\")\n\n","bdd21d7c":"train.head()","a97e682c":"print(f'Number of rows: {train.shape[0]};  Number of columns: {train.shape[1]}; No of missing values: {sum(train.isna().sum())}')","b0572926":"train.info()","8a142e0f":"train.describe().T","447dc09a":"fig, ax = plt.subplots(1, 1, figsize=(17, 8))\n\ntarget_count = train['Cover_Type'].value_counts().sort_index()\n\nax.bar(target_count.index, target_count, color=['#1520E6' if i%2==0 else '#93D1FF' for i in range(9)],\n       width=0.55, \n       edgecolor='black', \n       linewidth=0.7)\n\nax.margins(0.02, 0.05)\n\nfor i in range(1,8):\n    ax.annotate(f'{target_count[i]\/len(train)*100:.3}', xy=(i, target_count[i]+1000),\n                   va='center', ha='center',\n               )\n#Annotate the point xy with text text.\n\n#In the simplest form, the text is placed at xy.\n\nax.set_title('Cover_Type Distribution', weight='bold', fontsize=15)\nax.grid(axis='y', linestyle='-', alpha=0.4)\n\nfig.tight_layout()\nplt.show()","cdb74705":"target_count = train['Cover_Type'].value_counts().sort_index()\ntarget_count_df = pd.DataFrame(target_count)\n#pd.options.display.float_format = '{:,.2f}%'.format\ntarget_count_df['Cover_Type(%)'] = (target_count_df\/target_count.sum()*100)\ntarget_count_df.sort_values('Cover_Type(%)', ascending=False, inplace=True)\ndisplay(target_count_df)","70084af4":"train = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/train.csv\",nrows=400000)\n","7fe2a615":"train.drop([\"Id\"] , axis = 1 , inplace = True)","37965c0b":"y=train['Cover_Type']\nX=train.drop(labels=['Cover_Type'], axis=1)","610aca4d":"### It will zero variance features\nfrom sklearn.feature_selection import VarianceThreshold\nvar_thres=VarianceThreshold(threshold=0)\nvar_thres.fit(X)","0ce3a717":"var_thres.get_support()","83138baf":"### Finding non constant features\nsum(var_thres.get_support())","da2f10c1":"# Lets Find non-constant features \nlen(X.columns[var_thres.get_support()])","19660b64":"constant_columns = [column for column in X.columns\n                    if column not in X.columns[var_thres.get_support()]]\n\nprint(len(constant_columns))","621d5f97":"for column in constant_columns:\n    print(column)","7da19c6b":"from sklearn.datasets import load_boston","67403b1e":"X.corr()","c7dd42d1":"import seaborn as sns\n#Using Pearson Correlation\nplt.figure(figsize=(12,10))\ncor = X.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.CMRmap_r)\nplt.show()","57fbc854":"# with the following function we can select highly correlated features\n# it will remove the first feature that is correlated with anything other feature\n\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","796ed4da":"corr_features = correlation(X, 0.7)\nlen(set(corr_features))","5326d98f":"corr_features","fc880189":"train = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/train.csv\",nrows=400000)\n\ntrain.drop([\"Id\"] , axis = 1 , inplace = True)\ny=train['Cover_Type']\nX=train.drop(labels=['Cover_Type'], axis=1)\n","7a023829":"from sklearn.feature_selection import mutual_info_classif\n# determine the mutual information\nmutual_info = mutual_info_classif(X, y)\nmutual_info","9b54e4af":"mutual_info = pd.Series(mutual_info)\nmutual_info.index = X.columns\nmutual_info.sort_values(ascending=False)","d930008e":"#let's plot the ordered mutual_info values per feature\nmutual_info.sort_values(ascending=False).plot.bar(figsize=(20, 8))","850f874c":"from sklearn.feature_selection import SelectKBest","179794c7":"#No we Will select the  top 5 important features\nsel_five_cols = SelectKBest(mutual_info_classif, k=5)\nsel_five_cols.fit(X, y)\n","fdafdf2a":"sel_five_cols.get_support()","af5efcb4":"# Lets Find non-constant features \nlen(X.columns[sel_five_cols.get_support()])","6e47d505":"Five_columns = [column for column in X.columns\n                    if column  in X.columns[sel_five_cols.get_support()]]\n\nfor column in Five_columns:\n    print(column)","980efdc8":"Below is the first 5 rows of test dataset:","ea77edc5":"The dimension and number of missing values in the test dataset is as below:","b505447b":"## 3.Feature Selection-Information gain - mutual information In Classification Problem Statements","25bf83cc":"**Load and check data**","ba933af0":"train.drop(corr_features,axis=1)","a8a35f80":"The dimension and number of missing values in the train dataset is as below:\n","0b9a0992":"# Infos","d833ce7b":"### 1 Feature Selection- Dropping constant features\nIn this step we will be removing the features which have constant features which are actually not important\nfor solving the problem statement","4319d221":"train.drop(corr_features,axis=1)","d599daae":"Mutual Information\nMI Estimate mutual information for a discrete target variable.\n\nMutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n\nThe function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances.\n\nInshort\n\nA quantity called mutual information measures the amount of information one can obtain from one random variable given another.\n\nThe mutual information between two random variables X and Y can be stated formally as follows:\n\nI(X ; Y) = H(X) \u2013 H(X | Y) Where I(X ; Y) is the mutual information for X and Y, H(X) is the entropy for X and H(X | Y) is the conditional entropy for X given Y. The result has the units of bits.","1941f881":"# Summarie and statistics","2e6b3d24":"### 2. Feature Selection- With Correlation\nIn this step we will be removing the features which are highly correlated ","4f635b89":"# Load in our libraries"}}