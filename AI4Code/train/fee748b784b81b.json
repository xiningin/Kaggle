{"cell_type":{"961d8d6f":"code","9ee0d4aa":"code","f4097479":"code","49f54445":"code","65c08296":"code","6b467c88":"code","11043ccf":"code","99a28b70":"code","2feaad4a":"code","fa14acf7":"code","53cb24b5":"code","aa1449af":"code","6964dbe9":"code","0a11bdfb":"code","de6585af":"code","3f5559c9":"code","90a3072c":"code","3135bc16":"code","0d56f429":"code","36c79f48":"code","396e6381":"code","509c3b5e":"code","8297e27d":"code","12357768":"markdown","7822bff7":"markdown","cc3fbb01":"markdown","86d3117c":"markdown","8841d017":"markdown","02c1d65a":"markdown","b8039d1c":"markdown","52ce8312":"markdown","d2633145":"markdown"},"source":{"961d8d6f":"import numpy as np\nimport pandas as pd\nfrom keras.applications.mobilenet import MobileNet, preprocess_input\nfrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dropout, Dense,BatchNormalization, Flatten, MaxPool2D\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback\nfrom keras.layers import Conv2D, Reshape\nfrom keras.utils import Sequence\nfrom keras.backend import epsilon\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.python.keras.preprocessing.image import ImageDataGenerator\nimport cv2\n\n\nfrom tqdm.notebook import tqdm_notebook as tqdm\n\nimport os","9ee0d4aa":"print(os.listdir(\"..\/input\"))","f4097479":"real = \"..\/input\/real-and-fake-face-detection\/real_and_fake_face\/training_real\/\"\nfake = \"..\/input\/real-and-fake-face-detection\/real_and_fake_face\/training_fake\/\"\n\nreal_path = os.listdir(real)\nfake_path = os.listdir(fake)","49f54445":"def load_img(path):\n    image = cv2.imread(path)\n    image = cv2.resize(image,(224, 224))\n    return image[...,::-1]","65c08296":"fig = plt.figure(figsize=(10, 10))\n\nfor i in range(16):\n    plt.subplot(4, 4, i+1)\n    plt.imshow(load_img(real + real_path[i]), cmap='gray')\n    plt.suptitle(\"Real faces\",fontsize=20)\n    plt.axis('off')\n\nplt.show()","6b467c88":"fig = plt.figure(figsize=(10,10))\n\nfor i in range(16):\n    plt.subplot(4, 4, i+1)\n    plt.imshow(load_img(fake + fake_path[i]), cmap='gray')\n    plt.suptitle(\"Fakes faces\",fontsize=20)\n    plt.title(fake_path[i][:4])\n    plt.axis('off')\n\nplt.show()","11043ccf":"dataset_path = \"\/kaggle\/input\/real-and-fake-face-detection\/real_and_fake_face\"","99a28b70":"data_with_aug = ImageDataGenerator(horizontal_flip=True,\n                                   vertical_flip=False,\n                                   rescale=1.\/255,\n                                  validation_split=0.2)","2feaad4a":"train = data_with_aug.flow_from_directory(dataset_path,\n                                          class_mode=\"binary\",\n                                          target_size=(96, 96),\n                                          batch_size=32,\n                                          subset=\"training\")","fa14acf7":"val = data_with_aug.flow_from_directory(dataset_path,\n                                          class_mode=\"binary\",\n                                          target_size=(96, 96),\n                                          batch_size=32,\n                                          subset=\"validation\"\n                                          )","53cb24b5":"mnet = MobileNetV2(include_top = False, weights = \"imagenet\" ,input_shape=(96,96,3))","aa1449af":"tf.keras.backend.clear_session()\n\nmodel = Sequential([mnet,\n                    GlobalAveragePooling2D(),\n                    Dense(512, activation = \"relu\"),\n                    BatchNormalization(),\n                    Dropout(0.3),\n                    Dense(128, activation = \"relu\"),\n                    Dropout(0.1),\n                    # Dense(32, activation = \"relu\"),\n                    # Dropout(0.3),\n                    Dense(2, activation = \"softmax\")])\n\nmodel.layers[0].trainable = False\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=\"accuracy\")\n\nmodel.summary()","6964dbe9":"def scheduler(epoch):\n    if epoch <= 2:\n        return 0.001\n    elif epoch > 2 and epoch <= 15:\n        return 0.0001 \n    else:\n        return 0.00001\n\nlr_callbacks = tf.keras.callbacks.LearningRateScheduler(scheduler)","0a11bdfb":"hist = model.fit_generator(train,\n                    epochs=20,\n                    callbacks=[lr_callbacks],\n                    validation_data=val)","de6585af":"epochs = 20\ntrain_loss = hist.history['loss']\nval_loss = hist.history['val_loss']\ntrain_acc = hist.history['accuracy']\nval_acc = hist.history['val_accuracy']\nxc = range(epochs)\n\nplt.figure(1,figsize=(7,5))\nplt.plot(xc,train_loss)\nplt.plot(xc,val_loss)\nplt.xlabel('num of Epochs')\nplt.ylabel('loss')\nplt.title('train_loss vs val_loss')\nplt.grid(True)\nplt.legend(['train','val'])\n#print plt.style.available # use bmh, classic,ggplot for big pictures\nplt.style.use(['classic'])\n\nplt.figure(2,figsize=(7,5))\nplt.plot(xc,train_acc)\nplt.plot(xc,val_acc)\nplt.xlabel('num of Epochs')\nplt.ylabel('accuracy')\nplt.title('train_acc vs val_acc')\nplt.grid(True)\nplt.legend(['train','val'],loc=4)\n#print plt.style.available # use bmh, classic,ggplot for big pictures\nplt.style.use(['classic'])","3f5559c9":"train = data_with_aug.flow_from_directory(dataset_path,\n                                          class_mode=\"binary\",\n                                          target_size=(224, 224),\n                                          batch_size=98,\n                                          subset=\"training\")\n\nval = data_with_aug.flow_from_directory(dataset_path,\n                                          class_mode=\"binary\",\n                                          target_size=(224, 224),\n                                          batch_size=98,\n                                          subset=\"validation\"\n                                          )","90a3072c":"vgg16_model = tf.keras.applications.vgg16.VGG16(include_top=False, weights=\"imagenet\", input_shape=(224,224,3))","3135bc16":"vgg16_model.output[-1]","0d56f429":"# model = Sequential()\n# for layer in vgg16_model.layers[:-1]:\n#     model.add(layer)\n\n# for layer in model.layers:\n#     layer.trainable = False\n    \n# model.add(Dense(2, activation='softmax'))\n\n\nmodel = Sequential([vgg16_model,\n                    Flatten(),\n#                     GlobalAveragePooling2D(),\n#                     Dense(512, activation = \"relu\"),\n#                     BatchNormalization(),\n#                     Dropout(0.3),\n#                     Dense(128, activation = \"relu\"),\n#                     Dropout(0.1),\n#                     # Dense(32, activation = \"relu\"),\n#                     # Dropout(0.3),\n                    Dense(2, activation = \"softmax\")])\n\nmodel.layers[0].trainable = False\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=\"accuracy\")\n\nmodel.summary()","36c79f48":"hist =  model.fit_generator(train,\n                    epochs=20,\n                    callbacks=[lr_callbacks],\n                    validation_data=val)","396e6381":"epochs = 20\ntrain_loss = hist.history['loss']\nval_loss = hist.history['val_loss']\ntrain_acc = hist.history['accuracy']\nval_acc = hist.history['val_accuracy']\nxc = range(epochs)\n\nplt.figure(1,figsize=(7,5))\nplt.plot(xc,train_loss)\nplt.plot(xc,val_loss)\nplt.xlabel('num of Epochs')\nplt.ylabel('loss')\nplt.title('train_loss vs val_loss')\nplt.grid(True)\nplt.legend(['train','val'])\n#print plt.style.available # use bmh, classic,ggplot for big pictures\nplt.style.use(['classic'])\n\nplt.figure(2,figsize=(7,5))\nplt.plot(xc,train_acc)\nplt.plot(xc,val_acc)\nplt.xlabel('num of Epochs')\nplt.ylabel('accuracy')\nplt.title('train_acc vs val_acc')\nplt.grid(True)\nplt.legend(['train','val'],loc=4)\n#print plt.style.available # use bmh, classic,ggplot for big pictures\nplt.style.use(['classic'])","509c3b5e":"#Creating an array of predicted test images\n\npredictions = model.predict_generator(val)","8297e27d":"val_path = \"\/kaggle\/input\/real-and-fake-face-detection\/real_and_fake_face\/\"\n\nplt.figure(figsize=(15,15))\n\nstart_index = 250\n\nfor i in range(16):\n  plt.subplot(4,4, i+1)\n  plt.grid(False)\n  plt.xticks([])\n  plt.yticks([])\n  \n  preds = np.argmax(predictions[[start_index+i]])\n    \n  gt = val.filenames[start_index+i][9:13]\n\n  \n  if gt == \"fake\":\n    gt = 0\n  else:\n    gt = 1\n    \n  if preds != gt:\n    col =\"r\"\n  else:\n    col = \"g\"\n\n  plt.xlabel('i={}, pred={}, gt={}'.format(start_index+i,preds,gt),color=col)\n  plt.imshow(load_img(val_path+val.filenames[start_index+i]))\n  plt.tight_layout()\n\nplt.show()","12357768":"## Callbacks","7822bff7":"## Predictions","cc3fbb01":"## Data augumentation","86d3117c":"## Trying VGG16 model","8841d017":"## Import Library","02c1d65a":"## MobileNetV2","b8039d1c":"## Visualising the accuracy and loss","52ce8312":"## Visulaizing the real and fake faces","d2633145":"## Visualising the accuracy and loss"}}