{"cell_type":{"11ca1ed5":"code","945d2923":"code","71aa0e8d":"code","eb22add9":"code","eec63cbc":"code","cc157439":"code","e789667a":"code","621515da":"code","79056653":"code","7a2d8d72":"code","c47a97c9":"code","49f3499c":"code","237bd33a":"code","6444663c":"code","10f9ecfd":"code","b526a486":"code","37583066":"code","03af3af1":"code","0106e6bc":"code","0bb87948":"code","a1a636ed":"code","f8476da0":"code","0c55c1fb":"code","a9338250":"code","89b86a14":"code","c3a927a9":"code","5e5a4083":"code","b3bb7b4e":"code","516bac37":"code","56ad76fa":"code","f542ca07":"code","ec49c081":"code","c2956250":"code","ef16c83f":"code","382453de":"code","6e4645d2":"code","6d421de1":"code","045a62f7":"code","77e9b0cc":"markdown","36810348":"markdown","32cd6dde":"markdown","e6ac5444":"markdown","fefd2a38":"markdown","d449d904":"markdown","00d998ed":"markdown","6ad1ae3c":"markdown"},"source":{"11ca1ed5":"from sklearn.model_selection import KFold, cross_val_score, train_test_split\nimport xgboost as xgb","945d2923":"!pip install miceforest\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas_profiling import ProfileReport","71aa0e8d":"PATH = \"..\/input\/hackerearths-reduce-marketing-waste\/\"\ntrain = pd.read_csv(f\"{PATH}train.csv\",index_col='Deal_title')\ntest = pd.read_csv(f\"{PATH}test.csv\",index_col='Deal_title')\nsubmission= pd.read_csv(f\"{PATH}sample_submission.csv\",index_col='Deal_title')","eb22add9":"test_c=pd.read_csv(f\"{PATH}test.csv\",index_col='Deal_title')","eec63cbc":"train.head()","cc157439":"#adding test and train set together\n\ndf =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)","e789667a":"df.info()","621515da":"profile = ProfileReport(df)\nprofile","79056653":"#We will be removing some of the columns based on the count of unique values\ntrain = train.drop([\"Lead_name\",\"Contact_no\",\"POC_name\",\"Lead_POC_email\",\"Date_of_creation\"], axis=1)\ntest = test.drop([\"Lead_name\",\"Contact_no\",\"POC_name\",\"Lead_POC_email\",\"Date_of_creation\"], axis=1)","7a2d8d72":"test.isnull().sum()","c47a97c9":"train.isnull().sum()","49f3499c":"#Two of the columns has dollar sign.We will be removing them now.\ncolstocheck = train.columns\ntrain[colstocheck] = train[colstocheck].replace({r'\\$':''}, regex = True)\ntrain.head()","237bd33a":"colstocheck = test.columns\ntest[colstocheck] = test[colstocheck].replace({r'\\$':''}, regex = True)\ntest.head()","6444663c":"#Coverting deal_value and weighted_amount to type numeric\n\ntest['Weighted_amount'] = pd.to_numeric(test['Weighted_amount'], errors='coerce')\ntrain['Weighted_amount'] = pd.to_numeric(train['Weighted_amount'], errors='coerce')\ntrain['Deal_value'] = pd.to_numeric(train['Deal_value'], errors='coerce')\ntest['Deal_value'] = pd.to_numeric(test['Deal_value'], errors='coerce')","10f9ecfd":"train.info()","b526a486":"train[\"Industry\"].fillna(train[\"Industry\"].mode()[0], inplace = True)\ntest[\"Industry\"].fillna(test[\"Industry\"].mode()[0], inplace = True)","37583066":"train[\"Last_lead_update\"].fillna(train[\"Last_lead_update\"].mode()[0], inplace = True)\ntest[\"Last_lead_update\"].fillna(test[\"Last_lead_update\"].mode()[0], inplace = True)","03af3af1":"train[\"Resource\"].fillna(train[\"Resource\"].mode()[0], inplace = True)\ntest[\"Resource\"].fillna(test[\"Resource\"].mode()[0], inplace = True)","0106e6bc":"train['Deal_value'].fillna(train['Deal_value'].median(), inplace=True)\ntest['Deal_value'].fillna(test['Deal_value'].median(), inplace=True)","0bb87948":"train['Weighted_amount'].fillna(train['Weighted_amount'].median(), inplace=True)\ntest['Weighted_amount'].fillna(test['Weighted_amount'].median(), inplace=True)","a1a636ed":"train['Geography'].value_counts(dropna=False)","f8476da0":"train[train['Geography'].isnull()]","0c55c1fb":"train[['Place', 'State']] = train['Location'].str.split(' ', 1, expand=True)\ntrain['State'].fillna('0', inplace = True)\ntrain['Geography'].fillna('USA', inplace = True)\ntrain.loc[train['State'] == '0', 'Geography'] = 'India'\ntrain.drop(['Place', 'State'], axis = 1, inplace = True)","a9338250":"test[['Place', 'State']] = train['Location'].str.split(' ', 1, expand=True)\ntest['State'].fillna('0', inplace = True)\ntest['Geography'].fillna('USA', inplace = True)\ntest.loc[test['State'] == '0', 'Geography'] = 'India'\ntest.drop(['Place', 'State'], axis = 1, inplace = True)","89b86a14":"train[train['Location'].isnull()]","c3a927a9":"train['Success_probability'].describe()","5e5a4083":"#Probability cannot be more than 100.So, we will be dropping rows having values greater than 100\ntrain.drop(train.loc[train['Success_probability']>100].index, inplace=True)","b3bb7b4e":"train.nunique()","516bac37":"# Get list of categorical variables\ns = (train.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","56ad76fa":"from sklearn.preprocessing import LabelEncoder\n#coverting the object type columns to cat type\nfor i in object_cols :\n    train[i] = train[i].astype('category')\n    test[i] = test[i].astype('category')\n#coverting the categorical columns to numeric\n# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor col in object_cols:\n    train[col] = label_encoder.fit_transform(train[col])\n    test[col] = label_encoder.fit_transform(test[col])","f542ca07":"train.head()","ec49c081":"y = train.pop('Success_probability')\nX = train","c2956250":"# split into train dev and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)","ef16c83f":"model_xgb = xgb.XGBRegressor(eval_metric='mae')\nmodel_xgb.fit(X_train, y_train, early_stopping_rounds=20, eval_set=[(X_test, y_test)], verbose=1)","382453de":"y_pred=model_xgb.predict(X_test)\n\n","6e4645d2":"from sklearn import metrics\nscore=max(0,100-np.sqrt(metrics.mean_squared_error(y_test,y_pred)))\nprint(score)","6d421de1":"test","045a62f7":"final = model_xgb.predict(test)\ntest['Success_probability']= final\ntest = test.drop(['Industry','Deal_value','Weighted_amount','Pitch','Lead_revenue','Fund_category','Geography','Location','Designation','Hiring_candidate_role','Lead_source','Level_of_meeting','Last_lead_update','Internal_POC','Resource','Internal_rating'], axis=1)\ntest.to_csv('output.csv')","77e9b0cc":"Geography","36810348":"we can understand from the above data is that the area from USA has a , in between them but areas from India doesn't have them.\nSo, we can fillup the Gerography with USA where the area has a , and the one which doesn't have a , as India,","32cd6dde":"<h1 id=\"heading\">\n\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/deb009\/predict-customer-churn\/notebook#heading\">\u00b6<\/a>\n<\/h1>\n","e6ac5444":"# 1.1 Missing_values","fefd2a38":"Success_probability","d449d904":"As, there are missing values in train and test we will not be dropping the na values.","00d998ed":"# 1. Data Exploration","6ad1ae3c":"Location"}}