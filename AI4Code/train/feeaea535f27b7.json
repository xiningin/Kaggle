{"cell_type":{"1d07eab9":"code","9ae8d5af":"code","8efa92df":"code","fff42da9":"code","c2005fbf":"code","50c0fb0f":"code","dce0f6a7":"code","e708038c":"code","6a42297d":"code","11f94c58":"code","da630464":"code","330743d0":"code","0f780528":"markdown","eb983dca":"markdown","80b3e8b1":"markdown","fc455fb3":"markdown","afd5e058":"markdown","a8bb4a40":"markdown","2f332fd3":"markdown","b3872c95":"markdown","bc715f5c":"markdown","bfabdbd2":"markdown","7c315f95":"markdown","4747f42a":"markdown"},"source":{"1d07eab9":"import math, os, re, warnings, random, glob\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import Sequential\nfrom kaggle_datasets import KaggleDatasets\n","9ae8d5af":"# TPU or GPU detection\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f'Running on TPU {tpu.master()}')\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","8efa92df":"BATCH_SIZE = 16 * REPLICAS\nHEIGHT = 512\nWIDTH = 512 \nCHANNELS = 3\nN_CLASSES = 5\nTTA_STEPS = 3 # Do TTA if > 0 \nIMAGE_SIZE = [512, 512] # At this size, a GPU will run out of memory. Use the TPU.\n                        # For GPU training, please select 224 x 224 px image size.\nSEED =555    \nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nAUG_BATCH = BATCH_SIZE","fff42da9":"def data_augment(image, label):\n    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n    # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n    # of the TPU while the TPU itself is computing gradients.\n    # RandomCrop, VFlip, HFilp, RandomRotate\n    image = tf.image.rot90(image,k=np.random.randint(4))\n    image = tf.image.random_flip_left_right(image , seed=SEED)\n    image=  image = tf.image.random_flip_up_down(image, seed=SEED)\n    IMG_SIZE=IMAGE_SIZE[0]\n    # Add 6 pixels of padding\n    image = tf.image.resize_with_crop_or_pad(image, IMG_SIZE + 6, IMG_SIZE + 6) \n    # Random crop back to the original size\n    image = tf.image.random_crop(image, size=[IMG_SIZE, IMG_SIZE, 3])\n    image = tf.image.random_brightness(image, max_delta=0.5) # Random brightness\n    image = tf.image.random_saturation(image, 0, 2, seed=SEED)\n    image = tf.image.adjust_saturation(image, 3)\n    \n    #image = tf.image.central_crop(image, central_fraction=0.5)\n    return image, label   \n","c2005fbf":"# Datasets utility functions\ndef get_name(file_path):\n    parts = tf.strings.split(file_path, os.path.sep)\n    name = parts[-1]\n    return name\n\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0  # convert image to floats in [0, 1] range\n    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n    return image\n\ndef resize_image(image, label):\n    image = tf.image.resize(image, [HEIGHT, WIDTH])\n    image = tf.reshape(image, [HEIGHT, WIDTH, CHANNELS])\n    return image, label\n\ndef process_path(file_path):\n    name = get_name(file_path)\n    img = tf.io.read_file(file_path)\n    img = decode_image(img)\n    return img, name\n\ndef get_dataset(files_path, shuffled=False, tta=False, extension='jpg'):\n    dataset = tf.data.Dataset.list_files(f'{files_path}*{extension}', shuffle=shuffled)\n    dataset = dataset.map(process_path, num_parallel_calls=AUTO)\n    if tta:\n        dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n    dataset = dataset.map(resize_image, num_parallel_calls=AUTO)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef get_test_dataset(ordered=False, tta=False):\n    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n    if tta:\n        dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n    dataset = dataset.map(resize_image, num_parallel_calls=AUTO)    \n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n    return dataset\n\ndef read_unlabeled_tfrecord(example):\n    UNLABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    image_name = example['image_name']\n    return image, image_name # returns a dataset of image(s)\n\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","50c0fb0f":"database_base_path = '\/kaggle\/input\/cassava-leaf-disease-classification\/'\nsubmission = pd.read_csv(f'{database_base_path}sample_submission.csv')\ndisplay(submission.head())\nTEST_FILENAMES = tf.io.gfile.glob(f'{database_base_path}test_tfrecords\/*.tfrec') # predic\nNUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\nprint(f'GCS: test: {NUM_TEST_IMAGES}')\n","dce0f6a7":"model_path_list = glob.glob('\/kaggle\/input\/casavaleafclassificationdensenet201e20f3\/*.h5')\nmodel_path_list.sort()\n\nprint('Models to predict:')\nprint(*model_path_list, sep='\\n')","e708038c":"from tensorflow import keras\n    \nmodels = []    \ni = 0\nfor model_path in model_path_list:\n    print(model_path)\n    K.clear_session()\n    models.append(keras.models.load_model(model_path))\n","6a42297d":"models[0].summary()","11f94c58":"print(\" TTA_STEPS = {} \".format(TTA_STEPS))\nif TTA_STEPS > 0:\n    for step in range(TTA_STEPS):\n        test_ds = get_test_dataset(ordered=True, tta=True)\n        print(f'TTA step {step+1}\/{TTA_STEPS}')\n        test_images_ds = test_ds.map(lambda image, image_name: image)\n        probabilities = np.average([models[i].predict(test_images_ds) for i in range(len(models))], axis = 0)\nelse:\n    test_ds = get_test_dataset(ordered=True, tta=True)\n    test_images_ds = test_ds.map(lambda image, image_name: image)\n    probabilities = np.average([models[i].predict(test_images_ds) for i in range(1)], axis = 0)\n\npredictions = np.argmax(probabilities, axis=-1)\n","da630464":"    print('Generating submission.csv file...')\n    test_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\n    test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\n    np.savetxt('submission.csv', np.rec.fromarrays([test_ids, predictions]), fmt=['%s', '%d'], delimiter=',', header='image_id,label', comments='')","330743d0":"!head submission.csv","0f780528":"## Dependencies","eb983dca":"## Load pre-trained models ","80b3e8b1":"# Augmentation","fc455fb3":"### Hardware configuration","afd5e058":"## Generate submission file","a8bb4a40":"<center><img src=\"https:\/\/raw.githubusercontent.com\/dimitreOliveira\/MachineLearning\/master\/Kaggle\/Cassava%20Leaf%20Disease%20Classification\/banner.png\" width=\"1000\"><\/center>\n<br>\n<center><h1>Cassava Leaf Disease - TPU Tensorflow - Inference<\/h1><\/center>\n<br>\n\n[Credit] (https:\/\/www.kaggle.com\/dimitreoliveira\/cassava-leaf-disease-tpu-tensorflow-inference)\n\n","2f332fd3":"## Auxiliary functions","b3872c95":"# Load data","bc715f5c":"# Test set predictions","bfabdbd2":"## Generate Predictions","7c315f95":"# Model parameters","4747f42a":"## List Models loaded "}}