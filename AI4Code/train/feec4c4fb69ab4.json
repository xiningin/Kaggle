{"cell_type":{"e97334ea":"code","ede79e6c":"code","5cbb3a83":"code","faec9e1d":"code","371ab246":"code","94359449":"code","af42c9a2":"code","4921fab2":"code","ac7e51ba":"code","24a89871":"code","55134f7c":"code","be0e1a24":"code","4e3cf5c7":"code","bbaba018":"code","82aaf208":"code","83c2e95e":"code","07486ecf":"code","0f6c30b5":"code","d50ebf5c":"code","2251343b":"code","b8c96628":"code","9781853e":"code","b79bea31":"code","9fdd7016":"code","f65b3ba6":"code","f337eeda":"code","9c399c1d":"code","ef5cf66e":"code","7026d0aa":"code","0a9f59f9":"code","1cc304af":"code","b9f5deaa":"code","78ad8647":"code","88b03bbb":"code","58a9f22e":"code","96e84567":"code","44b2354c":"code","85eff640":"code","77d2a1e9":"code","6fe5e45e":"code","a03eef21":"code","55df267f":"code","076d9943":"code","50436d0b":"code","ca45d5d8":"code","c81f8a38":"markdown","dbca3aaa":"markdown","d326029e":"markdown","13f1a6b5":"markdown","94092082":"markdown","7a731b6f":"markdown","beb4c6ff":"markdown","730cf701":"markdown","3f0e7422":"markdown","91cca872":"markdown","71652571":"markdown","cd5b5d18":"markdown","fd48d378":"markdown","22e042d5":"markdown","c296c1a4":"markdown","94e5087e":"markdown","4e3a4b38":"markdown","7b80ccf9":"markdown","0953ff38":"markdown","f9d22372":"markdown","a24f1cdd":"markdown","7fb675f4":"markdown","e777faff":"markdown"},"source":{"e97334ea":"import numpy as np\nimport pandas as pd\nimport scipy.stats as stats\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly_express as px\n\nimport glob\nimport os","ede79e6c":"# Create a list of filepaths using glob and os.path.join\n\npath = '..\/input\/cyclistic-trip-data'\nall_files  = glob.glob(os.path.join(path, \"*.csv\"))  # list of filepath\nall_files","5cbb3a83":"# Loop through the filepath list to read csv files and append to merged_df\n\nmerged_df = pd.DataFrame()\nfor f in all_files:\n    df_each_file = pd.read_csv(f)\n    merged_df = merged_df.append(df_each_file, ignore_index=True)\n\nmerged_df.head()","faec9e1d":"merged_df.tail()","371ab246":"def explore_stats(df):\n    nrows, ncols = df.shape\n    print(\"Total records:\", nrows)\n    print(\"Total columns:\", ncols)\n    \n    # create columns list and check dtype\n    feature = []\n    type_lst = []\n    for key, value in df.dtypes.iteritems():\n        feature.append(key)\n        type_lst.append(value)\n        \n    # check distinct value\n    distinct = []\n    for i in df.columns:\n        num_distinct = df[i].unique().size\n        distinct_pct = num_distinct \/ nrows * 100\n        distinct.append(\"{} ({:0.2f}%)\".format(num_distinct, distinct_pct))\n        \n    # check null values\n    null = []\n    for i in df.columns:\n        num_null = df[i].isna().sum()\n        null_pct = num_null \/ nrows * 100\n        null.append(\"{} ({:0.2f}%)\".format(num_null, null_pct))\n        \n    # check negative values\n    negative = []\n    for i in df.columns:\n        try:\n            num_neg = (df[i].astype('float') < 0).sum()\n            neg_pct = num_neg \/ nrows * 100\n            negative.append(\"{} ({:0.2f}%)\".format(num_neg, neg_pct))\n        except:\n            negative.append(str(0) + \" (0%)\")\n            continue\n            \n    # check zeros\n    zeros = []\n    for i in df.columns:\n        try:\n            num_zero = (df[i] == 0).sum()\n            zero_pct = num_zero \/ nrows * 100\n            zeros.append(\"{} ({:0.2f}%)\".format(num_zero, zero_pct))\n        except:\n            zeros.append(str(0) + \" (0%)\")\n            continue\n            \n    # check stats measure\n    stats = df.describe().transpose()\n    \n    # put measures into a dataframe\n    data = {'feature': feature,\n           'data_type': type_lst,\n           'n_distinct': distinct,\n           'n_missing': null,\n           'n_negative': negative,\n           'n_zeros': zeros}\n    for y in stats.columns:\n        data[y] = []\n        for x in df.columns:\n            try:\n                data[y].append(stats.loc[x, y])\n            except:\n                data[y].append(0.0)\n    \n    df_stats = pd.DataFrame(data)\n    \n    return df_stats","94359449":"data_dict = explore_stats(merged_df)\ndata_dict","af42c9a2":"# Convert started_at and ended_at into datetime\n\nmerged_df['started_at'] = pd.to_datetime(merged_df['started_at'])\nmerged_df['ended_at'] = pd.to_datetime(merged_df['ended_at'])\nprint(merged_df['started_at'].dtype)\nprint(merged_df['ended_at'].dtype)","4921fab2":"# Calculate the duration of each ride\n\nmerged_df['ride_duration'] = (merged_df['ended_at'] - merged_df['started_at']) \/ pd.Timedelta(minutes=1)\nmerged_df['ride_duration']","ac7e51ba":"# Create column of date, hour, and day of week from starting time\n\nmerged_df['starting_hour'] = merged_df['started_at'].dt.hour\nmerged_df['starting_weekday'] = merged_df['started_at'].dt.dayofweek\nmerged_df['starting_day'] = merged_df['started_at'].dt.day\nmerged_df['starting_month'] = merged_df['started_at'].dt.month","24a89871":"# Create column of season from starting_month\n# create a list of our conditions\nconditions = [\n    (merged_df['starting_month'] <= 3),\n    (merged_df['starting_month'] >= 4) & (merged_df['starting_month'] <= 6),\n    (merged_df['starting_month'] >= 7) & (merged_df['starting_month'] <= 9),\n    (merged_df['starting_month'] >= 10) & (merged_df['starting_month'] <= 12)\n    ]\n\n# create a list of the values we want to assign for each condition\nvalues = ['spring', 'summer', 'autumn', 'winter']\n\n# create a new column and use np.select to assign values to it using our lists as arguments\nmerged_df['season'] = np.select(conditions, values)","55134f7c":"explore_stats(merged_df.loc[:, 'ride_duration':'season'])","be0e1a24":"merged_df_v2 = merged_df[merged_df['ride_duration'] > 0]  # drop 10,352 rows\nmerged_df_v2.shape","4e3cf5c7":"# Using Interquantile range to calculate outliers\n\nduration_data = sorted(merged_df_v2['ride_duration'])\nq1 = np.percentile(duration_data, 25)\nq3 = np.percentile(duration_data, 75)\n\niqr = q3 - q1\nlower_bound = q1 - 1.5 * iqr\nupper_bound = q3 + 1.5 * iqr\n\nprint(\"Lower bound and upper bound are: \"+str(lower_bound)+\", \"+str(upper_bound))\nprint(\"Percentile of lower bound: \" + str(stats.percentileofscore(duration_data, lower_bound)))\nprint(\"Percentile of upper bound: \" + str(stats.percentileofscore(duration_data, upper_bound)))","bbaba018":"merged_df_v3 = merged_df_v2[merged_df_v2['ride_duration'] < upper_bound]","82aaf208":"merged_df_v4 = merged_df_v3.dropna(axis=0, how='any', subset=['start_station_name', 'end_station_name'])\nexplore_stats(merged_df_v4).sort_values(by=['data_type'])","83c2e95e":"used_df = merged_df_v4.drop(['start_station_id', 'end_station_id'], axis=1)","07486ecf":"# data summary\n\nused_df.info()","0f6c30b5":"used_df.describe()","d50ebf5c":"used_df[['member_casual', 'ride_duration']].groupby(by=['member_casual']).describe(percentiles = [.01, .05, .25, .5, .75, .95, .99])","2251343b":"used_df.boxplot(column=['ride_duration'], by=['member_casual'], figsize=(10, 6), labels=['casual', 'member'])","b8c96628":"a = used_df[used_df['member_casual'] == 'member']['ride_duration']\nb = used_df[used_df['member_casual'] == 'casual']['ride_duration']","9781853e":"for i in range(7):\n    result = stats.ttest_ind(a+i, b, equal_var=False, alternative='two-sided')\n    print(\"Result of ttest with MEMBER smaller than CASUAL by \" + str(i))\n    print(result)\n    print(\"--------------------------------------------\")","b79bea31":"num_ride_by_hour = pd.pivot_table(used_df, values='ride_id', index=['starting_hour'], columns=['member_casual'], aggfunc='count')\nnum_ride_by_hour['pct_col_casual'] = num_ride_by_hour['casual']\/num_ride_by_hour['casual'].sum()\nnum_ride_by_hour['pct_col_member'] = num_ride_by_hour['member']\/num_ride_by_hour['member'].sum()","9fdd7016":"fig, ax = plt.subplots()\n\nx = np.arange(len(num_ride_by_hour.index))\nwidth = 0.4\n\ncasual = ax.bar(x-width\/2, num_ride_by_hour['pct_col_casual'], width=width, label='Casual')\nmember = ax.bar(x+width\/2, num_ride_by_hour['pct_col_member'], width=width, label='Member')\n\nplt.xticks(x, num_ride_by_hour.index)\nax.legend()\nplt.show()","f65b3ba6":"stat, p, dof, expected = stats.chi2_contingency(num_ride_by_hour[['casual', 'member']])\nprint(\"T-statistics: \" + str(stat))\nprint(\"P-value: \" + str(p))","f337eeda":"num_ride_by_weekday = pd.pivot_table(used_df, values='ride_id', index=['starting_weekday'], columns=['member_casual'], aggfunc='count')\nnum_ride_by_weekday['pct_col_casual'] = num_ride_by_weekday['casual']\/num_ride_by_weekday['casual'].sum()\nnum_ride_by_weekday['pct_col_member'] = num_ride_by_weekday['member']\/num_ride_by_weekday['member'].sum()","9c399c1d":"fig, ax = plt.subplots()\n\nx = np.arange(len(num_ride_by_weekday.index))\nwidth = 0.4\n\ncasual = ax.bar(x-width\/2, num_ride_by_weekday['pct_col_casual'], width=width, label='Casual')\nmember = ax.bar(x+width\/2, num_ride_by_weekday['pct_col_member'], width=width, label='Member')\n\nplt.xticks(x, num_ride_by_weekday.index)\nax.legend()\nplt.show()","ef5cf66e":"stat, p, dof, expected = stats.chi2_contingency(num_ride_by_weekday[['casual', 'member']])\nprint(\"T-statistics: \" + str(stat))\nprint(\"P-value: \" + str(p))","7026d0aa":"num_ride_by_month = pd.pivot_table(used_df, values='ride_id', index=['starting_month'], columns=['member_casual'], aggfunc='count')\nnum_ride_by_month['pct_col_casual'] = num_ride_by_month['casual']\/num_ride_by_month['casual'].sum()\nnum_ride_by_month['pct_col_member'] = num_ride_by_month['member']\/num_ride_by_month['member'].sum()","0a9f59f9":"fig, ax = plt.subplots()\n\nx = np.arange(len(num_ride_by_month.index))\nwidth = 0.4\n\ncasual = ax.bar(x-width\/2, num_ride_by_month['pct_col_casual'], width=width, label='Casual')\nmember = ax.bar(x+width\/2, num_ride_by_month['pct_col_member'], width=width, label='Member')\n\nplt.xticks(x, num_ride_by_month.index)\nax.legend()\nplt.show()","1cc304af":"stat, p, dof, expected = stats.chi2_contingency(num_ride_by_month[['casual', 'member']])\nprint(\"T-statistics: \" + str(stat))\nprint(\"P-value: \" + str(p))","b9f5deaa":"num_ride_by_season = pd.pivot_table(used_df, values='ride_id', index=['season'], columns=['member_casual'], aggfunc='count')\nnum_ride_by_season['pct_col_casual'] = num_ride_by_season['casual']\/num_ride_by_season['casual'].sum()\nnum_ride_by_season['pct_col_member'] = num_ride_by_season['member']\/num_ride_by_season['member'].sum()","78ad8647":"fig, ax = plt.subplots()\n\nx = np.arange(len(num_ride_by_season.index))\nwidth = 0.4\n\ncasual = ax.bar(x-width\/2, num_ride_by_season['pct_col_casual'], width=width, label='Casual')\nmember = ax.bar(x+width\/2, num_ride_by_season['pct_col_member'], width=width, label='Member')\n\nplt.xticks(x, num_ride_by_season.index)\nax.legend()\nplt.show()","88b03bbb":"stat, p, dof, expected = stats.chi2_contingency(num_ride_by_season[['casual', 'member']])\nprint(\"T-statistics: \" + str(stat))\nprint(\"P-value: \" + str(p))","58a9f22e":"num_ride_by_type = pd.pivot_table(used_df, values='ride_id', index=['rideable_type'], columns=['member_casual'], aggfunc=lambda x: x.count())\nnum_ride_by_type['pct_col_casual'] = num_ride_by_type['casual']\/num_ride_by_type['casual'].sum()\nnum_ride_by_type['pct_col_member'] = num_ride_by_type['member']\/num_ride_by_type['member'].sum()","96e84567":"fig, ax = plt.subplots()\n\nx = np.arange(len(num_ride_by_type.index))\nwidth = 0.4\n\ncasual = ax.bar(x-width\/2, num_ride_by_type['pct_col_casual'], width=width, label='Casual')\nmember = ax.bar(x+width\/2, num_ride_by_type['pct_col_member'], width=width, label='Member')\n\nplt.xticks(x, num_ride_by_type.index)\nax.legend()\nplt.show()","44b2354c":"stat, p, dof, expected = stats.chi2_contingency(num_ride_by_type[['casual', 'member']])\nprint(\"T-statistics: \" + str(stat))\nprint(\"P-value: \" + str(p))","85eff640":"num_ride_by_start = pd.pivot_table(used_df, values='ride_id', index=['start_station_name'], columns=['member_casual'], aggfunc=lambda x: x.count())\nnum_ride_by_start['pct_col_casual'] = num_ride_by_start['casual']\/num_ride_by_start['casual'].sum()\nnum_ride_by_start['pct_col_member'] = num_ride_by_start['member']\/num_ride_by_start['member'].sum()\nnum_ride_by_start.fillna(0, inplace=True)","77d2a1e9":"# Top 10 starting stations of members\n\nnum_ride_by_start[['member', 'pct_col_member']].sort_values(by='member', ascending=False).head(10)","6fe5e45e":"# Top 10 starting stations by casual riders\n\nnum_ride_by_start[['casual', 'pct_col_casual']].sort_values(by='casual', ascending=False).head(10)","a03eef21":"stat, p, dof, expected = stats.chi2_contingency(num_ride_by_start[['casual', 'member']])\nprint(\"T-statistics: \" + str(stat))\nprint(\"P-value: \" + str(p))","55df267f":"num_ride_by_end = pd.pivot_table(used_df, values='ride_id', index=['end_station_name'], columns=['member_casual'], aggfunc=lambda x: x.count())\nnum_ride_by_end['pct_col_casual'] = num_ride_by_end['casual']\/num_ride_by_end['casual'].sum()\nnum_ride_by_end['pct_col_member'] = num_ride_by_end['member']\/num_ride_by_end['member'].sum()\nnum_ride_by_end.fillna(0, inplace=True)","076d9943":"# Top 10 ending stations by members\n\nnum_ride_by_end[['member', 'pct_col_member']].sort_values(by='member', ascending=False).head(10)","50436d0b":"# Top 10 ending stations by casual riders\n\nnum_ride_by_end[['casual', 'pct_col_casual']].sort_values(by='casual', ascending=False).head(10)","ca45d5d8":"stat, p, dof, expected = stats.chi2_contingency(num_ride_by_start[['casual', 'member']])\nprint(\"T-statistics: \" + str(stat))\nprint(\"P-value: \" + str(p))","c81f8a38":"## 3. Data Cleaning\nFrom data_dict, I can see that:\n- **started_at** and **ended_at** column has dtype **object**, I will convert to **datetime**.\n- There are missing values in **start_station_name**, **end_station_name**, **start_station_id**, **end_station_id**, **eng_lat**, **eng_lng**. As I will these columns to map out the location of bike-sharing activities, I will remove all records that have missing values.\n- There are 713 distinct values in **start_station_name** and 714 in **end_station_name**, however, the numbers for **start_station_id** and **end_station_id** are 1361. There are stations that have more than one id. As I do not use station_id, and the metadata tells that there are no errors in station_name, therefore, I just remove station_id columns.","dbca3aaa":"#### Starting and ending station","d326029e":"#### Starting month\n- Casual riders have more rides in June - August\n- The same trend applies for members, however, in other months, members have more rides than casual riders\n","13f1a6b5":"## Business task\n1. Find the difference between casual riders and members\n2. Stakeholders:\n- Primary stakeholder: Chief Marketing Officer\n- Secondary stakeholder: Data team","94092082":"##  4. Exploratory Data Analysis","7a731b6f":"#### Starting season\n- Members have more rides in spring and winter while casual riders have more rides in autumn and summer\n- Overall, most of rides happen in autumn and summer","beb4c6ff":"#### 3.1.1. Create new time features","730cf701":"### 3.2. Remove rows with missing values","3f0e7422":"#### Starting hour\n- Members have more rides at 7 - 8 am and 16 - 18 pm\n- Casuals have rides mostly in the afternoon","91cca872":"I will check the statistics of new features to see any abnormal data. From the stats, I can see that:\n- **ride_duration** have 9872 negative values and 480 zeros\n- There are high chances that **ride_duration** have outliers","71652571":"## 1. Load data from csv files\n- First, I create a list of filepaths\n- Then, I merge all the csv into a dataframe","cd5b5d18":"### 4.2. Examine starting time with number of ride\nFor the relationship between time categorical variables and riders type, first, I draw barcharts to explore the relationship, then I use chi-square test to examine the hypotheses","fd48d378":"### 4.1. Examine ride length\nFirst, I create boxplot to see the relationship between **member_casual** and **ride_duration**. I see that the **ride_duration mean** for member is lower than for casual riders. I use t-test to see if this result is statistically significant.\n- Casual riders have longer ride than members, around 5 more minutes\n- Ride length of casual riders has more variability than members","22e042d5":"## Business context\n\nIn 2016, Cyclistic launched a successful bike-share offering. Since then, the program has grown to a fleet of 5,824 bicycles that are geotracked and locked into a network of 692 stations across Chicago. The bikes can be unlocked from one station and returned to any other station in the system anytime.\n\nCyclistic\u2019s finance analysts have concluded that annual members are much more profitable than casual riders. Although the pricing flexibility helps Cyclistic attract more customers, Moreno believes that maximizing the number of annual members will be key to future growth. Rather than creating a marketing campaign that targets all-new customers, Moreno believes there is a very good chance to convert casual riders into members. She notes that casual riders are already aware of the Cyclistic\nprogram and have chosen Cyclistic for their mobility needs.\n\nMoreno has set a clear goal: Design marketing strategies aimed at converting casual riders into annual members. In order to do that, however, the marketing analyst team needs to better understand how annual members and casual riders differ, why casual riders would buy a membership, and how digital media could affect their marketing tactics. Moreno and her team are interested in analyzing the Cyclistic historical bike trip data to identify trends.","c296c1a4":"#### Type of bikes\n- Members prefer classic bike more than casual riders\n- Docked bikes are used the most","94e5087e":"### 3.1. Cleaning datetime\n- First, I convert **started_at** and **ended_at** into datetime.\n- Second, I creates new features from time columns such as **ride_duration** (by minutes), **starting_month**, **starting_weekday**, **starting_day**, **starting_hour**, and **season**.","4e3a4b38":"#### Starting weekday\n- Members have almost equal number of rides on each day of the week\n- Casual riders have more rides on weekend","7b80ccf9":"#### 3.1.3. Remove rides with outliers in duration\n- The lower and upper bound are -18.9 and 51.63\n- There are only outliers in the upper side and take about 8% data. With 92% data, it's okay to solve the problem, therefore, I will eliminate these outliers","0953ff38":"This project is the capstone assignment of Google Data Analytics Professional Certificate course.","f9d22372":"### 4.3. Examine type of bikes, and location with number of rides","a24f1cdd":"- Places that casual riders come are entertainment places","7fb675f4":"####  3.1.2. Remove rides with 0 and negative duration\n","e777faff":"## 2. Data source summary\nI learn this function to explore the data from [https:\/\/nbviewer.jupyter.org\/github\/ongxuanhong\/data-science-mini-course\/blob\/master\/02_pandas\/05_explore_with_pandas.ipynb] From this exploratory stats, I can see:\n- Shape and type of data\n- Number of distinct values\n- Null values\n- Negative values\n- Zeros\n- Statistics measure"}}