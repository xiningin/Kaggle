{"cell_type":{"e0480fca":"code","9f5081e4":"code","0615e57c":"code","efad435d":"code","6c103b40":"code","dcc1937f":"code","fa6a57b4":"code","baec152f":"code","cddb8fcb":"code","aecc6a1e":"code","d974742a":"code","2135b4d0":"code","8f37f25c":"code","f36cbdb3":"code","b3dfb672":"code","2e2ae23a":"code","abed0090":"code","a8d1ed66":"code","565ed309":"code","9a11b7e7":"code","c5df39e0":"code","4c2a841e":"code","3ea38873":"code","0148b0ff":"code","014b36f1":"code","4dd3ac6d":"code","868bb531":"code","15ea8d5c":"code","acb23a69":"code","323596a3":"code","25008607":"code","cbc3c135":"code","19a152f0":"code","29e1445e":"code","8e4c1e7d":"code","2dac8772":"code","2b5384fe":"code","faa94139":"code","50ba6330":"code","0d30f258":"code","4e7d819f":"code","14565cca":"code","edaff12d":"code","ec779467":"code","9a35e92f":"code","44ae3bb0":"code","a623f4a4":"code","dec976ae":"code","2d1a0ee0":"code","f48a609d":"code","75d995be":"code","89ca088f":"code","4f0da9a6":"code","415f5d6c":"code","f37e61e2":"code","ab1de42b":"code","4c7e26d1":"code","eab37bb8":"markdown","b60ea064":"markdown","e9f07cb5":"markdown","b887b72e":"markdown","0fe77ec2":"markdown","6dcb01b9":"markdown","977c4ff9":"markdown","acd88369":"markdown","0d81b8a5":"markdown","36287b04":"markdown","e1a65c98":"markdown","45b399a8":"markdown","b52d1ec9":"markdown","333e7ff0":"markdown","62d65c38":"markdown","dee3fae4":"markdown","be3122bb":"markdown","1e01d004":"markdown","26f1d59c":"markdown","61a41f94":"markdown","e1b5d345":"markdown","bd5aa79d":"markdown","a705ae9f":"markdown","a6cede67":"markdown","9a802c07":"markdown"},"source":{"e0480fca":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp\nimport warnings\nimport os \nwarnings.filterwarnings(\"ignore\")\nimport datetime\n","9f5081e4":"data=pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\n","0615e57c":"data.head()      #displaying the head of dataset they gives the 1st to 5 rows of the data","efad435d":"data.describe()      #description of dataset ","6c103b40":"data.info()","dcc1937f":"data.shape       #569 rows and 33 columns","fa6a57b4":"data.columns     #displaying the columns of dataset","baec152f":"data.value_counts","cddb8fcb":"data.dtypes","aecc6a1e":"data.isnull().sum()","d974742a":"data.drop('Unnamed: 32', axis = 1, inplace = True)\n","2135b4d0":"data","8f37f25c":"data.corr()","f36cbdb3":"plt.figure(figsize=(18,9))\nsns.heatmap(data.corr(),annot = True, cmap =\"Accent_r\")\n\n\n\n","b3dfb672":"sns.barplot(x=\"id\", y=\"diagnosis\",data=data[160:190])\nplt.title(\"Id vs Diagnosis\",fontsize=15)\nplt.xlabel(\"Id\")\nplt.ylabel(\"Diagonis\")\nplt.show()\nplt.style.use(\"ggplot\")\n","2e2ae23a":"sns.barplot(x=\"radius_mean\", y=\"texture_mean\", data=data[170:180])\nplt.title(\"Radius Mean vs Texture Mean\",fontsize=15)\nplt.xlabel(\"Radius Mean\")\nplt.ylabel(\"Texture Mean\")\nplt.show()\nplt.style.use(\"ggplot\")\n","abed0090":" \nmean_col = ['diagnosis','radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\n\nsns.pairplot(data[mean_col],hue = 'diagnosis', palette='Accent')\n","a8d1ed66":"sns.violinplot(x=\"smoothness_mean\",y=\"perimeter_mean\",data=data)","565ed309":"plt.figure(figsize=(14,7))\nsns.lineplot(x = \"concavity_mean\",y = \"concave points_mean\",data = data[0:400], color='green')\nplt.title(\"Concavity Mean vs Concave Mean\")\nplt.xlabel(\"Concavity Mean\")\nplt.ylabel(\"Concave Points\")\nplt.show()\n\n","9a11b7e7":"worst_col = ['diagnosis','radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']\n\nsns.pairplot(data[worst_col],hue = 'diagnosis', palette=\"CMRmap\")","c5df39e0":"# Getting Features\n\nx = data.drop(columns = 'diagnosis')\n\n# Getting Predicting Value\ny = data['diagnosis']\n","4c2a841e":"\n#train_test_splitting of the dataset\nfrom sklearn.model_selection import train_test_split \nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)\n","3ea38873":"print(len(x_train))\n","0148b0ff":"print(len(x_test))","014b36f1":"print(len(y_train))","4dd3ac6d":"print(len(y_test))","868bb531":"from sklearn.linear_model import LogisticRegression\nreg = LogisticRegression()\nreg.fit(x_train,y_train)                         \n","15ea8d5c":"y_pred=reg.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",reg.score(x_train,y_train)*100)\n\n\n","acb23a69":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata\n\n\n\n\n","323596a3":"print(accuracy_score(y_test,y_pred)*100)","25008607":"from sklearn.model_selection import GridSearchCV\nparam = {\n         'penalty':['l1','l2'],\n         'C':[0.001, 0.01, 0.1, 1, 10, 20,100, 1000]\n}\nlr= LogisticRegression(penalty='l1')\ncv=GridSearchCV(reg,param,cv=5,n_jobs=-1)\ncv.fit(x_train,y_train)\ncv.predict(x_test)\n","cbc3c135":"print(\"Best CV score\", cv.best_score_*100)","19a152f0":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(max_depth=6, random_state=123)\n\ndtree.fit(x_train,y_train)\n\n#y_pred = dtree.predict(x_test)\n","29e1445e":"y_pred=dtree.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",dtree.score(x_train,y_train)*100)\n\n","8e4c1e7d":"print(accuracy_score(y_test,y_pred)*100)","2dac8772":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier()\nrfc.fit(x_train,y_train)\n\n","2b5384fe":"y_pred=rfc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",rfc.score(x_train,y_train)*100)\n","faa94139":"print(accuracy_score(y_test,y_pred)*100)","50ba6330":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=7)\n\nknn.fit(x_train,y_train)\n","0d30f258":"y_pred=knn.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",knn.score(x_train,y_train)*100)\nprint(knn.score(x_test,y_test))\n","4e7d819f":"print(accuracy_score(y_test,y_pred)*100)\n","14565cca":"from sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\n","edaff12d":"y_pred=svc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",svc.score(x_train,y_train)*100)\nprint(svc.score(x_test,y_test))\n","ec779467":"print(\"Training Score: \",svc.score(x_train,y_train)*100)","9a35e92f":"from sklearn.ensemble import AdaBoostClassifier\nadb = AdaBoostClassifier(base_estimator = None)\nadb.fit(x_train,y_train)\n\n\n\n\n\n","44ae3bb0":"y_pred=adb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",adb.score(x_train,y_train)*100)","a623f4a4":"print(accuracy_score(y_test,y_pred)*100)","dec976ae":"from sklearn.ensemble import GradientBoostingClassifier\ngbc=GradientBoostingClassifier()\ngbc.fit(x_train,y_train)\n","2d1a0ee0":"y_pred=gbc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",gbc.score(x_train,y_train)*100)\nprint(gbc.score(x_test,y_test))\n","f48a609d":"print(accuracy_score(y_test,y_pred)*100)","75d995be":"from xgboost import XGBClassifier\n\nxgb =XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)\n\nxgb.fit(x_train, y_train)\n","89ca088f":"y_pred=xgb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",xgb.score(x_train,y_train)*100)\nprint(xgb.score(x_test,y_test))\n","4f0da9a6":"print(\"Training Score: \",xgb.score(x_train,y_train)*100)","415f5d6c":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata","f37e61e2":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(x_train,y_train)","ab1de42b":"y_pred=gnb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(accuracy_score(y_test,y_pred))\nprint(\"Training Score: \",gnb.score(x_train,y_train)*100)\nprint(gnb.score(x_test,y_test))\n","4c7e26d1":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata","eab37bb8":"**So we get a accuracy score of 97.80 % using  XGBClassifier**","b60ea064":"#  7. Gradient Boosting Classifier","e9f07cb5":"**So we get a accuracy score of 63.29 % using Naive Bayes**","b887b72e":"**So we get a accuracy score of 98.24 % using AdaBoostClassifier**","0fe77ec2":"# LOADING THE DATASET","6dcb01b9":"**So we get a accuracy score of 95.61 % using GradientBoostingClassifier**","977c4ff9":"# 8. XGBClassifier","acd88369":"**So we get a accuracy score of 63.7 % using SVC**","0d81b8a5":"**So now we conclude the accuracy of different models:**\n\n**1. AdaBoost Classifier = 98.24 %**\n\n**2. XGB Classifier= 97.84 %**\n\n**3. Random Forest Classifier =96.57 %**\n\n**4. Gradient Boosting Classifier= 95.66%**\n\n**5. Decision Tree Classifier= 94.78 %**\n\n**6. K Neighbours Classifier= 70.18 %**\n\n**7. SVC = 63.80 %**\n\n**8. Naiye Bayes= 63.30 %**\n\n**9. Logistic Regression = 58.82%**\n","36287b04":"# 3. Random Forest Classifier","e1a65c98":"# MODELS","45b399a8":"# 6. AdaBoostClassifier","b52d1ec9":"# VISUALIZING THE DATA","333e7ff0":"**So we have to drop the Unnamed: 32 coulumn which contains NaN values**","62d65c38":"# 4. KNeighborsClassifier\n\n","dee3fae4":"# 1. Logistic Regression","be3122bb":"# IMPORTING THE LIBRARIES","1e01d004":"# 9. Naive Bayes","26f1d59c":"**So we get a accuracy score of 94.73 % using Decision Tree Classifier**","61a41f94":"# TRAINING AND TESTING DATA","e1b5d345":"**So we get a accuracy score of 70.17 % using KNeighborsClassifier**","bd5aa79d":"# 5. SVC","a705ae9f":"# 2. DECISION TREE CLASSIFIER","a6cede67":"**So we get a accuracy score of 96.49 % using Random Forest Classifier**","9a802c07":"**So we get a accuracy score of 58.7 % using logistic regression**"}}