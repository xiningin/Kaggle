{"cell_type":{"af28b463":"code","e0e23c67":"code","2144cf80":"code","e2256406":"code","d721938f":"code","f683eac6":"code","4793ce08":"code","da2fba70":"code","625839ef":"code","005edaeb":"code","d951256e":"code","c524f7f7":"code","3f825f52":"code","689f1014":"code","eaf9f46e":"code","f0b05b17":"code","2c3fab16":"code","ebf6dcd2":"code","fc076634":"code","42634441":"code","db679847":"markdown","ef727e41":"markdown","0e119d16":"markdown","fa1fa56d":"markdown","75d4ecfa":"markdown","920907ed":"markdown","f991b88e":"markdown","cf509744":"markdown","7f6284d5":"markdown","ac7fb501":"markdown","e240b8cd":"markdown","c5ce180d":"markdown","a2b72747":"markdown","49333e2c":"markdown","c54fb3ff":"markdown"},"source":{"af28b463":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nprint(tf.__version__)\nimport os\nimport shutil\nimport matplotlib.pyplot as plt","e0e23c67":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2144cf80":"train = pd.read_csv('..\/input\/plant-pathology-2020-fgvc7\/train.csv')\ntest = pd.read_csv('..\/input\/plant-pathology-2020-fgvc7\/test.csv')\nprint(train)\ntarget = train[['healthy', 'multiple_diseases', 'rust', 'scab']]\nprint(target)\n\ntest_ids = test['image_id']\nprint(test_ids)\n\ntrain_len = train.shape[0]\ntest_len = test.shape[0]\nprint(train_len)\nprint(train.shape)\n\ntrain.describe()","e2256406":"print(\"Shape of train data: \" + str(train.shape))\nprint(\"Shape of test data: \" + str(test.shape))","d721938f":"from keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom tqdm.notebook import tqdm\n\npath = '..\/input\/plant-pathology-2020-fgvc7\/images\/'\nsize = 224\n\ntrain_images = np.ndarray(shape=(train_len, size, size, 3))\nfor i in tqdm(range(train_len)):\n  img = load_img(path + f'Train_{i}.jpg', target_size=(size, size))\n  train_images[i] = np.uint8(img_to_array(img))\n\ntest_images = np.ndarray(shape=(test_len, size, size, 3))\nfor i in tqdm(range(test_len)):\n  img = load_img(path + f'Test_{i}.jpg', target_size=(size, size))\n  test_images[i] = np.uint8(img_to_array(img))\n\ntrain_images.shape, test_images.shape","f683eac6":"for i in range(4):\n\tplt.subplot(220 + 1 + i)\n# \tplt.title(train['image_id'][i])\n\tplt.imshow(np.uint8(train_images[i]), interpolation = 'nearest', aspect='auto')\nplt.show()\nplt.savefig('train_images.png')","4793ce08":"for i in range(4):\n\tplt.subplot(220 + 1 + i)\n\tplt.title(test['image_id'][i])\n\tplt.imshow(np.uint8(test_images[i]), interpolation = 'nearest', aspect='auto')\nplt.show()\nplt.savefig('test_images.png')","da2fba70":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(train_images, target.to_numpy(), test_size=0.1, random_state=289) \n\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","625839ef":"from imblearn.over_sampling import RandomOverSampler #\u8fc7\u91c7\u6837\u5904\u7406\n\nros = RandomOverSampler()\nx_train, y_train = ros.fit_resample(x_train.reshape((-1, size * size * 3)), y_train)\nx_train = x_train.reshape((-1, size, size, 3))\nx_train.shape, y_train.shape","005edaeb":"# samplewise_center\uff1a\u5e03\u5c14\u503c\uff0c\u4f7f\u8f93\u5165\u6570\u636e\u7684\u6bcf\u4e2a\u6837\u672c\u5747\u503c\u4e3a0\n# samplewise_std_normalization\uff1a\u5e03\u5c14\u503c\uff0c\u5c06\u8f93\u5165\u7684\u6bcf\u4e2a\u6837\u672c\u9664\u4ee5\u5176\u81ea\u8eab\u7684\u6807\u51c6\u5dee\nfrom keras_preprocessing.image import ImageDataGenerator\n\nbatch_size = 8\n\ntrain_datagen = ImageDataGenerator(samplewise_center = True,\n                                   samplewise_std_normalization = True,\n                                   horizontal_flip = True,\n                                   vertical_flip = True,\n                                   rotation_range=70)\n\ntrain_generator = train_datagen.flow(\n    x = x_train, \n    y = y_train,\n    batch_size = batch_size)\n\nvalidation_datagen = ImageDataGenerator(samplewise_center = True,\n                                        samplewise_std_normalization = True)\n\nvalidation_generator = validation_datagen.flow(\n    x = x_test, \n    y = y_test,\n    batch_size = batch_size)","d951256e":"idx = np.random.randint(8)\nprint(idx)\nx, y = train_generator.__getitem__(idx)\nprint(len(x))\nprint(len(y))\nplt.title(y[idx])\nplt.imshow(x[idx])\nplt.show()","c524f7f7":"def create_model():\n    pre_trained = tf.keras.applications.MobileNetV3Large(input_shape=(size, size, 3), weights='imagenet', include_top=False)\n\n    model = tf.keras.Sequential([\n      pre_trained,\n        tf.keras.layers.GlobalAveragePooling2D(),\n#       tf.keras.layers.Flatten(),\n      tf.keras.layers.Dropout(0.3),\n      tf.keras.layers.Dense(4, activation='softmax')\n      ])\n    model.compile(\n        loss = 'kullback_leibler_divergence', \n        optimizer = 'adam', \n        metrics = ['accuracy'])\n    return model\n\nmodel = create_model()\n\nmodel.summary()","3f825f52":"epochs = 150\nprint(batch_size)\nsteps_per_epoch = x_train.shape[0] \/\/ batch_size\nvalidation_steps = x_test.shape[0] \/\/ batch_size\nprint(steps_per_epoch)","689f1014":"# EarlyStopping\u662f\u7528\u4e8e\u63d0\u524d\u505c\u6b62\u8bad\u7ec3\u7684callbacks\u3002\u5f53\u8bad\u7ec3\u96c6\u4e0a\u7684loss\u4e0d\u5728\u51cf\u5c0f\uff08\u5373\u51cf\u5c0f\u7684\u7a0b\u5ea6\u5c0f\u4e8e\u67d0\u4e2a\u9608\u503c\uff09\u7684\u65f6\u5019\u505c\u6b62\u7ee7\u7eed\u8bad\u7ec3\u3002\n# patience: \u5f53early stop\u88ab\u6fc0\u6d3b(\u5982\u53d1\u73b0loss\u76f8\u6bd4\u4e0a\u4e00\u4e2aepoch\u8bad\u7ec3\u6ca1\u6709\u4e0b\u964d)\uff0c\u5219\u7ecf\u8fc7patience\u4e2aepoch\u540e\u505c\u6b62\u8bad\u7ec3\n# restore_best_weights\uff1a\u662f\u5426\u4ece\u65f6\u671f\u4ee5\u53d7\u76d1\u89c6\u6570\u91cf\u7684\u6700\u4f73\u503c\u6062\u590d\u6a21\u578b\u6743\u91cd\u3002\u5982\u679c\u4e3aFalse\uff0c\u5219\u4f7f\u7528\u5728\u8bad\u7ec3\u7684\u6700\u540e\u4e00\u6b65\u83b7\u5f97\u7684\u6a21\u578b\u6743\u91cd\u3002\nes = tf.keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True, verbose=1)\n# \u8be5\u56de\u8c03\u51fd\u6570\u5c06\u5728\u6bcf\u4e2aepoch\u540e\u4fdd\u5b58\u6a21\u578b\u5230filepath\n# verbose\uff1a\u4fe1\u606f\u5c55\u793a\u6a21\u5f0f\uff0c0\u62161\u3002\u4e3a1\u8868\u793a\u8f93\u51faepoch\u6a21\u578b\u4fdd\u5b58\u4fe1\u606f\uff0c\u9ed8\u8ba4\u4e3a0\u8868\u793a\u4e0d\u8f93\u51fa\u8be5\u4fe1\u606f\nmc = tf.keras.callbacks.ModelCheckpoint('model.hdf5', save_best_only=True, verbose=0)\n# \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5982\u679c\u51fa\u73b0\u4e86\u635f\u5931\u5e73\u53f0\uff08loss plateau\uff09\uff0c\u5373\u635f\u5931\u7387\u4e0d\u600e\u4e48\u53d8\u5316\u65f6\uff0c\u6539\u53d8\u5b66\u4e60\u7387\nrlr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=10, verbose=1)\n\nstart_lr = 0.00001\nmin_lr = 0.00001\nmax_lr = 0.00005\nrampup_epochs = 40\nsustain_epochs = 20\nexp_decay = .8\n\ndef lrfn(epoch):\n  if epoch < rampup_epochs:\n    return (max_lr - start_lr)\/rampup_epochs * epoch + start_lr\n  elif epoch < rampup_epochs + sustain_epochs:\n    return max_lr\n  else:\n    return min_lr\n# \u5b66\u4e60\u7387\u8c03\u5ea6\u51fd\u6570LearningRateScheduler\nlr = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=True)\n\nrang = np.arange(epochs)\ny = [lrfn(x) for x in rang]\nplt.plot(rang, y)\nprint('Learning rate per epoch:')","eaf9f46e":"history = model.fit(\n    x = train_generator,  \n    validation_data = validation_generator,\n    epochs = epochs,\n    steps_per_epoch = steps_per_epoch,\n    validation_steps = validation_steps,\n    verbose=1,\n    callbacks=[es, lr, mc, rlr])","f0b05b17":"# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper right')\nplt.show()","2c3fab16":"# samplewise_center\uff1a\u5e03\u5c14\u503c\uff0c\u4f7f\u8f93\u5165\u6570\u636e\u7684\u6bcf\u4e2a\u6837\u672c\u5747\u503c\u4e3a0\n# samplewise_std_normalization\uff1a\u5e03\u5c14\u503c\uff0c\u5c06\u8f93\u5165\u7684\u6bcf\u4e2a\u6837\u672c\u9664\u4ee5\u5176\u81ea\u8eab\u7684\u6807\u51c6\u5dee\ntest_datagen = ImageDataGenerator(samplewise_center = True,\n                                 samplewise_std_normalization = True)\n\ntest_generator = test_datagen.flow(\n    x = test_images,\n    shuffle = False)","ebf6dcd2":"probabilities = model.predict(test_generator, steps = len(test_generator))\nprint(probabilities)","fc076634":"def plot_image(i):\n    plt.title(test['image_id'][i])\n    plt.imshow(np.uint8(test_images[i]),cmap=plt.cm.binary)\n\ndef plot_value_array(results):\n    r= results\n    c = ['healthy','multiple_diseases','rush','scab']\n    plt.xticks(rotation=90)\n    plt.bar(c,r)\n    plt.ylim([0, 1])","42634441":"for i in range(50):\n    print(probabilities[i])\n    plt.figure(figsize=(6, 3))\n    plt.subplot(1, 2, 1)\n    plot_image(i)\n    plt.subplot(1, 2, 2)\n    plot_value_array(probabilities[i])\n    plt.show()","db679847":"\u4e0b\u9762\uff0c\u5c06\u8bad\u7ec3\u96c6\u5206\u6210\u8bad\u7ec3\u548c\u9a8c\u4e24\u90e8\u5206\u6570\u636e","ef727e41":"<h5>count:\u6bcf\u4e00\u5217\u975e\u7a7a\u503c\u7684\u6570\u91cf<\/h5>\n<h5>mean: \u6bcf\u4e00\u5217\u7684\u5e73\u5747\u503c<\/h5>\n<h5>std:\u6bcf\u4e00\u5217\u7684\u6807\u51c6\u5dee<\/h5>\n<h5>min\uff1a\u6700\u5c0f\u503c<\/h5>\n<h5>25%\uff1a25%\u5206\u4f4d\u6570\uff0c\u6392\u5e8f\u4e4b\u540e\u6392\u572825%\u4f4d\u7f6e\u7684\u6570<\/h5>\n<h5>50%\uff1a50%\u5206\u4f4d\u6570<\/h5>\n<h5>75%\uff1a75%\u5206\u4f4d\u6570<\/h5>\n<h5>max:\u6700\u5927\u503c<\/h5>\n<h5><\/h5>\n<h4>\u6211\u4eec\u7531mean\u884c\u770b\u5230\u591a\u91cd\u75be\u75c5\u6807\u7b7e\u7684\u56fe\u50cf\u6bd4\u5176\u4ed6\u6807\u7b7e\u7684\u56fe\u50cf\u8981\u5c11\u5f97\u591a\u3002\u56e0\u6b64\u4e0d\u80fd\u4ee5\u539f\u59cb\u6570\u636e\u5f62\u5f0f\u52a0\u8f7d\u56fe\u50cf\uff0c\u8fd9\u91cc\u5c06\u4f7f\u7528scikitlearn\u968f\u673a\u5730\u8fc7\u91c7\u6837\uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u4fee\u590d\u8fd9\u4e2a\u7c7b\u7684\u4e0d\u5e73\u8861\u3002<\/h4>\n<h5>\u73b0\u5728\u6765\u52a0\u8f7d\u56fe\u7247<\/h5>","0e119d16":"\u8ba9\u6211\u4eec\u770b\u4e00\u770b\u524d50\u5f20\u6d4b\u8bd5\u56fe\u7247\u7684\u9884\u6d4b\u7ed3\u679c","fa1fa56d":"<h4>\u73b0\u5728\u8ba9\u6211\u4eec\u770b\u4e00\u4e0b\u5206\u7c7b\u597d\u7684\u56fe\u7247<\/h4>\n<h5><\/h5>\n<h5>\u8bad\u7ec3\u96c6\u7684\u524d\u56db\u5f20\u56fe\u7247\uff1a<\/h5>","75d4ecfa":"<h2>\u52a0\u8f7d\u6570\u636e\u548c\u9884\u5904\u7406<\/h2>\n<h4>\u4e0b\u9762\uff0c\u52a0\u8f7d\u6570\u636e\uff0c\u770b\u4e00\u4e0b\u6211\u4eec\u9700\u8981\u5904\u7406\u4ec0\u4e48<\/h4>","920907ed":"\u73b0\u5728\u5b9a\u4e49\u4e00\u4e9b\u6a21\u578b\u53c2\u6570\u5e76\u8bbe\u7f6e\u4e00\u4e9b\u56de\u8c03,\u5e76\u5f00\u59cb\u8bad\u7ec3\u6a21\u578b","f991b88e":"\u9996\u5148\uff0c\u6211\u4eec\u5bfc\u5165\u9700\u8981\u7684\u5305","cf509744":"\u6d4b\u8bd5\u96c6\u7684\u524d\u56db\u5f20\u56fe\u7247\uff1a","7f6284d5":"<h5>\u73b0\u5728\u4e3a\u8fdb\u5165MobileNetV3\u6a21\u578b\u51c6\u5907\u6570\u636e\u3002<\/h5>\n<h5>\u5728\u8fd9\u91cc\uff0c\u6211\u4f7f\u7528ImageDataGenerator\u901a\u8fc7\u4f7f\u7528\u53c2\u6570\u6765\u65cb\u8f6c\u3001\u6c34\u5e73\u7ffb\u8f6c\u548c\u5782\u76f4\u7ffb\u8f6c\uff0c\u4e5f\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u66f4\u591a\u7684\u56fe\u50cf\u3002<\/h5>","ac7fb501":"<h2>\u57fa\u4e8eMobileNetV3\u7684\u679c\u6811\u75c5\u866b\u5bb3\u8bc6\u522b<\/h2>","e240b8cd":"\u4e0b\u9762\u6211\u4eec\u6765\u770b\u4e00\u770b\u9884\u6d4b\u51c6\u786e\u7387\u548c\u635f\u5931\u5ea6","c5ce180d":"\u8ba9\u6211\u4eec\u770b\u770b\u56fe\u50cf\u5904\u7406\u540e\u7684\u6837\u5b50\uff0c\u4ee5\u53ca\u5b83\u4eec\u8fdb\u5165\u6a21\u578b\u540e\u7684\u6837\u5b50\u3002","a2b72747":"## \u5e38\u660e\u6770\u7684\u6bd5\u4e1a\u8bbe\u8ba1\u4f5c\u54c1","49333e2c":"<h5>\u5728\u8fd9\u91cc\u5efa\u7acb\u6a21\u578b\u3002\u6211\u5c06\u4f7f\u7528\u9884\u5148\u8bad\u7ec3\u597d\u7684MobileNetV3Large\u8fdb\u884c\u6df1\u5ea6CNN\uff0c\u7136\u540e\u5c06\u5176\u8f93\u5165\u5230\u4e00\u4e2aDense\u5c42\u4e2d\u9884\u6d4b4\u4e2a\u7c7b\u3002<\/h5>\n<h5>\u8fd9\u91cc\u4f7f\u7528\u635f\u5931\u51fd\u6570KL\u6563\u5ea6\u3001Adam\u4f18\u5316\u5668\u548c\u7cbe\u5ea6\u5ea6\u91cf\u8fdb\u884c\u7f16\u8bd1\u3002<\/h5>","c54fb3ff":"\u73b0\u5728\u5229\u7528RandomOverSampler\u89e3\u51b3\u6837\u672c\u4e0d\u5e73\u8861\u7684\u95ee\u9898"}}