{"cell_type":{"ba1d00cc":"code","081a81fa":"code","f3141560":"code","109cf454":"code","9b3668a3":"code","88e16b9d":"code","0320f211":"code","84e544b3":"code","61a25e24":"code","051ea712":"code","d235c5b8":"code","d9feb706":"code","64c2bf4d":"code","72d8e44e":"code","f64a9207":"code","d52cf568":"code","e48b1a9d":"code","472cb8e5":"code","736aac51":"code","94691db0":"code","2dfd8a0f":"code","a299c0bf":"code","3dac3856":"code","4435f725":"code","939baef8":"code","8f440f6f":"code","3cd8a1f2":"code","fe4932ab":"code","ee94cc82":"code","b36d6693":"code","63d6f296":"code","91b8612c":"markdown","f17b7404":"markdown","c872f275":"markdown","a579da6d":"markdown","335dcf22":"markdown","4255b975":"markdown"},"source":{"ba1d00cc":"import numpy as np\nimport pandas as pd\nimport os\nimport random\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport cv2\nfrom tqdm import tqdm\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.python.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report, log_loss, accuracy_score\nfrom sklearn.model_selection import train_test_split","081a81fa":"directory = '..\/input\/face-mask-detection\/images'","f3141560":"anno=pd.read_csv('..\/input\/face-mask-get-annotation-info-from-xml\/annotation.csv')\nanno","109cf454":"path0='..\/input\/face-mask-detection\/images\/maksssksksss122.png'\nimage=cv2.imread(path0)\nprint(image.shape)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))","9b3668a3":"anno0=anno[anno['file']=='maksssksksss122']\nanno0","88e16b9d":"[xmin,ymin,xmax,ymax]=anno0.iloc[0:1,0:4].values.tolist()[0]\nymin,ymax,xmin,xmax","0320f211":"image2=image[ymin:ymax,xmin:xmax]\nplt.imshow(cv2.cvtColor(image2, cv2.COLOR_BGR2RGB))","84e544b3":"h=ymax-ymin\nw=xmax-xmin\nimage3=image[ymin+h*2\/\/5:ymax-h\/\/5,xmin+w\/\/5:xmax-w\/\/5]\nplt.imshow(cv2.cvtColor(image3, cv2.COLOR_BGR2RGB))","61a25e24":"image4=cv2.resize(image3,(60,40))\nprint(image4.shape)\nplt.imshow(cv2.cvtColor(image4, cv2.COLOR_BGR2RGB))","051ea712":"dataset=[]\nfor file in tqdm(anno['file']):\n    path=os.path.join(directory,file+'.png')\n    image=cv2.imread(path)\n    anno0=anno[anno['file']==file]\n    [xmin,ymin,xmax,ymax]=anno0.iloc[0:1,0:4].values.tolist()[0]\n    #image2=image[ymin:ymax,xmin:xmax]\n    h=ymax-ymin\n    w=xmax-xmin\n    image2=image[ymin+h*2\/\/5:ymax-h\/\/5,xmin+w\/\/5:xmax-w\/\/5]\n    if image2.shape[0]>10 and image2.shape[1]>10:\n        image3=cv2.resize(image2,(60,40))\n        dataset+=[image3]","d235c5b8":"print(anno['name'].unique())\nnormal_mapping={'with_mask':0, 'mask_weared_incorrect':1, 'without_mask':2}\ndatalabel=anno['name'].map(normal_mapping)\nprint(datalabel[0:5])","d9feb706":"dataset=np.array(dataset)\ndatalabel=np.array(datalabel)","64c2bf4d":"n=len(dataset)\nprint(n)\nD=list(range(n))\nrandom.seed(2021)\nrandom.shuffle(D)","72d8e44e":"trainY=np.array(datalabel)[D[0:(n\/\/4)*3]]\ntestY=np.array(datalabel)[D[(n\/\/4)*3:]]\ntrainX=np.array(dataset)[D[0:(n\/\/4)*3]]\ntestX=np.array(dataset)[D[(n\/\/4)*3:]]","f64a9207":"print(trainX.shape)\nprint(testX.shape)\nprint(trainY.shape)\nprint(testY.shape)","d52cf568":"labels1=to_categorical(trainY)\ntrainY2=np.array(labels1)","e48b1a9d":"tlabels1=to_categorical(testY)\ntestY2=np.array(tlabels1)","472cb8e5":"trainx,testx,trainy,testy=train_test_split(trainX,trainY2,test_size=0.2,random_state=44)","736aac51":"print(trainx.shape)\nprint(testx.shape)\nprint(trainy.shape)\nprint(testy.shape)","94691db0":"datagen = ImageDataGenerator(horizontal_flip=True,vertical_flip=True,rotation_range=20,zoom_range=0.2,\n                        width_shift_range=0.2,height_shift_range=0.2,shear_range=0.1,fill_mode=\"nearest\")","2dfd8a0f":"pretrained_model3 = tf.keras.applications.DenseNet201(input_shape=(40,60,3),include_top=False,weights='imagenet',pooling='avg')\npretrained_model3.trainable = False","a299c0bf":"inputs3 = pretrained_model3.input\nx3 = tf.keras.layers.Dense(128, activation='relu')(pretrained_model3.output)\noutputs3 = tf.keras.layers.Dense(3, activation='softmax')(x3)\nmodel = tf.keras.Model(inputs=inputs3, outputs=outputs3)","3dac3856":"model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])","4435f725":"his=model.fit(datagen.flow(trainx,trainy,batch_size=16),validation_data=(testx,testy),epochs=100)","939baef8":"y_pred=model.predict(testX)\npred=np.argmax(y_pred,axis=1)\nground=testY\nprint(classification_report(ground,pred))","8f440f6f":"get_acc = his.history['accuracy']\nvalue_acc = his.history['val_accuracy']\nget_loss = his.history['loss']\nvalidation_loss = his.history['val_loss']\n\nepochs = range(len(get_acc))\nplt.plot(epochs, get_acc, 'r', label='Accuracy of Training data')\nplt.plot(epochs, value_acc, 'b', label='Accuracy of Validation data')\nplt.title('Training vs validation accuracy')\nplt.legend(loc=0)\nplt.figure()\nplt.show()","3cd8a1f2":"epochs = range(len(get_loss))\nplt.plot(epochs, get_loss, 'r', label='Loss of Training data')\nplt.plot(epochs, validation_loss, 'b', label='Loss of Validation data')\nplt.title('Training vs validation loss')\nplt.legend(loc=0)\nplt.figure()\nplt.show()","fe4932ab":"pred2=model.predict(testX)\n\nPRED=[]\nfor item in pred2:\n    value2=np.argmax(item)      \n    PRED+=[value2]","ee94cc82":"ANS=testY","b36d6693":"accuracy=accuracy_score(ANS,PRED)\nprint(accuracy)","63d6f296":"pd.Series(PRED).value_counts()","91b8612c":"#### Focus on the area around mouth","f17b7404":"# Focus on the area around the mouth to improve the score \n* cf: https:\/\/www.kaggle.com\/stpeteishii\/annotated-face-mask-prediction","c872f275":"# The probability of correct judgement for 'without mouth' (recall value) improved\n* 0.23 (former 0.03, https:\/\/www.kaggle.com\/stpeteishii\/annotated-face-mask-prediction) ","a579da6d":"# Train and Test Setting","335dcf22":"# Annotated Area Extract","4255b975":"# Prepare Annotation Information\nhttps:\/\/www.kaggle.com\/stpeteishii\/face-mask-get-annotation-info-from-xml"}}