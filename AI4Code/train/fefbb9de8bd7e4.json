{"cell_type":{"7414828f":"code","788ccdfd":"code","4141a217":"code","4b4de605":"code","1c3ec3c0":"code","8bbe955e":"code","7159722b":"code","173d3817":"code","eedc24a2":"code","d59023d5":"code","bfd3dc39":"code","4ed6a374":"code","0015ef58":"code","d683288f":"code","9663d223":"code","e2c1ebce":"code","d1d4c1a0":"code","6b674350":"code","2870f470":"code","4f56be89":"code","230d0598":"code","091acbbc":"code","9ef8430b":"code","6d4d228d":"code","628bdbaf":"code","915075c1":"code","8cd68063":"code","cc7e0a60":"code","e6220f40":"code","b69e09f2":"code","187ba041":"code","19ff6f39":"markdown","fdd293a5":"markdown","e87a7941":"markdown","0ded0fd2":"markdown","d0214dfa":"markdown","5ff5b227":"markdown","7c64600b":"markdown","231d9ead":"markdown","34f171f9":"markdown","85b03bd0":"markdown","af0ec742":"markdown","27595f74":"markdown","3c4274c9":"markdown","a0259f98":"markdown","d29a116d":"markdown","f54396cb":"markdown","f58a3ce9":"markdown","fd2cb30d":"markdown","d354a529":"markdown","d7e8c6c0":"markdown","326c77a3":"markdown","fb65768c":"markdown"},"source":{"7414828f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor\n\n#sci-kit learn\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.pipeline import Pipeline\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","788ccdfd":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","4141a217":"def check_missing(df):\n    missing_cols = df.columns[df.isna().any()].tolist()\n    total_missing = df[missing_cols].isnull().sum().sort_values(ascending=False)\n    \n    #Also want to see the percentage of missing values for each column in missing_cols\n    percentage = (df[missing_cols].isnull().sum())\/(len(df[missing_cols]))\n    percentage = percentage.sort_values(ascending=False)\n    \n    print(pd.concat([total_missing, percentage], axis=1, keys= ['# Missing','Percentage']))","4b4de605":"check_missing(train)","1c3ec3c0":"check_missing(test)","8bbe955e":"# Create a copy of the datasets to avoid changing them\ntrain_cleaned = train.copy()\ntest_cleaned = test.copy()\n\n# PoolQC - A value of NA indicates that there is no pool present. Replace NA with 'None' string.\ntrain_cleaned['PoolQC'].fillna('None', inplace = True)\ntest_cleaned['PoolQC'].fillna('None', inplace = True)\n\n# MiscFeature - A value of NA should be replaced with 'None'\ntrain_cleaned['MiscFeature'].fillna('None', inplace = True)\ntest_cleaned['MiscFeature'].fillna('None', inplace = True)\n\n# Alley - A value of NA means no alley access. Should be replaced with 'None' \ntrain_cleaned['Alley'].fillna('None', inplace = True)\ntest_cleaned['Alley'].fillna('None', inplace = True)\n\n# Fence - A value of NA means no fence. Replace with 'None'\ntrain_cleaned['Fence'].fillna('None', inplace = True)\ntest_cleaned['Fence'].fillna('None', inplace = True)\n\n# FireplaceQu - A value of NA means no fireplace. Replace with 'None'\ntrain_cleaned['FireplaceQu'].fillna('None', inplace = True)\ntest_cleaned['FireplaceQu'].fillna('None', inplace = True)\n\n# GarageCond - A value of NA means no garage. Replace with 'None'\ntrain_cleaned['GarageCond'].fillna('None', inplace = True)\ntest_cleaned['GarageCond'].fillna('None', inplace = True)\n\n# GarageQual - A value of NA means no garage. Replace with 'None'\ntrain_cleaned['GarageQual'].fillna('None', inplace = True)\ntest_cleaned['GarageQual'].fillna('None', inplace = True)\n\n# GarageFinish - A value of NA means no garage. Replace with 'None'\ntrain_cleaned['GarageFinish'].fillna('None', inplace = True)\ntest_cleaned['GarageFinish'].fillna('None', inplace = True)\n\n# GarageType - A value of NA means no garage. Replace with 'None'\ntrain_cleaned['GarageType'].fillna('None', inplace = True)\ntest_cleaned['GarageType'].fillna('None', inplace = True)\n\n# GarageYrBlt - A value of NA means no garage. Replace with 0\ntrain_cleaned['GarageYrBlt'].fillna(0, inplace = True)\ntest_cleaned['GarageYrBlt'].fillna(0, inplace = True)\n\n# GarageArea - A value of NA means no garage. Replace with 0\ntest_cleaned['GarageArea'].fillna(0, inplace = True)\n\n# GarageCars - A value of NA means no garage. Replace with 0\ntest_cleaned['GarageCars'].fillna(0, inplace = True)\n\n# BsmtFinType2 - A value of NA means no basement. Replace with 'None'\ntrain_cleaned['BsmtFinType2'].fillna('None', inplace = True)\ntest_cleaned['BsmtFinType2'].fillna('None', inplace = True)\n\n# BsmtExposure - A value of NA means no basement. Replace with 'None'\ntrain_cleaned['BsmtExposure'].fillna('None', inplace = True)\ntest_cleaned['BsmtExposure'].fillna('None', inplace = True)\n\n# BsmtFinType1 - A value of NA means no basement. Replace with 'None'\ntrain_cleaned['BsmtFinType1'].fillna('None', inplace = True)\ntest_cleaned['BsmtFinType1'].fillna('None', inplace = True)\n\n# BsmtCond - A value of NA means no basement. Replace with 'None'\ntrain_cleaned['BsmtCond'].fillna('None', inplace = True)\ntest_cleaned['BsmtCond'].fillna('None', inplace = True)\n\n# BsmtQual - A value of NA means no basement. Replace with 'None'\ntrain_cleaned['BsmtQual'].fillna('None', inplace = True)\ntest_cleaned['BsmtQual'].fillna('None', inplace = True)\n\n# BsmtHalfBath - A value of NA means no basement half bathroom. Replace with 0\ntest_cleaned['BsmtHalfBath'].fillna(0, inplace = True)\n\n# BsmtFullBath - A value of NA means no basement full bathroom. Replace with 0\ntest_cleaned['BsmtFullBath'].fillna(0, inplace = True)\n\n# TotalBsmtSF - A value of NA means no basement. Replace with 0\ntest_cleaned['TotalBsmtSF'].fillna(0, inplace = True)\n\n# BsmtUnfSF - A value of NA means no unfinished basement square feet. Replace with 0\ntest_cleaned['BsmtUnfSF'].fillna(0, inplace = True)\n\n# BsmtFinSF2 - A value of NA means no type 2 finished basement square feet. Replace with 0\ntest_cleaned['BsmtFinSF2'].fillna(0, inplace = True)\n\n# BsmtFinSF1 - A value of NA means no type 1 finished basement square feet. Replace with 0\ntest_cleaned['BsmtFinSF1'].fillna(0, inplace = True)\n\n# MasVnrType - A value of NA most likely means no veneer. Replace with 'None'\ntrain_cleaned['MasVnrType'].fillna('None', inplace = True)\ntest_cleaned['MasVnrType'].fillna('None', inplace = True)\n\n# MasVnrArea - A value of NA most likely means no veneer which means area is zero. Replace with 0\ntrain_cleaned['MasVnrArea'].fillna(0, inplace = True)\ntest_cleaned['MasVnrArea'].fillna(0, inplace = True)\n\n# Electrical has 1 value missing, fill it with most common\n# print(test['Electrical'].value_counts()) # (uncomment to see most common value)\ntrain_cleaned['Electrical'].fillna('SBrkr', inplace=True)\n\n# KitchenQual has 1 value missing, fill it with most common\n# print(test['KitchenQual'].value_counts())\ntest_cleaned['KitchenQual'].fillna('TA', inplace=True)\n\n# Exterior1st has 1 value missing, fill it with most common\n# print(test['Exterior1st'].value_counts())\ntest_cleaned['Exterior1st'].fillna('VinylSd', inplace=True)\n\n# Exterior2nd has 1 value missing, fill it with most common\n# print(test['Exterior2nd'].value_counts())\ntest_cleaned['Exterior2nd'].fillna('VinylSd', inplace=True)\n\n# MSZoning - fill it with most common\n# print(test['MSZoning'].value_counts())\ntest_cleaned['MSZoning'].fillna('RL', inplace=True)\n\n# Functional - fill it with most common\n# print(test['Functional'].value_counts())\ntest_cleaned['Functional'].fillna('Typ', inplace=True)\n\n# Utilities - fill it with most common\n# print(test['Utilities'].value_counts())\ntest_cleaned['Utilities'].fillna('AllPub', inplace=True)\n\n# SaleType - fill it with most common\n# print(test['SaleType'].value_counts())\ntest_cleaned['SaleType'].fillna('WD', inplace=True)\n\n\n# For LotFrontage we can take the median value for houses in the same neighbourhood\n\n# Shows the median value for LotFrontage in each neighborhood:\nprint(train_cleaned.groupby('Neighborhood').LotFrontage.median())\n\n# Group the df by neighborhoods, fill in missing LotFrontage values using the median value for each neighborhood group\ntrain_cleaned['LotFrontage'] = train_cleaned.groupby('Neighborhood').LotFrontage.transform(lambda x: x.fillna(x.median()))\ntest_cleaned['LotFrontage'] = test_cleaned.groupby('Neighborhood').LotFrontage.transform(lambda x: x.fillna(x.median()))\n\n# Check to see if there are any missing values remaining\nprint(\"_\"*35)\nprint(check_missing(train_cleaned))\nprint(check_missing(test_cleaned))","7159722b":"sns.scatterplot(x=train_cleaned['GrLivArea'], y=train_cleaned['SalePrice'])","173d3817":"# Drop outliers\ntrain_cleaned = train_cleaned[train_cleaned.GrLivArea < 4000]\n\n# New plot\nsns.scatterplot(x=train_cleaned['GrLivArea'], y=train_cleaned['SalePrice'])","eedc24a2":"sns.distplot(a=train_cleaned['SalePrice'])","d59023d5":"train_cleaned['SalePrice'] = np.log1p(train_cleaned['SalePrice'])\n\nsns.distplot(a=train_cleaned['SalePrice']) # Now it looks more normally distributed","bfd3dc39":"train_ID = train_cleaned['Id']\ntest_ID = test_cleaned['Id']\n\ntrain_cleaned = train_cleaned.drop(['Id'], axis=1)\ntest_cleaned  = test_cleaned.drop(['Id'], axis=1)","4ed6a374":"#MSSubclass\ntrain_cleaned['MSSubClass']=train_cleaned['MSSubClass'].apply(str)\ntest_cleaned['MSSubClass']=test_cleaned['MSSubClass'].apply(str)\n\n#MoSold \ntrain_cleaned['MoSold']=train_cleaned['MoSold'].astype(str)\ntest_cleaned['MoSold']=test_cleaned['MoSold'].astype(str)","0015ef58":"sns.distplot(a=train_cleaned['LotArea']) # Before the transform","d683288f":"sns.distplot(a=np.log1p(train_cleaned['LotArea'])) # After the transform\ntrain_cleaned['LotArea'] = np.log1p(train_cleaned['LotArea'])\ntest_cleaned['LotArea'] = np.log1p(test_cleaned['LotArea'])","9663d223":"train_cleaned = train_cleaned.replace({\n\n                        'LotShape': {'IR3': 1, 'IR2': 2, 'IR1': 3, 'Reg': 4},\n                       'LandSlope': {'Sev': 1, 'Mod': 2, 'Gtl': 3},\n                       'ExterQual': {'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n                       'BsmtQual' : {'None':0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n                       'BsmtCond' : {'None':0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n                    'BsmtExposure': {'None':0, 'No' : 1, 'Mn' : 2, 'Av' : 3, 'Gd' : 4},\n                    'BsmtFinType1': {'None':0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n                    'BsmtFinType2': {'None':0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n                      'HeatingQC' : {'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n                    'KitchenQual' : {'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n                      'Functional': {'Sal': 1, 'Sev': 2, 'Maj2': 3, 'Maj1': 4, 'Mod': 5, 'Min2': 6, 'Min1': 7, 'Typ': 8},\n                     'FireplaceQu': {'None':0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n                    'GarageFinish': {'None':0, 'Unf': 1, 'RFn': 2, 'Fin': 3},  \n                      'GarageQual': {'None':0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n                       'BsmtCond' : {'None':0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n                     'PavedDrive' : {'N'  : 0, 'P'  : 1, 'Y'  : 2},\n                       'PoolQC'   : {'None':0, 'Fa' : 1, 'TA' : 2, 'Gd' : 3, 'Ex' : 4},    \n                        })\n\ntest_cleaned = test_cleaned.replace({\n    \n                        'LotShape': {'IR3': 1, 'IR2': 2, 'IR1': 3, 'Reg': 4},\n                       'LandSlope': {'Sev': 1, 'Mod': 2, 'Gtl': 3},\n                       'ExterQual': {'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n                       'BsmtQual' : {'None':0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n                       'BsmtCond' : {'None':0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n                    'BsmtExposure': {'None':0, 'No' : 1, 'Mn' : 2, 'Av' : 3, 'Gd' : 4},\n                    'BsmtFinType1': {'None':0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n                    'BsmtFinType2': {'None':0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n                      'HeatingQC' : {'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n                    'KitchenQual' : {'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n                      'Functional': {'Sal': 1, 'Sev': 2, 'Maj2': 3, 'Maj1': 4, 'Mod': 5, 'Min2': 6, 'Min1': 7, 'Typ': 8},\n                     'FireplaceQu': {'None':0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n                    'GarageFinish': {'None':0, 'Unf': 1, 'RFn': 2, 'Fin': 3},  \n                      'GarageQual': {'None':0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4},\n                       'BsmtCond' : {'None':0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n                     'PavedDrive' : {'N'  : 0, 'P'  : 1, 'Y'  : 2},\n                       'PoolQC'   : {'None':0, 'Fa' : 1, 'TA' : 2, 'Gd' : 3, 'Ex' : 4},    \n                        })","e2c1ebce":"# Total square footage of the house (basement and above ground SF)\ntrain_cleaned[\"TotalSF\"] = train_cleaned[\"GrLivArea\"] + train_cleaned[\"TotalBsmtSF\"]\ntest_cleaned[\"TotalSF\"] = test_cleaned[\"GrLivArea\"] + test_cleaned[\"TotalBsmtSF\"]\n\n# Total square footage for the 1st and 2nd floors\ntrain_cleaned[\"FlrsSF\"] = train_cleaned[\"1stFlrSF\"] + train_cleaned[\"2ndFlrSF\"]\ntest_cleaned[\"FlrsSF\"] = test_cleaned[\"1stFlrSF\"] + test_cleaned[\"2ndFlrSF\"]\n\n# Convert TotalSF to the same scale as OverallQual (1-10) and sum these features\ntrain_cleaned['QualScore'] = (train_cleaned['TotalSF'] \/ max(train_cleaned['TotalSF'])) * 10 + train_cleaned['OverallQual']\ntest_cleaned['QualScore'] = (test_cleaned['TotalSF'] \/ max(train_cleaned['TotalSF'])) * 10 + test_cleaned['OverallQual']\n\n# Bedrooms and bathrooms\ntrain_cleaned['BedBath'] = train_cleaned['BedroomAbvGr'] + train_cleaned['FullBath'] + 0.5*train_cleaned['HalfBath']\ntest_cleaned['BedBath'] = test_cleaned['BedroomAbvGr'] + test_cleaned['FullBath'] + 0.5*test_cleaned['HalfBath']\n","d1d4c1a0":"# Check which features have the highest correlation to the target (SalePrice)\ncorr = train_cleaned.corr()\ncorr.sort_values([\"SalePrice\"], ascending = False, inplace = True)\nprint(corr.SalePrice)","6b674350":"# Check the cardinality of categorical features\ns = (train_cleaned.dtypes == 'object')\nobject_cols = list(s[s].index)\nobject_cols\n\n# Get number of unique entries in each column with categorical data\nobject_nunique = list(map(lambda col: train_cleaned[col].nunique(), object_cols))\nd = dict(zip(object_cols, object_nunique))\n\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x: x[1])","2870f470":"# Only one-hot encode columns with cardinality <10, label-encode the rest. \ncols_to_onehot = [col for col in object_cols if train_cleaned[col].nunique() <10]\ncols_to_label = [col for col in object_cols if train_cleaned[col].nunique() >=10]\nprint(f\"The following features will be one-hot encoded: \\n{cols_to_onehot}\")\nprint(\"-\"*120)\nprint(f\"The following features will be label-encoded: \\n{cols_to_label}\")","4f56be89":"# Start with creating a joined dataset\nalldata = pd.concat([train_cleaned.iloc[:,:-1], test_cleaned]) #remove the last column (the target) in training set\n\n# Label Encode\nlabel_encoder = LabelEncoder()\nfor col in cols_to_label:\n    label_encoder.fit(alldata[col])\n    train_cleaned[col] = label_encoder.transform(train_cleaned[col])\n    test_cleaned[col] = label_encoder.transform(test_cleaned[col])\n\n# One-Hot Encode\nOH_encode = OneHotEncoder(handle_unknown='ignore', sparse= False )\nOH_encode.fit(alldata[cols_to_onehot])\nOH_train = pd.DataFrame(OH_encode.transform(train_cleaned[cols_to_onehot]))\nOH_test  = pd.DataFrame(OH_encode.transform(test_cleaned[cols_to_onehot]))\n\n# One-hot encoding removed the index - put it back\nOH_train.index = train_cleaned.index\nOH_test.index = test_cleaned.index\n\n# Add one-hot encoded columns to numerical features\ntrain_encoded = pd.concat([train_cleaned, OH_train], axis=1)\ntest_encoded = pd.concat([test_cleaned, OH_test], axis=1)\n\n# Now drop the categories you just encoded\ntrain_encoded = train_encoded.drop(cols_to_onehot, axis=1)\ntest_encoded  = test_encoded.drop(cols_to_onehot, axis=1)","230d0598":"# import lightgbm as lgb\n# from bayes_opt import BayesianOptimization\n\n# # Convert the data to LightGBM\u2019s Dataset object\n# dtrain = lgb.Dataset(X, label=y)\n\n# # BayesianOptimization function for lightgbm\n# # Specify which parameters you want to tune as keyword arguments\n# def bo_tune_lgb(num_leaves, learning_rate, n_estimators, max_bin, bagging_fraction,bagging_freq, feature_fraction,min_data_in_leaf, min_sum_hessian_in_leaf):\n#     params = {'objective': 'regression',\n#               'metric': 'rmse',\n#               'num_leaves': int(num_leaves),\n#               'learning_rate':learning_rate,\n#               'n_estimators': int(n_estimators),\n#               'max_bin': int(max_bin),\n#               'bagging_fraction': bagging_fraction,\n#               'bagging_freq': int(bagging_freq),\n#               'feature_fraction': feature_fraction,\n#               'min_data_in_leaf': int(min_data_in_leaf),\n#               'min_sum_hessian_in_leaf': min_sum_hessian_in_leaf,\n#               }\n    \n#     # Cross validate with the specified parameters in 5 folds\n#     cv_result = lgb.cv(params, dtrain, num_boost_round=100000, early_stopping_rounds = 50, nfold=5, stratified=False)\n    \n#     # Return the negative RMSE\n#     return (-1.0 * np.array(cv_result['rmse-mean'])).max()\n\n\n# # Specify the range of values for each parameter you'd like to tune with the Bayesian Optimizer\n# lgb_bo = BayesianOptimization(bo_tune_lgb, { 'num_leaves': (2, 5),\n#                                              'learning_rate': (0.01, 1),\n#                                              'n_estimators':(500,1000),\n#                                              'max_bin':(50,100),\n#                                              'bagging_fraction' : (0.7,1),\n#                                              'bagging_freq': (3, 5),\n#                                              'feature_fraction': (0.1, 0.5),\n#                                              'min_data_in_leaf': (5, 10),\n#                                              'min_sum_hessian_in_leaf': (8, 12),\n#                                             })\n\n# # Perform Bayesian optimization\n# lgb_bo.maximize(n_iter=25, init_points=10, acq='ei', xi=0.01)\n\n# # Extract the best parameters\n# params = lgb_bo.max['params']\n# print(params)","091acbbc":"# Best LightGBM Model (parameters obtained using BayesOptimization code above)\nmy_model = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.0473, n_estimators=576,\n                              max_bin = 95, bagging_fraction = 0.8195,\n                              bagging_freq = 5, feature_fraction = 0.1742,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =9, min_sum_hessian_in_leaf = 11.58)\n\n# Note: an XGBoost model was also optimized using BayesOptimization but gave a slightly lower score than LightGBM \n# (See Extras section below)","9ef8430b":"# Split into X and y\nX = train_encoded.drop(['SalePrice'], axis=1)\ny = train_encoded.SalePrice\n\n# Use five-fold cross-validation to get an idea of approximate score on the test set\nmodel_pipeline = Pipeline(steps=[\n                              ('model', my_model)\n                             ])\n\n# Multiply by -1 since sci-kit learn returns negative mean squared error\nscores = np.sqrt(-1*cross_val_score(model_pipeline, X, y, cv=5, scoring = 'neg_mean_squared_error'))\n\n\nprint(f\"RMSE scores:\\n {scores}\")\nprint(\"-\"*60)\nprint(f\"Average RMSE score:\\n {scores.mean()}\")","6d4d228d":"# Train the model\nmy_model.fit(X, y)\n\n# Generate test predictions \npreds_test = np.expm1(my_model.predict(test_encoded))\n\n\n# Save predictions in the format used for submitting to competition\noutput = pd.DataFrame({'Id': test_ID,\n                       'SalePrice': preds_test})\n\n# Submission score: 0.11956\noutput.to_csv('submission_ML.csv', index=False)","628bdbaf":"from fastai.tabular import *\n\n# Necessary for reproducibility in fast.ai\ndef random_seed(seed_value):\n    \n    import random \n    random.seed(seed_value)\n\n    np.random.seed(seed_value)\n    \n    import torch\n    torch.manual_seed(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n# Set seed for reproducibility (must be set multiple times)        \nrandom_seed(0)\n\n# Define the dependent (target) variable\ndep_var = 'SalePrice'\n\n# Data processing\nprocs = [Categorify, Normalize] \n\n# Drop the dependent variable from the dataset\ndataset_cleaned = train_cleaned.drop(['SalePrice'], axis=1)\n\n# Split the features into two groups: continuous and categorical \ncont_names = list(dataset_cleaned.select_dtypes(exclude = ['object', 'bool']).columns)\ncat_names = list(dataset_cleaned.select_dtypes(include = ['object', 'bool']).columns)\n\n# Create a databunch with TabularList (test data)\ntest = TabularList.from_df(test_cleaned, cont_names=cont_names, cat_names=cat_names, procs=procs)\n\n# Create a databunch with TabularList (train data)\ndata = ( TabularList.from_df(train_cleaned, path='.', cont_names=cont_names, cat_names=cat_names, procs=procs)\n                   .split_by_rand_pct(valid_pct = 0.2)\n                   .label_from_df(cols = dep_var, label_cls = FloatList, log = False ) # label_cls = FloatList needs to be passed for regression problems \n                   .add_test(test)\n                   .databunch(bs = 128, num_workers=0) )\n\n# Show the first five rows in the validation set\ndata.show_batch(rows=5, ds_type=DatasetType.Valid)","915075c1":"# Set seed for reproducibility\nrandom_seed(0)\n\n# Define the model\nlearn = tabular_learner(data, layers=[100,50], ps=[0.001,0.01], emb_drop=0.05, metrics=rmse)\n# learn = tabular_learner(data, layers=[200,100,50], ps=[0.001,0.01,0.1], emb_drop=0.05, metrics=rmse) #example of how to use more layers\n\n# Uncomment to see model architecture\n# print(learn.model)\n\n# Search for the best learning rate to use\nlearn.lr_find()\n\n# Typically choose a learning rate an order of magnitude before the minimum (the steepest part of the curve)  \nlearn.recorder.plot()","8cd68063":"# Set seed for reproducibility\nrandom_seed(0)\n\n# Fit the model with the chosen learning rate\nlearn.fit_one_cycle(40, max_lr = 1e-01)","cc7e0a60":"# Get predictions\npreds, _ = learn.get_preds(DatasetType.Test)\nprices = [np.expm1(p[0].data.item()) for p in preds]\n\n# Generate submission file\nsubmission = pd.DataFrame({'Id': test_ID, \n                           'SalePrice': prices})\n\n# Best submission score achieved: 0.12614\nsubmission.to_csv('submission_fastai.csv', index=False)","e6220f40":"from sklearn.preprocessing import MinMaxScaler \n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\n\n# Create scaler\nscaler = MinMaxScaler()\n\n# Access numpy (ndarray) version of the pandas df with .values\nX = train_encoded.drop(['SalePrice'], axis=1).values\ny = train_encoded.SalePrice.values\n\n# Normalize data\nX_scaled = scaler.fit_transform(X)\ntest_scaled = scaler.transform(test_encoded.values)\n\n# Set seed for reproducibility\ntf.random.set_seed(0)\n\n# Define model\nmodel = Sequential()\n\n# Input layer\n# Note: 193 is the # of columns in your training data (X)\n# Note: Running X.shape returns (1456, 193)\nmodel.add(Dense(193,  activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\n# Hidden layer\nmodel.add(Dense(256, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\n# Hidden layer\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\n# Hidden layer\nmodel.add(Dense(512, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\n# Output layer\nmodel.add(Dense(1))\n\n# Compile model\nmodel.compile(loss='mse', optimizer='adam')\n\n# Fit the model\nmodel.fit(X_scaled, y,\n          batch_size=64,\n          epochs=400,\n          validation_split = 0.2,\n          verbose = 0)\n\n# Plot losses\nlosses = pd.DataFrame(model.history.history)\nlosses.plot() \n\n# Generate test predictions\npreds_test = np.expm1(model.predict(test_scaled))\n\n# Generate submission file\noutput = pd.DataFrame({'Id': test_ID,\n                       'SalePrice': preds_test.flatten()})\n\n# Submission score:: 0.15450\noutput.to_csv('submission_TensorFlow.csv', index=False)","b69e09f2":"# import numpy\n# from sklearn.model_selection import GridSearchCV\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Dense\n# from tensorflow.keras.layers import Dropout\n# from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n\n# # KerasRegressor requires a function that creates your model\n# def create_model(num_nodes_layer1=16, num_nodes_layer2 =16, num_nodes_layer3 = 16, dropout_amt=0.2, learn_rate=0.001):\n    \n#     # Define model\n#     model = Sequential()\n    \n#     # Input layer\n#     model.add(Dense(193,  activation='relu'))\n#     model.add(Dropout(dropout_amt))\n\n#     # Hidden layer\n#     model.add(Dense(num_nodes_layer1, activation='relu'))\n#     model.add(Dropout(dropout_amt))\n\n#     # Hidden layer\n#     model.add(Dense(num_nodes_layer2, activation='relu'))\n#     model.add(Dropout(dropout_amt))\n\n#     # Hidden layer\n#     model.add(Dense(num_nodes_layer3, activation='relu'))\n#     model.add(Dropout(dropout_amt))\n\n#     # Output layer\n#     model.add(Dense(1))\n    \n#     optimizer = Adam(lr=learn_rate)\n\n#     # Compile model\n#     model.compile(loss='mse', optimizer=optimizer) #optimizer=optimizer\n    \n#     return model\n\n\n# # Set seed for reproducibility\n# tf.random.set_seed(0) \n\n# # Create the model with KerasRegressor\n# model = KerasRegressor(build_fn=create_model, verbose=0)\n\n# # Specify hyperparameter ranges\n# batch_size = [16] # example: [16,32,64]\n# epochs = [100] # example: [50,100,200]\n# num_nodes_layer1 = [128] # example: [32,64,128,256,512]\n# num_nodes_layer2 = [128]\n# num_nodes_layer3 = [256]\n# dropout_amt = [0.2] # example: [0.1,0.2,0.5]\n# learn_rate = [0.001] # example: [0.001,0.01,0.1]\n\n# # Specify hyperparameters\n# param_grid = dict(batch_size=batch_size, epochs=epochs, num_nodes_layer1=num_nodes_layer1,\n#                   num_nodes_layer2=num_nodes_layer2, \n#                   num_nodes_layer3=num_nodes_layer3, \n#                   dropout_amt=dropout_amt, learn_rate=learn_rate)\n\n# # Perform grid search\n# grid = GridSearchCV(estimator=model, param_grid=param_grid,scoring= 'neg_mean_squared_error', cv=5,  verbose=10)\n# grid_result = grid.fit(X_scaled, y)\n\n# # Output results\n# print(\"Best result:\\n %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# print(\"-\"*100)\n# means = grid_result.cv_results_['mean_test_score']\n# stds = grid_result.cv_results_['std_test_score']\n# params = grid_result.cv_results_['params']\n# for mean, stdev, param in zip(means, stds, params):\n#     print(\"Detailed results:\\n %f (%f) with: %r\" % (mean, stdev, param))","187ba041":"# from bayes_opt import BayesianOptimization\n# import xgboost as xgb\n\n# #Convert the data to XGBoost\u2019s Dmatrix object\n# dtrain = xgb.DMatrix(X, label=y)\n\n# # BayesianOptimization function for xgboost\n# # Specify which parameters you want to tune as keyword arguments\n# def bo_tune_xgb(max_depth, gamma, n_estimators ,learning_rate, colsample_bytree, reg_alpha, reg_lambda, min_child_weight,subsample):\n#     params = {'max_depth': int(max_depth),\n#               'gamma': gamma,\n#               'n_estimators': int(n_estimators),\n#               'learning_rate':learning_rate,\n#               'colsample_bytree' : colsample_bytree,\n#               'reg_alpha': reg_alpha,\n#               'reg_lambda': reg_lambda,\n#               'min_child_weight': min_child_weight,\n#               'subsample': subsample,\n#               'eta': 0.01,\n#               'eval_metric': 'rmse'}\n    \n#     # Cross validate with the specified parameters in 5 folds\n#     cv_result = xgb.cv(params, dtrain, num_boost_round=100000, early_stopping_rounds = 50, nfold=5)\n    \n#     # Return the negative RMSE\n#     return -1.0 * cv_result['test-rmse-mean'].iloc[-1]\n\n# # Specify the range of values for each parameter you'd like to tune with the Bayesian Optimizer\n# xgb_bo = BayesianOptimization(bo_tune_xgb, {'max_depth': (3, 10),\n#                                              'gamma': (0, 1),\n#                                              'learning_rate':(0.01,1),\n#                                              'n_estimators':(0,2500),\n#                                              'colsample_bytree' : (0,1),\n#                                              'reg_alpha': (0, 0.5),\n#                                              'reg_lambda': (0, 1),\n#                                              'min_child_weight': (1, 2),\n#                                              'subsample': (0.5 , 1),\n#                                             })\n\n# # Perform Bayesian optimization\n# xgb_bo.maximize(n_iter=25, init_points=10, acq='ei', xi=0.01)\n\n# # Extract the best parameters\n# params = xgb_bo.max['params']\n# print(params)","19ff6f39":"The \"Id\" feature can be removed as it doesn't contribute in predicting housing prices. Another option would be to reset the index to the \"Id\" values.\n\nNote: test_ID is stored as it will be useful when creating the submission file","fdd293a5":"# Deep Learning","e87a7941":"The next section uses TensorFlow to build models with custom architectures. ","0ded0fd2":"The target appears to be right skewed. \n\nApplying a log transform will make the feature more normally distributed which can improve model performance (constrains outliers and prediction errors for cheap vs. expensive houses will affect the model in the same way).\n\nWe use log1p(x) instead of log(x) because it is more accurate for smaller values of x.\n\nhttps:\/\/stackoverflow.com\/questions\/49538185\/what-is-the-purpose-of-numpy-log1p\/49538384","d0214dfa":"# Introduction\n\nThis notebook illustrates the steps taken towards applying machine learning and deep learning to predict housing prices in Ames, Iowa. \n\n**At the time of writing this, the notebook achieves a score of 0.11956 on the public leaderboard (top 15%)**.\n\nThe first half of the notebook focuses on the machine learning aspect as this gave the highest score for this competition (0.11956). The notebook is then extended to use deep learning in two ways: 1) Fast.ai (0.12614) and 2) TensorFlow (0.15450)\n\nA key idea in this notebook was to prevent data leakage as much as possible (as we noticed lots of notebooks submitted to this competition were susceptible to data leakage). As always, comments and suggestions for improvement are more than welcome.","5ff5b227":"# Missing Values\nThe following function checks which features have missing values","7c64600b":"A LightGBM model was optimized using BayesianOptimization to obtain the best parameters. The code used to do it is included for reference (commented out in the hidden cell). Note that each time you run it the model will converge slightly differently so it's something you need to play around with to maximize score. ","231d9ead":"Some numerical features should actually be categorical. The following cell changes them into categorical features.","34f171f9":"# Extras","85b03bd0":"The hidden code cell (included for reference) shows how to use GridSearchCV to optimize hyperparameters in the TensorFlow model. Uncomment and run to see how it works.","af0ec742":"Another feature that could benefit from a log transform is LotArea.","27595f74":"There appears to be a decent number of features with missing values in both the train and test datasets. The following cell goes feature-by-feature to fill in these missing values.\n\nNote: Information regarding each feature is found in data_description.txt","3c4274c9":"Great Resources:\n\n* https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\/notebook\n* https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset\n* https:\/\/analyticsindiamag.com\/implementing-bayesian-optimization-on-xgboost-a-beginners-guide\/\n* https:\/\/www.kaggle.com\/clair14\/tutorial-bayesian-optimization\/notebook\n* https:\/\/www.kaggle.com\/poltigo\/house-prices-prediction-with-fastai\n* https:\/\/machinelearningmastery.com\/grid-search-hyperparameters-deep-learning-models-python-keras\/\n* https:\/\/www.kaggle.com\/himanshumone11\/beginner-s-guide-house-prediction-tensorflow\/comments#Testing-Test-Data","a0259f98":"An XGBoost model was also optimized using BayesianOptimization to obtain the best parameters. The code used to do it is included for reference (commented out in the hidden cell). ","d29a116d":"Now that those outliers have been removed let's take a look at the target (SalePrice).","f54396cb":"# Feature Engineering\nThe author of the dataset recommends removing any houses with GrLivArea greater than 4000 square feet as these points in the dataset are outliers. (https:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/decock.pdf)\n\nThis can be seen by plotting SalePrice vs. GrLivArea.\n\n","f58a3ce9":"The next cell provides better-informed labels for ordinal features (categorical features that have an intrinsic order). \n\n\nThis manual label-encoding will perform better than having a LabelEncoder assign a random number for each unique value of the feature.","fd2cb30d":"# Load Train & Test Data:","d354a529":"# Machine Learning Models","d7e8c6c0":"The approach for label\/one-hot encoding will be to fit the encoder on the entire dataset (by concatenating the train and test set) but transform each individual dataset separately. \n\nThis allows the encoders to work properly even in the case where the encoder sees a value in the test set that it didn't see in the train set. \n\nNote: Data leakage should not be an issue since we only use \"alldata\" to gain access to all the unique values for each feature prior to encoding and don't continue using \"alldata\" after encoding.","326c77a3":"Generate new features that will help in predicting house prices.","fb65768c":"The tabular sub-module of the fast.ai library was used to quickly train an accurate deep learning model."}}