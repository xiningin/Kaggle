{"cell_type":{"d8d92786":"code","826a6e64":"code","13b9c9ca":"code","aa3f43a2":"code","456eae1b":"code","4f371a58":"code","bcf3f7b6":"code","7515c4ec":"code","a017797a":"code","5450e259":"code","ec0b1188":"code","76f925fa":"code","f22073e6":"code","a4714d57":"code","09309ee8":"code","4df9c98a":"code","40eb2895":"code","2351afcf":"code","0941cd3f":"code","719ff574":"code","efb75c46":"code","c89ef885":"code","f993d5d4":"code","05c7194e":"code","eb518dd9":"code","201725c1":"code","175572f8":"code","5f33bac5":"code","6ec74d0e":"code","c449a167":"code","a100ac66":"code","4e83e7ea":"code","094f01c4":"code","d04a0a4a":"code","31db87b3":"code","882e5d4f":"code","1b09b149":"code","b8de02e1":"code","0f85816b":"code","0c1e21ca":"code","45561da2":"markdown","5772a0ac":"markdown","871bc450":"markdown","45633ec8":"markdown","78d05749":"markdown","764f54ad":"markdown","8b07e225":"markdown","a48ec077":"markdown","0a331399":"markdown","921fd6de":"markdown","f2f94cd6":"markdown","11d2d77d":"markdown","b82a230e":"markdown","50023467":"markdown","96fe4cbe":"markdown","ba98a1ad":"markdown","7159e62b":"markdown"},"source":{"d8d92786":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g.| pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\npd.set_option('display.max_columns', None)","826a6e64":"train_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\", index_col=\"Id\")\ntest_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\", index_col=\"Id\")","13b9c9ca":"print(\"Length of train dataset is\", len(train_data))\nprint(\"Length of train dataset is\", len(test_data))","aa3f43a2":"#The columns in the dataset\nprint(train_data.columns)","456eae1b":"dataset = pd.concat([train_data, test_data])\nprint(\"The lenght of combined data is\", len(dataset))","4f371a58":"#detail info of the dataset\ndataset.info()","bcf3f7b6":"# null values columns in dataset\ndataset.isnull().sum()","7515c4ec":"dataset.describe()","a017797a":"# categorical features that can be turn into numerical features.\nrate_features = [\"GarageCond\",\"GarageQual\", \"FireplaceQu\", \"KitchenQual\",\"PoolQC\",\n                 \"HeatingQC\", \"BsmtCond\", \"BsmtQual\", \"ExterQual\", \"ExterCond\"]\ndataset.loc[:, rate_features] = dataset.loc[:, rate_features].fillna(\"NA\")\nrate_label = [\"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\", \"NA\"]\nrate_value = [5, 4, 3, 2, 1, 0]\ndataset[rate_features] = dataset[rate_features].replace(rate_label, rate_value)\n\n# BsmtExposure\ndataset.BsmtExposure.fillna(\"NA\", inplace = True)\ndataset[\"BsmtExposure\"].replace({\"Gd\" : 4, \"Av\" : 3, \"Mn\" : 2, \"No\" : 1, \"NA\" : 0}, inplace=True)\n\n#Fence\ndataset.Fence.fillna(\"NA\", inplace = True)\ndataset.Fence.replace({\"GdPrv\":4, \"MnPrv\":3, \"GdWo\":2, \"MnWw\":1, \"NA\":0}, inplace=True)\n\n#LotShape\ndataset[\"LotShape\"].replace({\"Reg\": 3, \"IR1\": 2, \"IR2\": 1, \"IR3\": 0}, inplace = True)\n\n# CentralAir\ndataset[\"CentralAir\"].replace({\"Y\":1, \"N\":0 }, inplace=True)\n\n#GarageFinish\ndataset.GarageFinish.fillna(\"NA\", inplace = True)\ndataset[\"GarageFinish\"].replace({\"Fin\":3, \"RFn\":2, \"Unf\":1, \"NA\":0}, inplace=True )\n\n#PavedDrive\ndataset[\"PavedDrive\"].replace({\"Y\":2, \"P\":1, \"N\":0}, inplace=True )\n\n#Functional\nfun = {\"Typ\":7, \"Min1\":6, \"Min2\":5, \"Mod\":4, \"Maj1\":3, \"Maj2\":2, \"Sev\":1, \"Sal\":0}\ndataset.Functional.replace(fun, inplace=True)\ndataset.Functional.fillna(dataset.Functional.mode()[0], inplace=True)\n\n#BsmtFinType1 and BsmtFinType2\ndataset[\"BsmtFinType1\"].fillna(\"NA\", inplace = True)\ndataset[\"BsmtFinType2\"].fillna(\"NA\", inplace = True)\nbsmt_label = [\"GLQ\", \"ALQ\", \"BLQ\", \"Rec\", \"LwQ\", \"Unf\", \"NA\"]\nbsmt_values = [6, 5, 4, 3, 2, 1, 0]\ndataset[[\"BsmtFinType1\", \"BsmtFinType2\"]].replace(bsmt_label, bsmt_label, inplace=True)\n\n#LandSlope\ndataset.LandSlope.replace({\"Glt\":2, \"Mod\":1, \"Sev\":0}, inplace=True)","5450e259":"#MSSubClass as Categorical feature\ndataset['MSSubClass'] = dataset['MSSubClass'].astype('O')","ec0b1188":"#MiscFeature replacing the Nan values with NA\ndataset.MiscFeature.fillna(\"NA\", inplace=True)\ndataset.Alley.fillna(\"NA\", inplace=True)\n\n#MasVnrType and MasVnrArea\n# The id 2611 of dataset has of the masvnrArea has 198 and the type of \"None\" on the MasVnrType\ndataset.MasVnrType.fillna(\"NA\", inplace=True)\ndataset.MasVnrArea.fillna(0, inplace=True)\n\n#LotFrontage\ndataset[\"LotFrontage\"] = dataset.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n\n# SalesType\ndataset[\"SaleType\"].fillna(dataset[\"SaleType\"].mode()[0], inplace=True)\n\n# Utilities\ndataset[\"Utilities\"].fillna(dataset[\"Utilities\"].mode()[0], inplace=True)\n\n#MSZoning\ndataset[\"MSZoning\"].fillna(dataset[\"MSZoning\"].mode()[0], inplace=True)\n\n#Exterior1st and Exterior2nd\ndataset[\"Exterior1st\"].fillna(dataset[\"Exterior1st\"].mode()[0], inplace=True)\ndataset[\"Exterior2nd\"].fillna(dataset[\"Exterior2nd\"].mode()[0], inplace=True)\n\n##Electrical\ndataset[\"Electrical\"].fillna(dataset[\"Electrical\"].mode()[0], inplace=True)\n","76f925fa":"#GarageYrBlt\ndataset.GarageYrBlt = dataset.apply(\n    lambda row: row['YrSold'] if np.isnan(row['GarageYrBlt']) \n    else row['GarageYrBlt'], axis=1)\n\n#GarageType\ndataset.GarageType.fillna(\"NA\", inplace=True)\n\n#GarageCars\ndataset.GarageCars = dataset.apply( lambda row: 0 if row['GarageCond'] == 0\n    else row['GarageCars'], axis=1)\n\n#GarageArea\ndataset.GarageArea = dataset.apply( lambda row: 0 if row['GarageCond'] == 0\n    else row['GarageArea'], axis=1)\n","f22073e6":"#Basement\ndataset.loc[dataset.BsmtFullBath.isnull()== True]\ndataset.BsmtFullBath.fillna(0, inplace=True)\ndataset.BsmtFinSF2.fillna(0, inplace=True)\ndataset.BsmtUnfSF.fillna(0, inplace=True)\ndataset.BsmtFinSF1.fillna(0, inplace=True)","a4714d57":"dataset.BsmtHalfBath.fillna(0, inplace=True)\ndataset.TotalBsmtSF.fillna(0, inplace=True)","09309ee8":"#year sold vs sales price graph\nplt.figure()\ntemp =  dataset.loc[dataset.SalePrice > 0].copy()\ntemp.groupby(\"YrSold\")[\"SalePrice\"].median()\nsns.lineplot(data=temp, x=\"YrSold\", y=\"SalePrice\")","4df9c98a":"date_features = [\"YearBuilt\", \"GarageYrBlt\", \"YrSold\", \"YearRemodAdd\"]\nfor feature in date_features:\n     if feature!='YrSold':\n        temp = dataset.loc[dataset.SalePrice > 0].copy()\n        temp.loc[:, feature] = temp['YrSold'] - temp[feature]\n        ## We will capture the difference between year variable and year the house was sold for\n        plt.scatter(temp[feature] , temp['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel('SalePrice')\n        plt.show()","40eb2895":"for col in date_features:\n    if col !='YrSold':\n        dataset[col] = dataset['YrSold'] -  dataset[feature]\n","2351afcf":"categorical_features = [feature for feature in dataset.columns if dataset[feature].dtypes == 'O']\nprint(categorical_features)","0941cd3f":"# list of numerical variables\nnumerical_features = [feature for feature in dataset.columns if dataset[feature].dtypes != 'O']\nprint(numerical_features)\n#descrete features\ndiscrete_feature=[feature for feature in numerical_features if len(dataset[feature].unique())<25 and feature not in date_features]\nprint(discrete_feature)\n#continous features\ncontinuous_feature=[feature for feature in numerical_features if feature not in discrete_feature + date_features +['Id']]\nprint(continuous_feature)","719ff574":"for feature in continuous_feature:\n    if 0 in list(dataset[feature]):\n        pass\n    else :\n        dataset[feature] = np.log(dataset[feature])","efb75c46":"# plot categorical variables\nfcols = 3\nfrows = np.ceil(len(categorical_features)\/fcols)\nplt.figure(figsize=(15,4*frows))\n\nfor i,col in enumerate(categorical_features):\n    plt.subplot(frows,fcols,i+1)\n    sns.violinplot(dataset[col],dataset['SalePrice'])","c89ef885":"# Descrete parameters\nfcols = 3\nfrows = np.ceil(len(discrete_feature)\/fcols)\nplt.figure(figsize=(15,4*frows))\n\nfor i,col in enumerate(discrete_feature):\n    plt.subplot(frows,fcols,i+1)\n    sns.violinplot(dataset[col],dataset['SalePrice'])","f993d5d4":"# continous features\nfrom scipy import stats\ntemp = dataset.loc[dataset.SalePrice > 0].copy()\nfcols = 2\nfrows = len(continuous_feature)\nplt.figure(figsize=(5*fcols,4*frows))\n\ni=0\nfor col in continuous_feature:\n    i+=1\n    ax=plt.subplot(frows,fcols,i)\n    sns.regplot(x=col, y='SalePrice', data=temp, ax=ax, \n                scatter_kws={'marker':'.','s':3,'alpha':0.3},\n                line_kws={'color':'k'});\n    plt.xlabel(col)\n    plt.ylabel('SalePrice')\n    \n    i+=1\n    ax=plt.subplot(frows,fcols,i)\n    sns.distplot(temp[col], kde=True, kde_kws={'bw':5}, fit=stats.norm)\n    plt.xlabel(col)","05c7194e":"dataset = pd.get_dummies(dataset)\ndataset.head()","eb518dd9":"train = dataset.loc[dataset.SalePrice > 0].copy()\ntest = dataset.loc[np.isnan(dataset.SalePrice)]\ntest.drop(columns = [\"SalePrice\"], inplace=True)\ny = train.loc[:, \"SalePrice\"]\ntrain.drop(columns = [\"SalePrice\"], inplace=True)","201725c1":"from sklearn.decomposition import PCA\npca = PCA()\npca.fit(train, y)\ncumsum = np.cumsum(pca.explained_variance_ratio_)*100\nd = [n for n in range(len(cumsum))]\nplt.figure()\nplt.plot(d,cumsum, color = 'red',label='cumulative explained variance')\nplt.title('Cumulative Explained Variance as a Function of the Number of Components')\nplt.ylabel('Cumulative Explained variance')\nplt.xlabel('Principal components')\nplt.axhline(y = 95, color='k', linestyle='--', label = '95% Explained Variance')\nplt.legend(loc='best')","175572f8":"#pca = PCA(.95, random_state=42)\n#train = pca.fit_transform(train)","5f33bac5":"#test = pca.transform(test)","6ec74d0e":"from sklearn import preprocessing\nnormalizer = preprocessing.MinMaxScaler()\nX_train_norm = normalizer.fit_transform(train)\nX_test_norm = normalizer.transform(test)","c449a167":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X_train_norm, y, test_size=0.2, random_state=42)","a100ac66":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn import svm","4e83e7ea":"l_reg = LinearRegression().fit(X_train, y_train)\nl_pred = l_reg.predict(X_valid)\nprint(np.sqrt(mean_squared_log_error(y_valid, l_pred)))","094f01c4":"# Fit regression model\nsvr_rbf = svm.SVR(kernel='rbf', C=1000, gamma=1e-5, epsilon=.0001)\nsvr_rbf_pred = svr_rbf.fit(X_train, y_train).predict(X_valid)\nprint(np.sqrt(mean_squared_log_error(y_valid, svr_rbf_pred)))","d04a0a4a":"svr_lin = svm.SVR(kernel='linear', C=100, gamma='auto', verbose=True)\nsvr_lin_pred = svr_lin.fit(X_train, y_train).predict(X_valid)\nprint(np.sqrt(mean_squared_log_error(y_valid, svr_lin_pred)))","31db87b3":"svr_poly = svm.SVR(kernel='poly', C=100, gamma='auto', degree=2.5, epsilon=.001, coef0=3,  verbose=True)\nsvr_poly_pred = svr_poly.fit(X_train, y_train).predict(X_valid)\nprint(np.sqrt(mean_squared_log_error(y_valid, svr_poly_pred)))","882e5d4f":"l_svm = svm.SVR().fit(X_train, y_train)\nl_svm_pred = l_svm.predict(X_valid)\nprint(np.sqrt(mean_squared_log_error(y_valid, l_svm_pred)))","1b09b149":"from sklearn import linear_model\nlasso_mdl = linear_model.Lasso(alpha=1e-8, random_state=42, max_iter=500, tol=1e-10 )\nlasso_mdl_pred = lasso_mdl.fit(X_train, y_train).predict(X_valid)\nprint(np.sqrt(mean_squared_log_error(y_valid, lasso_mdl_pred)))","b8de02e1":"test_pred = svr_poly.predict(X_test_norm)","0f85816b":"test_pred = pd.DataFrame({\"Id\":list(test_data.index), \"SalePrice\":  np.exp(test_pred) })","0c1e21ca":"test_pred.to_csv(\"submit_2.csv\", index=False)","45561da2":"**CHOOSING THE BEST MODEL**","5772a0ac":"# Categorical feature Handling","871bc450":"# GRAPHICAL REPRESENTATION","45633ec8":"3. **Lasso**","78d05749":"# TRAIN TEST SPLIT","764f54ad":"1. **LinearRegression**","8b07e225":"2. **SVM**","a48ec077":"<h3> Basement<\/h3>","0a331399":"1. svm.SVR() --> 0.01169431908847552\n2. svm.SVR(kernel='poly', C=100, gamma='auto', degree=3, epsilon=.1, coef0=1)  --> 0.009679333216715639\n3. svm.SVR(kernel='linear', C=100, gamma='auto') --> 0.01021784718990282\n4. l_reg  --> 3.2409014518222863\n5. svm.SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1) --> 0.013872325368680405","921fd6de":"# Ordinal Features CONVERSION\nThere are few categorical feature in the dataset that can be converted into the mathematical numbers. For eg. The Garage quality score from 0(lowest) to 5 (highest).","f2f94cd6":"**Date Features**","11d2d77d":"# MODELS","b82a230e":"<h2> NORMALIZATION<h2>","50023467":"# Handling missing values\nHandling missing value for categorical data can be done by replacing the None value with the NA for new categorical features.","96fe4cbe":"# Merging Train and Test\nWe are merging the train and test data so that we don't have to do seperate data cleaning for the train and test data and after that we will separate them into train and test.","ba98a1ad":"# Continous Features Handling","7159e62b":"<h3> Garage<\/h3>"}}