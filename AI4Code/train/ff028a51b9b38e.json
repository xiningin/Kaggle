{"cell_type":{"fc262156":"code","8c813176":"code","391c9216":"code","2ef1e3b9":"code","7cbed2db":"code","2b07bfd0":"code","9032295b":"code","fbca7d36":"code","50b5dce2":"code","5620b5b6":"code","14fda0f7":"code","f5fc0fd7":"code","24ee572c":"code","aa08fdd0":"code","d49d74f2":"markdown","ed7bceaa":"markdown","a495ffae":"markdown","796a5c1c":"markdown","cd712e7c":"markdown","d92cf97d":"markdown","c4c8eb87":"markdown","f9a2e21e":"markdown","381e3e59":"markdown","52a90efe":"markdown","777b8811":"markdown","1ce44f34":"markdown","e0e8a7cc":"markdown","44979e08":"markdown","f55b9217":"markdown","5ef29f04":"markdown","72f34834":"markdown","a0440036":"markdown","b22ab1b6":"markdown","5489bc51":"markdown","e8210b40":"markdown","8472c61d":"markdown","dd177cc2":"markdown"},"source":{"fc262156":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nprint(\"done importing libraries!\")","8c813176":"dataframe = pd.read_csv(\"..\/input\/train.csv\")  # read file into DataFrame\nprint(dataframe.head())  # preview\n\nsale_price = dataframe['SalePrice']  # holds all values of SalePrice column\nsale_price.describe()  # descriptive statistics","391c9216":"correlation_matrix = dataframe.corr()  # correlation coefficients of all paired columns\nfigure, axis = plt.subplots(figsize=(12, 10))  # make the graph bigger\nsns.heatmap(correlation_matrix, vmin=-0.7, vmax=0.7, cmap='RdBu', square=True)","2ef1e3b9":"# calculates the mean squared error as an evaluation metric\n# predicted: the observed values calculated by the model\n# actual: the expected values from the dataset\n\ndef calculate_loss(predicted, actual):\n  squared_errors = (actual - predicted)**2\n  n = len(predicted)\n  return 1.0 \/ (2*n) * squared_errors.sum()","7cbed2db":"# returns the weighted sum of the inputs as a prediction of y\n# x: rows of values for each column (inputs)\n# weights: factors representing relative importance of each column\n\ndef evaluate_prediction(x, weights):\n    return np.dot(x, weights)  # dot: add individual products","2b07bfd0":"# returns the new weights after taking a step in the direction of the negative gradient\n# x: rows of inputs\n# weights: coefficients for each column in x\n# y: rows of actual outputs\n# learning_rate: how quickly the model should learn (how big of a step)\n\ndef gradient_descent(x, weights, y, learning_rate):\n    predictions = evaluate_prediction(x, weights)\n    error = predictions - y  # how bad are the current weight values\n    gradient = np.dot(x.T,  error) \/ len(x)  # plug into derivative function\n    new_weights = weights - learning_rate * gradient  # update\n    return new_weights","9032295b":"# returns the final weights and list of losses after updating weights using repeated gradient descent\n# x: rows of inputs\n# y: rows of actual outputs\n# iterations: number of times to update (epochs)\n# learning_rate: how quickly the model should learn\n\ndef train_model(x, y, iterations, learning_rate):\n    weights = np.zeros(x.shape[1])  # initially all zeros\n    loss_history = []\n    for i in range(iterations):  # iterations aka epochs\n        prediction = evaluate_prediction(x, weights)\n        current_loss = calculate_loss(prediction, y)\n        loss_history.append(current_loss)\n        weights = gradient_descent(x, weights, y, learning_rate)  # update\n    return weights, loss_history","fbca7d36":"area = dataframe['GrLivArea']\nx_train, x_test, y_train, y_test = train_test_split(area, sale_price,test_size=0.2)\n\nstd_x_train = (x_train - x_train.mean()) \/ x_train.std()  # calculate z-scores\nstd_x_train = np.c_[std_x_train, np.ones(x_train.shape[0])]  # concatenate column of 1s\nstd_x_test = (x_test - x_test.mean()) \/ x_test.std()\nstd_x_test = np.c_[std_x_test, np.ones(x_test.shape[0])]\n\nweights, loss_history = train_model(std_x_train, y_train, 1000, 0.01)\nprint(weights)","50b5dce2":"plt.plot(loss_history)\nplt.title('Loss During Training')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.show()","5620b5b6":"plt.scatter(x_train, y_train, color='red')\nplt.plot(x_train, evaluate_prediction(std_x_train, weights), color='blue')\nplt.title('Sale Price vs. Living Area (Training set)')\nplt.xlabel('Living Area (sq. ft.)')\nplt.ylabel('Sale Price ($)')\nplt.show()","14fda0f7":"plt.scatter(x_test, y_test, color='red')\nplt.plot(x_test, evaluate_prediction(std_x_test, weights), color='blue')\nplt.title('Sale Price vs. Living Area (Test set)')\nplt.xlabel('Living Area (sq. ft.)')\nplt.ylabel('Sale Price ($)')\nplt.show()","f5fc0fd7":"multi_vars = dataframe[['GrLivArea', 'OverallQual', 'GarageCars']]\n\nstd_multi_vars = (multi_vars - multi_vars.mean()) \/ multi_vars.std()\nstd_multi_vars = np.c_[std_multi_vars, np.ones(std_multi_vars.shape[0])]\n\nweights, loss_history = train_model(std_multi_vars, sale_price, 1000, 0.01)\nprint(weights)","24ee572c":"plt.plot(loss_history)\nplt.title('Loss During Training')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.show()","aa08fdd0":"from sklearn.linear_model import LinearRegression\n\nregressor = LinearRegression()\nregressor.fit(std_x_train, y_train)\n\nprint(regressor.coef_)\nprint(regressor.intercept_)\n\nplt.scatter(x_train, y_train, color='red')\nplt.plot(x_train, regressor.predict(std_x_train), color='blue')\nplt.title('Sale Price vs. Living Area (Training set)')\nplt.xlabel('Living Area (sq. ft.)')\nplt.ylabel('Sale Price ($)')\nplt.show()\n\nplt.scatter(x_test, y_test, color='red')\nplt.plot(x_test, regressor.predict(std_x_test), color='blue')\nplt.title('Sale Price vs. Living Area (Test set)')\nplt.xlabel('Living Area (sq. ft.)')\nplt.ylabel('Sale Price ($)')\nplt.show()","d49d74f2":"**Why would we add a column of 1s to the x inputs?**\n\nThis formula y(x_1, ..., x_n) = w_1\\*x_1 + ... + w_n\\*x_n + w_0 can be rewritten as the dot product **y(x) = w \u00b7 x + w_0**, where x is the list (vector) of all x_1 to x_n and w is the list (vector) of all w_1 to w_n.\nThe dot product is a type of math function used on two vectors of the same length that adds the products of each of the elements.\n\nExample: If w = (2, 3, 4), x = (4, 3, 2), and w_0 = 5, what is y(x)?\n\ny(x) = w \u00b7 x + w_0\n\ny(x) = 2\\*4 + 3\\*3 + 4\\*2 + 5\n\ny(x) = 30\n\nInstead of thinking of y(x) as a dot product of two vectors plus another number, we can simplify the calculation to just a dot product of the w vector with an added column for w_0 and the x vector with an added column of 1s.\n\nSo now, w = (2, 3, 4, **5**) and x = (4, 3, 2, **1**), and there's no need for w_0 separately.\n\ny(x) = w \u00b7 x\n\ny(x) = 2\\*4 + 3\\*3 + 4\\*2 + 1\\*5\n\ny(x) = 30\n\n","ed7bceaa":"# Time for regression!\nFirst, we will look through the methods that go behind training a regression model. Then, we can use the sklearn library to train automatically.","a495ffae":"What do these weights mean?","796a5c1c":"**What is linear regression?**\n\nLinear regression is a type of **predictive analysis**: we want to predict an outcome based on given input(s). The outcome, aka the dependent variable, is referred to as y, and the input, aka the independent or explanatory variable, is x. When we say we want to create a regression model, essentially, we are trying to find a line of best fit.\n\nThere are two main types of linear regression:\n* **Simple Linear Regression**: one input variable and one output variable \n* **Multiple Linear Regression**: multiple input variables and one output variable","cd712e7c":"Now that we've understood how linear regression models are trained, we can use an sklearn package to do so automatically.","d92cf97d":"Notice how much repeated training reduces the loss! The loss over the first couple hundred iterations decrease dramatically, and then it slows down as we keep fine-tuning our weights.\n\nLet's plot this model as a line of best fit against the original training data.","c4c8eb87":"# Check Your Understanding\n* In your own words, what is a DataFrame?\n* What's the significance of the heat map?\n* Summarize how to train a linear regression model in machine learning.\n* What's the benefit of coding a machine learning algorithm to create a regression model over calculating the statistical regression formula, say, with a calculator?\n* Any questions, clarifications, comments?","f9a2e21e":"# Let's use these functions to train our models!","381e3e59":"# Data Preparation\nFirst, we will take a look at the dataset.","52a90efe":"Import the necessary Python libraries and submodules:\n* **numpy** - math including arrays\n* **pandas** - data processing and analysis\n* **matplotlib.pyplot** - plotting figures\n* **seaborn** - visualizations\n* **sklearn.model_selection** - machine learning","777b8811":"This first function, calculate_loss, calculates the **mean squared error** (MSE): the sum of all of the squared differences divided by the number of observations. MSE is one example of an **evaluation metric**, used to measure the quality of machine learning model.","1ce44f34":"Now, for training!\nEssentially, we repeat gradient descent, updating the weights after each iteration. I also keep track of every value of the loss function so that later, we can visualize how our model improves with training.","e0e8a7cc":"**How do we write all this in code?**\n\nWe find the optimal values for the linear regression model through training. The main purpose of training is to minimize the error between the predicted values and the actual values. With each step of training, we update the weights to keep minimizing the error.","44979e08":"**How do we find that line of best fit?**\n\nYou know that the equation of a line of best fit is y = mx + b, where m is the slope coefficient and b is the y-intercept. In machine learning, it's quite similar, except we adjust for possible multiple inputs: **y(x_1, ..., x_n) = w_1\\*x_1 + ... + w_n\\*x_n + w_0**.\n* x_1, ..., x_n represent the **explanatory variables** we are using to predict y\n* w_1 ... w_n represent the **weight** coefficients, representing each variable's relative importance\n* w_0 is the **intercept**\n    * to simplify product calculations, we will add another column of 1s to each of the inputs, so it becomes 1\\*w_0","f55b9217":"# Learn Linear Regression! House Sale Prices\nOn Day 1 of this AI\/ML workshop, we will learn simple and multiple linear regression by training a machine learning model to predict house sale prices.\n\nLive Coding Tutorial Recording (May 30): https:\/\/www.youtube.com\/watch?v=d-SyV8yZV1M\n\nTutorial and Notebook created by Eban Ebssa","5ef29f04":"Use the pandas library to create a **DataFrame**, which is a 2-D data structure labeled with rows (**observations**) and columns (**features**).\nThere are 80 features recorded with each of the 1460 house observations in the data set that we can use to predict the last column, SalePrice.","72f34834":"This function, evaluate_prediction, simply multiplies the x inputs by the weights of the model to return a prediction of the output y.","a0440036":"**Multiple Linear Regression Model**\n\nChoose GrLivArea (ground living area), OverallQual (overall finish), and GarageCars (car capacity) as three explanatory variables. Don't forget to standardize and concatenate the column of ones.","b22ab1b6":"Here's the graph of the decreasing loss function again :)","5489bc51":"Generate a heat map based on the pairwise correlations of all of the columns. What do you notice? What does the vertical bar on the right represent? How is that reflected in the graph?","e8210b40":"This function, gradient_descent, will return the new values of the weights after taking a step toward the descent of the gradient of the loss function. The **gradient** of a function is the **derivative** of that function, which is a new function representing the rate of change (slope) at each point.\n\nThe derivative (gradient) of the loss function is the dot product of the transposed flipped x values and the error, divided by the number of x values. Because this represents the rate of change of the errors, we want to go in the negative direction so that error will decrease (descent).\n\n![Gradient Descent Visual](http:\/\/miro.medium.com\/max\/1024\/1*G1v2WBigWmNzoMuKOYQV_g.png)","8472c61d":"**Simple Linear Regression Model**\n\nChoose GrLivArea (ground living area) as a single explanatory variable.\nThen split the data into training (80%) and testing (20%) sets.\nThen, standardize the inputs as z-scores, and concatenate a column of 1s.\n\nStandardizing allows the gradient descent to converge more quickly.","dd177cc2":"Now, let's see how well the model fits the test data."}}