{"cell_type":{"85a8cd50":"code","616aa08e":"code","bc792ac3":"code","82f7aff5":"code","92bb3464":"code","d1212437":"code","a2fe2043":"code","67afc05b":"code","3854245f":"code","9dbae864":"code","fa852f99":"code","75d3f4cd":"code","c6516d8d":"code","272979ab":"code","6941b9d9":"code","4122167c":"code","849b6c56":"code","e52a2d5e":"code","e53d0262":"code","1bc8f8d7":"code","bd3d729e":"code","ae951bb2":"code","edc4f41f":"code","d86cf13c":"code","5aa1bf0a":"code","788e994f":"code","41b601ef":"code","2d62ffb0":"code","2b9a0de1":"code","953d763c":"code","3fe45fec":"code","3006bafa":"code","20f0f8ea":"code","cac91c3f":"code","e85b6313":"code","c229e274":"code","836c8360":"code","f6ca6d88":"code","98c90ef3":"code","d9e368d4":"code","3b6dd99c":"code","e2e85ffe":"code","bb41f0cb":"code","71c2a662":"code","f3302493":"code","363e5f92":"code","5133f776":"code","7120ec96":"code","375561f5":"code","c6857d6a":"code","78932d33":"code","ecee911e":"code","f99d5533":"code","104f824d":"code","be2ee1c8":"code","9561cff5":"code","d67806d3":"code","ddffe6c6":"code","52d330e4":"code","407c7022":"code","e45591f6":"code","64b3c854":"code","c8927f87":"code","90bb362c":"code","b259a29e":"code","6a4889b7":"code","43de10dd":"markdown","c28482a4":"markdown","ec254fab":"markdown","5df8c6f2":"markdown","31d0ffc2":"markdown","08053555":"markdown","4ab6bac2":"markdown","26c00b33":"markdown","f6fddb79":"markdown","97199ce6":"markdown","46bdd66e":"markdown","2b919a82":"markdown","c3feea19":"markdown","1b6c703e":"markdown","037811ff":"markdown","ae5bad91":"markdown","88817cee":"markdown","0ede2f3d":"markdown","4073e002":"markdown","33485ddd":"markdown","3cdd177a":"markdown","678a8ffd":"markdown","2f228845":"markdown","15eed867":"markdown","24da65ae":"markdown","cfaa9191":"markdown","2e3b92f0":"markdown","9ee56afb":"markdown","95544f69":"markdown","15c43db7":"markdown","e1aeb647":"markdown","339c133c":"markdown","c5a1fabf":"markdown","23422dc6":"markdown","0d709f8c":"markdown","a8b5dfd5":"markdown","b2702235":"markdown","3f2c2c0e":"markdown","55f5fc24":"markdown","53dedc67":"markdown","c0eed537":"markdown"},"source":{"85a8cd50":"import numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport seaborn as sns","616aa08e":"train = pd.read_csv('..\/input\/haikujam\/BuyAffinity_Train.txt', delimiter='\\t', parse_dates=['F15', 'F16'])\ntest = pd.read_csv('..\/input\/haikujam\/BuyAffinity_Test.txt', delimiter='\\t', parse_dates=['F15', 'F16'])","bc792ac3":"train.shape, test.shape","82f7aff5":"# Creating a copy of our original dataframe.\ndf = train.copy()","92bb3464":"# Creating a function to find out all the features with missing values.\ndef missing_values(data):\n    missing = pd.DataFrame(data.isnull().sum())\n    missing.columns = ['Missing_Values(num)']\n    missing['Missing_Values(%)'] = pd.DataFrame((data.isnull().sum()\/len(data))*100)\n    missing['Dtype'] = data.dtypes\n    return missing.sort_values(by = 'Missing_Values(%)', ascending=False)\nmissing_values(df)","d1212437":"# Transpose of our dataframe.\ndf.head().T","a2fe2043":"import datetime as dt\ndf['F15']=df['F15'].map(dt.datetime.toordinal)\ndf['F16']=df['F16'].map(dt.datetime.toordinal)","67afc05b":"import matplotlib.ticker as ticker\nimport matplotlib.cm as cm\nimport matplotlib as mpl\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nfig = plt.figure()\nfig, ax = plt.subplots(1,1, figsize=(20,20))\nheatplot = ax.imshow(df.corr(), cmap='BuPu')\nax.set_xticklabels(df.columns)\nax.set_yticklabels(df.columns)\n\ntick_spacing = 1\nax.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\nax.yaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\nax.set_title(\"Heatmap of DataSet\")\nax.set_xlabel('Features')\nax.set_ylabel('Correlation')","3854245f":"fig, ax = plt.subplots(figsize=(10,6))\nsns.heatmap(df.corr(), center=0, cmap=\u2019Blues\u2019)\nax.set_title(\u2018Multi-Collinearity of Car Attributes\u2019)","9dbae864":"# Dropping target variable to create X dataframe. Let's also drop the Index Variable.\nX = df.drop(['C','Index'],axis=1)\n# Creating target variable with just `C`\ny = df['C']","fa852f99":"print(y.value_counts())\ny.value_counts().plot(kind='bar')","75d3f4cd":"(y.value_counts()\/len(y))*100","c6516d8d":"X","272979ab":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","6941b9d9":"def modeller(X_train, X_test, y_train, y_test):\n    np.random.seed(42)\n    #Random Forest\n    rf = make_pipeline(MinMaxScaler(),RandomForestClassifier())\n    rf.fit(X_train, y_train)\n    rf_pred = rf.predict(X_test)\n    accuracy_score_rf = accuracy_score(y_test, rf_pred)\n    print(f\"Accuracy Score of Random Forest Classifier is : {accuracy_score_rf}\")\n    print('\\n')\n    print(confusion_matrix(y_test, rf_pred))\n    print('\\n')\n    print(classification_report(y_test, rf_pred))\n    \n    #SGD Classifier\n    sgd_clf = make_pipeline(MinMaxScaler(),SGDClassifier(max_iter=1000, tol=1e-3))\n    sgd_clf.fit(X_train, y_train)\n    sgd_pred = sgd_clf.predict(X_test)\n    sgd_accuracy_score = sgd_clf.score(X_test, y_test)\n    print(f\"Accuracy Score of SGD Classifier is : {sgd_accuracy_score}\")\n    print('\\n')\n    print(confusion_matrix(y_test, sgd_pred))\n    print('\\n')\n    print(classification_report(y_test, sgd_pred))\n    \n    # KNeighbors Classifier\n    knn = make_pipeline(MinMaxScaler(),KNeighborsClassifier())\n    knn.fit(X_train, y_train)\n    knn_pred = knn.predict(X_test)\n    knn_accuracy_score = accuracy_score(y_test, knn_pred)\n    print(f\"Accuracy Score of KNeighbors Classifier is : {knn_accuracy_score}\")\n    print('\\n')\n    print(confusion_matrix(y_test, knn_pred))\n    print('\\n')\n    print(classification_report(y_test, knn_pred))\n    \n    # Extra Trees Classifier\n    etc = make_pipeline(MinMaxScaler(),ExtraTreesClassifier())\n    etc.fit(X_train, y_train)\n    etc_pred = etc.predict(X_test)\n    etc_accuracy_score = accuracy_score(y_test, etc_pred)\n    print(f\"Accuracy Score of Extra Trees Classifier is : {etc_accuracy_score}\")\n    print('\\n')\n    print(confusion_matrix(y_test, etc_pred))\n    print('\\n')\n    print(classification_report(y_test, etc_pred))\n\n    # Gradient Boosting Classifier\n    gbc = make_pipeline(MinMaxScaler(),GradientBoostingClassifier())\n    gbc.fit(X_train, y_train)\n    gbc_pred = gbc.predict(X_test)\n    gbc_accuracy_score = accuracy_score(y_test, gbc_pred)\n    print(f\"Accuracy Score of Gradient Boosting Classifier is : {gbc_accuracy_score}\")\n    print('\\n')\n    print(confusion_matrix(y_test, gbc_pred))\n    print('\\n')\n    print(classification_report(y_test, gbc_pred))","4122167c":"modeller(X_train, X_test, y_train, y_test)","849b6c56":"print(y.value_counts())\ny.value_counts().plot(kind='bar')","e52a2d5e":"((y.value_counts()\/len(y))*100)","e53d0262":"# Separate majority and minority classes\ndf_majority = df[df.C==0]\ndf_minority = df[df.C==1]","1bc8f8d7":"from sklearn.utils import resample\n# Upsample minority class\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples=76353,    # to match majority class\n                                 random_state=123) # reproducible results\n \n# Combine majority class with upsampled minority class\ndf_upsampled = pd.concat([df_majority, df_minority_upsampled])\n \n# Display new class counts\ndf_upsampled.C.value_counts()","bd3d729e":"df_upsampled","ae951bb2":"duplicateRowsDF = df_upsampled[df_upsampled.duplicated()]\nprint(\"Duplicate Rows except first occurrence based on all columns are :\")\nprint(duplicateRowsDF)","edc4f41f":"from collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import SMOTE\nprint('Original dataset shape %s' % Counter(y))\n#Original dataset shape Counter({1: 900, 0: 100})\nsm = SMOTE(random_state=42)\nX_res, y_res = sm.fit_resample(X, y)\nprint('Resampled dataset shape %s' % Counter(y_res))\n#Resampled dataset shape Counter({0: 900, 1: 900})","d86cf13c":"duplicateRowsDF = X_res[X_res.duplicated()]\nprint(\"Duplicate Rows except first occurrence based on all columns are :\")\nprint(duplicateRowsDF)\nprint(y_res.value_counts())","5aa1bf0a":"smo_x = X_res.copy()\nsmo_x","788e994f":"from sklearn.model_selection import train_test_split\nsmo_X_train, smo_X_test, smo_y_train, smo_y_test = train_test_split(X_res, y_res, test_size = 0.2, random_state = 42)\nsmo_X_train.shape, smo_X_test.shape, smo_y_train.shape, smo_y_test.shape","41b601ef":"modeller(smo_X_train, smo_X_test, smo_y_train, smo_y_test)","2d62ffb0":"# Lets use Light GBM to train the model as well.\nfrom lightgbm import LGBMClassifier\nnp.random.seed(42)\nlgbm = make_pipeline(MinMaxScaler(),LGBMClassifier())\n#lgbm = LGBMClassifier().fit(X_train, y_train)\nlgbm.fit(smo_X_train, smo_y_train)\nlgbm.score(smo_X_test, smo_y_test)\nlgbm_pred = lgbm.predict(smo_X_test)\nprint(classification_report(smo_y_test, lgbm_pred))\nprint(accuracy_score(smo_y_test, lgbm_pred))","2b9a0de1":"# Lets try Catboost Classifier\nfrom catboost import CatBoostClassifier\n\nclf = CatBoostClassifier(\n    iterations=5, \n    learning_rate=0.1, \n    #loss_function='CrossEntropy'\n)\n\n\nclf.fit(smo_X_train, smo_y_train, \n        eval_set=(smo_X_test, smo_y_test), \n        verbose=False\n)\n\nprint('CatBoost model is fitted: ' + str(clf.is_fitted()))\nprint('CatBoost model parameters:')\nprint(clf.get_params())","953d763c":"from catboost import CatBoostClassifier\nclf = CatBoostClassifier(\n    iterations=10,\n#     verbose=5,\n)\n\nclf.fit(\n    smo_X_train, smo_y_train,\n    eval_set=(smo_X_test, smo_y_test),\n)","3fe45fec":"# Separate majority and minority classes\ndf_majority = df[df.C==0]\ndf_minority = df[df.C==1]\n \n# Downsample majority class\ndf_majority_downsampled = resample(df_majority, \n                                 replace=False,    # sample without replacement\n                                 n_samples=24827,     # to match minority class\n                                 random_state=123) # reproducible results\n \n# Combine minority class with downsampled majority class\ndf_downsampled = pd.concat([df_majority_downsampled, df_minority])\n \n# Display new class counts\ndf_downsampled.C.value_counts()","3006bafa":"df_downsampled","20f0f8ea":"down_X = df_downsampled.drop(['C','Index'], axis=1)\ndown_y = df_downsampled['C']","cac91c3f":"down_X_train, down_X_test, down_y_train, down_y_test = train_test_split(down_X, down_y, test_size = 0.2, random_state = 42)\ndown_X_train.shape, down_X_test.shape, down_y_train.shape, down_y_test.shape","e85b6313":"modeller(down_X_train, down_X_test, down_y_train, down_y_test)","c229e274":"import sklearn\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel","836c8360":"train_copy = train.copy()\ntrain_copy.drop('Index', axis=1, inplace=True)","f6ca6d88":"train_copy","98c90ef3":"import datetime as dt\ntrain_copy['F15']=train_copy['F15'].map(dt.datetime.toordinal)\ntrain_copy['F16']=train_copy['F16'].map(dt.datetime.toordinal)","d9e368d4":"eda_X = train_copy.drop('C', axis=1)\neda_y = train_copy['C']","3b6dd99c":"eda_y","e2e85ffe":"from collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import SMOTE # doctest: +NORMALIZE_WHITESPACE\nprint('Original dataset shape %s' % Counter(y))\n#Original dataset shape Counter({1: 900, 0: 100})\nsm = SMOTE(random_state=42)\nX_res, y_res = sm.fit_resample(eda_X, eda_y)\nprint('Resampled dataset shape %s' % Counter(y_res))\n#Resampled dataset shape Counter({0: 900, 1: 900})","bb41f0cb":"train_copy = X_res\ntrain_copy['C'] = pd.Series(y_res)\ntrain_copy","71c2a662":"from sklearn.model_selection import train_test_split\ntrain_1, test_1 = train_test_split(train_copy, test_size = 0.2)","f3302493":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nClassifiers = [LogisticRegression(C=10),DecisionTreeClassifier(),RandomForestClassifier(n_estimators=200),GaussianNB(),KNeighborsClassifier(n_neighbors=3),GradientBoostingClassifier(n_estimators=200),ExtraTreesClassifier(n_estimators=200)]","363e5f92":"features = train_1.iloc[:,0:22]\nlabel = train_1['C']\nclf = ExtraTreesClassifier()\nclf = clf.fit(features, label)\nmodel = SelectFromModel(clf, prefit=True)\nNew_features = model.transform(features)\nprint(New_features.shape)","5133f776":"from sklearn.svm import LinearSVC\nlsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(features, label)\nmodel_2 = SelectFromModel(lsvc, prefit=True)\nNew_features_2 = model_2.transform(features)\nprint(New_features_2.shape)","7120ec96":"from sklearn.metrics import accuracy_score\nimport timeit\ntest_features= test_1.iloc[:,0:22]\nTime_1=[]\nModel_1=[]\nOut_Accuracy_1=[]\nfor clf in Classifiers:\n    start_time = timeit.default_timer()\n    fit=clf.fit(features,label)\n    pred=fit.predict(test_features)\n    print(clf.__class__.__name__)\n    print(classification_report(test_1['C'],pred,target_names=['0','1']))\n    print(confusion_matrix(test_1['C'], pred))\n    print(\"\\n\")\n    elapsed = timeit.default_timer() - start_time\n    Time_1.append(elapsed)\n    Model_1.append(clf.__class__.__name__)\n    Out_Accuracy_1.append(accuracy_score(test_1['C'],pred))","375561f5":"test_features= model_2.transform(test_1.iloc[:,0:22])\nTime_3=[]\nModel_3=[]\nOut_Accuracy_3=[]\nfor clf in Classifiers:\n    start_time = timeit.default_timer()\n    fit=clf.fit(New_features_2,label)\n    pred=fit.predict(test_features)\n    print(clf.__class__.__name__)\n    print(classification_report(test_1['C'],pred,target_names=['0','1']))\n    print(confusion_matrix(test_1['C'], pred))\n    print(\"\\n\")\n    elapsed = timeit.default_timer() - start_time\n    Time_3.append(elapsed)\n    Model_3.append(clf.__class__.__name__)\n    Out_Accuracy_3.append(accuracy_score(test_1['C'],pred))","c6857d6a":"test_features= model.transform(test_1.iloc[:,0:22])\nTime_2=[]\nModel_2=[]\nOut_Accuracy_2=[]\nfor clf in Classifiers:\n    start_time = timeit.default_timer()\n    fit=clf.fit(New_features,label)\n    pred=fit.predict(test_features)\n    print(clf.__class__.__name__)\n    print(classification_report(test_1['C'],pred,target_names=['0','1']))\n    print(confusion_matrix(test_1['C'], pred))\n    print(\"\\n\")\n    elapsed = timeit.default_timer() - start_time\n    Time_2.append(elapsed)\n    Model_2.append(clf.__class__.__name__)\n    Out_Accuracy_2.append(accuracy_score(test_1['C'],pred))","78932d33":"import matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\nind =  np.arange(7)   # the x locations for the groups\nwidth = 0.1     # the width of the bars\nfig, ax = plt.subplots()\nrects1 = ax.bar(ind, Out_Accuracy_1, width, color='r')\nrects2 = ax.bar(ind + width, Out_Accuracy_2, width, color='y')\nrects3 = ax.bar(ind + width + width ,Out_Accuracy_3, width, color='b')\nax.set_ylabel('Accuracy')\nax.set_title('Accuracy by Models and Selection Process')\nax.set_xticks(ind + width)\nax.set_xticklabels(Model_3,rotation=60)\nplt.show()","ecee911e":"import matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\nind =  np.arange(7)   # the x locations for the groups\nwidth = 0.1       # the width of the bars\nfig, ax = plt.subplots()\nrects1 = ax.bar(ind, Time_1, width, color='r')\nrects2 = ax.bar(ind + width, Time_2, width, color='y')\nrects3 = ax.bar(ind + width + width ,Time_3, width, color='b')\nax.set_ylabel('Running Time')\nax.set_title('Time by Models and Selection Process')\nax.set_xticks(ind + width)\nax.set_xticklabels(Model_3,rotation=60)\nax.legend((rects1[0], rects2[0],rects3[0]), ('No Selection', 'Tree_Based','L1_Based'))\nplt.show()","f99d5533":"test","104f824d":"test_copy = test.drop('Index', axis=1)\ntest_copy","be2ee1c8":"test_copy['F15']=test_copy['F15'].map(dt.datetime.toordinal)\ntest_copy['F16']=test_copy['F16'].map(dt.datetime.toordinal)\ntest_copy","9561cff5":"np.random.seed(42)\nrf = RandomForestClassifier(n_estimators=200)\nrf.fit(smo_X_train, smo_y_train)\nrf_pred = rf.predict(smo_X_test)\nprint(rf.score(smo_X_test, smo_y_test))\nprint(classification_report(smo_y_test, rf_pred))","d67806d3":"rf.feature_importances_","ddffe6c6":"import seaborn as sns\n\n# Helper function for plotting feature importance\ndef plot_features(columns, importances, n=10):\n    df = (pd.DataFrame({\"features\": columns,\n                        \"feature_importance\": importances})\n          .sort_values(\"feature_importance\", ascending=False)\n          .reset_index(drop=True))\n    \n    sns.barplot(x=\"feature_importance\",\n                y=\"features\",\n                data=df[:n],\n                orient=\"h\")","52d330e4":"plot_features(smo_X_train.columns, rf.feature_importances_)","407c7022":"from sklearn.metrics import fbeta_score, precision_score, recall_score, roc_auc_score, f1_score","e45591f6":"def show_scores(model):\n    train_pred = model.predict(smo_X_train)\n    val_pred = model.predict(smo_X_test)\n    #If model performs better on validation dataset, it means the model is overfitting.\n    scores = {\"Training F-Beta Score\":fbeta_score(smo_y_train, train_pred, average='weighted', beta=2),\n              \"Validation F-Beta Score\":fbeta_score(smo_y_test, val_pred, average='weighted', beta=2),\n              \"Training ROC AUC Score\":roc_auc_score(smo_y_train, train_pred),\n              \"Validation ROC AUC Score\":roc_auc_score(smo_y_test, val_pred)}\n    return scores","64b3c854":"show_scores(rf)","c8927f87":"final_pred = rf.predict(test_copy)\nfinal_pred = pd.DataFrame(final_pred)\nfinal_pred.columns = ['Predictions']","90bb362c":"subs = test['Index']\nsubs = pd.DataFrame(subs)\nsubs","b259a29e":"bigdata = pd.concat([subs, final_pred], ignore_index=True, sort=False, axis=1)","6a4889b7":"bigdata","43de10dd":"I've made a habit of making of copy before doing anything, in case I mess up something.","c28482a4":"L1-based feature selection:","ec254fab":"Now we make predictions on the test set","5df8c6f2":"Lets check if it has worked","31d0ffc2":"## Runtime of the models:","08053555":"Looks like around 52,644 duplicate datapoints are created, and they also lead to bias when you fit the model on this data. So I won't be using this data to train the model.\n\nInstead lets use SMOTE to balance the target classes.","4ab6bac2":"Looks all ... numbery. Lets try and visualize it.","26c00b33":"Seems like there are no missing values in the data.","f6fddb79":"# Alternate Approach.\n\nNow I've used different approach here in this section. The results are quite similar.\n\nHere we will cover different methods of feature selection: \n1. No Feature Selection.\n2. L1-based feature selection:\n3. Tree-Based feature selection:","97199ce6":"Let's take a look at the dimensions of the data.","46bdd66e":"# SMOTE\n\nSMOTE is an oversampling technique that generates synthetic samples from the minority class. It is used to obtain a synthetically class-balanced or nearly class-balanced training set, which is then used to train the classifier. ","2b919a82":"Splitting the data into X & y, which will make it easier for us to further split it into train and test set.","c3feea19":"Results are not as good compared to the data where we used`SMOTE` to balance the data.","1b6c703e":"Now I've built a function, that takes in `X_train, X_test, y_train, y_test` and fits different model on them, and returns back `accuracy_score, confusion_matrix, classification_report`.\n\nNow I've built two functions for modelling purposes :\n1. Where I've used `StandardScaler() - modeller_standardized`\n2. Where I've used `MinMaxScaler() - modeller_normalized`","037811ff":"Let's start with import the necessary models, and we'll import them later when necessary.","ae5bad91":"We can see that there are negative values as well in our data, we'll standardize\/normalize them while building our model.","88817cee":"Tree-Based feature selection:","0ede2f3d":"After trying to fit the model, I came across `invalid type error`, which was because of `F15, F16` features, which were in `datetime` format, so I re-worked on those to make sure, I won't face such errors again.","4073e002":"Lets check for missing values in the data ","33485ddd":"This gives us the idea of time taken by models to fit on X & Y.","3cdd177a":"## Upsampling \nNow that we've tried different models and there's no significant improvement evaluation metrics of the model, as you can see that all models have poor precision and recall. Lets try upsampling the target variable `C`, because it's imbalanced.\n\nLet's take a look at target class distribution.","678a8ffd":"## Accuracy of the models\n\nNow lets see the accuracy of ML models, visually.\n","2f228845":"As we can see that there's already improvement in terms of precision,recall and F1-score.\n\nNow that we were able to improve the model, let's try and see what happens if we under sample\/down sample, if the results will be just as good.\n\n# Down Sample","15eed867":"We can see that, of all the models RandomForestClassifier has the best accuracy.","24da65ae":"Let's take a look at different evaluation metrics for classification","cfaa9191":"Loading the datasets.","2e3b92f0":"Now df.head().T, gives us better idea of the data we're dealing with.","9ee56afb":"Nice, it has worked.\n\nBut it comes at a cost, it creates a lot of duplicates.\n\nLets see if there are duplicates created.","95544f69":"Wow ! It worked just as fine, now lets see if there are duplicates","15c43db7":"Target variable is imbalanced, with 75.46% of 0s and 24.53% of 1s.","e1aeb647":"Let's now split the data into train and test set.","339c133c":"Without Feature Selection:","c5a1fabf":"Looks good, lets fit our `modeller` function to this training and test set.","23422dc6":"Lets take a look at the distribution of our target variable","0d709f8c":"As we can see from the plot above, these are the top 10 important features.","a8b5dfd5":"Nice ! This works like a charm because, there are no duplicate values created here unlike created by upsampling.\n\nThis seems more robust. Now we use this data to train the model.","b2702235":"# Final Predictions\n\nNow time to make predictions on the test data","3f2c2c0e":"Splitting into X & Y.","55f5fc24":"Lets take a look at our down sampled data.","53dedc67":"Lets take a look at important features according to `.RandomForestClassifier`.","c0eed537":"# HaikuJAM Data Science Case Study.\n\n \nIt is an e-commerce dataset where each record is a combination of some user 'u' & some item 'i' & the task is to predict if user 'u' will buy item 'i' or not.\n\n \nThere are various features in the dataset but we don't know which feature is what hence, they are just labelled F1, F2, .., F22. We've been shared 2 files named, BuyAffinity_Train & BuyAffinity_Test. The train file has the label for the data points & the test file needs to be labelled based on your predictions."}}