{"cell_type":{"f584277f":"code","d3013e58":"code","b7ab8734":"code","e34cf4ca":"code","b85928e7":"code","76248639":"code","9dbffc19":"code","01bb5468":"code","bad11b82":"code","3971b994":"code","c68b27d1":"code","5ebbac35":"code","9742e9ef":"code","8485199f":"code","ab9d60e8":"code","17efe308":"code","2dbc3dd9":"code","cec73ad6":"code","f6b6cb9c":"code","341c03d3":"code","14dfeb7c":"code","3e75053b":"code","68abff0d":"code","7084be34":"code","be63bb6d":"code","4ee6e08b":"code","4903a03d":"code","1312093e":"code","bcf496bf":"code","4515803f":"code","c4377494":"code","421dd23a":"code","d73a7f28":"code","a087a146":"code","cd5e966f":"code","0db5c576":"code","5e871d36":"code","6269a2ba":"code","5b36148c":"code","41585f79":"code","4e45eda1":"code","6e796584":"code","90651803":"code","749458b3":"code","be232642":"code","abb42a5f":"code","dd033d6c":"code","af7c11eb":"markdown","7a8230e0":"markdown","480df54a":"markdown","109cda6d":"markdown","06a37727":"markdown","87edb1ff":"markdown","75082710":"markdown","8d31f4b7":"markdown","dd0cfcb4":"markdown","9fa85333":"markdown","082e59ea":"markdown","5f918764":"markdown","e8226598":"markdown","fd7ad465":"markdown","9835315e":"markdown","2b5f3fcb":"markdown","81eddbd1":"markdown","c8d67de6":"markdown","e8722244":"markdown","6156c514":"markdown","37a5613d":"markdown","de64a990":"markdown","91af8212":"markdown","35c7176b":"markdown"},"source":{"f584277f":"import pandas as pd\nimport numpy as np\nimport random\nimport warnings\nimport time\nimport datetime\nimport re\nimport string\nimport itertools\nimport pickle\nimport joblib\nimport nltk\nimport csv\n\nfrom nltk.corpus import stopwords, wordnet\nstop = set(stopwords.words('english'))\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\nfrom collections import Counter, defaultdict\n\nimport tensorflow as tf\nimport keras\nimport keras.backend as K\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Concatenate, Conv2D, Flatten, Dense, Embedding, LSTM\nfrom keras.models import Model\nfrom keras.utils import np_utils\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nfrom keras.regularizers import l2\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\nfrom keras.models import Model\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import Concatenate\nfrom keras.layers.core import Lambda, Flatten, Dense\nfrom keras.initializers import glorot_uniform\nfrom keras.layers import Input, Dense, Flatten, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Add, Conv2D\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","d3013e58":"df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf.drop(['keyword', 'location', 'id'], axis=1, inplace=True)\ndisplay(df.sample(5))","b7ab8734":"df.shape","e34cf4ca":"df['target'].value_counts()","b85928e7":"def remove_url(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    return re.sub(html, '', text)\n\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\ndf['clean_text'] = df['text'].apply(lambda x: remove_url(str(x)))\ndf['clean_text'] = df['clean_text'].apply(lambda x: remove_emoji(str(x)))\ndf['clean_text'] = df['clean_text'].apply(lambda x: remove_html(str(x)))\ndf['clean_text'] = df['clean_text'].apply(lambda x: remove_punct(str(x)))\ndf['clean_text'] = df['clean_text'].apply(word_tokenize)\ndf['clean_text'] = df['clean_text'].apply(lambda x: [word.lower() for word in x])\ndf['clean_text'] = df['clean_text'].apply(lambda x: [word for word in x if word not in stop])\ndf['clean_text'] = df['clean_text'].apply(nltk.tag.pos_tag)\n\ndef get_wordnet_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n    \ndf['clean_text'] = df['clean_text'].apply(\n    lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\nwnl = WordNetLemmatizer()\ndf['clean_text'] = df['clean_text'].apply(\n    lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\ndf['clean_text'] = df['clean_text'].apply(\n    lambda x: [word for word in x if word not in stop])\ndf['clean_text'] = [' '.join(map(str, l)) for l in df['clean_text']]","76248639":"display(df.sample(2))","9dbffc19":"disaster_initial = list(df[df['target'] == 1]['clean_text'])\nnon_disaster_initial = list(df[df['target'] == 0]['clean_text'])\n\n# Selecting only 1000 samples for siamese model\ndisaster = disaster_initial[:1000]\nnon_disaster = non_disaster_initial[:1000]\n\n# Creating pairs of data for siamese training => label 1 if pairs from same class otherwise 0\ndf2 = pd.DataFrame(columns=['text1', 'text2', 'label'])\n\nfor data in disaster:\n  data1 = data\n  data2 = random.choice(disaster)\n  data3 = random.choice(non_disaster)\n\n  df2.loc[len(df2)] = [data1, data2, 1]\n  df2.loc[len(df2)] = [data1, data3, 0]\n\n\nfor data in non_disaster:\n  data1 = data\n  data2 = random.choice(non_disaster)\n  data3 = random.choice(disaster)\n  \n  df2.loc[len(df2)] = [data1, data2, 1]\n  df2.loc[len(df2)] = [data1, data3, 0]","01bb5468":"df2.shape","bad11b82":"display(df2.sample(5))","3971b994":"X_train, X_val, y_train, y_val = train_test_split(df2[['text1', 'text2']], df2['label'], test_size=0.2, random_state=0)\nprint(X_train.shape, X_val.shape, y_train.shape, y_val.shape)","c68b27d1":"X_train['text'] = X_train[['text1', 'text2']].apply(lambda x: str(x[0])+\" \"+str(x[1]), axis=1)","5ebbac35":"t = Tokenizer()\nt.fit_on_texts(X_train['text'].values)\n\nX_train['text1'] = X_train['text1'].astype(str)\nX_train['text2'] = X_train['text2'].astype(str)\nX_val['text1'] = X_val['text1'].astype(str)\nX_val['text2'] = X_val['text2'].astype(str)\n\ntrain_q1_seq = t.texts_to_sequences(X_train['text1'].values)\ntrain_q2_seq = t.texts_to_sequences(X_train['text2'].values)\nval_q1_seq = t.texts_to_sequences(X_val['text1'].values)\nval_q2_seq = t.texts_to_sequences(X_val['text2'].values)\n\nmax_len = 200\ntrain_q1_seq = pad_sequences(train_q1_seq, maxlen=max_len, padding='post')\ntrain_q2_seq = pad_sequences(train_q2_seq, maxlen=max_len, padding='post')\nval_q1_seq = pad_sequences(val_q1_seq, maxlen=max_len, padding='post')\nval_q2_seq = pad_sequences(val_q2_seq, maxlen=max_len, padding='post')","9742e9ef":"#https:\/\/blog.keras.io\/using-pre-trained-word-embeddings-in-a-keras-model.html\nembeddings_index = {}\nf = open('..\/input\/glove6b300dtxt\/glove.6B.300d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","8485199f":"not_present_list = []\nvocab_size = len(t.word_index) + 1\nprint('Loaded %s word vectors.' % len(embeddings_index))\nembedding_matrix = np.zeros((vocab_size, len(embeddings_index['no'])))\nfor word, i in t.word_index.items():\n    if word in embeddings_index.keys():\n        embedding_vector = embeddings_index.get(word)\n    else:\n        not_present_list.append(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n    else:\n        embedding_matrix[i] = np.zeros(300)","ab9d60e8":"print(embedding_matrix.shape)","17efe308":"def euclidean_distance(vectors):\n    # unpack the vectors into separate lists\n    (featsA, featsB) = vectors\n    # compute the sum of squared distances between the vectors\n    sumSquared = K.sum(K.square(featsA - featsB), axis=1, keepdims=True)\n    # return the euclidean distance between the vectors\n    return K.sqrt(K.maximum(sumSquared, K.epsilon()))\n\ndef contrastive_loss(y, preds, margin=1):\n    # explicitly cast the true class label data type to the predicted\n    # class label data type (otherwise we run the risk of having two\n    # separate data types, causing TensorFlow to error out)\n    y = tf.cast(y, preds.dtype)\n    # calculate the contrastive loss between the true labels and\n    # the predicted labels\n    squaredPreds = K.square(preds)\n    squaredMargin = K.square(K.maximum(margin - preds, 0))\n    loss = K.mean(y * squaredPreds + (1 - y) * squaredMargin)\n    # return the computed contrastive loss to the calling function\n    return loss","2dbc3dd9":"def build_network():\n\n  network = Sequential()\n  network.add(Embedding(name=\"synopsis_embedd\",input_dim =len(t.word_index)+1, \n                       output_dim=len(embeddings_index['no']),weights=[embedding_matrix], \n                       input_length=train_q1_seq.shape[1],trainable=False))\n  network.add(LSTM(64,return_sequences=True, activation=\"relu\"))\n  network.add(Flatten())\n  network.add(Dense(128, activation='relu',\n                  kernel_regularizer=l2(1e-3),\n                  kernel_initializer='he_uniform'))\n  \n  network.add(Dense(2, activation=None,\n                  kernel_regularizer=l2(1e-3),\n                  kernel_initializer='he_uniform'))\n  \n  #Force the encoding to live on the d-dimentional hypershpere\n  network.add(Lambda(lambda x: K.l2_normalize(x,axis=-1)))\n\n  return network","cec73ad6":"input_1 = Input(shape=(train_q1_seq.shape[1],))\ninput_2 = Input(shape=(train_q2_seq.shape[1],))\n\nnetwork = build_network()\n\nencoded_input_1 = network(input_1)\nencoded_input_2 = network(input_2)\n\ndistance = Lambda(euclidean_distance)([encoded_input_1, encoded_input_2])\n\n# Connect the inputs with the outputs\nmodel = Model([input_1, input_2], distance)\n\nmodel.compile(loss=contrastive_loss, optimizer=Adam(0.001))","f6b6cb9c":"y_train = np.asarray(y_train).astype('float32')\ny_val = np.asarray(y_val).astype('float32')","341c03d3":"model.fit([train_q1_seq,train_q2_seq],y_train.reshape(-1,1), epochs = 5, \n          batch_size=64,validation_data=([val_q1_seq, val_q2_seq],y_val.reshape(-1,1)))","14dfeb7c":"# Save model for further use\n# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"siamesemodel-contrastive-loss.json\", \"w\") as json_file:\n    json_file.write(model_json)\n#serialize weights to HDF5\nmodel.save_weights(\"siamesemodel-contrastive-loss.h5\")\nprint(\"Saved model to disk\")\n\n# load json and create model\n# json_file = open('siamesemodel-contrastive-loss.json', 'r')\n# loaded_model_json = json_file.read()\n# json_file.close()\n# loaded_model = model_from_json(loaded_model_json)\n# # load weights into new model\n# loaded_model.load_weights(\"siamesemodel-contrastive-loss.h5\")\n# print(\"Loaded model from disk\")","3e75053b":"non_disaster_initial[122]","68abff0d":"prediction_data = \"usdjpy despite richter scale overbought fxstreet line swissquote bullish long jason sen mr ambulance chaser b\u2026\"\nprediction_vector = t.texts_to_sequences([prediction_data])\nprediction_vector = pad_sequences(prediction_vector,maxlen=200)\n\nassistant_data = disaster[11]\nassistant_vector = t.texts_to_sequences([assistant_data])\nassistant_vector = pad_sequences(assistant_vector,maxlen=200)\n\nmodel.predict([prediction_vector, assistant_vector])","7084be34":"prediction_data = \"usdjpy despite richter scale overbought fxstreet line swissquote bullish long jason sen mr ambulance chaser b\u2026\"\nprediction_vector = t.texts_to_sequences([prediction_data])\nprediction_vector = pad_sequences(prediction_vector,maxlen=200)\n\nassistant_data = non_disaster[11]\nassistant_vector = t.texts_to_sequences([assistant_data])\nassistant_vector = pad_sequences(assistant_vector,maxlen=200)\n\nmodel.predict([prediction_vector, assistant_vector])","be63bb6d":"disaster_initial[12]","4ee6e08b":"prediction_data = \"cameroon bir soldier 05012020 invaded southerncameroons village kimar set ablaze total of\"\nprediction_vector = t.texts_to_sequences([prediction_data])\nprediction_vector = pad_sequences(prediction_vector,maxlen=200)\n\nassistant_data = disaster[11]\nassistant_vector = t.texts_to_sequences([assistant_data])\nassistant_vector = pad_sequences(assistant_vector,maxlen=200)\n\nmodel.predict([prediction_vector, assistant_vector])","4903a03d":"prediction_data = \"cameroon bir soldier 05012020 invaded southerncameroons village kimar set ablaze total of\"\nprediction_vector = t.texts_to_sequences([prediction_data])\nprediction_vector = pad_sequences(prediction_vector,maxlen=200)\n\nassistant_data = non_disaster[11]\nassistant_vector = t.texts_to_sequences([assistant_data])\nassistant_vector = pad_sequences(assistant_vector,maxlen=200)\n\nmodel.predict([prediction_vector, assistant_vector])","1312093e":"disaster_initial = list(df[df['target'] == 1]['clean_text'])\nnon_disaster_initial = list(df[df['target'] == 0]['clean_text'])\n\n# Taking 1000 samples from the entire data\ndisaster = disaster_initial[:1000]\nnon_disaster = non_disaster_initial[:1000]\n\n# Creating pairs of data for siamese training => labels for identity loss and class for disaster or non disaster\ndf2 = pd.DataFrame(columns=['Anchor', 'Positive', 'Negative', 'label', 'class'])\n\nfor data in disaster:\n  a = data\n  p = random.choice(disaster)\n  n = random.choice(non_disaster)\n\n  df2.loc[len(df2)] = [a, p, n, 1, 1]\n\n\nfor data in non_disaster:\n  a = data\n  p = random.choice(non_disaster)\n  n = random.choice(disaster)\n  \n  df2.loc[len(df2)] = [a, p, n, 1, 0]","bcf496bf":"df2.shape","4515803f":"display(df2.sample(5))","c4377494":"X, X_test, y, y_test = train_test_split(df2[['Anchor', 'Positive', 'Negative']], df2[['label', 'class']], test_size=0.2, random_state=0)\nX_train, X_val, y_train, y_val = train_test_split(X[['Anchor', 'Positive', 'Negative']], y[['label', 'class']], test_size=0.2, random_state=0)\nprint(X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape)","421dd23a":"X_train['text'] = X_train[['Anchor', 'Positive', 'Negative']].apply(lambda x: str(x[0])+\" \"+str(x[1])+\" \"+str(x[2]), axis=1)","d73a7f28":"t = Tokenizer()\nt.fit_on_texts(X_train['text'].values)\n\nX_train['Anchor'] = X_train['Anchor'].astype(str)\nX_train['Positive'] = X_train['Positive'].astype(str)\nX_train['Negative'] = X_train['Negative'].astype(str)\nX_val['Anchor'] = X_val['Anchor'].astype(str)\nX_val['Positive'] = X_val['Positive'].astype(str)\nX_val['Negative'] = X_val['Negative'].astype(str)\nX_test['Anchor'] = X_test['Anchor'].astype(str)\nX_test['Positive'] = X_test['Positive'].astype(str)\nX_test['Negative'] = X_test['Negative'].astype(str)\n\ntrain_q1_seq = t.texts_to_sequences(X_train['Anchor'].values)\ntrain_q2_seq = t.texts_to_sequences(X_train['Positive'].values)\ntrain_q3_seq = t.texts_to_sequences(X_train['Negative'].values)\nval_q1_seq = t.texts_to_sequences(X_val['Anchor'].values)\nval_q2_seq = t.texts_to_sequences(X_val['Positive'].values)\nval_q3_seq = t.texts_to_sequences(X_val['Negative'].values)\ntest_q1_seq = t.texts_to_sequences(X_test['Anchor'].values)\ntest_q2_seq = t.texts_to_sequences(X_test['Positive'].values)\ntest_q3_seq = t.texts_to_sequences(X_test['Negative'].values)\n\nmax_len = 200\ntrain_q1_seq = pad_sequences(train_q1_seq, maxlen=max_len, padding='post')\ntrain_q2_seq = pad_sequences(train_q2_seq, maxlen=max_len, padding='post')\ntrain_q3_seq = pad_sequences(train_q3_seq, maxlen=max_len, padding='post')\nval_q1_seq = pad_sequences(val_q1_seq, maxlen=max_len, padding='post')\nval_q2_seq = pad_sequences(val_q2_seq, maxlen=max_len, padding='post')\nval_q3_seq = pad_sequences(val_q3_seq, maxlen=max_len, padding='post')\ntest_q1_seq = pad_sequences(test_q1_seq, maxlen=max_len, padding='post')\ntest_q2_seq = pad_sequences(test_q2_seq, maxlen=max_len, padding='post')\ntest_q3_seq = pad_sequences(test_q3_seq, maxlen=max_len, padding='post')","a087a146":"#https:\/\/blog.keras.io\/using-pre-trained-word-embeddings-in-a-keras-model.html\nembeddings_index = {}\nf = open('..\/input\/glove6b300dtxt\/glove.6B.300d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","cd5e966f":"not_present_list = []\nvocab_size = len(t.word_index) + 1\nprint('Loaded %s word vectors.' % len(embeddings_index))\nembedding_matrix = np.zeros((vocab_size, len(embeddings_index['no'])))\nfor word, i in t.word_index.items():\n    if word in embeddings_index.keys():\n        embedding_vector = embeddings_index.get(word)\n    else:\n        not_present_list.append(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n    else:\n        embedding_matrix[i] = np.zeros(300)","0db5c576":"print(embedding_matrix.shape)","5e871d36":"def identity_loss(y_true, y_pred):\n    return K.mean(y_pred)\n\ndef triplet_loss(x, alpha = 0.2):\n    # Triplet Loss function.\n    anchor,positive,negative = x\n    # distance between the anchor and the positive\n    pos_dist = K.sum(K.square(anchor-positive),axis=1)\n    # distance between the anchor and the negative\n    neg_dist = K.sum(K.square(anchor-negative),axis=1)\n    # compute loss\n    basic_loss = pos_dist-neg_dist+alpha\n    loss = K.maximum(basic_loss,0.0)\n    return loss","6269a2ba":"def embedding_model():\n\n  network = Sequential()\n  network.add(Embedding(name=\"synopsis_embedd\",input_dim =len(t.word_index)+1, \n                       output_dim=len(embeddings_index['no']),weights=[embedding_matrix], \n                       input_length=train_q1_seq.shape[1],trainable=False))\n  network.add(LSTM(64,return_sequences=True, activation=\"relu\"))\n  network.add(Flatten())\n  network.add(Dense(128, activation='relu',\n                  kernel_regularizer=l2(1e-3),\n                  kernel_initializer='he_uniform'))\n  \n  network.add(Dense(2, activation=None,\n                  kernel_regularizer=l2(1e-3),\n                  kernel_initializer='he_uniform'))\n  \n  #Force the encoding to live on the d-dimentional hypershpere\n  # network.add(Lambda(lambda x: K.l2_normalize(x,axis=-1)))\n\n  return network\n\n\ndef build_network(base_model):\n\n  input_1 = Input(shape=(train_q1_seq.shape[1],))\n  input_2 = Input(shape=(train_q2_seq.shape[1],))\n  input_3 = Input(shape=(train_q3_seq.shape[1],))\n\n  A = base_model(input_1)\n  P = base_model(input_2)\n  N = base_model(input_3)\n\n  loss = Lambda(triplet_loss)([A, P, N])\n  model = Model(inputs = [input_1, input_2, input_3], outputs = loss)\n  model.compile(loss = identity_loss, optimizer = Adam(0.001))\n  return model","5b36148c":"base_model = embedding_model()\nmodel = build_network(base_model)\nmodel.summary()","41585f79":"y_train_label = np.asarray(y_train['label']).astype('float32')\ny_val_label = np.asarray(y_val['label']).astype('float32')\ny_test_label = np.asarray(y_test['label']).astype('float32')\n\ny_train_class = np.asarray(y_train['class']).astype('float32')\ny_val_class = np.asarray(y_val['class']).astype('float32')\ny_test_class = np.asarray(y_test['class']).astype('float32')","4e45eda1":"history = model.fit([train_q1_seq,train_q2_seq, train_q3_seq],y_train_label.reshape(-1,1), epochs = 5, \n          batch_size=64,validation_data=([val_q1_seq, val_q2_seq, val_q3_seq],y_val_label.reshape(-1,1)))","6e796584":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Training and Validation Losses',size = 12)\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper right')\nplt.show()","90651803":"# Save model for further use\n# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"siamesemodel-triplet-loss.json\", \"w\") as json_file:\n    json_file.write(model_json)\n#serialize weights to HDF5\nmodel.save_weights(\"siamesemodel-triplet-loss.h5\")\nprint(\"Saved model to disk\")\n\n# load json and create model\n# json_file = open('siamesemodel-triplet-loss.json', 'r')\n# loaded_model_json = json_file.read()\n# json_file.close()\n# loaded_model = model_from_json(loaded_model_json)\n# # load weights into new model\n# loaded_model.load_weights(\"siamesemodel-triplet-loss.h5\")\n# print(\"Loaded model from disk\")","749458b3":"X_train_eval = base_model.predict(train_q1_seq)\nX_test_eval = base_model.predict(test_q1_seq)\n\n# TSNE - dimensionality reduction for data visualization\ntsne = TSNE()\ntrain_tsne_embeds = tsne.fit_transform(X_train_eval)","be232642":"def scatter(x, labels, subtitle=None):\n    # Create a scatter plot of all the \n    # the embeddings of the model.\n    # We choose a color palette with seaborn.\n    palette = np.array(sns.color_palette(\"hls\", 2))\n    # We create a scatter plot.\n    f = plt.figure(figsize=(8, 8))\n    ax = plt.subplot(aspect='equal')\n    sc = ax.scatter(x[:,0], x[:,1], lw=0,alpha = 0.5, s=40,\n                    c=palette[labels.astype(np.int)] )\n    plt.xlim(-25, 25)\n    plt.ylim(-25, 25)\n    ax.axis('off')\n    ax.axis('tight')\n\nscatter(train_tsne_embeds, y_train_class)","abb42a5f":"# classifier that computes the class of a specific embedding for prediction purposes\nclassifier_input = Input(shape=(2,))\nclassifier_output = Dense(2, activation='softmax')(classifier_input)\nclassifier_model = Model(classifier_input, classifier_output)\n\n# onehot encoding vectors to 2 classes\nY_train_onehot = np_utils.to_categorical(y_train_class, 2)\nY_test_onehot = np_utils.to_categorical(y_test_class, 2)\n\nclassifier_model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\nclassifier_model.fit(X_train_eval,Y_train_onehot, validation_data=(X_test_eval, Y_test_onehot),epochs=5)","dd033d6c":"input = X_test_eval[:10]\nprediction = classifier_model.predict(input)\nprint(prediction)","af7c11eb":"## Data Preparation","7a8230e0":"## Glove Embeddings","480df54a":"## Siamese Model","109cda6d":"## Prediction","06a37727":"### Non Disaster Tweet","87edb1ff":"## Data Preparation","75082710":"### Disaster Tweet","8d31f4b7":"## Glove Embeddings","dd0cfcb4":"## Data Split","9fa85333":"## Data Split","082e59ea":"So in this case, although the data set is large enough to perform text classification using general neural netowrks, siamese is just used for information purposes and to see how different types of losses affect training and finally looking at the evaluation part.","5f918764":"# Data Cleaning","e8226598":"# Contrastive Loss","fd7ad465":"## Evaluation","9835315e":"Siamese Neural Networks are type of neural networks that contain multiple instances of same model, share the same architecture and weights. This type of neural network works well when the training data is less and hence the concept of one shot learning and few shot learning comes into the picture which are concepts which utilize the siamese model architecture. \nThe architecture of a general neural network is shown below. ","2b5f3fcb":"While a basic architecture of a Siamese model is shown below.","81eddbd1":"# Data Loading","c8d67de6":"# Import Libraries","e8722244":"![image.png](attachment:ef87e33f-db26-4e50-833a-87aad58499eb.png)","6156c514":"## Prediction","37a5613d":"## Siamese Model","de64a990":"# Triplet Loss","91af8212":"![image.png](attachment:37d536b0-81af-4140-96ef-938c69120fe2.png)","35c7176b":"# Text Classification using Siamese Neural Network"}}