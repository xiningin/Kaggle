{"cell_type":{"f25729e9":"code","fc3dc17e":"code","3808668d":"code","bfbc2a76":"code","4c6242f4":"code","5e54f3b5":"code","9849be37":"code","25b83a30":"code","fbc919ae":"code","2cb845ac":"code","6318238e":"code","67a8d161":"code","48176a15":"code","11a21ab7":"code","d16e4af1":"code","b1c94f5a":"code","f78cac23":"code","08ba674d":"code","e80728dc":"code","4204db36":"code","49ed1c1f":"code","ecbf4a0f":"code","4583d7d2":"code","5ab6804d":"code","d358e5c0":"code","045e208b":"code","37274adf":"code","a6f14830":"code","01b9fd16":"code","a19261de":"code","a3de7b40":"code","d4d1aa4c":"code","1d2ae9a6":"code","52b7d4b5":"code","aa2c373b":"code","c8b07b15":"markdown","7904ae91":"markdown","d1282709":"markdown","611489dd":"markdown","d49aff49":"markdown","b16ae52a":"markdown","afc2592d":"markdown","e44d8816":"markdown","157c0fdb":"markdown","8047ba73":"markdown","45cec426":"markdown","570b0cdc":"markdown","b13624ba":"markdown","f97be701":"markdown","bd7d9b26":"markdown","e73dac3b":"markdown","e9d4a051":"markdown","7e8812c0":"markdown","cb6be5ab":"markdown","f74fffd0":"markdown","7fed48b7":"markdown","2dd5cd51":"markdown","df847a61":"markdown","189d257b":"markdown","b55fe1b4":"markdown","c764df1f":"markdown","8501a51e":"markdown","ab82a662":"markdown","d56f7aa3":"markdown","48252bc8":"markdown","a75793be":"markdown"},"source":{"f25729e9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nfrom sklearn import preprocessing\nimport numpy as np\nimport lightgbm as lgb\nimport gc\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import cohen_kappa_score\nfrom functools import partial\nimport scipy as sp\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fc3dc17e":"df_data=pd.read_csv('..\/input\/diabetes-globant\/diabetic_data.csv')","3808668d":"df_data.columns","bfbc2a76":"print (len(df_data['encounter_id'].unique())\/len(df_data))\n\nprint (len(df_data['patient_nbr'].unique())\/len(df_data))","4c6242f4":"df_data['patient_nbr'].value_counts()","5e54f3b5":"##limpieza\n\nprint(df_data['age'].value_counts())\n\ndf_data['age']=df_data['age'].map({'[0-10)':1,'[10-20)':2,'[20-30)':3,'[30-40)':4,'[40-50)':5,'[50-60)':6,'[60-70)':7,'[70-80)':8})\n\nprint(df_data['age'].value_counts())\n\ndf_data=df_data.replace(\"?\",np.nan)\ndf_data=df_data.replace(\"None\",np.nan)\n\ndf_data.loc[df_data['max_glu_serum']=='Norm','max_glu_serum']=1\ndf_data.loc[df_data['max_glu_serum']=='>200','max_glu_serum']=2\ndf_data.loc[df_data['max_glu_serum']=='>300','max_glu_serum']=3\n\n\ndf_data.loc[df_data['readmitted']=='NO','readmitted']=0\ndf_data.loc[df_data['readmitted']=='<30','readmitted']=1\ndf_data.loc[df_data['readmitted']=='>30','readmitted']=2\n\n \ndf_data['weight']=df_data['weight'].map({'[0-25)':1,'[25-50)':2,'[50-75)':3,'[75-100)':4,'[100-125)':5,'[125-150)':6,'[150-175)':7,'[175-200)':8,'>200\/':9 })\n\n\n\n","9849be37":"df_data['readmitted'].value_counts()","25b83a30":"df_data.isnull().mean()","fbc919ae":"\n\ndef entrena_lgb(data,features,categorical,target):\n\n    kfold=KFold(n_splits=5,shuffle=True,random_state=2021)\n\n\n    i=1\n\n    r=[]\n\n    importancias=pd.DataFrame()\n\n    importancias['variable']=features\n    \n    \n    cat_ind=[features.index(x) for x in categorical if x in features]\n    \n    dict_cat={}\n    \n    categorical_numerical = data[categorical].dropna().select_dtypes(include=np.number).columns.tolist()\n    \n    categorical_transform=[x for x in categorical if x not in categorical_numerical]\n    \n    for l in categorical_transform:\n        le = preprocessing.LabelEncoder()\n        le.fit(list(data[l].dropna()))\n\n        dict_cat[l]=le\n\n        data.loc[~data[l].isnull(),l]=le.transform(data.loc[~data[l].isnull(),l])\n        \n        \n\n    for train_index,test_index in kfold.split(data):\n\n        lgb_data_train = lgb.Dataset(data.loc[train_index,features].values,data.loc[train_index,target].values)\n        lgb_data_eval = lgb.Dataset(data.loc[test_index,features].values,data.loc[test_index,target].values, reference=lgb_data_train)\n\n        params = {\n            'task': 'train',\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': { 'rmse'},\n            \"max_depth\":-1,\n            \"num_leaves\":32,\n            'learning_rate': 0.1,\n        \"min_child_samples\": 100,\n            'feature_fraction': 0.9,\n         \"bagging_freq\":1,\n            'bagging_fraction': 0.9,\n            \"lambda_l1\":10,\n            \"lambda_l2\":10,\n           # \"scale_pos_weight\":30,\n\n            'verbose': 1    \n        }\n\n\n\n\n        modelo = lgb.train(params,lgb_data_train,num_boost_round=13100,valid_sets=lgb_data_eval,early_stopping_rounds=50,verbose_eval=25,categorical_feature=cat_ind)\n\n        importancias['gain_'+str(i)]=modelo.feature_importance(importance_type=\"gain\")\n\n\n        data.loc[test_index,'estimator']=modelo.predict(data.loc[test_index,features].values, num_iteration=modelo.best_iteration)\n\n        print (\"Fold_\"+str(i))\n        a= (mean_squared_error(data.loc[test_index,target],data.loc[test_index,'estimator']))**0.5\n        r.append(a)\n        print (a)\n        print (\"\")\n\n        i=i+1\n        \n    for l in categorical_transform:\n\n            data.loc[~data[l].isnull(),l]=dict_cat[l].inverse_transform(data.loc[~data[l].isnull(),l].astype(int))\n            \n    importancias[\"gain_avg\"]=importancias[[\"gain_1\",\"gain_2\",\"gain_3\",\"gain_4\",\"gain_5\"]].mean(axis=1)\n    importancias=importancias.sort_values(\"gain_avg\",ascending=False).reset_index(drop=True)\n\n    print (\"mean: \"+str(np.mean(np.array(r))))\n    print (\"std: \"+str(np.std(np.array(r))))\n    \n    return importancias","2cb845ac":"print(df_data['max_glu_serum'].value_counts())\nprint(df_data['diag_1'].value_counts())\nprint(df_data['diag_2'].value_counts())\nprint(df_data['diag_3'].value_counts())\n","6318238e":"### notar la alta cardinalidad de algunas variables categoricas\n\nfor x in ['race','gender','admission_type_id', 'discharge_disposition_id', 'admission_source_id',\n             'payer_code', 'medical_specialty','A1Cresult',\n       'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide',\n       'glimepiride', 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide',\n       'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone',\n       'tolazamide', 'examide', 'citoglipton', 'insulin',\n       'glyburide-metformin', 'glipizide-metformin',\n       'glimepiride-pioglitazone', 'metformin-rosiglitazone','metformin-pioglitazone','change', 'diabetesMed','diag_1','diag_2','diag_3']:\n    \n    print(x)\n    print(df_data[x].nunique())\n    print('')\n    \n    ","67a8d161":"\n\ncategorical=['patient_nbr','race','gender','admission_type_id', 'discharge_disposition_id', 'admission_source_id',\n             'payer_code', 'medical_specialty','A1Cresult',\n       'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide',\n       'glimepiride', 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide',\n       'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone',\n       'tolazamide', 'examide', 'citoglipton', 'insulin',\n       'glyburide-metformin', 'glipizide-metformin',\n       'glimepiride-pioglitazone', 'metformin-rosiglitazone','metformin-pioglitazone','change', 'diabetesMed','diag_1','diag_2','diag_3']\n\nno_usar=['encounter_id','readmitted','estimator']\n\nfeatures=[x for x in df_data.columns if x not in no_usar]\n\nimportancias=entrena_lgb(data=df_data,features=features,categorical=categorical,target='readmitted')\n\n","48176a15":"importancias","11a21ab7":"## basado en esto https:\/\/www.kaggle.com\/naveenasaithambi\/optimizedrounder-improved\nclass OptRounder(object):\n    def __init__(self):\n        self.res_ = []\n        self.coef_ = []\n        \n    def get_res(self):\n        return self.res_\n    \n    # objective function\n    def func(self, coef, X, y):\n\n        mse = mean_squared_error(self.bincut(coef, X), y)\n        return mse\n    \n    \n    def bincut(self, coef, X):\n        return pd.cut(X,\n                      [-np.inf] + list(np.sort(coef)) + [np.inf],\n                      labels = [0, 1, 2])\n        \n    def fit(self, X, y):\n        pfunc = partial(self.func, X=X, y=y)\n        self.res_ = sp.optimize.minimize(fun = pfunc,           # objective func\n                                         x0 = [0.5, 1.2],  # initial coef\n                                         method='nelder-mead')  # solver\n        \n        \n        self.coef_ = self.res_.x\n        \n    def predict(self, X, coef):\n        return self.bincut(coef, X)","d16e4af1":"optR = OptRounder()\noptR.fit(df_data[\"estimator\"].values.reshape(-1,), df_data['readmitted'].astype(int))\nres = optR.get_res() ","b1c94f5a":"coefficients = res.x        \n\n\n(mean_squared_error(optR.predict(df_data[\"estimator\"].values, coefficients).astype(int),df_data['readmitted'].astype(int)))**0.5","f78cac23":"res","08ba674d":"no_usar=['encounter_id','readmitted','estimator', 'diag_1','diag_2','diag_3']\n\nfeatures=[x for x in df_data.columns if x not in no_usar]\n\nimportancias_2=entrena_lgb(data=df_data,features=features,categorical=categorical,target='readmitted')","e80728dc":"### probamos haciendo algo de feature engineering a las variables mas importantes, en caso alguna salga como relevante, se puede discutir la razon de su relevancia\n\n#for x in [ 'diag_1',\n# 'diag_2',\n# 'diag_3']:\n    \n#    df_data[x+\"_size\"]=df_data.groupby(x)['encounter_id'].transform('size')\n    \n#for x in [ 'diag_1',\n# 'diag_2',\n# 'diag_3']:\n    \n#    df_data[x+\"_number_inpatient_mean\"]=df_data.groupby(x)['number_inpatient'].transform('mean')\n#    df_data[x+\"_number_inpatient_var\"]=df_data.groupby(x)['number_inpatient'].transform('var')\n\nfor x in [ 'discharge_disposition_id',\n 'admission_source_id',\n 'medical_specialty']:\n    \n    df_data[x+\"_number_inpatient_mean\"]=df_data.groupby(x)['number_inpatient'].transform('mean')\n    df_data[x+\"_number_inpatient_var\"]=df_data.groupby(x)['number_inpatient'].transform('var')\n      ","4204db36":"no_usar=['encounter_id','readmitted','estimator']\n\nfeatures=[x for x in df_data.columns if x not in no_usar]\n\nimportancias_2=entrena_lgb(data=df_data,features=features,categorical=categorical,target='readmitted')","49ed1c1f":"importancias_2","ecbf4a0f":"def target_encoding(data,variable,target,threshold=20,function='mean'):\n\n    list_part=[]\n\n\n    kfold=KFold(n_splits=5,shuffle=True,random_state=2021)\n\n    for train_index,test_index in kfold.split(data):\n\n        temp=data.loc[train_index].groupby(variable)[target].agg([function,'size']).reset_index()\n\n        temp=temp.loc[temp['size']>threshold].reset_index(drop=True)\n\n        del temp['size'] ; gc.collect()\n\n        temp=temp.rename(columns={function:function+'_enconding_'+variable+\"-\"+target})\n        \n        part=data.loc[test_index,['encounter_id',variable]]\n\n        part=part.merge(temp,on=variable,how='left')\n\n        list_part.append(part)\n\n    df_part=pd.concat(list_part,ignore_index=True)\n    \n    del df_part[variable] ; gc.collect()\n    \n    data=data.merge(df_part,on='encounter_id',how='left')\n    \n    return data\n\n\n\n\n    ","4583d7d2":"df_data=target_encoding(data=df_data,variable='diag_1',target='readmitted')\ndf_data=target_encoding(data=df_data,variable='diag_2',target='readmitted')\ndf_data=target_encoding(data=df_data,variable='diag_3',target='readmitted')\ndf_data=target_encoding(data=df_data,variable='discharge_disposition_id',target='readmitted')\ndf_data=target_encoding(data=df_data,variable='admission_source_id',target='readmitted')\ndf_data=target_encoding(data=df_data,variable='medical_specialty',target='readmitted')\n\ndf_data=target_encoding(data=df_data,variable='diag_1',target='readmitted',function='var')\ndf_data=target_encoding(data=df_data,variable='diag_2',target='readmitted',function='var')\ndf_data=target_encoding(data=df_data,variable='diag_3',target='readmitted',function='var')\ndf_data=target_encoding(data=df_data,variable='discharge_disposition_id',target='readmitted',function='var')\ndf_data=target_encoding(data=df_data,variable='admission_source_id',target='readmitted',function='var')\ndf_data=target_encoding(data=df_data,variable='medical_specialty',target='readmitted',function='var')","5ab6804d":"no_usar=['encounter_id','readmitted','estimator']\n\nfeatures=[x for x in df_data.columns if x not in no_usar]\n\nimportancias_3=entrena_lgb(data=df_data,features=features,categorical=categorical,target='readmitted')","d358e5c0":"importancias_3","045e208b":"no_usar=['encounter_id','readmitted','estimator','diag_1','diag_2','diag_3']\n\nfeatures=[x for x in df_data.columns if x not in no_usar]\n\nimportancias_4=entrena_lgb(data=df_data,features=features,categorical=categorical,target='readmitted')","37274adf":"importancias_4","a6f14830":"optR = OptRounder()\noptR.fit(df_data[\"estimator\"].values.reshape(-1,), df_data['readmitted'].astype(int))\nres = optR.get_res() \n\n\ncoefficients = res.x        \n\n(mean_squared_error(optR.predict(df_data[\"estimator\"].values, coefficients).astype(int),df_data['readmitted'].astype(int)))**0.5\n\n","01b9fd16":"features_selected=importancias_4.loc[importancias_4['gain_avg']>0,'variable'].tolist()\n\nprint(len(features_selected))\n\nimportancias_5=entrena_lgb(data=df_data,features=features_selected,categorical=categorical,target='readmitted')","a19261de":"def get_feature_importance( data,features,categorical,target,shuffle=True, seed=None):\n\n    \n    cat_ind=[features.index(x) for x in categorical if x in features]\n    \n    dict_cat={}\n    \n    categorical_numerical = data[categorical].dropna().select_dtypes(include=np.number).columns.tolist()\n    \n    categorical_transform=[x for x in categorical if x not in categorical_numerical]\n    \n    for l in categorical_transform:\n        le = preprocessing.LabelEncoder()\n        le.fit(list(data[l].dropna()))\n\n        dict_cat[l]=le\n\n        data.loc[~data[l].isnull(),l]=le.transform(data.loc[~data[l].isnull(),l])\n    \n    y = data[target].copy()\n    if shuffle:\n        y = data[target].copy().sample(frac=1.0,random_state=seed)\n    \n    dtrain = lgb.Dataset(data[features].values, y.values, free_raw_data=False, silent=True)\n    lgb_params = {\n        'objective': 'regression',\n        'boosting_type': 'gbdt',\n     \"learning_rate\": 0.2,\n        'metric': { 'rmse'},\n        'subsample': 0.9,\n        'colsample_bytree': 0.9,\n            \"min_child_samples\": 100,\n        'num_leaves': 64,\n        'max_depth': -1,\n        'seed': seed,\n        'bagging_freq': 1,\n    }\n    \n    # Fit the model\n    clf = lgb.train(params=lgb_params, train_set=dtrain, num_boost_round=90,categorical_feature=cat_ind)\n\n    # Get feature importances\n    imp_df = pd.DataFrame()\n    imp_df[\"feature\"] = list(features)\n    imp_df[\"importance_gain\"] = clf.feature_importance(importance_type='gain')\n    imp_df['trn_score'] = mean_squared_error(y, clf.predict(data[features].values))\n    \n    for l in categorical_transform:\n\n            data.loc[~data[l].isnull(),l]=dict_cat[l].inverse_transform(data.loc[~data[l].isnull(),l].astype(int))\n    \n    return imp_df\n\n\ndef get_importance_permutation(data,variables,categorical,target,n_iterations=50):\n\n    for x in range(1,n_iterations):\n        temp=get_feature_importance(data=data,features=variables,categorical=categorical,target=target,shuffle=True, seed=x)\n        temp[\"run\"]=x\n\n        if x==1:\n            general=temp.copy()\n\n        else :\n            general=general.append(temp,ignore_index=True)\n\n        print(x)\n\n\n    ranking=get_feature_importance(data=data,features=variables,categorical=categorical,target=target,shuffle=False, seed=2021)\n\n    features_quantile=general.groupby(\"feature\")[\"importance_gain\"].quantile(0.95).reset_index()\n\n    features_quantile=features_quantile.rename(columns={\"importance_gain\":\"quantile_0.95\"})\n    ranking=ranking.merge(features_quantile,on=\"feature\",how=\"left\")\n\n    ranking=ranking.sort_values(\"importance_gain\",ascending=False).reset_index(drop=True)\n    \n    return ranking","a3de7b40":"importances_6=get_importance_permutation(data=df_data,variables=features_selected,categorical=categorical,target='readmitted')","d4d1aa4c":"importances_6","1d2ae9a6":"features_selected_2=importances_6.loc[importances_6['importance_gain']>importances_6['quantile_0.95'],'feature'].tolist()\n\nprint(len(features_selected_2))\n\nimportancias_7=entrena_lgb(data=df_data,features=features_selected_2,categorical=categorical,target='readmitted')","52b7d4b5":"importancias_7['variable'].tolist()","aa2c373b":"no_usar=['encounter_id','readmitted','estimator']\n\nvariables=[x for x in df_data.columns if x not in no_usar]\n\nvariables_descartadas=[x for x in variables if x not in features_selected_2]\n\nvariables_descartadas","c8b07b15":"Entonces probemos el score optimizando los puntos de corte como se hablo anteriormente","7904ae91":"Confirmemos si la id 'encounter_id' es unica y si el id del paciente se repite.","d1282709":"Observamos que pasando de las 65 variables originales a 50 variables, obtenemos un modelo de resultados muy similares al del mejor escore que es el del experimento anterior.","611489dd":"Seleccionemos variables usando el metodo de target perumutation que basicamente hace que para cada una de las variables es permutar el target muchas veces y compara si el score sin el target permutado tiene diferencia significativa con los score con target permutado. En caso eso sea cierto, la variable queda seleccionada.\n\nLa metodologia y implementacion se base en los siguientes enlaces:\n\nhttps:\/\/www.kaggle.com\/ogrellier\/feature-selection-with-null-importances\n\nhttps:\/\/academic.oup.com\/bioinformatics\/article\/26\/10\/1340\/193348\n\n","d49aff49":"Aqui vemos que el score el mismo o incluso ligeramente peor que en la baseline","b16ae52a":"Probamos sacando el promedio y varianza del 'number_patient'(variable numerica mas relevante) en variables de alta cardinalidad , volveremos a poner las 3 variables extraidas anteriormente y veremos mas abajo que el score sera practicamente el mismo de la baseline. El codigo que esta comentado tambien fueron variables que se crearon con la logica hablada pero los resultados seguian siendo basicamente los mismos.","afc2592d":"\n1. Conocer las variables mas importantes para predecir la readmision en pacientes diabeticos y que el modelo hipotetico sea de alta prediccion, del menor numero de variables posible para que pueda ser explicado y totalmente implementables.\n\n2. Detallar como son los experimentos con que se llego a los resultados\n\n3. Explorar algunas tecnicas de feature engineering segun sea el tipo de datos que se encontrara\n\n4. Explorar como la seleccion de variables ayuda a mejorar la prediccion y a la vez ayudar a la explicacion del modelo al hacerlo mas compacto","e44d8816":"# **Septimo Experimento**","157c0fdb":"Creare a conintuacion el modelo baseline que sus variables seran tal y como vienen en la data luego de la limpiaza inicial que se realizo.\n\nPara predecir la readmision se opta por una regresion para aprovechar el orden que hay entre los valores, el cual no ocurriria si se optaria por una clasificacion multinomial.\n\nSe realizara una validacion cruzada al no observar un patron de dependencia del tiempo entre las observaciones, luego el modelo se optimizara con rmse y finalmente con un optimizador se convertira los valores decimales que arroja la prediccion a 0 , 1 y 2.","8047ba73":"Observamos que la hipotesis se hace cierta y superamos al score del segundo experimento","45cec426":"Vemos que 0.913 es el valor de nuestra baseline","570b0cdc":"# **Primer Experimento - Baseline**","b13624ba":"Observamos que en la importancia de variables del primer experimento que 'diag_1','diag_2' y 'diag_3' son las variables mas importantes y tienen alta cardinalidad. En estos casos muchas veces estas variables tienden a overfitear por mas que salgan como las variables mas relevantes. Probemos extrayendo estas variables en nuestro segundo experimento.","f97be701":"Se usa el siguiente optimizador para elegir los puntos de corte que convertiran los valores predicho en 0 , 1 2 en el caso que este modelo tenga que ser pasado a un ambiente productivo. Esto es mucho mas ventajoso que redondear las predicciones solamente al entero mas cercano.\n\nEl codigo de dicho optimizador esta basado en el codigo del siguiente enlace:\n\nhttps:\/\/www.kaggle.com\/naveenasaithambi\/optimizedrounder-improved","bd7d9b26":"Las variables 'age','max_glu_serum','readmitted'(variable a predecir) y 'weight' se convierten a enteros para explotar el orden que tienen sus valores. Luego algunas etiquetas que aparecne en la data se convierten a nulo para unfirmoizar los valores perdidos en la data.","e73dac3b":"# **Los objetivos de este analisis son :**","e9d4a051":"Vemos que el target encoding incluso llega a descartar variables que estan adelante en el ranking de importancias, como 'var_enconding_diag_1-readmitted'.Finalmente con el target permutation obtenemos un modelo de solo 30 variables con un score que supera a la baseline y de casi el mismo score que el mejor modelo en los experimentos realizados.\n\nLas variables seleccionadas en orden de relevancia para el modelo bajo los test respectivos son las siguientes:","7e8812c0":"# **Quinto Experimento**","cb6be5ab":" Notar la alta cardinalidad de algunas variables categoricas:","f74fffd0":"Las variales que se descartaron fueron:\n","7fed48b7":"# **Sexto Experimento**","2dd5cd51":"Tan solo sacando esas 3 variables el score bajo de 0.870 a 0.866 !","df847a61":"# **Cuarto Experimento**","189d257b":"Si bien vimos que las 3 variables categoricas que sacamos en el segundo experimento estaban overfiteando y tambien puede ser confirmado al regresarlas en los 2 experimentos anteriores. Tal vez algunos de los niveles de dichas variables si podrian tener poder predictivo y podrian estar representados en el target encoding.\n\nEntonces mantengamos las variables creadas  el target encoding pero volvamos a extraer las 3 variables con mas alta cardinalidad como en el segundo experimento y veamos los resultados.","b55fe1b4":"Usaremos el **target encoding** para las 6 variables con mas alta cardinalidad pero usando las mismas particiones de la validacion cruzada usada para evitar el **data leakage.**\n\nRecordemos que a diferencia del segundo experimento, no se ha sacado ninguna variable categorica.\n\nLa funcion de target encoding creada,hace que cuando el tama;o de muestra es menor a 20, se asigna como valor nulo a la variable creada para evitar efectos de tama\u00f1o de muestra peque\u00f1o. Dicho tama\u00f1o de muestra es una parametro de la funcion que se puede variar.","c764df1f":"Extraigamos las variables con 'gain_avg' mayor a 0 y esperemos encontrar un score mejor o similar. Lo comprobaremos en los resultados de mas abajo.","8501a51e":"Aqui se confirma que se vuelve a superar el score.","ab82a662":"# **Tercer Experimento**","d56f7aa3":"Observemos los valores nulos por cada variable:","48252bc8":"A pesar de las variables agregadas, el score continua siendo practicamente el mismo de la baseline.","a75793be":"# **Segundo Experimento**"}}