{"cell_type":{"5487752f":"code","98d4d0bf":"code","24a7eac8":"code","681c09ee":"code","bbc67add":"code","47b6fe4d":"code","29337242":"code","5081320f":"code","7b6a2d36":"code","701319f8":"code","b8589752":"code","c2571466":"code","283dda64":"code","934fa227":"code","f26541a1":"code","0ce11b65":"code","b298cc5b":"code","029b6d8a":"code","43978b28":"code","9a8bc8ec":"code","e42df47b":"code","3646a4d8":"code","9b5493a7":"code","5a99f395":"code","c3596614":"code","f4d29631":"code","077c68f6":"code","516be833":"code","6eb9f74c":"code","95277700":"code","3f14cb39":"code","f07e691b":"code","721cd5e1":"code","331d51e2":"code","7b7ff0b0":"code","dd2f697e":"code","8738c0ed":"code","918a0411":"code","a041bb29":"code","9f33149b":"code","3ff3f86b":"code","9f042f1c":"code","8d356a28":"code","2647b3fe":"code","6ae64d97":"code","c7eb047d":"code","fcca9921":"code","0b691169":"code","b3f84a9d":"code","46c77b1f":"code","6e3cb94e":"code","6a57e333":"code","a20197b0":"code","baa1dbe1":"code","aec314f1":"code","e8fcde98":"code","8f5f7a08":"code","6b0651a2":"code","2542918a":"code","f8b2e9e5":"code","d3578c4d":"code","cc34b6c8":"code","092594f1":"code","40503843":"code","81d0fdbf":"code","0e10bcba":"code","e059d360":"code","d12533ce":"code","cc41ca70":"code","8e3a6e50":"code","34ae056f":"code","44734915":"code","2e65c779":"code","988df3c4":"code","0ae8b9e8":"code","cb1936ff":"code","fcd8fbae":"code","15bba04c":"code","e3e2923e":"code","f6ad25d6":"code","a6978ec0":"code","f2d741fe":"code","2d5ce7b5":"code","e64fe6ed":"code","536cb20a":"code","cb590025":"code","56bcf4c2":"code","a5445710":"code","02106e05":"code","c26522b1":"markdown","0f18627f":"markdown","8b47c418":"markdown","32534d00":"markdown","727a02e5":"markdown","7d1361bb":"markdown","e99065dc":"markdown","fcbf0a4e":"markdown","7c4919f2":"markdown","37c8d9b4":"markdown","48322acf":"markdown","fbe4ce23":"markdown","b1dc3230":"markdown","ca9b2fb4":"markdown","ffbfe4cd":"markdown","ead9ebb6":"markdown","af0f9ab6":"markdown","b527d5cd":"markdown","78281c32":"markdown","2067360b":"markdown","cbe52b6d":"markdown","e162b0bb":"markdown","ac49481d":"markdown","84aee3f4":"markdown","19a4751b":"markdown","48e23fa1":"markdown","80cd7ee6":"markdown","e7ee5a9d":"markdown","b35d4821":"markdown","2bfbe96b":"markdown","b3050c37":"markdown","c003e670":"markdown","32baae6a":"markdown","f15d18ca":"markdown","710f8561":"markdown","1e92baea":"markdown","4b139865":"markdown","fd2e9f8a":"markdown","38319284":"markdown","97fc97d2":"markdown","e741bbcf":"markdown","edd06959":"markdown","f50be517":"markdown","2a46f023":"markdown","078597d4":"markdown","62eb2a04":"markdown","cb30a89e":"markdown","015a6455":"markdown","054b8590":"markdown","76b10d0a":"markdown","70510f1c":"markdown","23e6cdab":"markdown","7ddea614":"markdown","bd23f9c5":"markdown","9b336d2c":"markdown","e500ce7d":"markdown","04db6c67":"markdown","866547d3":"markdown","73d5baa5":"markdown","39479308":"markdown","492e1a24":"markdown","8975483b":"markdown","d4b99459":"markdown","55b0c667":"markdown","1476d0cf":"markdown","848dbe9a":"markdown","42947454":"markdown","e3282138":"markdown","cabfe314":"markdown","ab4a6b70":"markdown","b40bc151":"markdown","5d9dce04":"markdown"},"source":{"5487752f":"import warnings\nimport os # Get Current Directory\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nimport pandas as pd # data processing, CSV file I\/O (e.i. pd.read_csv)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nfrom time import time\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.decomposition import PCA\nfrom scipy import stats\nimport subprocess\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.metrics import roc_curve, auc #plot_roc_curve\nfrom sklearn.utils.multiclass import unique_labels\nimport itertools\nimport math\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n","98d4d0bf":"warnings.filterwarnings(\"ignore\")\npd.set_option('mode.chained_assignment', None)","24a7eac8":"currentDirectory=os.getcwd()","681c09ee":"def folder_path(path):\n    try:\n        os.mkdir(path)\n    except OSError:\n        print (\"Creation of the directory %s failed\" % path)\n        return(path)\n    else:\n        print (\"Successfully created the directory %s \" % path)\n        return(path)\n    return(path)","bbc67add":"# OUTPUTS: Folder for storing OUTPUTS\nOUTPUT_path=folder_path(currentDirectory+'\/Outputs')\n\n# Models: Folder for storing models\nmodels_path=folder_path(OUTPUT_path+'\/Models')\n# Figures: Folder for storing figures\nfigures_path=folder_path(OUTPUT_path+'\/Figures')","47b6fe4d":"\ntry:\n    train= pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n    test= pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nexcept OSError:\n    print (\"Input file not found at location:\",'\/kaggle\/input\/titanic\/train.csv')\n    print (\"Input file not found at location:\",'\/kaggle\/input\/titanic\/test.csv')\n    INPUT_path=currentDirectory+'\/Inputs'\n    train_path=os.path.join(INPUT_path,'train.csv')\n    train= pd.read_csv(train_path)\n    test_path=os.path.join(INPUT_path,'test.csv')\n    test= pd.read_csv(test_path)\n    print (\"Successfully loaded Input file from:\",INPUT_path)\nelse:\n    print (\"Successfully loaded Input file from Kaggle:\")\n\n\n\n\ntrain.head(10) #notice Survival is provided for train dataset\ntest.head(10) #notice Survival is NOT provided for test dataset","29337242":"test_clean=pd.DataFrame(test)\ntrain_clean=pd.DataFrame(train)","5081320f":"train_clean.info()","7b6a2d36":"test_clean.info()","701319f8":"DropList=['Survived','PassengerId','Name','Cabin','Ticket','Embarked']\nsqrt_length=round(len(list(train_clean.drop(DropList,axis=1)))**(1\/2))\nncols=sqrt_length+1\nnrows=sqrt_length\nfig, axes = plt.subplots(ncols=ncols,nrows=nrows,figsize=[20,10])\nfig.suptitle('Figure 0: Histogram Plot Raw Data', y=1.05, fontsize=24)\nfig.tight_layout()\nslog_train = train_clean['Survived'] == 1\nfor name, ax in zip(list(train_clean.drop(['Survived','PassengerId','Name','Cabin','Ticket','Embarked'],axis=1)),axes.flat):\n    x1=train_clean[~slog_train][name]\n    x2=train_clean[slog_train][name]\n    x3=train_clean[name]\n    x4=test_clean[name]\n    labels=['Training-Died','Training-Survived','Training-ALL','Test-ALL']\n    ax.hist([x1,x2,x3,x4],label=labels)\n    ax.legend()\n    ax.set_xlabel(name)\n    ax.set_ylabel('# of people')\n    plt.tight_layout()\nhistplot_fig = os.path.join(figures_path,'Figure 0.Histogram Plot Raw Data.png')\nplt.savefig(histplot_fig,dpi=300,bbox_inches='tight')","b8589752":"missing_train=train_clean.isnull().sum() # notice Age [177], Cabin [687], Embarked [2]\nmissing_test=test_clean.isnull().sum() # notice Age [86], Fare [1], Cabin [327]\n\n","c2571466":"print(missing_train)","283dda64":"print(missing_test)","934fa227":"train_clean['Age'].fillna(train_clean['Age'].mean(),inplace=True) \ntest_clean['Age'].fillna(test_clean['Age'].mean(),inplace=True)           ","f26541a1":"def AgeConvertCategory(data):\n    datatest=pd.DataFrame(data)\n    datatest['Age_Child']=np.where((datatest['Age']<10),1,0)\n    datatest['Age_Teen']=np.where((datatest['Age'] >10) & (datatest['Age']<20),1,0)\n    datatest['Age_YoungAdult']=np.where((datatest['Age']>20) & (datatest['Age']<30),1,0)\n    datatest['Age_MidAdult']=np.where((datatest['Age']>30) & (datatest['Age']<50),1,0)\n    datatest['Age_OldAdult']=np.where((datatest['Age']>50),1,0)\n    return datatest","0ce11b65":"train_clean=AgeConvertCategory(pd.DataFrame(train_clean))\ntest_clean=AgeConvertCategory(pd.DataFrame(test_clean))","b298cc5b":"# Look at the unique Cabin identifiers\nprint(len(train_clean['Cabin'].unique()))\n\n# make a function to conver to the first letter\ndef oneletter_conv(data):\n    newlist=[None]*len(data)\n    count=0\n    for name in list(data):\n        newlist[count]=str(data[count])[0]\n        count=count+1\n    return pd.DataFrame(newlist)","029b6d8a":"# Assign missing values with 'Unknown' and known values with the first letter of their cabin identifier.\n# The first letter of the cabin id could represent a unique physical location for the passenger that correlates with survival.\ntrain_clean['Cabin']=oneletter_conv(train_clean['Cabin'])\ntest_clean['Cabin']=oneletter_conv(test_clean['Cabin'])","43978b28":"fig,ax = plt.subplots()\n\nsns.pointplot(x='Cabin',y='Survived',data=train_clean,color='red')\nplt.legend(['Cabin'])\nplt.title('Figure 1.1A:  Point Plot [training data] \"Cabin\" vs. \"Survived\"')\npointplot_fig=os.path.join(figures_path,'Figure1.1A.Pointplot.png')\nplt.savefig(pointplot_fig,dpi=300,bbox_inches='tight')\nplt.show()","9a8bc8ec":"pivot_plot = train_clean.pivot_table(index='Cabin',values='Survived',aggfunc='mean')\npivot_plot.plot.bar(color='red')\nplt.ylabel('Average Survived')\nplt.title('Figure 1.1.B:  Pivot Table [training data] \"Cabin\" vs. \"Survived\"')\npivotplot_fig=os.path.join(figures_path,'Figure1.1.B_Pivotplot.png')\nplt.savefig(pivotplot_fig,dpi=300,bbox_inches='tight')\nplt.show()","e42df47b":"plt.figure()\nsns.pointplot(x='Sex',y='Survived',data=train_clean,color='blue')\nplt.legend(['Sex'])\nplt.title('Figure 1.2:  Point Plot [training data] \"Sex\" vs. \"Survived\"')\npointplot_fig=os.path.join(figures_path,'Figure1.2.Pointplot.png')\nplt.savefig(pointplot_fig,dpi=300,bbox_inches='tight')\nplt.show()","3646a4d8":"train_clean=train_clean.dropna(subset=['Embarked'],axis=0)","9b5493a7":"mean_Fare=test_clean['Fare'].mean()\nmean_Fare_noCabin=test_clean.loc[test_clean.loc[:,'Cabin']=='n','Fare'].mean()\ntest_clean['Fare'].fillna(mean_Fare,inplace=True) ","5a99f395":"def pd_concat(x1,x2): #for binning, combine both datasets by the variable to bin\n    return pd.concat([x1,x2],sort=False).reset_index(drop=True)","c3596614":"dummyall=pd.qcut(pd_concat(train_clean['Fare'],test_clean['Fare']),5)","f4d29631":"dummy1=dummyall[:len(train_clean['Fare'])]\ndummy2=dummyall[len(train_clean['Fare']):]\ndummy1.reset_index(drop=True,inplace=True)\ndummy2.reset_index(drop=True,inplace=True)\ntrain_clean['Fare']=dummy1\ntest_clean['Fare']=dummy2","077c68f6":"mean_Fare=dummyall.describe().top\ntest_clean['Fare'].fillna(mean_Fare,inplace=True) \ntrain_clean['Fare'].fillna(mean_Fare,inplace=True) \nprint(mean_Fare)","516be833":"encoder = LabelEncoder()\ntest_clean['Fare']=encoder.fit_transform(test_clean['Fare'])\ntrain_clean['Fare']=encoder.fit_transform(train_clean['Fare'])","6eb9f74c":"plt.figure()\nsns.pointplot(x='Survived',y='Fare',data=train_clean,color='purple')\nplt.legend(['Survived'])\nplt.title('Figure 1.3.A:  Point Plot [training data] \"Fare\" vs. \"Survived\"')\npointplot_fig=os.path.join(figures_path,'Figure1.3.A_Pointplot.png')\nplt.savefig(pointplot_fig,dpi=300,bbox_inches='tight')\nplt.show()","95277700":"print(train_clean.head())","3f14cb39":"missing_train_clean=train_clean.isnull().sum() # notice Age [177], Cabin [687], Embarked [2]\nmissing_test_clean=test_clean.isnull().sum() # notice Age [86], Fare [1], Cabin [327]","f07e691b":"print(missing_train_clean)","721cd5e1":"print(missing_test_clean)","331d51e2":"def family_conv(data):\n    lastname=[]\n    for kk in list(data): # converts string in an array to only the Last Name\n        lastname.append(kk[0:kk.index(',')])\n    return lastname\ntrain_clean['Family']=family_conv(train_clean['Name'])\ntest_clean['Family']=family_conv(test_clean['Name'])\n# View Unique Family\nUnique_Family_Train=(list(set(train_clean['Family'])))\nUnique_Family_Test=(list(set(train_clean['Family'])))","7b7ff0b0":"def prefix_conv(data):\n    prefix=[]\n    for kk in list(data): # converts string in an array to only the Prefix\n        prefix.append(kk[kk.index(',')+1:kk.index('.')+1])\n    return prefix\ntrain_clean['Name']=prefix_conv(train_clean['Name'])\ntest_clean['Name']=prefix_conv(test_clean['Name'])\n# View Unique Names\nUnique_Names_Train=(list(set(train_clean['Name'])))\nUnique_Names_Test=(list(set(train_clean['Name'])))","dd2f697e":"# Combine SibSp (siblings and spouses) & Parch (parents & children)\ntrain_clean['Family_cnt']=1+train_clean['SibSp']+train_clean['Parch']\ntest_clean['Family_cnt']=1+test_clean['SibSp']+test_clean['Parch']\n\n\n# Combine the family size with the average survival rate for that family\nsize_surv_family_train = train_clean.groupby('Family')['Survived','Family','Family_cnt'].mean()\nsize_surv_family_train['Family_Rate']=size_surv_family_train['Survived']*size_surv_family_train['Family_cnt']\nsize_surv_family_test=[]\nsize_surv_family_train.head()\ntest_clean['Family_Rate']=[0]*len(test_clean['Family'])\ntrain_clean['Family_Rate']=[0]*len(train_clean['Family'])\ncount1=0\ncount2=0\nfor name in list(size_surv_family_train.index):\n    if name in list(test_clean['Family']):\n        value_=size_surv_family_train['Family_Rate'][name]\n        test_clean['Family_Rate'][count1]=value_\n        count1=count1+1\n    if name in list(train_clean['Family']):\n        value_=size_surv_family_train['Family_Rate'][name]\n        train_clean['Family_Rate'][count2]=value_\n        count2=count2+1","8738c0ed":"train_clean['Family_Rate'].unique()","918a0411":"print(Unique_Names_Train)","a041bb29":"pivot_plot = train_clean.pivot_table(index='Name',values='Survived',aggfunc='mean')\npivot_plot.plot.barh(color='green')\nplt.xlabel('Average Survived')\nplt.title('Figure 1.4.A:  Pivot Table [training data] \"Name\" vs. \"Survived\"')\npivotplot_fig=os.path.join(figures_path,'Figure1.4.A_Pivotplot.png')\nplt.savefig(pivotplot_fig,dpi=300,bbox_inches='tight')\nplt.show()","9f33149b":"print(train_clean['Name'].value_counts())\nprint(test_clean['Name'].value_counts())","3ff3f86b":"print(Unique_Names_Train+Unique_Names_Test)","9f042f1c":"def make_dummies(data,col_name): #Make binary values for unique category names\n    dummy = pd.get_dummies(data[col_name],prefix=col_name)\n    data = pd.concat([data,dummy],axis=1)\n    return data","8d356a28":"# Combine SibSp (siblings and spouses) & Parch (parents & children)\ntrain_clean['Family_cnt']=train_clean['SibSp']+train_clean['Parch']\ntest_clean['Family_cnt']=test_clean['SibSp']+test_clean['Parch']","2647b3fe":"# Convert Sex to numeric\ngender_num={'male':0,'female':1}\ntrain_clean['Sex']=train_clean['Sex'].map(gender_num)\ntest_clean['Sex']=test_clean['Sex'].map(gender_num)","6ae64d97":"for col in [\"Name\",\"Cabin\",\"Embarked\",\"Pclass\"]:\n    train_clean = make_dummies(train_clean,col)\n    test_clean = make_dummies(test_clean,col)\n    train_clean.drop(col,axis=1,inplace=True)\n    test_clean.drop(col,axis=1,inplace=True)\n    \n#print(train_clean.head(10))\nUnique_Cols_Train=(list(set(train_clean.columns.values)))\nUnique_Cols_Test=(list(set(test_clean.columns.values)))\nprint('The Following were not part of the test values so those columns are dropped')\nfor col in Unique_Cols_Train:\n    if col not in Unique_Cols_Test:\n        if col != 'Survived':\n            train_clean.drop(col,axis=1,inplace=True)\n            print(col)\n            \n\nprint('The Following were not part of the train values so those columns are dropped')\nfor col in Unique_Cols_Test:\n    if col not in Unique_Cols_Train:\n        if col != 'Survived':\n            test_clean.drop(col,axis=1,inplace=True)\n            print(col)\n","c7eb047d":"train_clean['Family'].head()","fcca9921":"encoder = LabelEncoder()\ntest_clean['Family']=encoder.fit_transform(test_clean['Family'])\ntrain_clean['Family']=encoder.fit_transform(train_clean['Family'])","0b691169":"train_clean['Family'].head()","b3f84a9d":"train_clean.drop(['Ticket','PassengerId'],axis=1,inplace=True)\ntest_clean.drop(['Ticket','PassengerId'],axis=1,inplace=True)","46c77b1f":"# Scaler for Training\ncolumns = train_clean.columns\nscaler = preprocessing.MinMaxScaler()\nscaled_tc = pd.DataFrame(scaler.fit_transform(train_clean),columns=columns)\ntrain_clean=scaled_tc\n#scaled_tc = pd.DataFrame(scaled_tc.transform(test_clean))\n#test_clean=scaled_tc","6e3cb94e":"# Scaler for Testing\ncolumns = test_clean.columns\nscaler = preprocessing.MinMaxScaler()\nscaled_tc = pd.DataFrame(scaler.fit_transform(test_clean),columns=columns)\ntest_clean=scaled_tc","6a57e333":"#fix,ax = plt.subplots(figsize=(25,25))\nfix,ax = plt.subplots(figsize=(30,30))\nheatmap_data = train_clean\nsns.heatmap(heatmap_data.corr(),vmax=1,linewidths=0.01,square=True,annot=True,linecolor=\"white\")\nbottom,top=ax.get_ylim()\nax.set_ylim(bottom+0.5,top-0.5)\nheatmap_title='Figure 2:  Heatmap with Pearson Correlation Coefficient for Features'\nax.set_title(heatmap_title)\nheatmap_fig=os.path.join(figures_path,'Figure2.Heatmap.png')\nplt.savefig(heatmap_fig,dpi=300,bbox_inches='tight')\nplt.show()","a20197b0":"droplist=pd.DataFrame((heatmap_data[heatmap_data.columns[:]].corr().abs()<.1)['Survived'][:-1])\ndroplist=list(droplist.loc[droplist['Survived']==True].index)\nprint(droplist)","baa1dbe1":"droplist=['Parch','SibSp'] # don't want to drop that many\ntrain_clean.drop(droplist,axis=1,inplace=True)\ntest_clean.drop(droplist,axis=1,inplace=True)","aec314f1":"#fix,ax = plt.subplots(figsize=(25,25))\nfix,ax = plt.subplots(figsize=(30,30))\nheatmap_data = train_clean\nsns.heatmap(heatmap_data.corr(),vmax=1,linewidths=0.01,square=True,annot=True,linecolor=\"white\")\nbottom,top=ax.get_ylim()\nax.set_ylim(bottom+0.5,top-0.5)\nheatmap_title='Figure 3:  Heatmap with Pearson Correlation Coefficient for Features'\nax.set_title(heatmap_title)\nheatmap_fig=os.path.join(figures_path,'Figure3.Heatmap.png')\nplt.savefig(heatmap_fig,dpi=300,bbox_inches='tight')\nplt.show()\n","e8fcde98":"sqrt_length=math.ceil(len(list(train_clean))**(1\/2))\nncols=sqrt_length\nnrows=sqrt_length\nfig, axes = plt.subplots(ncols=ncols,nrows=nrows,figsize=[30,10])\nfig.suptitle('Figure 4: Histogram Plot', y=1.05, fontsize=24)\nfig.tight_layout()\nfor i in range((ncols*nrows-sqrt_length**2)+2):\n    #print(i)\n    axes[-1,(i+1)*-1].axis('off')\nslog_train = train_clean['Survived'] == 1\nfor name, ax in zip(list(train_clean),axes.flat):\n    x1=train_clean[~slog_train][name]\n    x2=train_clean[slog_train][name]\n    labels=['Died','Survived']\n    ax.hist([x1,x2],label=labels)\n    #sns.distplot(x1,label='Died',ax=ax)\n    #sns.distplot(x2,label='Survived',ax=ax)\n    ax.legend()\n    ax.set_xlabel(name)\n    ax.set_ylabel('# of people')\n    plt.tight_layout()\nhistplot_fig = os.path.join(figures_path,'Figure 4.Histogram Plot.png')\nplt.savefig(histplot_fig,dpi=300,bbox_inches='tight')","8f5f7a08":"features = train_clean.drop('Survived',axis=1)\nlabels = train_clean['Survived']\nX_train, X_val, y_train, y_val = train_test_split(features,labels, test_size=0.3, random_state=5) #hold out 30% of data\nX_train = pd.DataFrame(X_train)\nX_val=pd.DataFrame(X_val)\ny_train = pd.DataFrame(y_train)\ny_val=pd.DataFrame(y_val)\nfor dataset in [y_train, y_val]:\n    print(round(len(dataset),2))\n    \ntr_features=X_train\ntr_labels=y_train\n\nval_features = X_val\nval_labels=y_val","6b0651a2":"def print_results(results,name,filename_pr):\n    with open(filename_pr, mode='w') as file_object:\n        print(name,file=file_object)\n        print(name)\n        print('BEST PARAMS: {}\\n'.format(results.best_params_),file=file_object)\n        print('BEST PARAMS: {}\\n'.format(results.best_params_))\n        means = results.cv_results_['mean_test_score']\n        stds = results.cv_results_['std_test_score']\n        for mean, std, params in zip(means, stds, results.cv_results_['params']):\n            print('{} {} (+\/-{}) for {}'.format(name,round(mean, 3), round(std * 2, 3), params),file=file_object)\n            print('{} {} (+\/-{}) for {}'.format(name,round(mean, 3), round(std * 2, 3), params))","2542918a":"print(GridSearchCV)","f8b2e9e5":"#print(LogisticRegression())","d3578c4d":"# Make Directory\npath=folder_path(OUTPUT_path+'\/Models\/LR')","cc34b6c8":"LR_model_dir=os.path.join(path,'LR_model.pkl')\nLR_params_dir=os.path.join(path,'LR_params.pkl')\nif os.path.exists(LR_model_dir) == False:\n    lr = LogisticRegression()\n    parameters = {\n            'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n            }\n    cv=GridSearchCV(lr, parameters, cv=5)\n    cv.fit(tr_features,tr_labels.values.ravel())      \n    print_results(cv,'Logistic Regression (LR)',os.path.join(path,'LR_GridSearchCV_results.txt'))\n    cv.best_estimator_\n    LR_best=cv.best_estimator_\n    LR_best_param=cv.best_params_\n    joblib.dump(cv.best_estimator_,LR_model_dir)\n    joblib.dump(cv.best_params_,LR_params_dir)\nelse:\n    print('Already have LR')","092594f1":"#print(SVC())","40503843":"# Make Directory\npath=folder_path(OUTPUT_path+'\/Models\/SVM')","81d0fdbf":"SVM_model_dir=os.path.join(path,'SVM_model.pkl')\nSVM_params_dir=os.path.join(path,'SVM_params.pkl')\nif os.path.exists(SVM_model_dir) == False:\n    svc = SVC()\n    parameters = {\n            'kernel': ['linear','rbf'],\n            'C': [0.1, 1, 10]\n            }\n    cv=GridSearchCV(svc,parameters, cv=3)\n    cv.fit(tr_features, tr_labels.values.ravel())\n    print_results(cv,'Support Vector Machine (SVM)',os.path.join(path,'SVM_GridSearchCV_results.txt'))\n    cv.best_estimator_\n    SVM_best=cv.best_estimator_\n    SVM_best_param=cv.best_params_\n    joblib.dump(cv.best_estimator_,SVM_model_dir)\n    joblib.dump(cv.best_params_,SVM_params_dir)\nelse:\n    print('Already have SVM')","0e10bcba":"#print(MLPClassifier())","e059d360":"# Make Directory\npath=folder_path(OUTPUT_path+'\/Models\/MLP')","d12533ce":"MLP_model_dir=os.path.join(path,'MLP_model.pkl')\nMLP_params_dir=os.path.join(path,'MLP_params.pkl')\nif os.path.exists(MLP_model_dir) == False:\n    mlp = MLPClassifier()\n    parameters = {\n            'hidden_layer_sizes': [(100,)],\n            'activation': ['relu','tanh','logistic'],\n            'learning_rate': ['constant','invscaling','adaptive']\n            }\n    cv=GridSearchCV(mlp, parameters, cv=3)\n    cv.fit(tr_features, tr_labels.values.ravel())\n    print_results(cv,'Neural Network (MLP)',os.path.join(path,'MLP_GridSearchCV_results.txt'))\n    cv.best_estimator_\n    MLP_best=cv.best_estimator_\n    MLP_best_param=cv.best_params_\n    joblib.dump(cv.best_estimator_,MLP_model_dir)\n    joblib.dump(cv.best_params_,MLP_params_dir)\nelse:\n    print('Already have MLP')","cc41ca70":"#print(RandomForestClassifier())","8e3a6e50":"# Make Directory\npath=folder_path(OUTPUT_path+'\/Models\/RF')","34ae056f":"RF_model_dir=os.path.join(path,'RF_model.pkl')\nRF_params_dir=os.path.join(path,'RF_params.pkl')\nif os.path.exists(RF_model_dir) == False:\n    rf = RandomForestClassifier(oob_score=True)\n    parameters = {\n            'n_estimators': [500,1000],\n            'max_features': [0.25],\n            'criterion': [\"entropy\"],\n            'max_depth': [1,None]\n            }\n    cv = GridSearchCV(rf, parameters, cv=3)\n    cv.fit(tr_features, tr_labels.values.ravel())\n    print_results(cv,'Random Forest (RF)',os.path.join(path,'RF_GridSearchCV_results.txt'))\n    cv.best_estimator_\n    RF_best=cv.best_estimator_\n    RF_best_param=cv.best_params_\n    joblib.dump(cv.best_estimator_,RF_model_dir)\n    joblib.dump(cv.best_params_,RF_params_dir)\nelse:\n    print('Already have RF')","44734915":"#print(GradientBoostingClassifier())","2e65c779":"# Make Directory\npath=folder_path(OUTPUT_path+'\/Models\/GB')","988df3c4":"GB_model_dir=os.path.join(path,'GB_model.pkl')\nGB_params_dir=os.path.join(path,'GB_params.pkl')\nif os.path.exists(GB_model_dir) == False:\n    gb = GradientBoostingClassifier()\n    parameters = {\n            'n_estimators': [500],\n            'max_depth': [1],\n            'learning_rate': [0.01, 0.1, 1]\n            }\n    cv=GridSearchCV(gb, parameters, cv=3)\n    cv.fit(tr_features, tr_labels.values.ravel())\n    print_results(cv,'Gradient Boost (GB)',os.path.join(path,'GR_GridSearchCV_results.txt'))\n    cv.best_estimator_\n    GB_best=cv.best_estimator_\n    GB_best_param=cv.best_params_\n    joblib.dump(cv.best_estimator_,GB_model_dir)\n    joblib.dump(cv.best_params_,GB_params_dir)\nelse:\n    print('Already have GB') ","0ae8b9e8":"#print(XGBClassifier())","cb1936ff":"# Make Directory\npath=folder_path(OUTPUT_path+'\/Models\/XGB')","fcd8fbae":"XGB_model_dir=os.path.join(path,'XGB_model.pkl')\nXGB_params_dir=os.path.join(path,'XGB_params.pkl')\nif os.path.exists(XGB_model_dir) == False:\n    xgb = XGBClassifier()\n    parameters = {\n            'n_estimators': [500],\n            'max_depth': [1],\n            'learning_rate': [0.01, 0.1, 1]\n            }\n    cv=GridSearchCV(xgb, parameters, cv=3)\n    cv.fit(tr_features, tr_labels.values.ravel())\n    print_results(cv,'eXtreme Gradient Boost (XGB)',os.path.join(path,'XGB_GridSearchCV_results.txt'))\n    cv.best_estimator_\n    XGB_best=cv.best_estimator_\n    XGB_best_param=cv.best_params_\n    joblib.dump(cv.best_estimator_,XGB_model_dir)\n    joblib.dump(cv.best_params_,XGB_params_dir)\nelse:\n    print('Already have XGB')  ","15bba04c":"## all models\nmodels = {}\nparams = {}\n\n#for mdl in ['LR', 'SVM', 'MLP', 'RF', 'GB','XGB']:\nfor mdl in ['LR', 'SVM','MLP', 'RF', 'GB','XGB']:\n    model_path=os.path.join(OUTPUT_path,'Models\/{}\/{}_model.pkl')\n    params_path=os.path.join(OUTPUT_path,'Models\/{}\/{}_params.pkl')\n    models[mdl] = joblib.load(model_path.format(mdl,mdl))\n    params[mdl] = joblib.load(model_path.format(mdl,mdl))","e3e2923e":"def evaluate_model(fig_path,name, model, features, labels, y_test_ev, fc):\n        CM_fig=os.path.join(fig_path,'Figure{}.A_{}_Confusion_Matrix.png'.format(fc,name))\n        VI_fig=os.path.join(fig_path,'Figure{}.B_{}_Variable_Importance_Plot.png'.format(fc,name))\n        \n        start = time()\n        pred = model.predict(features)\n        end = time()\n        y_truth=y_test_ev\n        accuracy = round(accuracy_score(labels, pred), 3)\n        precision = round(precision_score(labels, pred), 3)\n        recall = round(recall_score(labels, pred), 3)\n        print('{} -- Accuracy: {} \/ Precision: {} \/ Recall: {} \/ Latency: {}ms'.format(name,\n                                                                                       accuracy,\n                                                                                       precision,\n                                                                                       recall,\n                                                                                       round((end - start)*1000, 1)))\n        \n        \n        pred=pd.DataFrame(pred)\n        pred.columns=['Survived']\n        # Convert Diagnosis for Cancer from Binary to Categorical\n        diagnosis_name={0:'Died',1:'Lived'}\n        y_truth['Survived']=y_truth['Survived'].map(diagnosis_name)\n        pred['Survived']=pred['Survived'].map(diagnosis_name)\n        class_names = ['Died','Lived']        \n        cm = confusion_matrix(y_test_ev, pred, class_names)\n        \n        FP_L='False Positive'\n        FP = cm[0][1]\n        FN_L='False Negative'\n        FN = cm[1][0]\n        TP_L='True Positive'\n        TP = cm[1][1]\n        TN_L='True Negative'\n        TN = cm[0][0]\n\n        #TPR_L= 'Sensitivity, hit rate, recall, or true positive rate'\n        TPR_L= 'Sensitivity'\n        TPR = round(TP\/(TP+FN),3)\n        #TNR_L= 'Specificity or true negative rate'\n        TNR_L= 'Specificity'\n        TNR = round(TN\/(TN+FP),3) \n        #PPV_L= 'Precision or positive predictive value'\n        PPV_L= 'Precision'\n        PPV = round(TP\/(TP+FP),3)\n        #NPV_L= 'Negative predictive value'\n        NPV_L= 'NPV'\n        NPV = round(TN\/(TN+FN),3)\n        #FPR_L= 'Fall out or false positive rate'\n        FPR_L= 'FPR'\n        FPR = round(FP\/(FP+TN),3)\n        #FNR_L= 'False negative rate'\n        FNR_L= 'FNR'\n        FNR = round(FN\/(TP+FN),3)\n        #FDR_L= 'False discovery rate'\n        FDR_L= 'FDR'\n        FDR = round(FP\/(TP+FP),3)\n\n        ACC_L= 'Accuracy'\n        ACC = round((TP+TN)\/(TP+FP+FN+TN),3)\n        \n        stats_data = {'Name':name,\n                     ACC_L:ACC,\n                     FP_L:FP,\n                     FN_L:FN,\n                     TP_L:TP,\n                     TN_L:TN,\n                     TPR_L:TPR,\n                     TNR_L:TNR,\n                     PPV_L:PPV,\n                     NPV_L:NPV,\n                     FPR_L:FPR,\n                     FNR_L:FDR}\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n        cax = ax.matshow(cm,cmap=plt.cm.gray_r)\n        plt.title('Figure {}.A: {} Confusion Matrix on Unseen Test Data'.format(fc,name),y=1.08)\n        fig.colorbar(cax)\n        ax.set_xticklabels([''] + class_names)\n        ax.set_yticklabels([''] + class_names)\n        # Loop over data dimensions and create text annotations.\n        for i in range(len(class_names)):\n            for j in range(len(class_names)):\n                text = ax.text(j, i, cm[i, j],\n                               ha=\"center\", va=\"center\", color=\"b\")\n        plt.xlabel('Predicted')\n        plt.ylabel('True')\n        plt.savefig(CM_fig,dpi=400,bbox_inches='tight')\n        #plt.show()\n        \n        if  name == 'RF' or name == 'GB' or name == 'XGB': \n            # Get numerical feature importances\n            importances = list(model.feature_importances_)\n            importances=100*(importances\/max(importances))               \n            feature_list = list(features.columns)\n            sorted_ID=np.argsort(importances)   \n            plt.figure(figsize=[10,10])\n            plt.barh(sort_list(feature_list,importances),importances[sorted_ID],align='center')\n            plt.title('Figure {}.B: {} Variable Importance Plot'.format(fc,name))\n            plt.xlabel('Relative Importance')\n            plt.ylabel('Feature') \n            plt.savefig(VI_fig,dpi=300,bbox_inches='tight')\n            #plt.show()\n        \n        return accuracy,name, model, stats_data","f6ad25d6":"    def sort_list(list1, list2): \n        zipped_pairs = zip(list2, list1)   \n        z = [x for _, x in sorted(zipped_pairs)]       \n        return z ","a6978ec0":"def plot_roc_cur(fper, tper,mdl_i,ax): \n    roc_auc=str(round(auc(fper,tper),3))\n    label_i='ROC-'+mdl_i+'  (AUC = '+roc_auc+')'\n    auc\n    ax.plot(fper, tper,label=label_i)\n    ax.set_xlabel('False Positive Rate', size=15, labelpad=20)\n    ax.set_ylabel('True Positive Rate', size=15, labelpad=20)\n    ax.set_xlim([-0.05,1.05])\n    ax.set_ylim([-0.05,1.05])\n\n","f2d741fe":"ev_accuracy=[None]*len(models)\nev_name=[None]*len(models)\nev_model=[None]*len(models)\nev_stats=[None]*len(models)\ncount=1\nf, ax2 = plt.subplots(figsize=(15,15))\nplt.title('Figure 11.  Receiver Operating Characteristic (ROC) Curve')\nplt.xlabel('False Positive Rate',size=15,labelpad=20)\nplt.ylabel('True Positive Rate',size=15,labelpad=20)\nfor name, mdl in models.items():\n        y_test_ev=val_labels\n        fper,tper,thresholds = [],[],[]\n        ev_accuracy[count-1],ev_name[count-1],ev_model[count-1], ev_stats[count-1] = evaluate_model(figures_path,\n                                                                                                    name,\n                                                                                                    mdl,\n                                                                                                    val_features,\n                                                                                                    val_labels,\n                                                                                                    y_test_ev,\n                                                                                                    count+4)\n        diagnosis_name={'Died':0,'Lived':1}\n        val_labels['Survived']=val_labels['Survived'].map(diagnosis_name)\n        if name != 'SVM':\n            y_pred=pd.DataFrame(mdl.predict_proba(pd.DataFrame(val_features))[:,1])\n            fper,tper,thresholds = roc_curve(val_labels,y_pred)\n            plot_disp = plot_roc_cur(fper,tper,name,ax=ax2)\n            \n        count=count+1\nax2.plot([0, 1], [0, 1], color='darkblue', linestyle='--',label='Coin Flip (AUC = 0.5)')\nax2.legend()\nplt.savefig(os.path.join(figures_path,'Figure11.png'),dpi=300,bbox_inches='tight')\n","2d5ce7b5":"pd.DataFrame(ev_stats).head()\n","e64fe6ed":"best_name=ev_name[ev_accuracy.index(max(ev_accuracy))]    #picks the maximum accuracy\nprint('Best Model:',best_name,'with Accuracy of ',max(ev_accuracy))   \nbest_model=ev_model[ev_accuracy.index(max(ev_accuracy))]    #picks the maximum accuracy\n\nif best_name == 'RF' or best_name == 'GB' or best_name == 'XGB': \n    # Get numerical feature importances\n    importances = list(best_model.feature_importances_)\n    importances=100*(importances\/max(importances))               \n    feature_list = list(val_features.columns)\n    sorted_ID=np.argsort(importances)   \n    plt.figure(figsize=[10,10])\n    plt.barh(sort_list(feature_list,importances),importances[sorted_ID],align='center')\n    plt.title('Figure 12:  Variable Importance Plot -- {}'.format(best_name))\n    plt.xlabel('Relative Importance')\n    plt.ylabel('Feature') \n    plt.savefig(os.path.join(figures_path,'Figure12.png'),dpi=300,bbox_inches='tight')\n    plt.show()","536cb20a":"def predict_model(name, model, features):\n    pred = model.predict(features)\n    return pred","cb590025":"VC = VotingClassifier(estimators=[('rfc', RF_best),('MLP',MLP_best),('gbc',GB_best),('xgb',XGB_best)], voting='soft', n_jobs=3)\n\nVC = VC.fit(tr_features, tr_labels.values.ravel())","56bcf4c2":"Use_VC = True\nif Use_VC == True:\n    y_pred=pd.DataFrame(VC.predict(pd.DataFrame(test_clean)))\nelse:\n    for name, mdl in models.items():\n        y_pred=predict_model(best_name,mdl,pd.DataFrame(test_clean))\n        y_pred=pd.DataFrame(y_pred)\n","a5445710":"try:\n    test= pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nexcept OSError:\n    print (\"Input file not found at location:\",'\/kaggle\/input\/titanic\/test.csv')\n    INPUT_path=currentDirectory+'\/Inputs'\n    test_path=os.path.join(INPUT_path,'test.csv')\n    test= pd.read_csv(test_path)\n    print (\"Successfully loaded Input file from:\",INPUT_path)\nelse:\n    print (\"Successfully loaded Input file from Kaggle:\")\n\ntest.head(10) #notice Survival is NOT provided for test dataset\nKaggle_Submission=test\nKaggle_Submission['Survived']=y_pred.astype(int)\nKaggle_Submission=Kaggle_Submission[['PassengerId','Survived']]","02106e05":"Kaggle_Submission_path=os.path.join(currentDirectory,'Kaggle_Submission_Steven_Smiley.csv')\nKaggle_Submission.to_csv(Kaggle_Submission_path,index=False)","c26522b1":"I wanted to use the VotingClassifier feature in order to capture the best response from all of them since they all had high scores.  ","0f18627f":"## eXtreme Gradient Boosting:  \n### Hyperparameter used in GridSearchCV\n#### HP1, n_estimators:  (int) \u2013 Number of trees to fit.\n###### Details\nUsually 500 does the trick and the accuracy and out of bag error doesn't change much after. \n###### Values chosen\n'n_estimators': [5, 50, 250, 500],\n\n#### HP2, max_depth:  (int) \u2013 \nMaximum tree depth for base learners.\n###### Details\nA variety of shallow trees are tested. \n###### Values chosen\n'max_depth': [1, 3, 5, 7, 9],\n\n#### HP3, learning_rate: (float) \u2013 \nBoosting learning rate (xgb\u2019s \u201ceta\u201d)\n###### Details\nA variety was chosen because of the trade-off.\n###### Values chosen\n'learning_rate': [0.01, 0.1, 1]","8b47c418":"#### Unique_Family_Train","32534d00":"# Search for best model","727a02e5":"## Random Forest:  \n### Hyperparameter used in GridSearchCV\n#### HP1, n_estimators:  integer, optional (default=100)\nThe number of trees in the forest.\n\nChanged in version 0.22: The default value of n_estimators changed from 10 to 100 in 0.22.\n###### Details\nUsually 500 does the trick and the accuracy and out of bag error doesn't change much after. \n###### Values chosen\n'n_estimators': [500],\n\n#### HP2, max_depth:  integer or None, optional (default=None)\nThe maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n###### Details\nNone usually does the trick, but a few shallow trees are tested. \n###### Values chosen\n'max_depth': [5,7,9, None]","7d1361bb":"## Neural Network:  \n### Hyperparameter used in GridSearchCV\n#### HP1, hidden_layer_sizes:  tuple, length = n_layers - 2, default (100,)\nThe ith element represents the number of neurons in the ith hidden layer.\n###### Details\nA rule of thumb is (2\/3)*(# of input features) = neurons per hidden layer. \n###### Values chosen\n'hidden_layer_sizes': [(10,),(50,),(100,)]\n\n#### HP2, activation:  {\u2018identity\u2019, \u2018logistic\u2019, \u2018tanh\u2019, \u2018relu\u2019}, default \u2018relu\u2019\nActivation function for the hidden layer.\n###### Details\n* \u2018identity\u2019, no-op activation, useful to implement linear bottleneck, returns f(x) = x\n* \u2018logistic\u2019, the logistic sigmoid function, returns f(x) = 1 \/ (1 + exp(-x)).\n* \u2018tanh\u2019, the hyperbolic tan function, returns f(x) = tanh(x).\n* \u2018relu\u2019, the rectified linear unit function, returns f(x) = max(0, x)   \n\n###### Values chosen\n'activation': ['relu','tanh','logistic'],\n\n#### HP3, learning_rate:  {\u2018constant\u2019, \u2018invscaling\u2019, \u2018adaptive\u2019}, default \u2018constant\u2019\nLearning rate schedule for weight updates.\n###### Details\n* \u2018constant\u2019 is a constant learning rate given by \u2018learning_rate_init\u2019.\n* \u2018invscaling\u2019 gradually decreases the learning rate at each time step \u2018t\u2019 using an inverse scaling exponent of \u2018power_t\u2019. effective_learning_rate = learning_rate_init \/ pow(t, power_t)\n* \u2018adaptive\u2019 keeps the learning rate constant to \u2018learning_rate_init\u2019 as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol if \u2018early_stopping\u2019 is on, the current learning rate is divided by 5.\n\nOnly used when solver='sgd'.\n  \n###### Values chosen\n'learning_rate': ['constant','invscaling','adaptive']","e99065dc":"#### Function:  make_dummies","fcbf0a4e":"### Code_Objective_5.1.4\n### 5.1.4: RF Model","7c4919f2":"### Code_Objective_4.1.1\n###   4.1.1:   Convert Categorical Variables to Continuous\n#### Code_Objective_4.1.1.1\n####   4.1.1.1: Name, Embarked, Sex","37c8d9b4":"## Gradient Boosting:  \n### Hyperparameter used in GridSearchCV\n#### HP1, n_estimators:  int (default=100)\nThe number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.\n###### Details\nUsually 500 does the trick and the accuracy and out of bag error doesn't change much after. \n###### Values chosen\n'n_estimators': [5, 50, 250, 500],\n\n#### HP2, max_depth:  integer, optional (default=3)\nmaximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables.\n###### Details\nA variety of shallow trees are tested. \n###### Values chosen\n'max_depth': [1, 3, 5, 7, 9],\n\n#### HP3, learning_rate:  float, optional (default=0.1)\nlearning rate shrinks the contribution of each tree by learning_rate. There is a trade-off between learning_rate and n_estimators.\n###### Details\nA variety was chosen because of the trade-off.\n###### Values chosen\n'learning_rate': [0.01, 0.1, 1]","48322acf":"#### Function:  sort_list","fbe4ce23":"# Code_Objective_1\n# 1: Import Libraries","b1dc3230":"#### Function: folder_path","ca9b2fb4":"# Table of Contents (TOC)","ffbfe4cd":"### Code_Objective_4.0.0\n### 4.0.0: Copy & View Data","ead9ebb6":"All of the models were similar, therefore only the highest accuracy model was chosen.  The variable importance plot shows the feature importance, which is similar to the intensities of the heatmaps previously. ","af0f9ab6":"#### Convert Sex to numeric","b527d5cd":"# Code_Objective_3\n# 3: Import Data and View Data","78281c32":"#### The Histogram Plot\nI love this figure below.  It really gives us some insight into how the data is distributed.  \nFirst, I need to drop `'Survived','PassengerId','Name','Cabin','Ticket','Embarked'` because:\n* `Survived` is not in the test data to look at (but can be seen for training with the blue and orange columns); \n* `PassengerId` does not tell us anything because each passenger has their own unique Id;\n* `Name` is an \"object\" we need to sort, and we can see the importance for it later;  \n* `Cabin` has a bunch of null values and we need to sort, and we can see the importance for it as well later;\n* `Ticket` is an \"object\" that we will look at later.\n* `Embarked` is an \"object\" that we will look at later as well.\n\nThe `train_clean` data is shown with the `test_clean` data in this histogram.  \n\n  * Blue is for the people that died from the training set, `Training-Died`\n  * Orange is for the people that survived from the training set, `Training-Survived`\n  * Green is a combination of all the people in the training set, `Training-ALL`\n  * Red is a combination of all the people in the test set, `Test-ALL` [We don't have the survival info for this!]\n  \nWe can see for that the distribution for each feature is relatively equally sampled between `train_clean` and `test_clean` by looking at the heights of all of the Green and Red histogram bars for `Training-ALL` and `Test-ALL` respectively.  The Green [training] columns are ~double the height of the Red [testing] columns.  This is good because we don't need to worry about training the models for an unequal distribution between the test and training sets based on features.\n\nLooking at the training data alone between people that `Died` and people that `Survived`:\n* `Pclass` The Social-Class of the passenger (i.e. PClass=1 is upper class, PClass=2 is middle class, PClass=3 is lower class.)\n    * `Training-Died`[Blue] vs. `Training-Survived`[Orange]\n        * PClass 1: More people survived!,\n        * PClass 2: Roughly equal,\n        * PClass 3: Way more people died!\n* `Sex` The sex of the individual (male or female).\n    * `Training-Died`[Blue] vs. `Training-Survived`[Orange]\n        * male vs. female: way more men died!\n* `Age` The age of the passenger.\n    * `Training-Died`[Blue] vs. `Training-Survived`[Orange]\n        * Age 0-10 years: nearly twice as many young kids survived!\n        * Age >10 years: more died than survived!\n* `SibSp` Number of siblings and spouses the passenger had.\n    * `Training-Died`[Blue] vs. `Training-Survived`[Orange]\n        * SibSp 0: more likely to die by nearly double!\n        * SibSp 1: a little more likely to survive, but not much,\n        * SibSp >1: more likely to die.\n* `Parch` Number of parents and kids the passenger had.\n    * `Training-Died`[Blue] vs. `Training-Survived`[Orange]\n        * Parch 0: more likely to die by nearly double!\n        * Parch 1-2: Roughly equal,\n        * Parch >1: hard to say, not much to see on this one.\n* `Fare` The cost (dollars) of the ticket for the passenger.\n    * `Training-Died`[Blue] vs. `Training-Survived`[Orange]\n        * Fare ~0-75: more likely to die by nearly double!\n        * Fare >75: more likely to survive!\n        \nNow that we have an idea of the data we got, lets get it organized and cleaned up.  A key to machine learning is to making sure the data feeding the algorithms is in the proper form.  Since this is a Binary Classification problem (Survived vs. Died), we should try to get the input data in a numerical form that is standardized.  With some data cleaning, we can convert some of these features into Binary and also extract features that might help the algorithms make better models for predicting Survival.  Also, we want to fill in or look at the missing values and see if we can get rid of any data that is not adding much value to training our algorithms.\n","2067360b":"# Code_Objective_5\n# 5:  Machine Learning\n## Code_Objective_5.0\n##  5.0 Training Data Split","cbe52b6d":"We want to make a copy of the data so that we don't overwrite the original.  I will call the copy datasets as `test_clean` and `train_clean` for the raw `test` and `train` datasets respectively.  ","e162b0bb":"### Code_Objective_5.1.5\n### 5.1.5: GB Model","ac49481d":"### GridSearch CV\n\nclass sklearn.model_selection.GridSearchCV(estimator, param_grid, scoring=None, n_jobs=None, iid='deprecated', refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)[source]\u00b6\n\nExhaustive search over specified parameter values for an estimator.\n\nImportant members are fit, predict.\n\nGridSearchCV implements a \u201cfit\u201d and a \u201cscore\u201d method. It also implements \u201cpredict\u201d, \u201cpredict_proba\u201d, \u201cdecision_function\u201d, \u201ctransform\u201d and \u201cinverse_transform\u201d if they are implemented in the estimator used.\n\nThe parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.","84aee3f4":"# Steven Smiley's Kaggle Competition for the Titanic Dataset Notebook:\n* ## Data Cleaning, Data Exploring, Feature Extraction\n* ## Machine Learning, Supervised \n* ### Random Forest, Neural Networks, Logistic Regression, Support Vector Machine, Gradient Boosting, eXtreme Gradient Boosting\n    \"\"\"\n    Created on Fri Dec 27 20:58:05 2019\n\n    @author: stevensmiley\n\n    Kaggle Competition Project Description: \n\n       This code by Steven Smiley is for\n       evaluating the \"Titanic: Machine Learning from Disaster\" dataset from Kaggle.\n\n       Two datasets were given for the competition:\n       1) test.csv\n       2) train.csv\n\n       The \"train.csv\" dataset is used to create a model \n       to predict Survival Rate with the \"test.csv\" dataset.","19a4751b":"There are different names between the Training and Test set.  Therefore, we will get rid of the columns we make below that are not unique to the Test set since they will not help predict by themselves anyt","48e23fa1":"### Code_Objective_5.1.0\n### 5.1.0 Hyperparameter using GridSearch CV","80cd7ee6":"### Code_Objective_4.0.1\n###   4.0.1 Find Missing Values","e7ee5a9d":"### Code_Objective_5.1.2\n### 5.1.2:  SVM Model","b35d4821":"## Code_Objective_5.1\n## 5.1: Machine Learning          \n","2bfbe96b":"The first step is to explore the data.","b3050c37":"## Logistic Regression:  Hyperparameter used in GridSearchCV\n### HP1, C:  float, optional (default=1.0)\nInverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n##### Details\nRegularization is when a penality is applied with increasing value to prevent overfitting.  The inverse of regularization strength means as the value of C goes up, the value of the regularization strength goes down and vice versa.  \n##### Values chosen\n'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]","c003e670":"### Figure 2:  Heatmap [training data]","32baae6a":"### missing_train\n   Shows the missing values in the training set","f15d18ca":"In order to find a good model, several algorithms are tested on the training dataset.  A senstivity study using different Hyperparameters of the algorithms are iterated on with GridSearchCV in order optimize each model.  The best model is the one that has the highest accuracy without overfitting by looking at both the training data and the validation data results.  Computer time does not appear to be an issue for these models, so it has little weight on deciding between models.  ","710f8561":"#### Function:  evaluate_model","1e92baea":"#### Convert Name, Embarked, and Pclass to unique binary arrays","4b139865":"We want to explore the data by looking for missing or null values as well as overall trends.  Then clean the data based on what we find so it is better able to be used with a Machine Learning algorithm to make a model.  We then extract some features that we think would give a model stronger predictive power.","fd2e9f8a":"## Before Numeric Conversion\n","38319284":"The heatmap shows how well correlated each feature is to the other features in the training set.  Before cleaning, some features don't correlate with the Survived feature that much.  For example, the PassengerId doesn't show much correlation at all to the Survived feature, which makes sense because it is the index of the passenger.  ","97fc97d2":"## Support Vector Machine:  \n### Hyperparameter used in GridSearchCV\n#### HP1,  kernelstring, optional (default=\u2019rbf\u2019)\nSpecifies the kernel type to be used in the algorithm. It must be one of \u2018linear\u2019, \u2018poly\u2019, \u2018rbf\u2019, \u2018sigmoid\u2019, \u2018precomputed\u2019 or a callable. If none is given, \u2018rbf\u2019 will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; that matrix should be an array of shape (n_samples, n_samples).\n###### Details\nA linear kernel type is good when the data is Linearly seperable, which means it can be separated by a single Line.\nA radial basis function (rbf) kernel type is an expontential function of the squared Euclidean distance between two vectors and a constant.  Since the value of RBF kernel decreases with distance and ranges between zero and one, it has a ready interpretation as a similiarity measure.  \n###### Values chosen\n'kernel': ['linear','rbf']\n\n#### HP2,  C:  float, optional (default=1.0)\nRegularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.\n###### Details\nRegularization is when a penality is applied with increasing value to prevent overfitting.  The inverse of regularization strength means as the value of C goes up, the value of the regularization strength goes down and vice versa.  \n###### Values chosen\n'C': [0.1, 1, 10]","e741bbcf":"### missing_train_clean\n   Verify there is no missing values in the training set","edd06959":"### missing_test\n   Shows the missing values in the test set","f50be517":"#### Function:  predict_model","2a46f023":"#### Code_Objective_4.1.2.1\n####   4.1.2.1 Ticket & Passenger ID\nThe Ticket appears random and is assumed to have no relationship with the Survival.  Therefore, Ticket is removed.\nAlso, the PassengerID holds no value because each passenger has a unique number.","078597d4":"#### Combine SibSp & Parch for Family Count","62eb2a04":"#### Code_Objective_4.0.2.2\n####      4.0.2.2: Cabin","cb30a89e":"## Hide Warnings","015a6455":"* [1: Import Libraries](#Code_Objective_1)\n* [2: Get Current Directory](#Code_Objective_2)\n* [3: Import and View Data:](#Code_Objective_3)\n* [4: Explore Data, Clean Data, & Feature Extraction:](#Code_Objective_4)\n    * [4.0:  Explore Data:](#Code_Objective_4.0)\n        * [4.0.0: Copy Data](#Code_Objective_4.0.0)\n        * [4.0.1: Find Missing Values](#Code_Objective_4.0.1)\n        * [4.0.2: Explore Data: Fix Missing Values](#Code_Objective_4.0.2)\n           * [4.0.2.1: Age](#Code_Objective_4.0.2.1)\n           * [4.0.2.2: Cabin](#Code_Objective_4.0.2.2)\n           * [4.0.2.3: Age](#Code_Objective_4.0.2.1)\n           * [4.0.2.4: Cabin](#Code_Objective_4.0.2.2)\n    * [4.1:  Clean Data:](#Code_Objective_4.1)\n        * [4.1.0:  Change Name to Prefix:](#Code_Objective_4.1.0)\n        * [4.1.1:  Convert Categorical Variables to Continuous:](#Code_Objective_4.1.1)\n           * [4.1.1.1: Embarked, Sex, Name:](#Code_Objective_4.1.1.1)\n           * [4.1.2.1: Ticket:](#Code_Objective_4.1.2.1)\n        * [4.2.0:  Drop:  PassengerId, Embarked, Parch, SibSp](#Code_Objective_4.2.0)\n        * [4.3.0:  Principal Component Analysis:](#Code_Objective_4.3.0)\n* [5:  Machine Learning:](#Code_Objective_5)\n    * [5.0:  Training Data Split:](#Code_Objective_5.0)\n    * [5.1:  Models:](#Code_Objective_5.1)            \n        * [5.1.1:  Logistic Regression Model](#Code_Objective_5.1.1)\n        * [5.1.2:  SVM Model](#Code_Objective_5.1.2)\n        * [5.1.3:  MLP Model](#Code_Objective_5.1.3)\n        * [5.1.4:  RF Model](#Code_Objective_5.1.4)\n        * [5.1.5:  GB Model](#Code_Objective_5.1.5)\n        * [5.1.6:  XGB Model](#Code_Objective_5.1.6)\n    * [5.2:  Evaluate Models](#Code_Objective_5.2)\n    * [5.3:  Predict with Best Model](#Code_Objective_5.3)\n* [6: Export Results for Kaggle Competition](#Code_Objective_6) \n                        ","054b8590":"####       5.0.2.3 Explore Data: Fix Missing Values: Embarked\n\nMissing 2 values in train set of 891, and 0 values in test set of 418.  Therefore, not missing too much information, so we can delete these rows.\n","76b10d0a":"#### Unique_Names_Train","70510f1c":"## Code_Objective_4.1\n##   4.1      Data Cleaning:\n\n### Code_Objective_4.1.0\n###   4.1.0    Change Name to Prefix","23e6cdab":"## Code_Objective_5.2\n## 5.2: Evaluate Models\n","7ddea614":"### Code_Objective_4.0.2\n###   4.0.2: Fix Missing Values\n####       Code_Objective_4.0.2.1\n####       4.0.2.1: Age\nMissing 177 values in train set of 891, and 86 values in test set of 418.  Therefore, missing significant portion of the data.  Fill missing values with the average value to prevent bias.\n","bd23f9c5":"From the `train_clean` data, we can see that there are 891 unique passengers that boarded the Titanic with 12 different features of the form: integers, real numbers, and objects.  On the otherhand, the `test_clean` data has 418 unique passengers that boarded the ship with 11 different features of the same form.  However, the `test_clean` data does not include the `Survived` column, which is why it is heldout for the competition.  Thus, we want to create a model that will predict the missing `Survived` column for the `test_clean` dataset.  In order to create such a model, we should look for as many relationships to surival with our training dataset.\n","9b336d2c":"### Code_Objective_5.1.3\n### 5.1.3: MLP Model","e500ce7d":"### Code_Objective_5.1.1\n### 5.1.1:  Logistic Regression Model ","04db6c67":"### Code_Objective_5.1.6\n### 5.1.6: XGB Model","866547d3":"## Make Directories for Output Files","73d5baa5":"# Code_Objective_6\n# 6: Export Results for Kaggle Competition ","39479308":"\n\nVerify missing cabin correlates with survival using point plot.  The point plot represents the estimate of central tendency for a numeric variable (Cabin) by the position of scatter plot points and provides some indication of the uncertainty around that estimate using error bars. \n        \n","492e1a24":"#### Function:  print_results","8975483b":"### Code_Objective_4.2.0\n###   4.2.0: Drop:  Parch, SibSp, Fare, Age","d4b99459":"#### Code_Objective_4.0.2.5\n####       4.0.2.5 Store Missing Trian Information","55b0c667":"# Code_Objective_2\n# 2: Get Current Directory","1476d0cf":"#### View Raw Data ","848dbe9a":"## Code_Objective_4.0\n## 4.0: Explore Data","42947454":"The heatmap shows how well correlated each feature is to the other features in the training set.  After cleaning, the Survived feature has a higher correlation with the other features used to train a model on it. ","e3282138":"### missing_test_clean\n   Verify there is no missing values in the test set","cabfe314":"# Code_Objective_4\n# 4: Explore Data, Clean Data, Feature Extraction","ab4a6b70":"#### Code_Objective_4.0.2.4\n####       4.0.2.4: Fare\n\nMissing 0 values in train set of 891, and 1 values in test set of 418.  Therefore, not missing too much information, but it is in the test data. This person did not have a cabin, so the fare is probably cheaper than just the average.  So we can use the average of the fare for peopel without a cabin for this value without causing bias.\n","b40bc151":"### Figure 3:  Heatmap [training data, cleaned]","5d9dce04":"## Code_Objective_5.3\n## 5.3: Predict with Best Model"}}