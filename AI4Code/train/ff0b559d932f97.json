{"cell_type":{"a185d097":"code","6bdda6a8":"code","96e8cd3d":"code","379bee41":"code","3cc295a7":"code","1a614488":"code","43950664":"code","041be2af":"code","a1dd0c94":"code","0740a308":"code","ddb28606":"code","0c9f5e61":"code","64f735d4":"code","905cab75":"code","bf629531":"code","7381b64a":"code","72474ad2":"code","ba61e96e":"code","ce78cf6e":"code","41b923a4":"code","de5b43e1":"code","d192ae86":"code","ff78f317":"code","bbb18061":"code","f1172be3":"code","40c5402e":"code","54830a3f":"code","de735775":"code","2192f1aa":"code","6a44fa27":"code","3197f967":"code","bea1b16e":"code","57442441":"code","7d5e0330":"code","9b9df5db":"code","77c9612d":"code","edb380c0":"code","3bd70465":"code","11128d70":"code","1c4aa07e":"code","8e46ee44":"code","a3ac09d3":"code","1f03bde2":"code","7a5f7f2e":"code","62945a0c":"code","95f0e653":"code","fb5fd310":"code","a60c6ba0":"code","11f41df3":"code","57757976":"code","1ab3c555":"code","456d9682":"code","96f1c696":"code","016241bb":"code","aac34554":"code","617bc875":"code","c9224c92":"code","22d92366":"code","f055b4bd":"code","b0ac3832":"code","98f79eac":"code","4c05fd0a":"code","bf86523e":"code","be3763f9":"code","3f2ec021":"code","44f99198":"code","bad85898":"code","7b0bc337":"code","5536f6d8":"code","ccccc0b1":"code","a130c032":"code","8dfc7796":"code","bc94d782":"code","407ae952":"code","4d6b43c0":"code","2d76e926":"code","e695f826":"code","43d38bd0":"code","4723763a":"code","3f1a5c04":"code","cf3e7341":"code","f7618847":"code","8c4e0684":"code","3c94a75a":"code","9c1d9a05":"code","8fb32f9f":"code","d010819b":"code","ae792807":"code","ffa425b9":"code","8abf4826":"code","b364357c":"code","9f1d6752":"code","52154fcf":"code","eb74eb5b":"code","4c277c0d":"markdown","170d5fac":"markdown","d695c0ec":"markdown","efb64501":"markdown","577af4be":"markdown","79870e3c":"markdown","dcc16d8a":"markdown","96249da4":"markdown","ba95a21e":"markdown","e96e7df7":"markdown","7cf1fdd0":"markdown","cb879c45":"markdown","171b5675":"markdown","feb70445":"markdown","48dc27ff":"markdown","ae1ac67f":"markdown","5ec053bb":"markdown","f1960c7e":"markdown","b5967905":"markdown","a9a44ae9":"markdown","8f78f75d":"markdown","fc7f095a":"markdown","ade43f10":"markdown","3b344126":"markdown","d07c4610":"markdown","bf039faf":"markdown","854fd194":"markdown","9f476e1f":"markdown","3aa22eab":"markdown","b2609c65":"markdown","d43a0981":"markdown","c8ad15ec":"markdown","3afc10b4":"markdown","e3785bcd":"markdown","ccde6e6b":"markdown","669dd858":"markdown","0a78bb88":"markdown","39bd6211":"markdown","d3434da0":"markdown","35a1d3d9":"markdown","6ccb7c10":"markdown","8eb33ecc":"markdown","672a68aa":"markdown","0c3ca5ef":"markdown","08a3b5ce":"markdown","2c774a6c":"markdown","2686eeca":"markdown","20c71a6e":"markdown"},"source":{"a185d097":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6bdda6a8":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","96e8cd3d":"train=pd.read_csv('..\/input\/titanic\/train.csv')\ntest=pd.read_csv('..\/input\/titanic\/test.csv')","379bee41":"#We will combine train and test data for better Analysis\ntrain['source']='train'\ntest['source']='test'\ntest['Survived']=np.NaN\n\n#Concatenating the data in df variable\ndf=pd.concat([train,test],sort=False,ignore_index=True)","3cc295a7":"df.head()","1a614488":"df.shape","43950664":"df.nunique()","041be2af":"df.isnull().sum()","a1dd0c94":"df.dtypes","0740a308":"df.describe()","ddb28606":"sns.countplot(df['Survived'])","0c9f5e61":"sns.countplot(df['Pclass'])","64f735d4":"sns.countplot(df['Sex'])","905cab75":"sns.histplot(df['Age'])","bf629531":"sns.countplot(df['SibSp'])","7381b64a":"sns.countplot(df['Parch'])","72474ad2":"sns.distplot(df['Fare'])","ba61e96e":"sns.countplot(df['Embarked'])","ce78cf6e":"data=df.copy()\ndata.drop(['PassengerId','Name','Ticket','Cabin','Sex','Embarked','Survived','Pclass','source'],axis=1,inplace=True)\ndata.head()","41b923a4":"for i in data.columns:\n    sns.boxplot(data[i])\n    plt.figure()","de5b43e1":"sns.distplot(data['Age'])","d192ae86":"sns.distplot(data['Fare'])","ff78f317":"sns.violinplot(data['Age'])","bbb18061":"sns.violinplot(data['Fare'])","f1172be3":"sns.barplot(x=df['Sex'],y=df['Survived'])","40c5402e":"sns.barplot(x='SibSp',y='Survived',data=df)","54830a3f":"sns.barplot(x='Embarked',y='Survived',data=df)","de735775":"sns.barplot(x='Pclass',y='Survived',data=df)","2192f1aa":"sns.barplot(x='Parch',y='Survived',data=df)","6a44fa27":"sns.scatterplot(x=df['Age'],y=df['Fare'],hue=df['Survived'])","3197f967":"plt.figure(figsize=(8,8))\nsns.swarmplot(x=df['Pclass'],y=df['Age'],hue=df['Survived'])","bea1b16e":"fig = plt.figure(figsize=(15,5))\nax1 = fig.add_subplot(1,2,1)\nax2 = fig.add_subplot(1,2,2)\n\nsns.violinplot(x='Pclass', y='Age', hue='Survived', data=df, split=True, ax=ax1)\nsns.violinplot(x='Sex', y='Age', hue='Survived', data=df, split=True, ax=ax2)","57442441":"plt.figure(figsize=(8,8))\nsns.factorplot(x='Sex', y='Survived', hue='Pclass', data=df)","7d5e0330":"plt.figure(figsize=(15,10))\nsns.heatmap(df.corr(),annot=True,cmap='Greys')","9b9df5db":"#Extracting titles from Name column\ndf['Title']=df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)","77c9612d":"df.head()","edb380c0":"pd.crosstab(df['Title'],df['Sex'])","3bd70465":"#We replace rarely ocuurling titles with 'rare'\ndf['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir',\n                                   'Jonkheer', 'Dona'], 'Rare')","11128d70":"df['Title'].value_counts()","1c4aa07e":"df['Title'].replace(['Mlle','Ms'],'Miss',inplace=True)\ndf['Title'].replace('Mme','Mrs',inplace=True)\ndf['Title'].value_counts()","8e46ee44":"mean = train[\"Age\"].mean()\nstd = test[\"Age\"].std()\nis_null = df[\"Age\"].isnull().sum()\n# compute random numbers between the mean, std and is_null\nrand_age = np.random.randint(mean - std, mean + std, size = is_null)\n# fill NaN values in Age column with random values generated\nage_slice = df[\"Age\"].copy()\nage_slice[np.isnan(age_slice)] = rand_age\ndf[\"Age\"] = age_slice","a3ac09d3":"embarked_mode = train['Embarked'].mode()\ndf['Embarked'] = df['Embarked'].fillna(embarked_mode)","1f03bde2":"df['Embarked'].fillna(train['Embarked'].mode()[0],inplace=True)","7a5f7f2e":"df.isnull().sum()","62945a0c":"#Since too many null values we drop this column\ndf.drop('Cabin',axis=1,inplace=True)","95f0e653":"#We add another column by adding siblings and parent column\ndf['Family']=df['SibSp']+df['Parch']","fb5fd310":"df.head()","a60c6ba0":"#We drop the identifier columns\ndf.drop(['PassengerId','Name','Ticket'],axis=1,inplace=True)","11f41df3":"#we Encode Sex, Embarked and title column\nfrom sklearn.preprocessing import OrdinalEncoder\ne=OrdinalEncoder()","57757976":"df['Sex']=e.fit_transform(df['Sex'].values.reshape(-1,1))\ndf['Title']=e.fit_transform(df['Title'].values.reshape(-1,1))\ndf['Embarked']=e.fit_transform(df['Embarked'].values.reshape(-1,1))","1ab3c555":"train = df.loc[df['source']=='train']\ntest = df.loc[df['source']=='test']","456d9682":"train.drop('source',axis=1,inplace=True)\ntest.drop(['source','Survived'],axis=1,inplace=True)","96f1c696":"#We separate the data\nx=train.iloc[:,1:]\ny=train.iloc[:,0]","016241bb":"from imblearn.over_sampling import SMOTE\nover=SMOTE()","aac34554":"x,y=over.fit_resample(x,y)","617bc875":"#We scale the independent variables\nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()","c9224c92":"#train dataset\nxd=scaler.fit_transform(x)\nx=pd.DataFrame(xd,columns=x.columns)","22d92366":"#test dataset\ntestd=scaler.fit_transform(test)\ntest=pd.DataFrame(testd,columns=test.columns)","f055b4bd":"x.head()","b0ac3832":"test.head()","98f79eac":"#We import Classification Models\nfrom sklearn.naive_bayes import  GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier","4c05fd0a":"from sklearn.model_selection import train_test_split, cross_val_score","bf86523e":"from sklearn.metrics import accuracy_score,confusion_matrix,classification_report,roc_auc_score,roc_curve","be3763f9":"#Function to find the best random state\ndef randomstate(x,y):\n    maxx=0\n    model=LogisticRegression()\n    for i in range(1,201):\n        xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.25,random_state=i)\n        model.fit(xtrain,ytrain)\n        p=model.predict(xtest)\n        accu=accuracy_score(p,ytest)\n        if accu>maxx:\n            maxx=accu\n            j=i\n    return j","3f2ec021":"#To evakuate performances of all the models\ndef performance(p,ytest,m,xtest,s):\n    print('------------------------------------',m,'------------------------------------')\n    print('Accuracy',np.round(accuracy_score(p,ytest),4))\n    print('----------------------------------------------------------')\n    print('Mean of Cross Validation Score',np.round(s.mean(),4))\n    print('----------------------------------------------------------')\n    print('AUC_ROC Score',np.round(roc_auc_score(ytest,m.predict_proba(xtest)[:,1]),4))\n    print('----------------------------------------------------------')\n    print('Confusion Matrix')\n    print(confusion_matrix(p,ytest))\n    print('----------------------------------------------------------')\n    print('Classification Report')\n    print(classification_report(p,ytest))","44f99198":"models=[GaussianNB(),KNeighborsClassifier(),SVC(probability=True),LogisticRegression(),DecisionTreeClassifier(),\n        RandomForestClassifier(),AdaBoostClassifier(),GradientBoostingClassifier(),XGBClassifier(verbosity=0)]","bad85898":"#Creates and trains model from the models list\ndef createmodel(trainx,testx,trainy,testy):\n    for i in models:\n        model=i\n        model.fit(trainx,trainy)\n        p=model.predict(testx)\n        score=cross_val_score(model,x,y,cv=10)\n        performance(p,testy,model,testx,score)\n        ","7b0bc337":"xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.25,random_state=randomstate(x,y))\ncreatemodel(xtrain,xtest,ytrain,ytest)","5536f6d8":"from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nxtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.25,random_state=randomstate(x,y))","ccccc0b1":"params={'base_estimator':[SVC(),LogisticRegression(),DecisionTreeClassifier()],\n      'n_estimators':[50, 100, 150, 200, 250, 300],\n      'learning_rate':[0.0001,0.001,0.01,0.1,]}","a130c032":"g=GridSearchCV(AdaBoostClassifier(),params,cv=10)","8dfc7796":"g.fit(xtrain,ytrain)","bc94d782":"print(g.best_params_)\nprint(g.best_score_)\nprint(g.best_estimator_)","407ae952":"m=AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), learning_rate=0.1,n_estimators=250)\nm.fit(xtrain,ytrain)\np=m.predict(xtest)\nscore=cross_val_score(m,x,y,cv=10)\nperformance(p,ytest,m,xtest,score)","4d6b43c0":"params={'n_estimators':[100, 200, 300, 400, 500],\n            'criterion':['gini','entropty'],\n            'max_depth':[None,1,2,3,4,5,6,7,8,9,10,15,20,25,30,35,40],\n           'max_features':['int','float','auto','log2']}","2d76e926":"g=GridSearchCV(RandomForestClassifier(),params,cv=10)","e695f826":"g.fit(xtrain,ytrain)","43d38bd0":"print(g.best_params_)\nprint(g.best_score_)\nprint(g.best_estimator_)","4723763a":"m=RandomForestClassifier(max_depth=7, n_estimators=400)\nm.fit(xtrain,ytrain)\np=m.predict(xtest)\nscore=cross_val_score(m,x,y,cv=10)\nperformance(p,ytest,m,xtest,score)","3f1a5c04":"params={\n \"learning_rate\"    : [0.01,0.05, 0.10] ,\n \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15,20,25,30,35,40,None],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n    \n}","cf3e7341":"g=GridSearchCV(XGBClassifier(),params,cv=10)","f7618847":"g.fit(xtrain,ytrain)","8c4e0684":"print(g.best_params_)\nprint(g.best_score_)\nprint(g.best_estimator_)","3c94a75a":"m=XGBClassifier(colsample_bytree=0.4, gamma=0.4, learning_rate=0.05, max_depth= 12, min_child_weight= 1)\nm.fit(xtrain,ytrain)\np=m.predict(xtest)\nscore=cross_val_score(m,x,y,cv=10)\nperformance(p,ytest,m,xtest,score)","9c1d9a05":"model=XGBClassifier(colsample_bytree=0.4, gamma=0.4, learning_rate=0.05, max_depth= 12, min_child_weight= 1)\nmodel.fit(xtrain,ytrain)\np=model.predict(xtest)\nscore=cross_val_score(model,x,y,cv=10)","8fb32f9f":"performance(p,ytest,model,xtest,score)","d010819b":"fpred=pd.Series(model.predict_proba(xtest)[:,1])\nfpr,tpr,threshold=roc_curve(ytest,fpred)","ae792807":"plt.plot(fpr,tpr,color='k',label='ROC')\nplt.plot([0,1],[0,1],color='b',linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC-AUC curve')\nplt.legend()","ffa425b9":"predictions=model.predict(test)","8abf4826":"predictions=pd.DataFrame(predictions,columns=['Survived'])","b364357c":"predictions.head()","9f1d6752":"predictions['Survived']=predictions['Survived'].apply(lambda x: int(x))","52154fcf":"predictions.head()","eb74eb5b":"predictions.to_csv('test_predictions.csv')","4c277c0d":"# Modelling Phase","170d5fac":"Survived, Sex, Pclass, SibSp, Parch and Embarked columns have categorical values rest are continuous.\n","d695c0ec":"Less than 200 passengers had parents\/guardian travelling with them and with some there were 6 guardians with a single child.","efb64501":"##### Univariate Analysis","577af4be":"There we almost double the number of male passengers than female","79870e3c":"Most of the people boarded the ship from S harbor followed by C and least from Q","dcc16d8a":"##### XGBoost","96249da4":"More than 70% women survived and less than 20% men survived","ba95a21e":"We choose XGBClassifier as our final model which seems to give the best mean cross validation score","e96e7df7":"### Finalizing the model","7cf1fdd0":"Survived feature shows positive correlation with fare paid, and very little with parch column whereas it shows string negative correlation with Pclass.","cb879c45":"# EDA","171b5675":"People with age between 20 to 30 were the majority among ages, while people also travelled with children of age below 5","feb70445":"Most people paid fare less than 50 and some even paid 500","48dc27ff":"##### Ada Boost","ae1ac67f":"Survival rate of passengers having one sibling or spouse was maximum.","5ec053bb":"# Pre-Processing Pipeline","f1960c7e":"****Handling Imbalanced Dataset","b5967905":"##### Filling NaN Values","a9a44ae9":"Age, Fare, Cabin and Embarked have missing values present, while survived have missing values from test set.","8f78f75d":"People boarding from C harbor had most survival rate","fc7f095a":"### Evaluation Metrics","ade43f10":"Almost all Women from 1st and 2nd class survived whereas very few men survived from 1st class and least from 2nd and 3rd classes","3b344126":"##### Multivariate Analysis","d07c4610":"****Separating the data frame into train and test\u00b6","bf039faf":"Most passengers were from age between 20 and 40.","854fd194":"Passesngers who had paid highe fares have survived the most","9f476e1f":"![](https:\/\/pavbca.com\/walldb\/original\/9\/7\/f\/456066.jpg)","3aa22eab":"Data skewed to the right","b2609c65":"![](https:\/\/i.imgflip.com\/36pwb5.jpg)","d43a0981":"Name, Sex, Ticket, Cabin, Embarked columns are of object type rest of the columns are numerical.","c8ad15ec":"Majority of the people had paid less than 100 for the fare whereas there were elite classes also present who had paid 500.","3afc10b4":"Data is almost normally distributed","e3785bcd":"There are more people that have died than people who survived.","ccde6e6b":"### Predicting the Test dataset","669dd858":"Mostly siblings or spouses travelling with each passenger were none, one or two but there were passengers present with 8 siblings also.","0a78bb88":"Dataframe have 1309 rows and 13 columns including the source column","39bd6211":"##### Bivariate Analysis","d3434da0":"There are more old people in 1st class, most of the children srom 2nd class survived also children from 3rd also survived. Most male children survived and women from 20 to 40 had much better survival rate.","35a1d3d9":"Variance of Survived, Pclass, Pclass and Parch column is close to zero. Mean is greater than median in Pclass column, showing that data is skewed towards left, in rest of the columns data is skewed towards right as median is greater. Outliers seem to be present as min interquartile range and maximun ranged do not have same difference.","6ccb7c10":"Apart from tiles,there aare Mlle, Mme, and Ms we are misspelled","8eb33ecc":"Survival rate of passenges having parents\/guardians 1,2 or was more than 50%. Amost none of the passenger survived who had Parch 4 or 5.","672a68aa":"Most people travelled with 3rd class ticket followed by 1st class the 2nd class","0c3ca5ef":"People from 1st class survived the most and almost all the children from 2 class survived while very few passengers survived from 3rd class","08a3b5ce":"A large no. of outliers are present in Fare and Age.","2c774a6c":"##### Random Forest","2686eeca":"Almost all models are giving same performance, still top performing models are SVC, AdaBoost, Random Forest, XGBoost and Gradient Boost. So we apply Hyperparameter tuning on them","20c71a6e":"# Hyperparameter Tuning"}}