{"cell_type":{"97b42fb5":"code","71e7f589":"code","1af26a64":"code","c47efa23":"code","67b95ab4":"code","59bf9d42":"code","27fd6546":"code","7a4003ea":"code","264a0f47":"code","e2cf7ac2":"code","7d1d18ee":"code","302bcc5c":"code","647dddad":"code","591c5c6b":"code","ad63e464":"code","85cd50ff":"code","46a9632e":"code","565fd9ff":"code","781f9f71":"code","4b23dc10":"code","f8cd282a":"code","dd59d1cd":"code","fea7b2f3":"code","524e757f":"code","550c4026":"code","e90419d6":"code","cc6ff38b":"code","00c7e17d":"code","a2086fad":"code","8f4f00c2":"code","8bfb7af0":"code","3ec553d4":"code","c5a2bcaa":"code","bd101115":"code","b5b1ce25":"code","478a72df":"code","3fca0123":"code","f762ed3e":"code","bd6d5fac":"code","5a71b87c":"code","1391a702":"code","dc9b28f7":"code","fe8e56e7":"code","03da8226":"code","f476ae37":"code","d34a1178":"code","2b71d892":"code","59211570":"code","af4b47fc":"code","597bb50c":"code","d908a21f":"code","07ab6850":"code","8ae86276":"code","c88b4173":"code","25f3ac87":"code","78f8ed55":"code","8ff4ab59":"code","7f1940cc":"code","38ade03a":"code","b2b48981":"code","cf0516f8":"code","da69ed48":"code","8b0f24c0":"code","3733356a":"code","567644bb":"code","db630086":"code","dbc3a761":"code","76b2231c":"code","5c6a1d04":"code","bed0363c":"code","d56a491d":"code","bf64a208":"code","96fee442":"code","a01cbb9a":"code","59a02ddb":"code","fce17f29":"code","1e1eabdf":"code","7292736b":"code","d4e0d664":"code","ed523656":"code","9f8498b5":"code","d6b2defd":"code","7f631699":"code","0bb33ab9":"code","10cf1595":"markdown","a855f6c2":"markdown","1872054e":"markdown","ffb0ff3c":"markdown","bad4df10":"markdown","0cd3b490":"markdown","8c0467d6":"markdown","caedbb4b":"markdown","d398a2b1":"markdown","eb7197b8":"markdown","f35426cd":"markdown","597288f7":"markdown","f4c5341e":"markdown","5ee4006d":"markdown","0cea20a4":"markdown","dfed194f":"markdown","d1b28675":"markdown","24461beb":"markdown","1a41ddd0":"markdown","a7322090":"markdown","0ecb3bf9":"markdown","0b11913a":"markdown","ef39f01f":"markdown","177b74d9":"markdown","22abfb5e":"markdown"},"source":{"97b42fb5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","71e7f589":"import os\nfilename_list=[]\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        filename_list.append(os.path.join(dirname, filename))","1af26a64":"#Joining all csv files into one dataset with one line of code :)\ndata=pd.concat(map(pd.read_csv,filename_list))","c47efa23":"data.info()","67b95ab4":"data.drop([\"engine size\",\"mileage2\",\"fuel type2\",\"engine size2\",\"reference\"],axis=1,inplace=True)\ndata.head(2)","59bf9d42":"import math as m\ntax=list(data[\"tax\"])\ntaxe=list(data[\"tax(\u00a3)\"])\ntax_final=[]\nfor i in range(0, len(tax)):\n    if m.isnan(tax[i]):\n        if m.isnan(taxe[i]):\n            tax_final.append(np.nan)\n        else:\n            tax_final.append(taxe[i])\n    else:\n        tax_final.append(tax[i])\ndata.drop([\"tax\",\"tax(\u00a3)\"],axis=1,inplace=True)\ndata[\"tax\"]=tax_final","27fd6546":"data.info()","7a4003ea":"data.dropna(inplace=True)\ndata.info()","264a0f47":"data.head()","e2cf7ac2":"price_list=list(data.price)\nnew_price_list=[]\nfor i in price_list:\n    new_price_list.append(float(i))\ndata[\"price\"]=new_price_list","7d1d18ee":"data.info()","302bcc5c":"mileage_list=list(data.mileage)\nnew_mileage_list=[]\nfor i in mileage_list:\n    new_mileage_list.append(float(i))\ndata[\"mileage\"]=new_mileage_list","647dddad":"data.info()","591c5c6b":"# importing visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns","ad63e464":"data.describe()","85cd50ff":"sns.pairplot(data)","46a9632e":"sns.regplot(data=data, x=\"tax\", y=\"price\")","565fd9ff":"sns.regplot(data=data, x=\"engineSize\", y=\"price\")","781f9f71":"data[data[\"year\"]>2020]","4b23dc10":"data.model.nunique()","f8cd282a":"data.transmission.unique()","dd59d1cd":"data.fuelType.unique()","fea7b2f3":"data=data[data[\"year\"]<=2020]","524e757f":"data.info()","550c4026":"i_mileage=[1\/x for x in list(data.mileage)]\ni_mpg=[1\/x for x in list(data.mpg)]","e90419d6":"data.drop(\"model\", axis=1, inplace=True)","cc6ff38b":"plt.scatter(i_mileage, data[\"price\"])","00c7e17d":"plt.scatter(i_mpg, data[\"price\"])","a2086fad":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import explained_variance_score","8f4f00c2":"data.info()","8bfb7af0":"data=data.reset_index().drop(\"index\",axis=1)\ndata","3ec553d4":"#fuel type dummies\ndummies_ft=pd.get_dummies(data[\"fuelType\"],drop_first=True)\ndummies_ft[\"Other FT\"]=dummies_ft[\"Other\"]\ndummies_ft.drop(\"Other\",axis=1,inplace=True)\ndata=data.join(dummies_ft)","c5a2bcaa":"data.drop(\"fuelType\", axis=1, inplace=True)","bd101115":"data000=data.copy()\ndata100=data.copy()\ndata010=data.copy()\ndata001=data.copy()\ndata101=data.copy()\ndata110=data.copy()\ndata011=data.copy()\ndata111=data.copy()","b5b1ce25":"data100[\"mileage\"]=i_mileage\ndata101[\"mileage\"]=i_mileage\ndata110[\"mileage\"]=i_mileage\ndata111[\"mileage\"]=i_mileage","478a72df":"data010[\"mpg\"]=i_mpg\ndata011[\"mpg\"]=i_mpg\ndata110[\"mpg\"]=i_mpg\ndata111[\"mpg\"]=i_mpg","3fca0123":"#dummies and cardinals for transmission\ndummies_transmission=pd.get_dummies(data[\"transmission\"],drop_first=True)\ndummies_transmission[\"Other T\"]=dummies_transmission[\"Other\"]\ndummies_transmission.drop(\"Other\", axis=1, inplace=True)\ncardinal_transmission={'Automatic':2, 'Manual':0, 'Semi-Auto':1, 'Other':3} \n# Other gets value 3 because it is very likely that any other type of transmission is superior to the ones listed\n\ndata000.drop(\"transmission\",axis=1,inplace=True)\ndata100.drop(\"transmission\",axis=1,inplace=True)\ndata010.drop(\"transmission\",axis=1,inplace=True)\ndata110.drop(\"transmission\",axis=1,inplace=True)\ndata000=data000.join(dummies_transmission)\ndata100=data100.join(dummies_transmission)\ndata010=data010.join(dummies_transmission)\ndata110=data110.join(dummies_transmission)\n\ndata001[\"transmission\"]=data001[\"transmission\"].map(cardinal_transmission)\ndata101[\"transmission\"]=data101[\"transmission\"].map(cardinal_transmission)\ndata011[\"transmission\"]=data011[\"transmission\"].map(cardinal_transmission)\ndata111[\"transmission\"]=data111[\"transmission\"].map(cardinal_transmission)\n","f762ed3e":"y000=data000[\"price\"]\nX000=data000.drop(\"price\",axis=1)\n\ny100=data100[\"price\"]\nX100=data100.drop(\"price\",axis=1)\n\ny010=data010[\"price\"]\nX010=data010.drop(\"price\",axis=1)\n\ny001=data001[\"price\"]\nX001=data001.drop(\"price\",axis=1)\n\ny101=data101[\"price\"]\nX101=data101.drop(\"price\",axis=1)\n\ny110=data000[\"price\"]\nX110=data000.drop(\"price\",axis=1)\n\ny011=data011[\"price\"]\nX011=data011.drop(\"price\",axis=1)\n\ny111=data111[\"price\"]\nX111=data111.drop(\"price\",axis=1)\n\nX_train000, X_test000, y_train000, y_test000 = train_test_split(X000, y000, test_size=0.3)\n\nX_train001, X_test001, y_train001, y_test001 = train_test_split(X001, y001, test_size=0.3)\n\nX_train010, X_test010, y_train010, y_test010 = train_test_split(X010, y010, test_size=0.3)\n\nX_train100, X_test100, y_train100, y_test100 = train_test_split(X100, y100, test_size=0.3)\n\nX_train110, X_test110, y_train110, y_test110 = train_test_split(X110, y110, test_size=0.3)\n\nX_train101, X_test101, y_train101, y_test101 = train_test_split(X101, y101, test_size=0.3)\n\nX_train011, X_test011, y_train011, y_test011 = train_test_split(X011, y011, test_size=0.3)\n\nX_train111, X_test111, y_train111, y_test111 = train_test_split(X111, y111, test_size=0.3)","bd6d5fac":"lr=LinearRegression()\nlr.fit(X_train000, y_train000)\nlr_result=lr.predict(X_test000)\nprint(explained_variance_score(y_test000,lr_result))","5a71b87c":"lr=LinearRegression()\nlr.fit(X_train001, y_train001)\nlr_result=lr.predict(X_test001)\nprint(explained_variance_score(y_test001,lr_result))","1391a702":"lr=LinearRegression()\nlr.fit(X_train010, y_train010)\nlr_result=lr.predict(X_test010)\nprint(explained_variance_score(y_test010,lr_result))","dc9b28f7":"lr=LinearRegression()\nlr.fit(X_train100, y_train100)\nlr_result=lr.predict(X_test100)\nprint(explained_variance_score(y_test100,lr_result))","fe8e56e7":"lr=LinearRegression()\nlr.fit(X_train011, y_train011)\nlr_result=lr.predict(X_test011)\nprint(explained_variance_score(y_test011,lr_result))","03da8226":"lr=LinearRegression()\nlr.fit(X_train110, y_train110)\nlr_result=lr.predict(X_test110)\nprint(explained_variance_score(y_test110,lr_result))","f476ae37":"lr=LinearRegression()\nlr.fit(X_train101, y_train101)\nlr_result=lr.predict(X_test101)\nprint(explained_variance_score(y_test101,lr_result))","d34a1178":"lr=LinearRegression()\nlr.fit(X_train111, y_train111)\nlr_result=lr.predict(X_test111)\nprint(explained_variance_score(y_test111,lr_result))","2b71d892":"values=[]\nfor i in [0.1,1,10,100,1000,10000]:\n    lasso=Lasso(alpha=i)\n    lasso.fit(X_train010,y_train010)\n    values.append(list(lasso.coef_))\nval_arr=np.array(values)    \nval_arr=np.transpose(val_arr)\nvalues=val_arr.tolist()\n\nplt.figure(figsize=(20,12))\nfor i in range(0,len(values)):\n    plt.plot([0.1,1,10,100,1000,10000],values[i])\nplt.xscale(\"log\")\nplt.legend(X010.columns)\nplt.xlabel(\"\\alpha value\")\nplt.ylabel(\"Coefficient value\")","59211570":"lasso=Lasso(alpha=1)\nlasso.fit(X_train010,y_train010)\nlasso_result=lasso.predict(X_test010)\nprint(explained_variance_score(y_test010,lasso_result))\nprint(lasso.coef_)","af4b47fc":"lasso=Lasso(alpha=10)\nlasso.fit(X_train010,y_train010)\nlasso_result=lasso.predict(X_test010)\nprint(explained_variance_score(y_test010,lasso_result))\nprint(lasso.coef_)","597bb50c":"values=[]\nfor i in [0.1,1,10,100,1000,10000]:\n    lasso=Lasso(alpha=i)\n    lasso.fit(X_train000,y_train000)\n    values.append(list(lasso.coef_))\nval_arr=np.array(values)    \nval_arr=np.transpose(val_arr)\nvalues=val_arr.tolist()\n\nplt.figure(figsize=(20,12))\nfor i in range(0,len(values)):\n    plt.plot([0.1,1,10,100,1000,10000],values[i])\nplt.xscale(\"log\")\nplt.legend(X000.columns)","d908a21f":"lasso=Lasso(alpha=10)\nlasso.fit(X_train000,y_train000)\nlasso_result=lasso.predict(X_test000)\nprint(explained_variance_score(y_test000,lasso_result))\nprint(lasso.coef_)","07ab6850":"lasso=Lasso(alpha=100)\nlasso.fit(X_train000,y_train000)\nlasso_result=lasso.predict(X_test000)\nprint(explained_variance_score(y_test000,lasso_result))\nprint(lasso.coef_)","8ae86276":"values=[]\nfor i in [0.1,1,10,100,1000,10000]:\n    lasso=Lasso(alpha=i)\n    lasso.fit(X_train100,y_train100)\n    values.append(list(lasso.coef_))\nval_arr=np.array(values)    \nval_arr=np.transpose(val_arr)\nvalues=val_arr.tolist()\n\nplt.figure(figsize=(20,12))\nfor i in range(0,len(values)):\n    plt.plot([0.1,1,10,100,1000,10000],values[i])\nplt.xscale(\"log\")\nplt.legend(X100.columns)","c88b4173":"lasso=Lasso(alpha=10)\nlasso.fit(X_train100,y_train100)\nlasso_result=lasso.predict(X_test100)\nprint(explained_variance_score(y_test100,lasso_result))\nprint(lasso.coef_)","25f3ac87":"lasso=Lasso(alpha=100)\nlasso.fit(X_train100,y_train100)\nlasso_result=lasso.predict(X_test100)\nprint(explained_variance_score(y_test100,lasso_result))\nprint(lasso.coef_)","78f8ed55":"values=[]\nfor i in [0.1,1,10,100,1000,10000]:\n    lasso=Lasso(alpha=i)\n    lasso.fit(X_train001,y_train001)\n    values.append(list(lasso.coef_))\nval_arr=np.array(values)    \nval_arr=np.transpose(val_arr)\nvalues=val_arr.tolist()\n\nplt.figure(figsize=(20,12))\nfor i in range(0,len(values)):\n    plt.plot([0.1,1,10,100,1000,10000],values[i])\nplt.xscale(\"log\")\nplt.legend(X001.columns)","8ff4ab59":"lasso=Lasso(alpha=10)\nlasso.fit(X_train001,y_train001)\nlasso_result=lasso.predict(X_test001)\nprint(explained_variance_score(y_test001,lasso_result))\nprint(lasso.coef_)","7f1940cc":"lasso=Lasso(alpha=100)\nlasso.fit(X_train001,y_train001)\nlasso_result=lasso.predict(X_test001)\nprint(explained_variance_score(y_test001,lasso_result))\nprint(lasso.coef_)","38ade03a":"values=[]\nfor i in [0.1,1,10,100,1000,10000]:\n    lasso=Lasso(alpha=i)\n    lasso.fit(X_train110,y_train110)\n    values.append(list(lasso.coef_))\nval_arr=np.array(values)    \nval_arr=np.transpose(val_arr)\nvalues=val_arr.tolist()\n\nplt.figure(figsize=(20,12))\nfor i in range(0,len(values)):\n    plt.plot([0.1,1,10,100,1000,10000],values[i])\nplt.xscale(\"log\")\nplt.legend(X110.columns)","b2b48981":"lasso=Lasso(alpha=10)\nlasso.fit(X_train110,y_train110)\nlasso_result=lasso.predict(X_test110)\nprint(explained_variance_score(y_test110,lasso_result))\nprint(lasso.coef_)","cf0516f8":"lasso=Lasso(alpha=100)\nlasso.fit(X_train110,y_train110)\nlasso_result=lasso.predict(X_test110)\nprint(explained_variance_score(y_test110,lasso_result))\nprint(lasso.coef_)","da69ed48":"values=[]\nfor i in [0.1,1,10,100,1000,10000]:\n    lasso=Lasso(alpha=i)\n    lasso.fit(X_train101,y_train101)\n    values.append(list(lasso.coef_))\nval_arr=np.array(values)    \nval_arr=np.transpose(val_arr)\nvalues=val_arr.tolist()\n\nplt.figure(figsize=(20,12))\nfor i in range(0,len(values)):\n    plt.plot([0.1,1,10,100,1000,10000],values[i])\nplt.xscale(\"log\")\nplt.legend(X101.columns)","8b0f24c0":"lasso=Lasso(alpha=10)\nlasso.fit(X_train101,y_train101)\nlasso_result=lasso.predict(X_test101)\nprint(explained_variance_score(y_test101,lasso_result))\nprint(lasso.coef_)","3733356a":"lasso=Lasso(alpha=100)\nlasso.fit(X_train101,y_train101)\nlasso_result=lasso.predict(X_test101)\nprint(explained_variance_score(y_test101,lasso_result))\nprint(lasso.coef_)","567644bb":"values=[]\nfor i in [0.1,1,10,100,1000,10000]:\n    lasso=Lasso(alpha=i)\n    lasso.fit(X_train011,y_train011)\n    values.append(list(lasso.coef_))\nval_arr=np.array(values)    \nval_arr=np.transpose(val_arr)\nvalues=val_arr.tolist()\n\nplt.figure(figsize=(20,12))\nfor i in range(0,len(values)):\n    plt.plot([0.1,1,10,100,1000,10000],values[i])\nplt.xscale(\"log\")\nplt.legend(X011.columns)","db630086":"lasso=Lasso(alpha=10)\nlasso.fit(X_train011,y_train011)\nlasso_result=lasso.predict(X_test011)\nprint(explained_variance_score(y_test011,lasso_result))\nprint(lasso.coef_)","dbc3a761":"lasso=Lasso(alpha=100)\nlasso.fit(X_train011,y_train011)\nlasso_result=lasso.predict(X_test011)\nprint(explained_variance_score(y_test011,lasso_result))\nprint(lasso.coef_)","76b2231c":"values=[]\nfor i in [0.1,1,10,100,1000,10000]:\n    lasso=Lasso(alpha=i)\n    lasso.fit(X_train111,y_train111)\n    values.append(list(lasso.coef_))\nval_arr=np.array(values)    \nval_arr=np.transpose(val_arr)\nvalues=val_arr.tolist()\n\nplt.figure(figsize=(20,12))\nfor i in range(0,len(values)):\n    plt.plot([0.1,1,10,100,1000,10000],values[i])\nplt.xscale(\"log\")\nplt.legend(X111.columns)","5c6a1d04":"lasso=Lasso(alpha=10)\nlasso.fit(X_train111,y_train111)\nlasso_result=lasso.predict(X_test111)\nprint(explained_variance_score(y_test111,lasso_result))\nprint(lasso.coef_)","bed0363c":"lasso=Lasso(alpha=1000)\nlasso.fit(X_train111,y_train111)\nlasso_result=lasso.predict(X_test111)\nprint(explained_variance_score(y_test111,lasso_result))\nprint(lasso.coef_)","d56a491d":"values=[]\nfor i in [0.1,1,10,100,1000,10000]:\n    ridge=Ridge(alpha=i)\n    ridge.fit(X_train010,y_train010)\n    values.append(list(ridge.coef_))\nval_arr=np.array(values)    \nval_arr=np.transpose(val_arr)\nvalues=val_arr.tolist()\n\nplt.figure(figsize=(20,12))\nfor i in range(0,len(values)):\n    plt.plot([0.1,1,10,100,1000,10000],values[i])\nplt.xscale(\"log\")\nplt.legend(X010.columns)","bf64a208":"ridge=Ridge(alpha=1)\nridge.fit(X_train010,y_train010)\nridge_result=ridge.predict(X_test010)\nprint(explained_variance_score(y_test010,ridge_result))\nprint(ridge.coef_)","96fee442":"ridge=Ridge(alpha=10)\nridge.fit(X_train010,y_train010)\nridge_result=ridge.predict(X_test010)\nprint(explained_variance_score(y_test010,ridge_result))\nprint(ridge.coef_)","a01cbb9a":"ridge=Ridge(alpha=100)\nridge.fit(X_train010,y_train010)\nridge_result=ridge.predict(X_test010)\nprint(explained_variance_score(y_test010,ridge_result))\nprint(ridge.coef_)","59a02ddb":"filename_list\n# entries 3,4 and 5 will be removed","fce17f29":"filename_list1=filename_list[0:3]+filename_list[6:]\nfilename_list1","1e1eabdf":"data2=pd.read_csv(filename_list1[0])\nbrand_name=filename_list1[0].split(\"\/\")[-1].split(\".\")[0]\ndata2[\"brand\"]=[brand_name]*len(data2)\nfor i in filename_list1[1:]:\n    aux_df=pd.read_csv(i)\n    brand_name=i.split(\"\/\")[-1].split(\".\")[0]\n    aux_df[\"brand\"]=[brand_name]*len(aux_df)\n    data2=pd.concat([data2,aux_df])\n# coincidently, a lot of the features that weren't present in most datasets belong to the data we just removed \ndata2=data2[data2[\"year\"]<=2020]\ndata2.head()","7292736b":"import math as m\ntax=list(data2[\"tax\"])\ntaxe=list(data2[\"tax(\u00a3)\"])\ntax_final=[]\nfor i in range(0, len(tax)):\n    if m.isnan(tax[i]):\n        if m.isnan(taxe[i]):\n            tax_final.append(np.nan)\n        else:\n            tax_final.append(taxe[i])\n    else:\n        tax_final.append(tax[i])\ndata2.drop([\"tax\",\"tax(\u00a3)\"],axis=1,inplace=True)\ndata2[\"tax\"]=tax_final\n\ndata2.dropna(inplace=True)\ndata2.drop(\"model\", axis=1, inplace=True)","d4e0d664":"data2.info()","ed523656":"g = sns.FacetGrid(data2, col=\"brand\",col_wrap=3)\ng.map(sns.distplot,\"price\")","9f8498b5":"#dummies and cardinals for transmission\ndata2=data2.reset_index().drop(\"index\",axis=1)\ndummies_transmission=pd.get_dummies(data2[\"transmission\"],drop_first=True)\ndummies_transmission[\"Other T\"]=dummies_transmission[\"Other\"]\ndummies_transmission.drop(\"Other\",axis=1, inplace=True)\ndummies_brand=pd.get_dummies(data2[\"brand\"], drop_first=True)\ndummies_ft=pd.get_dummies(data2[\"fuelType\"],drop_first=True)\ndummies_ft[\"Other FT\"]=dummies_ft[\"Other\"]\ndummies_ft.drop(\"Other\",axis=1, inplace=True)\ndata2[\"mpg\"]=i_mpg\ndata2.drop([\"transmission\",\"brand\",\"fuelType\"], axis=1, inplace=True)\ndata2=data2.join(dummies_transmission)\ndata2=data2.join(dummies_brand)\ndata2=data2.join(dummies_ft)\ndata2.head()","d6b2defd":"data2.info()","7f631699":"y=data2[\"price\"]\nX=data2.drop(\"price\",axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","0bb33ab9":"lr=LinearRegression()\nlr.fit(X_train, y_train)\nlr_res=lr.predict(X_test)\nprint(explained_variance_score(y_test,lr_res))","10cf1595":"First observation: There is an entry from year 2060, which must be removed","a855f6c2":"# Part 3: Model Selection","1872054e":"# What can be observed from the EDA\n\n* Mileage and mileage per gallon seem to have an inverse relation to price and I'll attempt a fit with the inverse value of both features in order to improve model performance.\n* There is an entry with an inconsistent value: Since we are in year 2020, no year entry can exceed this value, but this happens with a Fiesta which is labeled as year 2060.\n* There is a positive relation between price and year, but there seems to be no visible relation of engine size and tax to the price.\n* There are 195 different car models in this dataset. Although it is possible to convert them to dummy variables, we will disconsider this feature for regression. Two cars with the exact same specs are supposed to be sold at the same price and any difference coming from model could be attributed to the model error.\n* The transmission feature can be reworked as both dummies and cardinals. It's plausible that automatic transmissions are superior to semi-auto and manual transmissions, and a hierarchy can be estabilished in this feature. We will implement both alternatives to check if this improves model performance.\n* The same could be said for fuel type, but since the hierarchy is not as clear and objective as in the transmission feature case, the dummy variable approach will be taken","ffb0ff3c":"# Retry: Reworking model feature","bad4df10":"# Similarly to the lasso, ridge regression was also unable to improve performance\n\n# Given that these methods are suposed to improve model performance, the hypothesis of the model feature not being important starts to lose credibility and we will attempt to enable this feature with reduced dimensionality","0cd3b490":"# Hi there! In this notebook I have made my first attempt at using lasso and ridge regression, alongside linear regression in order to evaluate if these methods could improve model performance. I have tried to ommit some of the repetitive parts since I ended up builing 8 different datasets, but the important parts are highlighted at the end.\n\n# The goal here is to build a model that can use all given datasets together and still have a nice performance score","8c0467d6":"# This will be done by adding the brand column, which will contain the model brand. It's a more comprehensive feature and will not increase dimensionality by a huge value. Only problem is: 3 datasets don't have brand name on the file name, and for simplicity, we will remove these from our final dataset","caedbb4b":"# Linear Regression Attempt","d398a2b1":"# Part 2: EDA","eb7197b8":"# The huge amount of missing data on some columns have made it clear that using those would not be benefitial to the model, since there is no feasible way to fill in all of the missing data. Therefore, columns that are missing the vast majority of the entry values will be excluded from the model\n\nFor this dataset, the following will be excluded:\n* mileage2 \n* fuel type2 \n* engine size2 \n* reference   \n\nThe \"tax(\u00a3)\" column will not be excluded as there is a similar \"tax\" column and I'll attempt to merge the two columns into one","f35426cd":"# Without any visible difference between average price, we're going to use the dummy variable approach, combined with the best performing model from the last attempt","597288f7":"# Part 1: Data Wrangling","f4c5341e":"# A quick check on the relation between brand and price","5ee4006d":"# Lasso modeling involves choosing an alpha value, and since we already have 8 different datasets, I chose to optimize the one with the best R2 value from the linear model","0cea20a4":"# Model 1: Linear Regression","dfed194f":"# Model 2: Lasso Regression","d1b28675":"# Last but not least: price and mileage features should be numeric, and they are described as objects","24461beb":"# Considering all brands over 90,000 entries, the best model could explain 75% of the variance. The model could be improved by making one model for each dataset, but this attempt had as it's goal an all-rounder model.\n\n* The implementation of the \"model\" dummy feature can possibily do more harm than good given there would be an extra 194 features for the model. I didn't want to try it, but any feedback on this implementation would be greatly appreciated.\n* I had hopes that the shinkage methods would remove the features that didn't seem to have much of a visible relation to the price feature, such as engineSize. Maybe a more iterative method such as stepwise selection could perform better in this scenario.\n* The shrinkage methods seemed to perform worse when dealing with many dummy variables. Shrinkage methods remove or lessen the weight of some variables in the model and since dummy variables always go together (either all of them are in the model or none of them are) the variable selection process only diminished the model performance.\n* The 1\/mpg rework improved model performance, but the 1\/mileage rework didn't have the same effect, but still presented a negative linear coefficient.\n* The creation of the brand feature didn't have the impact I expected, probably beacause all of the price distributions divided by brand seemed very similar, as shown in this notebook","1a41ddd0":"# As seen above, all cases of variable selection ended up lowering model performance, so the lasso regression is not useful in this case.","a7322090":"# From now on we will have the following names for datasets\n\n# x y and z will have either 0 or 1 value corresponding to false or true\n\n* x is corresponding to incorporating the inverse values of mileage\n* y is corresponding to incorporating the inverse values of mpg\n* z is corresponding to using cardinal values for the transmission feature, if it is 0 the dataset will use dummies\n\nAnd the dataset will be named as dataxyz","0ecb3bf9":"# For this regression we will use the following models:\n\n* Linear regression \n* Lasso regression\n* Ridge regression","0b11913a":"# Model 3: Ridge Regression","ef39f01f":"# One could try to fill in the missing values using proper interpolation techniques (for numerical values), but since the dataset is extensive compared to the number of features, the choice taken here will be to filter out entries that have any amount of null values","177b74d9":"# Unfortunately, the lasso removed some of the dummy variables in the process, so the model performed worse since there was a lack of information none of the remaining features could explain. This pushed me to also try optimizing the other datasets","22abfb5e":"# A quick check shows that the files have different numbers of columns, and we need to select the common columns for all files "}}