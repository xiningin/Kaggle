{"cell_type":{"f7d7e8f6":"code","aed0e630":"code","4eecdb92":"code","65598fa9":"code","9fb7503c":"code","b8c23556":"code","f0b8efac":"code","ad814976":"code","950a87b2":"code","3e9075d3":"code","9220f209":"code","aecd4628":"code","5af71104":"code","292e6900":"code","ac74bd51":"code","e49e1a30":"code","1ac2131a":"code","51920805":"code","1c97561e":"code","f7711f14":"code","c0c9bb20":"code","16345014":"code","d753f18e":"code","1f7725d5":"code","ac59f3b8":"code","a87ca901":"code","c0e3c0fe":"code","5d7c9306":"code","bfe555ae":"code","0731c3cd":"code","133fbad1":"code","8bde2723":"code","5b850861":"code","1838f038":"code","94ecf11d":"code","4f383144":"code","fabcbfa6":"code","07f76553":"code","8bbf0986":"code","d435deed":"code","0c99f559":"code","acf45667":"code","d2a29c8d":"code","8b358b06":"code","10f07670":"code","920c56c3":"code","7a9d4739":"markdown","02359b6f":"markdown","89f28a27":"markdown","bebda0d3":"markdown","b1ce1a74":"markdown","0c8ebd06":"markdown","776efaea":"markdown","0415697f":"markdown","4ed311b9":"markdown","b39a3a5c":"markdown","9466e341":"markdown","bf27c6b0":"markdown","9ad4b9be":"markdown","a4100eaa":"markdown","a47e0b2c":"markdown","3f646b0e":"markdown","8d75b8a4":"markdown","e8f94ef4":"markdown","ab4cd7b2":"markdown","19367ff1":"markdown","6bcfc7e5":"markdown"},"source":{"f7d7e8f6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aed0e630":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.style.use(\"fivethirtyeight\")\nsns.set_style(\"darkgrid\")","4eecdb92":"from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, auc, roc_curve, roc_auc_score\n\nfrom xgboost import XGBClassifier, plot_importance\nfrom lightgbm import LGBMClassifier, plot_importance\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.preprocessing import LabelEncoder","65598fa9":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv\")\nsub = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv\")","9fb7503c":"trainoriginal = train.copy()\ntestoriginal = test.copy()","b8c23556":"display(train.head())\ndisplay(test.head())\ndisplay(sub.head())","f0b8efac":"print(\"Size of the train:\", train.shape)\nprint(\"\\nSize of the test:\", test.shape)\nprint(\"\\nSize of the Submission:\", sub.shape)","ad814976":"print('Columns in Train data:\\n\\n', train.columns)\nprint('-'*80)\nprint('\\n\\nColumns in Test data:\\n\\n', test.columns)","950a87b2":"print('Datatypes of Train dataset:\\n', train.info())\nprint('-'*50)\nprint('\\n\\nDatatypes of Train dataset:\\n', test.info())","3e9075d3":"train = train.drop(['id'], axis=1)\ntest = test.drop(['id'], axis=1)","9220f209":"train.isnull().sum()","aecd4628":"test.isnull().sum()","5af71104":"train['target'].value_counts().sort_index(ascending=True)","292e6900":"plt.figure(figsize=(8,6))\nsns.countplot(x='target', data=train, order=train['target'].value_counts().index);","ac74bd51":"train['target1'] = train['target'].map({'Class_1':0, 'Class_2':1, 'Class_3':2, 'Class_4':3})","e49e1a30":"train['feature_0'].value_counts()","1ac2131a":"train['feature_2'].value_counts()","51920805":"train['feature_38'].value_counts().sort_index(ascending=False)","1c97561e":"plt.figure(figsize=(18,25))\nsns.boxplot(data=train, orient=\"h\");","f7711f14":"plt.figure(figsize=(18,25))\nsns.boxplot(data=test.iloc[:,1:], orient=\"h\");","c0c9bb20":"# Pearson Correlation\nplt.figure(figsize=(18,10))\nsns.heatmap(train.corr(method='pearson'), cbar=False, annot=True, fmt='.1f', linewidth=0.2, cmap='coolwarm');","16345014":"# Spearman Correlation\nplt.figure(figsize=(18,12))\nsns.heatmap(train.corr(method='spearman'), cbar=False, annot=True, fmt='.1f', linewidth=0.2, cmap='coolwarm');","d753f18e":"fig, ax = plt.subplots(figsize=(18, 12))\ncorr = train.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nax.text(-1.1, -0.7, 'Correlation between the Features', fontsize=20, fontweight='bold', fontfamily='serif')\nsns.heatmap(corr, mask=mask, annot=False, fmt='.2f', linewidth=0.2, cbar=True, cmap='coolwarm');","1f7725d5":"# kendall\nfig, ax = plt.subplots(1, 3, figsize=(17 , 5))\n\nfeature_lst = ['feature_0', 'feature_1', 'feature_2','feature_3','feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9']\n\ncorr = train[feature_lst].corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\nfor idx, method in enumerate(['pearson', 'kendall', 'spearman']):\n    sns.heatmap(train[feature_lst].corr(method=method), ax=ax[idx],\n            square=True, annot=True, fmt='.1f', center=0, linewidth=2,\n            cbar=False, cmap=sns.diverging_palette(240, 10, as_cmap=True),\n            mask=mask\n           ) \n    ax[idx].set_title(f'{method.capitalize()} Correlation', loc='left', fontweight='bold')     \n\nplt.show()","ac59f3b8":"train.corr()['target1'].sort_values(ascending=False)","a87ca901":"a = train.drop(['target','target1'], axis=1)\na.corrwith(train['target1']).plot(kind='bar', figsize=(18,11), color=['salmon'])\nplt.title('Correlation b\/n target and Independant features')\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.show()","c0e3c0fe":"train.skew()","5d7c9306":"test.skew()","bfe555ae":"# Independant variable\nX = train.iloc[:,:-2]\n\n# Dependant variable\ny = train['target']","0731c3cd":"# split  data into training and testing sets of 80:20 ratio\n# 20% of test size selected\n# random_state is random seed\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","133fbad1":"print(\"Length of X_train is: {X_train}\".format(X_train = len(X_train)))\nprint(\"Length of X_test is: {X_test}\".format(X_test = len(X_test)))\nprint(\"Length of y_train is: {y_train}\".format(y_train = len(y_train)))\nprint(\"Length of y_test is: {y_test}\".format(y_test = len(y_test)))","8bde2723":"# model = XGBClassifier(tree_method='cpu_hist', use_label_encoder=True)\nmodel = XGBClassifier(random_state=42, use_label_encoder=True)\nmodel.fit(X, y)","5b850861":"# fig, ax = plt.subplots(figsize=(10,10))\n\n# plot_importance(model,\n#                height=0.5,\n#                max_num_features=None,\n#                title='Feature importance',\n#                xlabel='F score', \n#                ylabel='Features',\n#                ax=ax)","1838f038":"# Feature Importance\nmain_colors = ['#f03aa5', '#40c2f3', '#c489ce', '#bb3ca9']\n\nf, ax = plt.subplots(1, 1, figsize=(18, 18))\n\nplot_importance(model, \n                max_num_features=None,\n                color=main_colors[0],\n                ax=ax)\nplt.title('Feature Importance', fontsize=20)\nplt.show()","94ecf11d":"y_pred_xgb = model.predict_proba(test)","4f383144":"submission_xgb = pd.DataFrame(y_pred_xgb, columns=['Class_1','Class_2','Class_3','Class_4'])\nsubmission_xgb['id'] = sub['id']\nsubmission_xgb","fabcbfa6":"LGB = LGBMClassifier(random_state=42, use_label_encoder=True)\nLGB.fit(X, y)","07f76553":"plot_importance(LGB, figsize=(18, 15));","8bbf0986":"pip install -U lightautoml","d435deed":"# Imports from our package\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.tasks import Task\nfrom sklearn.metrics import log_loss","0c99f559":"N_THREADS = 4 # threads cnt for lgbm and linear models\nN_FOLDS = 5 # folds cnt for AutoML\nRANDOM_STATE = 2021 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTIMEOUT = 60 * 60 # Time in seconds for automl run","acf45667":"le = LabelEncoder()\ntrain['target'] = le.fit_transform(train['target'])","d2a29c8d":"%%time\n\nautoml = TabularUtilizedAutoML(task = Task('multiclass',), \n                               timeout = TIMEOUT,\n                               cpu_limit = N_THREADS,\n                               verbose=0,\n                               reader_params = {'n_jobs': N_THREADS},\n)","8b358b06":"target_column = 'target'\n\nroles = {\n    'target': target_column,\n    'drop': ['id'],\n}\n\nlightml_pred = automl.fit_predict(train.iloc[:,:-1], roles = roles)\nprint('lightml_pred:\\n{}\\nShape = {}'.format(lightml_pred[:10], lightml_pred.shape))","10f07670":"%%time\n\ntest_pred = automl.predict(test)\nprint('Prediction for test set:\\n{}\\nShape = {}'.format(test_pred[:5], test_pred.shape))","920c56c3":"sub.iloc[:, 1:] = test_pred.data\nsub.to_csv('Submission1.csv', index = False)","7a9d4739":"<h1 style=\"background-color:magenta; font-family:newtimeroman; font-size:180%; text-align:left;\"> 3.1) Drop Unwanted Columns <\/h1>","02359b6f":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 4) Model Building and Evaluation <\/h1>","89f28a27":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 2) Read Data <\/h1>","bebda0d3":"<h1 style=\"background-color:magenta; font-family:newtimeroman; font-size:180%; text-align:left;\"> 3.2) Variable Analysis <\/h1>","b1ce1a74":"<h1 style=\"background-color:magenta; font-family:newtimeroman; font-size:180%; text-align:left;\"> 3.2) Missing Values <\/h1>","0c8ebd06":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:160%; text-align:left;\"> 4.2) LightAutoML <\/h1>","776efaea":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:180%; text-align:left;\"> 3.4.1) The correlation between the continuos variables <\/h1>\n\na. Pearson Correlation\n\nb. Spearman Correlation\n\nc. kendall","0415697f":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 3) EDA (Exploratory Data Analysis) <\/h1>","4ed311b9":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> Submission <\/h1>","b39a3a5c":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:180%; text-align:left;\"> 3.4.2) The correlation between this continuos features and the target <\/h1>","9466e341":"### Below features have negative values\n> feature_19 (-1, -2)\n\n> feature_30 (-1)\n\n> feature_31 (-1)\n\n> feature_32 (-1, -2)\n\n> feature_35 (-2)\n\n> feature_38 (-1, -2, -3, -5, -8)\n\n> feature_39 (-1, -2, -3, -5)\n\n> feature_42 (-1, -2)","bf27c6b0":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:160%; text-align:left;\"> 4.2) LGBM <\/h1>","9ad4b9be":"submission = pd.DataFrame(y_pred_xgb_test, columns=xgb.classes_)\nsubmission","a4100eaa":"<h2 style=color:green align='left'> Table of Contents <\/h2>\n\n##### 1) Load Required Libraries\n##### 2) Read Data\n##### 3) EDA (Exploratory Data Analysis)\n\n>    3.1) Drop Unwanted Columns\n\n>    3.2) Missing Values\n\n>    3.3) Variable Analysis\n\n>    3.4) Outliers\n\n>    3.5) Relation between Features \n\n>    3.6) Skewness and Kurtosis \n\n##### 4) Model Building and Evaluation\n\n>    4.1) XGBoost\n\n>    4.2) LightAutoML","a47e0b2c":"submission.insert(0, 'id', testoriginal['id'])\nsubmission","3f646b0e":"<h1 style=\"background-color:magenta; font-family:newtimeroman; font-size:180%; text-align:left;\"> 3.5) Skewness and Kurtosis <\/h1>","8d75b8a4":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:160%; text-align:left;\"> 4.1) XGBoost <\/h1>","e8f94ef4":"<h1 style=\"background-color:magenta; font-family:newtimeroman; font-size:180%; text-align:left;\"> 3.4) Relation between Features <\/h1>","ab4cd7b2":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 1) Load Required Libraries <\/h1>","19367ff1":"<h1 style=\"background-color:magenta; font-family:newtimeroman; font-size:180%; text-align:left;\"> 3.3) Outliers <\/h1>","6bcfc7e5":"submission.to_csv(\"submission.csv\", index = False)"}}