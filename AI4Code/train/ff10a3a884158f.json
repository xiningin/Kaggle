{"cell_type":{"7517c0fb":"code","57d32809":"code","784a59de":"code","ca0823a4":"code","b3e1b621":"code","df4df8c5":"code","81e82148":"code","ebeedeb6":"code","b45e1145":"code","c6a79d2e":"code","64e66812":"code","d6fbca2f":"code","577186d4":"code","11f99846":"code","0a2d1d16":"code","5a0b3a8a":"code","23dceef3":"code","bc10721f":"code","70530357":"code","c04dc75c":"code","5dae9d53":"code","96626ddb":"code","f300f9e7":"code","ac46fe4a":"code","3f5614a2":"code","f093eb30":"code","2425b141":"code","63eb1c8d":"code","5353e946":"code","e89a6f81":"code","109351ad":"code","71ef6fc7":"code","11a9c684":"code","410e0e07":"code","80d04e65":"code","b2cc3c34":"code","677e7047":"code","1b18b56a":"code","92bca2de":"code","787874ad":"code","75739163":"code","2eee49f6":"code","852e087e":"code","df6e4478":"code","767cca24":"code","2f3b3868":"code","22e70f76":"code","aec53068":"code","db3575a3":"code","44416e67":"markdown","e19d7a0f":"markdown","d4feb7f6":"markdown","29f2e3a3":"markdown","eb10c5e2":"markdown","336d4bd4":"markdown","16508aa3":"markdown","d5e55f70":"markdown","242df043":"markdown","72c12cc0":"markdown","ec78ac91":"markdown","c47d0d36":"markdown","1ad5072c":"markdown"},"source":{"7517c0fb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","57d32809":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats as st\n%matplotlib inline\nimport xgboost\n","784a59de":"with open('..\/input\/house-prices-advanced-regression-techniques\/data_description.txt','r') as f:\n    info = f.read()","ca0823a4":"print(info)\n","b3e1b621":"df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nquantitative = [f for f in df.columns if df.dtypes[f] != 'object']\nquantitative.remove('SalePrice')\nquantitative.remove('Id')\nqualitative = [f for f in df.columns if df.dtypes[f] == 'object']\n\ntest_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n","df4df8c5":"print(\"Quantitative Features: \",quantitative)\nprint()\nprint(\"Qualitative Attributes: \",qualitative)","81e82148":"df.head()","ebeedeb6":"print(df.info())","b45e1145":"total = df.isnull().sum().sort_values(ascending=False)\npercent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","c6a79d2e":"plt.style.use('seaborn')\nmissing = df.isnull().sum()\nmissing = missing[missing>0]\nmissing.sort_values(inplace=True)\nbars = missing.plot.bar(missing,color='aqua',edgecolor='black',linewidth=1)\nplt.title(\"Missing Values\")\nplt.ylabel('Counts')\nplt.xlabel('features')\nplt.show()\n","64e66812":"columns_to_drop = ['PoolQC','MiscFeature','Alley','Fence','FireplaceQu','Id']","d6fbca2f":"df['SalePrice'].describe()","577186d4":"\nplt.figure(figsize=(10,6))\nsns.distplot(df['SalePrice'],fit=st.norm)\nsns.distplot(df['SalePrice'],fit=st.lognorm)\nplt.show()","11f99846":"print('Skewness= {:.3f}'.format(df['SalePrice'].skew()))\nprint('Kurtosis= {:.3f}'.format(df['SalePrice'].kurt()))","0a2d1d16":"df['LogSalePrice'] = np.log1p(df['SalePrice'])\nplt.figure(figsize=(10,6))\nsns.distplot(df['LogSalePrice'],fit=st.norm,kde=False)\nsns.distplot(df['LogSalePrice'],fit=st.lognorm,kde=False)\nplt.show()","5a0b3a8a":"def relation_with_numerical_feature(VarName,limit):\n    data = pd.concat([df['SalePrice'],df[VarName]],axis=1)\n    data.plot.scatter(x=VarName,y = 'SalePrice',ylim=(0,limit))\n    plt.show()","23dceef3":"relation_with_numerical_feature('GrLivArea',900000)","bc10721f":"relation_with_numerical_feature(\"TotalBsmtSF\",900000)","70530357":"f = pd.melt(df, value_vars=quantitative)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False)\ng = g.map(sns.distplot, \"value\",kde_kws={'bw':1})","c04dc75c":"for c in qualitative:\n    df[c] = df[c].astype('category')\n    if df[c].isnull().any():\n        df[c] = df[c].cat.add_categories(['Missing'])\n        df[c] = df[c].fillna('Missing')\n        \ndef boxplot(x,y,**kwargs):\n    sns.boxplot(x=x,y=y)\n    x = plt.xticks(rotation=90)\n    \nf = pd.melt(df,id_vars=['SalePrice'],value_vars = qualitative)\ng = sns.FacetGrid(f,col='variable',col_wrap=2, sharex=False, sharey=False, size=5)\ng.map(boxplot,'value','SalePrice')\nplt.show()\n\n","5dae9d53":"def anova(frame):\n    anv = pd.DataFrame()\n    anv['features'] = qualitative\n    pvals = []\n    for c in qualitative:\n        samples = []\n        for clas in frame[c].unique():\n            s = frame[frame[c] == clas]['SalePrice'].values\n            samples.append(s)\n        pval = st.f_oneway(*samples)[1]\n        pvals.append(pval)\n        \n    anv['pval'] = pvals\n    return anv.sort_values('pval')\n\na = anova(df)\na['disparity'] = np.log(1.\/a['pval'].values)\nsns.barplot(data=a, x='features', y='disparity')\nx=plt.xticks(rotation=90)","96626ddb":"train = df.drop(columns_to_drop,axis=1)\ntest = test_data.drop(columns_to_drop,axis=1)\n","f300f9e7":"sns.heatmap(train.isnull(),cbar=False)","ac46fe4a":"def fill_nan_train(feature):\n    if train[feature].isnull().sum()>0:\n        if feature in qualitative:\n            train[feature].fillna(train[feature].mode()[0],inplace=True)\n        elif feature in quantitative:\n            train[feature].fillna(train[feature].mean(),inplace=True)","3f5614a2":"columns_with_nan_values = ['LotFrontage','GarageFinish','GarageType','GarageCond','GarageQual','GarageYrBlt','BsmtFinType2','BsmtExposure','BsmtFinType1','BsmtCond',\n                          'BsmtQual','MasVnrType','Electrical','MasVnrArea','MSZoning','BsmtFullBath','Utilities','BsmtHalfBath','Functional','TotalBsmtSF','GarageArea','BsmtFinSF2','BsmtUnfSF','SaleType','Exterior2nd','Exterior1st','KitchenQual','GarageCars','BsmtFinSF1']","f093eb30":"for col in columns_with_nan_values:\n    fill_nan_train(col)","2425b141":"train.isnull().sum().sort_values(ascending=False)","63eb1c8d":"print(qualitative,len(qualitative),quantitative,len(quantitative),sep='\\n')\n","5353e946":"sns.heatmap(test.isnull(),cbar=False,cmap = 'ocean')","e89a6f81":"def fill_nan_test(feature):\n    if test[feature].isnull().sum()>0:\n        if feature in qualitative:\n            test[feature].fillna(test[feature].mode()[0],inplace=True)\n        elif feature in quantitative:\n            test[feature].fillna(test[feature].mean(),inplace=True)","109351ad":"for col in columns_with_nan_values:\n    fill_nan_test(col)","71ef6fc7":"print(test.isnull().sum().sort_values(ascending=False).head(12))","11a9c684":"print(test.shape)\nprint(train.shape)","410e0e07":"for i in columns_to_drop:\n    if i in qualitative:\n        qualitative.remove(i)\n    elif i in quantitative:\n        quantitative.remove(i)","80d04e65":"df_complete = pd.concat([train,test],axis=0)\nprint('train_shape',train.shape)\nprint('test_shape',test.shape)\nprint('df_complete shape',df_complete.shape)","b2cc3c34":"final_df = df_complete.copy()","677e7047":"def category_onehot(features):\n    new_df = df_complete\n    num = 0\n    print(new_df.shape)\n    for col in features:\n        onehot = pd.get_dummies(new_df[col],drop_first=True)\n        new_df.drop(col,axis=1,inplace=True) #drop entire column\n        new_df = pd.concat([new_df,onehot],axis=1)\n        num += onehot.shape[1]\n        print(col)\n    print(new_df.shape)\n    print(num)\n    return new_df\n    ","1b18b56a":"new_df = category_onehot(qualitative)","92bca2de":"new_df = new_df.loc[:,~new_df.columns.duplicated()]","787874ad":"new_df.shape","75739163":"X_train = new_df[:1460]\nX_test = new_df[1460:]\nY_train = X_train[['SalePrice','LogSalePrice']]\n","2eee49f6":"X_train = X_train.drop(['SalePrice','LogSalePrice'],axis=1)\nX_test = X_test.drop(['SalePrice','LogSalePrice'],axis=1)","852e087e":"Y_train1 = Y_train['LogSalePrice']","df6e4478":"print('X_train',X_train.shape)\nprint('X_test',X_test.shape)\nprint('Y_train',Y_train.shape)","767cca24":"classifier = xgboost.XGBRegressor()\nclassifier.fit(X_train,Y_train1)\n","2f3b3868":"result = np.exp(classifier.predict(X_test))-1","22e70f76":"id_column = test_data['Id']\nresult = result.reshape((-1,1))\nresult = pd.DataFrame(result,columns=['SalePrice'])","aec53068":"prediction = pd.concat([id_column,result],axis=1)\n","db3575a3":"#prediction.to_csv('submisison.csv',index=False,header=True)","44416e67":"## Handling Missing Values","e19d7a0f":"#### some Features are more diversified in relation to the target(SalePrice). Categories of some features displays a very uniform pattern while there are some which resides in distinct ranges. Features like Neighbourhood, cond1, cond2, Housestyle, Garagetype, Garage Quality shows larger impact on target.\n#### feartures like PoolQc shows having a pool affects the price of the house greatly and Exterior Quality(ExterQual) shows similar behavious as of PoolQC.\n","d4feb7f6":"The Kde distribution is positively skewed and it is evident LogNormal Distribution fits best for the Target.","29f2e3a3":"### ANOVA TEST","eb10c5e2":"### Here is quick estimation of influence of categorical variable on SalePrice. For each variable SalePrices are partitioned to distinct sets based on category values. Then check with ANOVA test if sets have similar distributions. If variable has minor impact then set means should be equal. Decreasing pval is sign of increasing diversity in partitions.\n### The same results can also be seen from box plots of individual features.\n","336d4bd4":"#### Setting up a threshold as 15%, features having more than 15% of missing data will be discarded and will not be used for trainig purposes.\n#### COLUMNS = 'Alley', 'PoolQC', 'Fence', 'MiscFeature' ,'FireplaceQu','LotFrontage'  contains inadequate data points. and exceeds the threshold for missing data percentage, hence these columns can be discardede for the training purpose.","16508aa3":"1. ### Relationship with Numerical Features (GrLiveArea and TotalBsmtSF)","d5e55f70":"## 2.Relationship (boxplots) with qualitative features.","242df043":"#### In test data, there are occurences of categories of categorical data which are not present in train data. So inorder to prevent any misprediction\/error due to this, so, before converting our categorical variables to One hot encodings, will use the entire data i.e. test + train inorder to obtain a Label for that missing category in our train data as well. ","72c12cc0":"#### Extract test and train back from the complete df. train shape --> 1460 \n","ec78ac91":"### One hot encodings for Categorical Features using df_complete","c47d0d36":"#### Replacing all nan values with mode of that variable","1ad5072c":"### Missing Values in Test Data"}}