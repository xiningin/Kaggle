{"cell_type":{"5ba3e89f":"code","1818d9ab":"code","52a739ad":"code","1c2516d1":"code","76d56046":"code","35ba87b0":"code","9cc1ab74":"code","188679a6":"code","0b0c1fb8":"code","81556537":"code","832395d5":"code","9f58454b":"code","e23224fa":"code","717524aa":"code","e1a7401e":"code","52785cda":"code","872698d7":"code","3d5e59cf":"code","153e8086":"code","12198c03":"code","c0525672":"markdown","c21d4039":"markdown"},"source":{"5ba3e89f":"import pandas as pd\nimport numpy as np\nimport keras\nimport sklearn\nfrom keras.layers import Dense, Dropout\nfrom keras.models import Sequential\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.set(style = 'whitegrid')\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n\n\n%matplotlib inline","1818d9ab":"df = pd.read_csv('..\/input\/water-potability\/water_potability.csv')\n\ndf.head()","52a739ad":"df.info()","1c2516d1":"df.isnull().sum()","76d56046":"print(df.shape)\ndf.describe()","35ba87b0":"df['ph'] = df['ph'].replace(np.nan, df['ph'].median())\ndf['Sulfate'] = df['Sulfate'].replace(np.nan, df['Sulfate'].median())\ndf['Trihalomethanes'] = df['Trihalomethanes'].replace(np.nan, df['Trihalomethanes'].median())\n\ndf.isnull().sum()","9cc1ab74":"print(df.shape)\ndf.describe()","188679a6":"plt.figure(figsize = (11,11))\nsns.set(style = 'whitegrid')\nsns.heatmap(df.corr(), annot = True, cmap = 'Blues')","0b0c1fb8":"plt.figure(figsize = (8,6))\nsns.distplot(df['ph'], kde = True, bins = 45)","81556537":"plt.figure(figsize = (10,10))\nsns.jointplot(x = 'ph', y = 'Potability', data = df)","832395d5":"plt.figure(figsize = (8,6))\nplt.title('Counts of Potability')\nsns.countplot(x = 'Potability', data = df)","9f58454b":"sns.pairplot(df)","e23224fa":"X = df.drop(['Potability'], axis = 1)\ny = df['Potability']\n\nX.head()","717524aa":"X = (X - X.mean()) \/ X.std()\n\n\n\nprint(X.shape)\nX.head()","e1a7401e":"'''\nscalar = StandardScalar()\n'''","52785cda":"from keras.layers import BatchNormalization\n\ndef nn():\n    model = Sequential()\n    model.add(Dense(120, input_dim = X.shape[1], activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.8))\n    \n    model.add(Dense(120, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.6))\n    \n    model.add(Dense(120, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.63))\n    \n    model.add(Dense(120, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.7))\n    \n    model.add(Dense(1, activation = 'sigmoid'))\n    \n    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    \n    return model","872698d7":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 12)\n\nprint(X_train.shape)\nprint(y_train.shape)","3d5e59cf":"model = nn()\n\nhistory = model.fit(X_train, y_train, validation_split = 0.25, batch_size = 28, epochs = 350, verbose = 2)","153e8086":"scores = model.evaluate(X_test, y_test, verbose = 2)\nprint('Test Accuracy: ', scores[1])","12198c03":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Accuracy Error')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(['Training', 'Validation'], loc = 'upper right')\nplt.show()","c0525672":"Let's go and normalize our data to make it much more accurate","c21d4039":"Wow. Minor correlation with everything. Looks like every data column matters as much as the next"}}