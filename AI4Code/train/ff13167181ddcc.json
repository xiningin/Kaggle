{"cell_type":{"94895899":"code","afd5006c":"code","9415e8d4":"code","2d8d0feb":"code","d66bee16":"code","118b3d6a":"code","a459c480":"code","6288e7fa":"code","2fe56225":"code","b3d92390":"code","85fdab85":"code","5021753d":"code","2b93d77b":"code","90435059":"code","cdfadc68":"code","cc9200e6":"code","9e70bc4b":"code","27fd5e94":"code","a7c4acf3":"code","f3f513ea":"code","706e8a2b":"code","55e04792":"code","29ad3f0a":"code","46167d1c":"code","6537ae4d":"code","82b62382":"code","2bae9cef":"code","9278c989":"code","43799781":"code","05ac6bcb":"code","a2c8a805":"code","d36a3cc1":"code","201ad1cf":"code","6c65ea13":"code","7e384660":"code","e5361919":"code","75c1f90a":"code","30e3132b":"code","c91e2ba6":"code","cf6ff31f":"code","f934fcd7":"code","0f96381c":"code","077cb968":"code","85da1b55":"code","6a879e1d":"code","fbbf4659":"code","6bb90d31":"code","95300dc0":"code","7dfbf614":"code","8d030545":"markdown","994ab183":"markdown","544b943c":"markdown","77959d79":"markdown","d7f67cab":"markdown","4998a9e1":"markdown","d230f19e":"markdown","fbbc3648":"markdown","8d46c677":"markdown","be73c348":"markdown","cccfa643":"markdown","c4b5de6c":"markdown","27ca3844":"markdown","79883b13":"markdown","de167d16":"markdown","7d959e46":"markdown","380d4560":"markdown","4273e578":"markdown","008c6460":"markdown"},"source":{"94895899":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model as lm\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom pandas.tools import plotting\nfrom scipy import stats as stats\nfrom scipy.stats import kurtosis, skew\nfrom scipy.stats import chisquare, chi2_contingency, chi2\nfrom scipy.stats import pearsonr\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nplt.style.use(\"ggplot\")\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","afd5006c":"data = pd.read_csv(\"..\/input\/TermPaper1.csv\",header=0,sep=';')\ndata.head()","9415e8d4":"data.columns\n","2d8d0feb":"data.describe()","d66bee16":"x1=data.WDI\nx2=data.TDI\nx3=data[\"CONC.\"]\nx4=data.RISK\n\nprint( 'excess kurtosis of normal distribution (should be 0): {}'.format( kurtosis(x1) ))\nprint( 'skewness of normal distribution (should be 0): {}'.format( skew(x1) ))\n\nprint( 'excess kurtosis of normal distribution (should be 0): {}'.format( kurtosis(x2) ))\nprint( 'skewness of normal distribution (should be 0): {}'.format( skew(x2) ))\n\nprint( 'excess kurtosis of normal distribution (should be 0): {}'.format( kurtosis(x3) ))\nprint( 'skewness of normal distribution (should be 0): {}'.format( skew(x3) ))\n\nprint( 'excess kurtosis of normal distribution (should be 0): {}'.format( kurtosis(x4) ))\nprint( 'skewness of normal distribution (should be 0): {}'.format( skew(x4) ))\n","118b3d6a":"plt.figure(figsize = (10,10))\ndf = pd.DataFrame(data,\n                  columns=['BW','INCOME', 'EDUCATION', 'WDI', 'TDI','CONC.','RISK'])\nboxplot = df.boxplot(column=['BW','INCOME', 'EDUCATION', 'WDI', 'TDI','CONC.','RISK'])","a459c480":"plt.figure(figsize = (5,5))\ndf = pd.DataFrame(data,\n                  columns=['CONC.'])\nboxplot = df.boxplot(column=['CONC.'])","6288e7fa":"plt.figure(figsize = (5,5))\ndf = pd.DataFrame(data,\n                  columns=['RISK'])\nboxplot = df.boxplot(column=['RISK'])","2fe56225":"\n#This histogram has been made for the RISK outputs depended on the AREA types\n\n#Since there are 34 columns it is appropriate to use about squaroot of the 34, nearly 6\nu=plt.hist(data[data[\"AREA\"]==1].RISK,bins=6,fc=(0,0,0.1,0.5),label = \"Urban\")\nn_u=plt.hist(data[data[\"AREA\"] == 2].RISK,bins=6,fc = (1,0,0,0.5),label = \"Non-Urban\")\n\nplt.legend()\nplt.xlabel(\"RISK\")\nplt.ylabel(\"Frequency\")\nplt.title(\"RISK Histogram for the Urban and Non-Urban Areas\")\nplt.show()\n\nmax_RISK_Urban = u[0].max()\nmax_RISK_Non_Urban = n_u[0].max()\nindex_RISK_Urban = list(u[0]).index(max_RISK_Urban)\nindex_RISK_Non_Urban = list(n_u[0]).index(max_RISK_Non_Urban)\nmost_RISK_Urban = u[1][index_RISK_Urban]\nmost_RISK_Non_Urban = n_u[1][index_RISK_Urban]\nprint(\"Most frequent RISK for Urban and Non-Urban areas are: \",most_RISK_Urban,\"and\",most_RISK_Non_Urban)\n","b3d92390":"f=plt.hist(data[data[\"GENDER\"]==1].RISK,bins=6,fc=(0,0,0.1,0.5),label = \"Female\")\nm=plt.hist(data[data[\"GENDER\"] == 2].RISK,bins=6,fc = (1,0,0,0.5),label = \"Male\")\n\nplt.legend()\nplt.xlabel(\"RISK\")\nplt.ylabel(\"Frequency\")\nplt.title(\"RISK Histogram for the Male and Female Gender Types\")\nplt.show()\n\nmax_RISK_Female = f[0].max()\nmax_RISK_Male = m[0].max()\nindexFemale = list(f[0]).index(max_RISK_Female)\nindexMale= list(m[0]).index(max_RISK_Male)\nmost_RISK_Female = f[1][indexFemale]\nmost_RISK_Male = m[1][indexMale]\nprint(\"Most frequent RISK for the Female and Male genders are: \",most_RISK_Female,\"and\",most_RISK_Male)","85fdab85":"f=plt.hist(data[data[\"BW\"]<=70].RISK,bins=6,fc=(0,0,0.1,0.5),label = \"Under Average\")\nm=plt.hist(data[data[\"BW\"] >= 70].RISK,bins=6,fc = (1,0,0,0.5),label = \"Above Average\")\n\nplt.legend()\nplt.xlabel(\"RISK\")\nplt.ylabel(\"Frequency\")\nplt.title(\"RISK Histogram Depending on the Average Weight\")\nplt.show()\n\nmax_RISK_UA = f[0].max()\nmax_RISK_AA = m[0].max()\nindexUA = list(f[0]).index(max_RISK_UA)\nindexAA= list(m[0]).index(max_RISK_AA)\nmost_RISK_UA = f[1][indexUA]\nmost_RISK_AA = m[1][indexAA]\nprint(\"Most frequent RISK for the body weights of people depending on the under or above the 70kg are: \",most_RISK_UA,\"and\",most_RISK_AA)","5021753d":"f=plt.hist(data[data[\"EDUCATION\"]<=4].RISK,bins=6,fc=(0,0,0.1,0.5),label = \"Under High School\")\nm=plt.hist(data[data[\"EDUCATION\"] >4].RISK,bins=6,fc = (1,0,0,0.5),label = \"Above High School\")\n\nplt.legend()\nplt.xlabel(\"RISK\")\nplt.ylabel(\"Frequency\")\nplt.title(\"RISK Histogram Depending on EDUCATION\")\nplt.show()\n\nmax_RISK_UA = f[0].max()\nmax_RISK_AA = m[0].max()\nindexUA = list(f[0]).index(max_RISK_UA)\nindexAA= list(m[0]).index(max_RISK_AA)\nmost_RISK_UA = f[1][indexUA]\nmost_RISK_AA = m[1][indexAA]\nprint(\"Most frequent RISK for the High School or less and college or more  are: \",most_RISK_UA,\"and\",most_RISK_AA)","2b93d77b":"f=plt.hist(data[data[\"INCOME\"]<=4].RISK,bins=6,fc=(0,0,0.1,0.5),label = \"Normal-Poor\")\nm=plt.hist(data[data[\"INCOME\"] >4].RISK,bins=6,fc = (1,0,0,0.5),label = \"Normal-Wealthy\")\n\nplt.legend()\nplt.xlabel(\"RISK\")\nplt.ylabel(\"Frequency\")\nplt.title(\"RISK Histogram Depending on Income\")\nplt.show()\n\nmax_RISK_UA = f[0].max()\nmax_RISK_AA = m[0].max()\nindexUA = list(f[0]).index(max_RISK_UA)\nindexAA= list(m[0]).index(max_RISK_AA)\nmost_RISK_UA = f[1][indexUA]\nmost_RISK_AA = m[1][indexAA]\nprint(\"Most frequent RISK depending on the income are: \",most_RISK_UA,\"and\",most_RISK_AA,\"for the normal-poor and normal-wealthy people,respectively\")","90435059":"data_Urban = data[data[\"AREA\"] == 1]\ndata_NonUrban = data[data[\"AREA\"] == 2]\ndesc = data_Urban.RISK.describe()\nQ1 = desc[4]\nQ3 = desc[6]\nIQR = Q3-Q1\nlower_bound = Q1 - 1.5*IQR\nupper_bound = Q3 + 1.5*IQR\nprint(\"Anything outside this range is an outlier: (\", lower_bound ,\",\", upper_bound,\")\")\ndata_Urban[data_Urban.RISK < lower_bound].RISK\nprint(\"Outliers: \",data_Urban[(data_Urban.RISK < lower_bound) | (data_Urban.RISK > upper_bound)].RISK.values)","cdfadc68":"melted_data = pd.melt(data,id_vars = \"AREA\",value_vars = ['RISK', 'CONC.','WDI','EDUCATION','TDI','INCOME','AGE'])\nplt.figure(figsize = (10,10))\nsns.boxplot(x = \"variable\", y = \"value\", hue=\"AREA\",data= melted_data)\nplt.show()","cc9200e6":"# RISK factor for the Urban and Non-Urban Areas are zoomed in \n\nmelted_data = pd.melt(data,id_vars = \"AREA\",value_vars = ['RISK'])\nplt.figure(figsize = (10,10))\nsns.boxplot(x = \"variable\", y = \"value\", hue=\"AREA\",data= melted_data)\nplt.show()","9e70bc4b":"# RISK factor for the Urban and Non-Urban Areas are zoomed in \n\nmelted_data = pd.melt(data,id_vars = \"AREA\",value_vars = ['CONC.'])\nplt.figure(figsize = (10,10))\nsns.boxplot(x = \"variable\", y = \"value\", hue=\"AREA\",data= melted_data)\nplt.show()","27fd5e94":"#Probability plot for the WDI parameter\nstats.probplot(data.WDI, sparams=(), dist='norm', fit=True, plot=plt, rvalue=False)","a7c4acf3":"#Probability plot for the Risk parameter\nstats.probplot(data.RISK, sparams=(), dist='norm', fit=True, plot=plt, rvalue=False)","f3f513ea":"#Probability plot for the CONCENTRATION parameter\nstats.probplot(data['CONC.'], sparams=(), dist='norm', fit=True, plot=plt, rvalue=False)","706e8a2b":"#Probability plot for the INCOME\nstats.probplot(data.INCOME, sparams=(), dist='norm', fit=True, plot=plt, rvalue=False)","55e04792":"#Probability plot for the EDUCATION\nstats.probplot(data.EDUCATION, sparams=(), dist='norm', fit=True, plot=plt, rvalue=False)","29ad3f0a":"#Probability plot for the TDI parameter\nstats.probplot(data.TDI, sparams=(), dist='norm', fit=True, plot=plt, rvalue=False)","46167d1c":"a=stats.normaltest(data.TDI, axis=0, nan_policy='omit')\nb=stats.shapiro(data.TDI)\nc=stats.anderson(data.TDI, dist='norm')\nprint(a,\"and Shapiro Results are :\",b,\"where the variables are test statistics and p-value, respectively\")\nprint(\"Anderson Test Result is:\",c)\nprint(\n)\na2=stats.normaltest(data.WDI, axis=0, nan_policy='omit')\nb2=stats.shapiro(data.WDI)\nc2=stats.anderson(data.WDI, dist='norm')\nprint(a2,\"and Shapiro Results are :\",b2,\"where the variables are test statistics and p-value, respectively\")\nprint(\"Anderson Test Result is:\",c2)\nprint(\n)\n\na3=stats.normaltest(data.RISK, axis=0, nan_policy='omit')\nb3=stats.shapiro(data.RISK)\nc3=stats.anderson(data.RISK, dist='norm')\nprint(a3,\"and Shapiro Results are :\",b3,\"where the variables are test statistics and p-value, respectively\")\nprint(\"Anderson Test Result is:\",c3)\nprint(\n)\n\n\na4=stats.normaltest(data.EDUCATION, axis=0, nan_policy='omit')\nb4=stats.shapiro(data.EDUCATION)\nc4=stats.anderson(data.EDUCATION, dist='norm')\nprint(a4,\"and Shapiro Results are :\",b4,\"where the variables are test statistics and p-value, respectively\")\nprint(\"Anderson Test Result is:\",c4)\nprint(\n)\n\na5=stats.normaltest(data['CONC.'], axis=0, nan_policy='omit')\nb5=stats.shapiro(data['CONC.'])\nc5=stats.anderson(data['CONC.'], dist='norm')\nprint(a5,\"and Shapiro Results are :\",b5,\"where the variables are test statistics and p-value, respectively\")\nprint(\"Anderson Test Result is:\",c5)\nprint(\n)\n\na6=stats.normaltest(data.INCOME, axis=0, nan_policy='omit')\nb6=stats.shapiro(data.INCOME)\nc6=stats.anderson(data.INCOME, dist='norm')\nprint(a6,\"and Shapiro Results are :\",b6,\"where the variables are test statistics and p-value, respectively\")\nprint(\"Anderson Test Result is:\",c6)\nprint(\n)","6537ae4d":"a=data.groupby('GENDER').size()\nprint(a)\na.plot.pie(figsize=(4,4))\n","82b62382":"a=data.groupby('AREA').size()\nprint(a)\na.plot.pie(figsize=(4,4))","2bae9cef":"a=data.groupby('EDUCATION').size()\nprint(a)\na.plot.pie(figsize=(4,4))","9278c989":"a=data.groupby('INCOME').size()\nprint(a)\na.plot.pie(figsize=(4,4))","43799781":"# Categorizing by gender, some descriptive parameters have been given here.  \n\nf=data[data[\"GENDER\"]==1]\nm=data[data[\"GENDER\"]==2]\nf.head()\nfemale_Risk_Mean=f.RISK.mean()\nF_Median=np.median(f.RISK)\nmale_Risk_Mean=m.RISK.mean()\nM_Median=np.median(m.RISK)\n#Print Median \nprint(\"Median of the Risk for the female and male types are:\",F_Median,\"and\",M_Median)\n#Standard Deviaton of F and M\nF_std=np.std(f.RISK)\nM_std=np.std(m.RISK)\nprint(\"Standard deviation of the Risk for the female and male types are:\",F_std,\"and\",M_std)\n#Print Mean Values\nprint(\"The average risk for the female and male types are:\",female_Risk_Mean,\"and\",male_Risk_Mean,\"And the given critical Level is\", 0.001)\n#Population Risk\nPop_risk=data.RISK.mean()\nPop_risk_std=np.std(data.RISK)\nPop_risk_median=np.median(data.RISK)\nprint(\"mean-std-median values of the population are:\",Pop_risk,Pop_risk_std,\"and\",Pop_risk_median)\n","05ac6bcb":"df=data.groupby('GENDER',axis=0).mean()\ndf","a2c8a805":"df=data.groupby('EDUCATION',axis=0).mean()\ndf\n","d36a3cc1":"df=data.groupby('AREA',axis=0).mean()\ndf","201ad1cf":"df=data.groupby('INCOME',axis=0).mean()\ndf","6c65ea13":"#H0:m\u00fc=1x10^-4\n#H1:m\u00fc>1x10^-4, One sided alternative hypothesis\n#alfa=0.05\nm\u00fc=0.0001\ns=Pop_risk_std\nprint(\"Standard deviation of the sample is\",s)\nt0=(Pop_risk-m\u00fc)\/(s\/np.sqrt(35))\n#alfa=P(T<t0)\nprint(\"t0 values is:\",t0)\nprint(\"Q1-[g]\",\"Conclusion: Since\",t0,\">1.691, null hyphothesis is rejected at the 0.05 level of significance that the average cancer risk level exceeds the acceptable carcinogenic level and p-value<0.005 from table IV,Appendix A, page 656\")\n\n\n","7e384660":"#H0:ki2_0=ki2_alfa_v\n#H1:ki2_0>ki2_alfa_v\n\nsigma2=0.001\ns=Pop_risk_std\nn=35\nki2_0=(n-1)*s*s\/(sigma2*sigma2)\nki2_alfa_v=49\n\nprint(\"Q1-[h]-1st Option\",\"Conclusion: X^2_0=\",ki2_0,\">\",\"X^2_(0.05,34)=\",ki2_alfa_v,\"There is strong evidence that null hypothesis is fail to rejected\")","e5361919":"t=stats.chisquare(data.TDI)\ns=stats.chisquare(data.RISK)\nprint(\"results for TDI and RISK are:\",t,\"and\",s)","75c1f90a":"stat,p,dof,expected=chi2_contingency(data.RISK)\nstat,p,dof,expected","30e3132b":"CI=0.95\ncritical=0.001\nif abs(stat)>=critical:\n    print('Dependent(reject H0)')\nelse:\n        print('Independent(fail to reject H0)',\"and p-Value is:\",p)\nprint(\"Q1-[h]-2nd Option: Null hypothesis is fail to rejected with strong evidence\")","c91e2ba6":"#Correlation between parameters\nf,ax=plt.subplots(figsize = (18,18))\nsns.heatmap(data.corr(),annot= True,linewidths=0.5,fmt = \".1f\",ax=ax)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.title('Correlation Map')\nplt.savefig('graph.png')\nplt.show()","cf6ff31f":"plt.figure(figsize = (9,9))\nplt.scatter(data.WDI,data.TDI,color='black')\nplt.show()","f934fcd7":"regression_model =lm.LinearRegression()\n\n\nregression_model.fit(X=pd.DataFrame(data.WDI),\n                     y=data.TDI)\nprint(\"Model is: y=m+[n]x, where m is the intercept and n is slope\")\nprint(\"Intercept of the plot is:\",regression_model.intercept_)\nprint(\"Slope or regression model coefficient is:\",regression_model.coef_)\n\n\n","0f96381c":"regression_model.score(X = pd.DataFrame(data.WDI), \n                       y = data.TDI)\n","077cb968":"train_prediction = regression_model.predict(X = pd.DataFrame(data.WDI))\n\n# Actual - prediction = residuals\nresiduals = data.TDI - train_prediction\n\nresiduals.describe()","85da1b55":"\nSSResiduals = (residuals**2).sum()\n\nSSTotal = ((data.TDI - data.TDI.mean())**2).sum()\n\n# R-squared\nR_square=1 - (SSResiduals\/SSTotal)\nprint(\"SSR and SST are:\",SSResiduals,\"and\",SSTotal,\",respectively. So R-Square is:\",R_square)","6a879e1d":"plt.scatter(data.WDI,data.TDI,color='black')\n\n# Plot regression line\nplt.plot(data[\"WDI\"],      # Explanitory variable\n         train_prediction,  # Predicted values\n         color=\"blue\")","fbbf4659":"plt.figure(figsize=(9,9))\n\nstats.probplot(residuals, dist=\"norm\", plot=plt)","6bb90d31":"\ndef rmse(predicted, targets):\n      return (np.sqrt(np.mean((targets-predicted)**2)))\n\nRMSE=rmse(train_prediction, data[\"TDI\"])\nprint('Root mean squared error is:', RMSE)","95300dc0":"regression_model =lm.LinearRegression()\n\n\nregression_model.fit(X=pd.DataFrame(data.BW),\n                     y=data.RISK)\nprint(\"Model is: y=m+[n]x, where m is the intercept and n is slope\")\nprint(\"Intercept of the plot is:\",regression_model.intercept_)\nprint(\"Slope or regression model coefficient is:\",regression_model.coef_)\n\n\n\nregression_model.score(X = pd.DataFrame(data.BW), \n                       y = data.RISK)\n\n\n\ntrain_prediction = regression_model.predict(X = pd.DataFrame(data.BW))\n\n# Actual - prediction = residuals\nresiduals = data.RISK - train_prediction\n\nresiduals.describe()\n\n\n\n\nSSResiduals = (residuals**2).sum()\n\nSSTotal = ((data.RISK - data.RISK.mean())**2).sum()\n\n# R-squared\nR_square=1 - (SSResiduals\/SSTotal)\nprint(\"SSR and SST are:\",SSResiduals,\"and\",SSTotal,\",respectively. So R-Square is:\",R_square)\n\n\nplt.scatter(data.BW,data.RISK,color='black')\n\n# Plot regression line\nplt.plot(data[\"BW\"],      # Explanitory variable\n         train_prediction,  # Predicted values\n         color=\"blue\")\n\n\n\nplt.figure(figsize=(9,9))\n\nstats.probplot(residuals, dist=\"norm\", plot=plt)","7dfbf614":"def rmse(predicted, targets):\n      return (np.sqrt(np.mean((targets-predicted)**2)))\n\nRMSE=rmse(train_prediction, data[\"RISK\"])\nprint('Root mean squared error is:', RMSE)","8d030545":"Results for the ID, AREA, and GENDER shouldn't have considered. Parameters such as Gender or area can be analysed after counting-count() function- later those number can be considered. Otherwise number between 1 and 2 shows nothing. But the frequencies of the 1 and 2 are important as well as AREA parameter's freq.","994ab183":"As it can be seen above depending on the test p values can differ. In Anderson Test, if the returned value is bigger than the critical values, H0 is rejected. Saphiro and NormalTest also give results that are not exactly same. ","544b943c":"Chi-square method have used. Firstly, the concept is given and values are calculated by hand. Later, defined codes have been used to make chi-squared test.  ","77959d79":"[b]- Discussing","d7f67cab":"[g]- Test of the Acceptable Risk","4998a9e1":"If the outliers would removed(left and right tail), distribution shows normal characteristic.  ","d230f19e":"After checking skewnnes, kurtosis, mean, median, and range, It can be said that: WDI has negative kurtosis which means it has lighter tail than normal distribution and positive skewness indicates long tail in positive direction. Most obvious one here is CONCENTRATION parameter which has biggest kurtosis, that is 9, and skewness is 2.69. So it is obviosly not normal distribution. TDI and RISK parameters also have positive kurtosis and skewness values where kurtosis of the risk is 0.22 and TDI 1.539 and skewness values are 1.083 and 1.13, respectively. RISK distribution shows better normal distribution characteristics than the TDI, even mean and median close each other in the boxplots. ","fbbc3648":"[e]- Pie Charts","8d46c677":"[c]-Histograms and normal-probability plots","be73c348":"Here correlation among the all parameters can be seen. But in the question, BW-risk and WDI-TDI are wanted, only. So, values are -0.1 and 0.7, respectively. That shows BW-risk relation is not linear or another saying is, that cannot be modeled as linear. But contrary WDI and TDI have very powerful linear relationship.  ","cccfa643":"Using chisqure function of the python and easily find the whether H0 is rejected or fail to rejected. Conclusion is either way it is fail to rejected with strong evidence since p value is 1.","c4b5de6c":"NOW, same steps will be applied for BW and RISK factor.","27ca3844":"[i] Simple Lineer Regression between parameters. \n\nCorrelation results for all parameters have shown by heat map where dark colors show non-linearity.","79883b13":"[a]- Descriptive Statistic Results ","de167d16":"[h]- Critical Risk Test ","7d959e46":"Following part gives statistical explanation by categorizing variable.","380d4560":"[f]-Test cancer risk by categorizing","4273e578":"[d]- Normality TESTS for:\nTDI,\nWDI,\nRISK,\nAGE,\nBW,\nCONC, and\nINCOME","008c6460":"**QUESTION 2**"}}