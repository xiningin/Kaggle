{"cell_type":{"c32d4cc9":"code","6d4aedb4":"code","5a2ba9a5":"code","acff0212":"code","c4bb4e05":"code","1636fcd2":"code","5754b81a":"code","04b02b35":"code","39be0fc1":"code","b26ff8d0":"code","fdbf0ff7":"code","d97c418b":"code","80ad8505":"markdown"},"source":{"c32d4cc9":"import pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import figure\n\nimport time\nfrom IPython.display import clear_output\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","6d4aedb4":"sns.set(rc={'figure.figsize':(15, 10)})\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 5})\n\nsns.set_style(\"whitegrid\", {'axes.grid' : False})","5a2ba9a5":"states = ['A','B','C','D','E','F','G','T','M','recycling','trash']\nx_list = [4,3,2,1,1,1,2,3,3,4,4]\ny_list = [1,1,1,1,2,3,3,3,2,3,2]\n\n# The low-level actions the agent can make in the environment\nactions = ['left','right','up','down']\nrewards = [-0.04,-0.04,-0.04,-0.04,-0.04,-0.04,-0.04,-0.04,-0.04,1,-1]\n\ninitial_policy = ['left','up','right','up','up','right','right','right','up','up','up']","acff0212":"for n,state in enumerate(states):\n    if state == 'recycling':\n        plt.scatter(x_list[n],y_list[n], s=150, color='g', marker='+')\n    elif state == 'trash':\n        plt.scatter(x_list[n],y_list[n], s=150, color='r', marker='x')\n    else:\n        plt.scatter(x_list[n],y_list[n], s=150, color='b')\n    plt.text(x_list[n]+0.05,y_list[n]+0.05,states[n])\nplt.title(\"Grid World Diagram for Classroom Paper Throwing Environment\")\nplt.xticks([])\nplt.yticks([])\nplt.ylim(0,4)\nplt.xlim(0,5)\nplt.show()\n","c4bb4e05":"def action_outcome(state_x,state_y,action):\n    if action == 'left':\n        u = -1\n        v = 0\n    elif action == 'right':\n        u = 1\n        v = 0\n    elif action == 'up':\n        u = 0\n        v = 1\n    elif action == 'down':\n        u = 0\n        v = -1\n    else:\n        print(\"Error: Invalid action given\")\n        \n    # Override if action hits wall to not move\n    if (state_x == 1) & (u == -1):\n        u = 0\n        v = v\n    elif (state_x == 4) & (u == 1):\n        u = 0\n        v = v\n    elif (state_y == 1) & (v == -1):\n        u = u\n        v = 0\n    elif (state_y == 3) & (v == 1):\n        u = u\n        v = 0\n    elif (state_x == 2)&(state_y == 1) & (v == 1):\n        u = u\n        v = 0\n    elif (state_x == 1)&(state_y == 2) & (u == 1):\n        u = 0\n        v = v  \n    elif (state_x == 2)&(state_y == 3) & (v == -1):\n        u = u\n        v = 0         \n    elif (state_x == 3)&(state_y == 2) & (u == -1):\n        u = 0\n        v = v \n    # Make so it cannot get out of bin\n    elif (state_x == 4)&(state_y == 3):\n        u = 0\n        v = 0\n    elif (state_x == 4)&(state_y == 2):\n        u = 0\n        v = 0\n    return(u,v)","1636fcd2":"state_action_pairs = pd.DataFrame()\nfor n1,state in enumerate(states):\n    action_list = pd.DataFrame()\n    for n2,action in enumerate(actions):\n        \n        state_x = x_list[n1]\n        state_y = y_list[n1]\n        u = action_outcome(state_x,state_y,action)[0]\n        v = action_outcome(state_x,state_y,action)[1]\n        action_list  = action_list.append(pd.DataFrame({'state':state,'action':action,'x':x_list[n1],'y':y_list[n1],\n                                                               'u':u,'v':v}, index=[(n1*len(actions)) + n2]))\n    \n    state_action_pairs = state_action_pairs.append(action_list)\nstate_action_pairs.head()","5754b81a":"for n,state in enumerate(states):\n    if state == 'recycling':\n        plt.scatter(x_list[n],y_list[n], s=150, color='g', marker='+')\n    elif state == 'trash':\n        plt.scatter(x_list[n],y_list[n], s=150, color='r', marker='x')\n    else:\n        plt.scatter(x_list[n],y_list[n], s=150, color='b')\n    plt.text(x_list[n]+0.05,y_list[n]+0.05,states[n])\n    \n    state_action_pairs_state = state_action_pairs[state_action_pairs['state']==state]\n    plt.quiver([x_list[n],x_list[n],x_list[n],x_list[n]],[y_list[n],y_list[n],y_list[n],y_list[n]],\n               state_action_pairs_state['u'],state_action_pairs_state['v'], alpha = 0.5)\n    \nplt.title(\"Grid World Diagram for Classroom Paper Throwing Environment: \\n Possible Action Outcomes for each State \\n (if action hits wall, next state is same as current)\")\nplt.xticks([])\nplt.yticks([])\nplt.ylim(0,4)\nplt.xlim(0,5)\nplt.show()\n","04b02b35":"initial_policy_table = pd.DataFrame()\n\nfor n,state in enumerate(states):\n    state_x = x_list[n]\n    state_y = y_list[n]\n    policy_a = initial_policy[n]\n    \n    u = action_outcome(state_x,state_y,policy_a)[0]\n    v = action_outcome(state_x,state_y,policy_a)[1]\n    \n    initial_policy_table = initial_policy_table.append(pd.DataFrame({'state':state,'x':state_x,'y':state_y, 'action':policy_a,\n                                                                       'u':u,'v':v}, index=[n]))\ninitial_policy_table","39be0fc1":"for n,state in enumerate(states):\n    if state == 'recycling':\n        plt.scatter(x_list[n],y_list[n], s=150, color='g', marker='+')\n    elif state == 'trash':\n        plt.scatter(x_list[n],y_list[n], s=150, color='r', marker='x')\n    else:\n        plt.scatter(x_list[n],y_list[n], s=150, color='b')\n    plt.text(x_list[n]+0.05,y_list[n]+0.05,states[n])\n    \n    initial_policy_table_state = initial_policy_table[initial_policy_table['state']==state]\n    plt.quiver([x_list[n],x_list[n],x_list[n],x_list[n]],[y_list[n],y_list[n],y_list[n],y_list[n]],\n               initial_policy_table_state['u'],initial_policy_table_state['v'], alpha = 0.5)\n    \nplt.title(\"Grid World Diagram for Classroom Paper Throwing Environment: \\n Initial Policy\")\nplt.xticks([])\nplt.yticks([])\nplt.ylim(0,4)\nplt.xlim(0,5)\nplt.show()\n","b26ff8d0":"def environment(state, action):\n    # Outcome probabilities\n    if (state=='recycling')|(state=='trash'):\n        prob = 0\n    elif (state=='T'):\n        prob = 1\n\n    elif (state=='M'):\n        prob = 0.7\n   \n    elif (state=='B'):\n        prob = 0.7\n\n    elif (state=='A'):\n        prob = 0.7\n\n    elif (state=='C'):\n        prob = 0.7\n\n    elif (state=='D'):\n        prob = 0.7\n\n    elif (state=='E'):\n        prob = 0.7\n\n    elif (state=='F'):\n        prob = 0.7\n\n    elif (state=='G'):\n        prob = 0.7\n\n    else:\n        prob = \"Error\"\n        print(\"Error state\", state)\n\n    action_rng = np.random.rand()\n    if action_rng<=prob:\n        action = action\n    else:\n        action_sub_list = actions.copy()\n        action_sub_list.remove(action)\n        action = random.choice(action_sub_list)\n        \n        \n    state_x = x_list[states.index(state)]\n    state_y = y_list[states.index(state)]\n    u = action_outcome(state_x,state_y,action)[0]\n    v = action_outcome(state_x,state_y,action)[1]\n    next_state_x = state_x + u\n    next_state_y = state_y + v\n    # Returns index of x + y position to then find the state name\n    next_state = states[' '.join(str(x_list[i])+ \"_\" + str(y_list[i]) for i in range(0,len(x_list))).split().index(str(next_state_x) + \"_\" + str(next_state_y))]\n    reward = rewards[states.index(next_state)]\n    return(state, action, state_x, state_y, u, v, next_state, next_state_x, next_state_y, reward)","fdbf0ff7":"for i in range(0,10):\n    print(environment('A', 'left'))","d97c418b":"import pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import figure\n\nimport time\nfrom IPython.display import clear_output\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nsns.set(rc={'figure.figsize':(15, 10)})\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 5})\n\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\n\n#------------------------------------------------------------------------------------------\n\nstates = ['A','B','C','D','E','F','G','T','M','recycling','trash']\nx_list = [4,3,2,1,1,1,2,3,3,4,4]\ny_list = [1,1,1,1,2,3,3,3,2,3,2]\n\n# The low-level actions the agent can make in the environment\nactions = ['left','right','up','down']\nrewards = [-0.04,-0.04,-0.04,-0.04,-0.04,-0.04,-0.04,-0.04,-0.04,1,-1]\n\ninitial_policy = ['left','up','right','up','up','right','right','right','up','up','up']\n\n#------------------------------------------------------------------------------------------\n\ndef action_outcome(state_x,state_y,action):\n    if action == 'left':\n        u = -1\n        v = 0\n    elif action == 'right':\n        u = 1\n        v = 0\n    elif action == 'up':\n        u = 0\n        v = 1\n    elif action == 'down':\n        u = 0\n        v = -1\n    else:\n        print(\"Error: Invalid action given\")\n        \n    # Override if action hits wall to not move\n    if (state_x == 1) & (u == -1):\n        u = 0\n        v = v\n    elif (state_x == 4) & (u == 1):\n        u = 0\n        v = v\n    elif (state_y == 1) & (v == -1):\n        u = u\n        v = 0\n    elif (state_y == 3) & (v == 1):\n        u = u\n        v = 0\n    elif (state_x == 2)&(state_y == 1) & (v == 1):\n        u = u\n        v = 0\n    elif (state_x == 1)&(state_y == 2) & (u == 1):\n        u = 0\n        v = v  \n    elif (state_x == 2)&(state_y == 3) & (v == -1):\n        u = u\n        v = 0         \n    elif (state_x == 3)&(state_y == 2) & (u == -1):\n        u = 0\n        v = v \n    # Make so it cannot get out of bin\n    elif (state_x == 4)&(state_y == 3):\n        u = 0\n        v = 0\n    elif (state_x == 4)&(state_y == 2):\n        u = 0\n        v = 0\n    return(u,v)\n\ndef environment(state, action):\n    # Outcome probabilities\n    if (state=='recycling')|(state=='trash'):\n        prob = 0\n    elif (state=='T'):\n        prob = 1\n\n    elif (state=='M'):\n        prob = 0.7\n   \n    elif (state=='B'):\n        prob = 0.7\n\n    elif (state=='A'):\n        prob = 0.7\n\n    elif (state=='C'):\n        prob = 0.7\n\n    elif (state=='D'):\n        prob = 0.7\n\n    elif (state=='E'):\n        prob = 0.7\n\n    elif (state=='F'):\n        prob = 0.7\n\n    elif (state=='G'):\n        prob = 0.7\n\n    else:\n        prob = \"Error\"\n        print(\"Error state\", state)\n\n    action_rng = np.random.rand()\n    if action_rng<=prob:\n        action = action\n    else:\n        action_sub_list = actions.copy()\n        action_sub_list.remove(action)\n        action = random.choice(action_sub_list)\n        \n        \n    state_x = x_list[states.index(state)]\n    state_y = y_list[states.index(state)]\n    u = action_outcome(state_x,state_y,action)[0]\n    v = action_outcome(state_x,state_y,action)[1]\n    next_state_x = state_x + u\n    next_state_y = state_y + v\n    # Returns index of x + y position to then find the state name\n    next_state = states[' '.join(str(x_list[i])+ \"_\" + str(y_list[i]) for i in range(0,len(x_list))).split().index(str(next_state_x) + \"_\" + str(next_state_y))]\n    reward = rewards[states.index(next_state)]\n    return(state, action, state_x, state_y, u, v, next_state, next_state_x, next_state_y, reward)\n\n#------------------------------------------------------------------------------------------\n","80ad8505":"## Introducing the Environment\n\nWe formalise the problem into an example classroom with rows of students and a teacher. \n\nThe teacher is the controller of the system and would like for the paper to be passed to him\/her and placed into the recycling bin, not the general trash. More formally:\n\n1. the paper should be passed along the class, \n2. in as few steps as possible, \n3. until it can be passed to the lecturer (controller) and will be placed in the bin. \n\nWe first show this as a real diagram and then how this can be formalised into a 'Grid World' example. In this example, students A and M are 'risky' individuals in that there is a chance that they attempt to throw the paper. As before, each student may pass or hold onto the paper (denoted by actions that go into a wall).\n\n### Formalising the MDP\n\nThe MDP for the environment is de\ufb01ned by: \n- States are the students, teacher and the bin with 2-d(x,y) positions in the classroom\n- Actions are commands given to each state and are de\ufb01ned by [up, down, left, right]\n- The transition probability function is de\ufb01ned by the probability each state object has for successfully following the given action.\n- The reward function is simply de\ufb01ned as +1 for reaching the positive goal, -1 for reaching the negative goal and a small negative reward (e.g. -0.04) otherwise to encourage decisions that lead to the positive goal\n\n\nThe probabilities are defined by:\n- The next state will depend on the current [x,y] state, action and probability distribution. If a \u201cwall\u201d is hit (e.g. C := [2,1] and action \u201cup\u201d) then the next state will remain the same as the current state.\n- The teacher will always follow their command.\n- Other students have the probability of following the action set to 0.8 and 0.1 for every other action\n- The episode ends when the paper reaches the bin.\n\n\n\n![Real](https:\/\/i.imgur.com\/nOIUKlg.png)\n\n![GridWorld](https:\/\/i.imgur.com\/WeJnqs2.png)"}}