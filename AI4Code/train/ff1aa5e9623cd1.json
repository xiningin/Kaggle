{"cell_type":{"bfd63254":"code","87ca4acf":"code","354e7b33":"code","91ad1c4e":"code","c463f2b5":"code","010098cc":"code","87491c27":"code","23e23bb8":"code","b687a230":"code","99490446":"code","fde32b26":"code","be249957":"code","21772182":"code","a7a555cf":"code","50136a65":"code","b9d37c39":"code","84dec93e":"code","7a2a4f7b":"code","bc653e5f":"markdown","927dca46":"markdown","46fc2bf7":"markdown","e8a789a4":"markdown","7a5a4d8a":"markdown","f68e451e":"markdown","35757b07":"markdown","2f15fc07":"markdown","6af1daea":"markdown","e2bef34f":"markdown","5fcf0dc0":"markdown","1f51514f":"markdown","44cdbd55":"markdown","0a4a903d":"markdown","7c0c263a":"markdown","d866823d":"markdown","95c80461":"markdown","340c286a":"markdown","6235665b":"markdown","a7096298":"markdown"},"source":{"bfd63254":"#Libraries\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\nimport json\nimport string\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom tqdm.autonotebook import tqdm\nfrom functools import partial\nfrom wordcloud import WordCloud, STOPWORDS\nimport nltk\nimport spacy\nnlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\nnlp.max_length = 4000000\nfrom nltk.probability import FreqDist\n","87ca4acf":"train = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\ntrain.head()","354e7b33":"train.columns","91ad1c4e":"train.info()","c463f2b5":"for col in train.columns:\n    print(col + \":\" + str(len(train[col].unique())))","010098cc":"sample_sub = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')\nsample_sub.head()","87491c27":"train_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\ntest_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/test'","23e23bb8":"def json_to_text(filename, train_files_path=train_files_path, output='text'):\n    json_path = os.path.join(train_files_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","b687a230":"tqdm.pandas()\ntrain['text'] = train['Id'].progress_apply(json_to_text)","99490446":"train.head()","fde32b26":"tqdm.pandas()\nsample_sub['text'] = sample_sub['Id'].progress_apply(partial(json_to_text, train_files_path=test_files_path))","be249957":"sample_sub.head()","21772182":"def text_cleaning(text):\n    text = ''.join([k for k in text if k not in string.punctuation])\n    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n    text = re.sub(' +', ' ', text)\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    return text","a7a555cf":"tqdm.pandas()\ntrain['text'] = train['text'].progress_apply(text_cleaning)","50136a65":"text = ' '.join(train['text'].sample(frac=0.3))\nwordcloud = WordCloud(background_color='white', stopwords=STOPWORDS, width=2000, height=1200).generate(text)\n\nbarplot_dim = (15, 15)\nax = plt.subplots(figsize=barplot_dim, facecolor='w')\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title('Top 100 Most Common Words in Publications Text', fontsize=50)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()\n","b9d37c39":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\ntemp_1 = [x.lower() for x in train['dataset_label'].unique()]\ntemp_2 = [x.lower() for x in train['dataset_title'].unique()]\ntemp_3 = [x.lower() for x in train['cleaned_label'].unique()]\n\nexisting_labels = set(temp_1 + temp_2 + temp_3)\nid_list = []\nlables_list = []\nfor index, row in tqdm(sample_sub.iterrows()):\n    sample_text = row['text']\n    row_id = row['Id']\n    temp_df = train[train['text'] == text_cleaning(sample_text)]\n    cleaned_labels = temp_df['cleaned_label'].to_list()\n    for known_label in existing_labels:\n        if known_label in sample_text.lower():\n            cleaned_labels.append(clean_text(known_label))\n    cleaned_labels = [clean_text(x) for x in cleaned_labels]\n    cleaned_labels = set(cleaned_labels)\n    lables_list.append('|'.join(cleaned_labels))\n    id_list.append(row_id)","84dec93e":"submission = pd.DataFrame()\nsubmission['Id'] = id_list\nsubmission['PredictionString'] = lables_list","7a2a4f7b":"submission.head()","bc653e5f":"<h3><b>Data Processing<\/b><\/h3>","927dca46":"<b>Currently working on BERT model for this competition for better results. Hopefully share with you soon. Thank You!!<\/b>","46fc2bf7":"<b>So we have no 'NULL' values in the train data<\/b>","e8a789a4":"<b>Train Data Exploration<\/b>","7a5a4d8a":"<h3><b>Data Description<\/b><\/h3>\n<p><\/p>\n<ul>\n    <li><b>id-<\/b> publication id <\/li>\n    <li><b>PredictionString-<\/b>To be filled with equivalent of cleaned_label of train data..<\/li>\n    \n<\/ul>","f68e451e":"<center><img src = \"data:image\/png;base64,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\" width = \"350\" height = \"350\"\/><\/center>             \n","35757b07":"<h3>References<\/h3>\n\n\nhttps:\/\/www.kaggle.com\/ishandutta\/coleridge-complete-eda-in-one-notebook\n\nhttps:\/\/www.kaggle.com\/harshsharma511\/starter-competition-data-eda-and-modelling\n\nhttps:\/\/www.kaggle.com\/harshsharma511\/starter-competition-data-eda-and-modelling\n\nhttps:\/\/www.kaggle.com\/harshsharma511\/coleridge-initiative-eda-baseline-model","2f15fc07":"<h4><center>First Understand What is Coleridge Initiative?<\/center><\/h4>\n<ul>\n<li>The Coleridge Initiative is a not-for-profit organization, originally established at New York University, that is working with governments to ensure that data are more effectively used for public decision-making<\/li>\n<p><\/p>\n<li>To achieve this goal Coleridge Initiative is working with government agencies to create value for the taxpayer from the careful use of data, by building new technologies to enable secure access to and sharing of confidential microdata, and by training agency staff to acquire modern data skills.<\/li>\n<\/ul>\n\n\n","6af1daea":"<h1><center>Coleridge: Start To End<\/center><\/h1>\n<h3><center>In this notebook I will drive you through all the important steps required to understand and complete this competition. <center><\/h3>","e2bef34f":"<b>Now apply the function to submission Data<\/b>","5fcf0dc0":"<h3><center>EDA with Visualization<\/h3>","1f51514f":"<h3><b>Baseline Model<\/b><\/h3>","44cdbd55":"<b>Let's see the Train Data now<\/b>","0a4a903d":"<h3><b>Data Description<\/b><\/h3>\n<p><\/p>\n<ul>\n    <li><b>id-<\/b> publication id - note that there are multiple rows for some training documents, indicating multiple mentioned datasets.<\/li>\n    <li><b>pub_title-<\/b>title of the publication (a small number of publications have the same title).<\/li>\n    <li><b>dataset_title-<\/b>the title of the dataset that is mentioned within the publication.<\/li>\n    <li><b>dataset_label-<\/b>a portion of the text that indicates the dataset.<\/li>\n    <li><b>cleaned_label-<\/b>the dataset_label, as passed through the clean_text function from the Evaluation page.<\/li>\n<\/ul>","7c0c263a":"<h4> <center>Competition Goal<\/center> <\/h4>\nIn this competition, we need to develop an algorithm using natural language processing (NLP) to automate the discovery of how scientific data is referenced in publications. We have to identify the data sets that publications authors used in their work. For this we have full text of scientific publications from numerous research areas.\nThis model will eventually enable government agencies and researchers to quickly find the information they need. The approach will be used to develop data usage scorecards to better enable agencies to show how their data are used and bring down a critical barrier to the access and use of public data.\n<p><\/p>\n<b>In short, we have to build a model to identify what are the datasets that a publication is using.<\/b>\n\n","d866823d":"<b>Create a function to Preprocess the data using Basic NLP Filters (all text to lower case, Removes special charecters, emojis and multiple spaces)<\/b>","95c80461":"<b>Now we will create a function to get the text from the JSON file and append it to the new column in table<\/b>","340c286a":"<h4>Inference<\/h4>\n\n\n- The Training Dataset has 19,661 samples but only 14,316 unique IDs in the dataset. This means that some publications include a multitude of datasets. \n\n\n- The pub_title unique count is also less than the Id unique counts. This points to the precense of several occurences of having 2 separate publications, each with a unique ID, but sharing the exact same title.\n\n\n- Also, there are a total of 45 unique dataset_title and 130 unique dataset_label. It means that a single dataset could have multible labels throughout different publications.","6235665b":"<h4><center>Let's understand the DATA (EDA)<\/center><\/h4>\n<p><\/p>\n<b>Files Provided<\/b>\n<ul>\n    <li><b>train.csv-<\/b> CSV file contains metadata of the publications<\/li>\n    <li><b>train-<\/b>JSON file contains publications that are referenced in train.csv<\/li>\n    <li><b>test-<\/b>CSV file contains publications for testing purpose<\/li>\n    <li><b>sample_submission.csv-<\/b>CSV file conatins publications IDs column and prediction columns<\/li>\n<\/ul>","a7096298":"<b>Sample Submission<\/b>"}}