{"cell_type":{"a2484859":"code","52b4a7ef":"code","88394ac5":"code","fa3a24fe":"code","8c7bb5ed":"code","42d3f537":"code","90fee060":"code","c22f831e":"code","216ae8b9":"code","3b6a1a8b":"code","3459d0e7":"code","92b4378c":"code","69d09b62":"code","86b4d2f1":"code","a14e3db3":"code","074ac437":"markdown","cce03ab2":"markdown","b8c2352b":"markdown","165f95d1":"markdown","7353999a":"markdown"},"source":{"a2484859":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","52b4a7ef":"from IPython.display import display, HTML\nimport warnings\nfrom tqdm import tqdm\nwarnings.filterwarnings('ignore')\n\ntrain_df = pd.read_csv(\"\/kaggle\/input\/scaling-3\/new_train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/scaling-3\/new_test.csv\")\ntrain_df['batch']=((train_df.time-0.0001)\/\/50).astype(int)\ntest_df['batch']=((test_df.time-0.0001)\/\/50).astype(int)\ntrain_df['mini_batch']=((train_df.time-0.0001)\/\/10).astype(int)\ntest_df['mini_batch']=((test_df.time-0.0001)\/\/10).astype(int)\ntrain_df['mini_mini_batch']=((train_df.time-0.0001)\/\/0.5).astype(int)\ntest_df['mini_mini_batch']=((test_df.time-0.0001)\/\/0.5).astype(int)\n\nshifted = [4,9]\ntrain_df.loc[train_df.batch.isin(shifted),'signal'] = train_df.loc[train_df.batch.isin(shifted),'signal'] #+ 2.72\nshifted = [55,57]\ntest_df.loc[test_df.mini_batch.isin(shifted),'signal'] = test_df.loc[test_df.mini_batch.isin(shifted),'signal'] #+ 2.72\n\n# # Dirty Batches\n# dirty = [x for x in range(728,765)]\n# train_df = train_df[~train_df.mini_mini_batch.isin(dirty)]\n\n# for batch in [10,9,8]: train_df.loc[(train_df.batch==batch) ,'batch'] = batch+1\n    \n# train_df.loc[(train_df.batch==7) & (train_df.mini_mini_batch>=765),'batch'] = 8\n\ntrain_df = train_df.reset_index(drop=True)\n\ntrain_df.batch.value_counts()","88394ac5":"train_df.groupby(['batch','open_channels']).signal.agg(['mean','std'])","fa3a24fe":"import os\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n\nfrom scipy.stats import norm","8c7bb5ed":"from sklearn.metrics import f1_score,accuracy_score\nsignal_stats = train_df[['open_channels','signal']].groupby('open_channels').agg(['mean','std']).reset_index()\nwindow = 0.1\nsignal_lb = signal_stats[('signal', 'mean')] - window\nsignal_ub = signal_stats[('signal', 'mean')] + window\n\nfor oc in range(11): train_df.loc[(train_df.signal>signal_lb.loc[oc])&(train_df.signal<signal_ub.loc[oc]),'prediction']=oc\n    \n#Estimating Percentiles  0.9275004465932014\ndistribution = train_df[['signal','mini_batch','prediction']].groupby(['mini_batch','prediction']).agg('count').reset_index()\ndistribution = distribution[distribution.signal>=5]\ndistribution['signal'] = distribution.groupby(['mini_batch'])['signal'].apply(lambda x: x.cumsum())\ndistribution = distribution.pivot_table(values='signal',index=distribution.mini_batch,columns='prediction')\ndistribution = distribution.div(distribution.max(axis=1),axis=0).fillna(-1)\ntrain_distribution = distribution\nActual_Transitions = {}\nTransition_matrices = {}\n\nfor mini_batch in tqdm(range(50)):\n#     print('mini_batch',mini_batch)\n    temp_df = train_df[train_df.mini_batch==mini_batch]\n    if temp_df.shape[0]>1000:\n        oc=0\n        percentile = distribution.loc[mini_batch,oc]\n        temp_df['prediction'] = -1\n        while(percentile==-1):\n            oc+=1\n            percentile = distribution.loc[mini_batch,oc]\n        while(percentile!=-1 and oc<11):\n            threshold = temp_df.signal.quantile(percentile)\n            temp_df.loc[(temp_df.prediction==-1) & (temp_df.signal<=threshold),'prediction'] =oc\n            oc+=1\n            if oc<11: percentile = distribution.loc[mini_batch,oc]\n                \n            \n            temp_df['next_oc'] = temp_df.open_channels.shift(-1)\n            trans_mat = temp_df[['open_channels','next_oc','signal']].groupby(['open_channels','next_oc']).agg('count').reset_index()\n            trans_mat = trans_mat.pivot_table(values='signal',index=trans_mat.open_channels,columns='next_oc')\n            transition = pd.DataFrame(columns = np.unique(temp_df.open_channels),index = np.unique(temp_df.open_channels))\n            transition.loc[:,:] = 1 #Smoothing Factor\n            transition += trans_mat\n            transition = transition.fillna(1)\n            transition = transition.div(transition.sum(axis=1),axis=0)\n            Actual_Transitions[mini_batch] = transition\n            \n            temp_df['next_oc'] = temp_df.prediction.shift(-1)\n            trans_mat = temp_df[['prediction','next_oc','signal']].groupby(['prediction','next_oc']).agg('count').reset_index()\n            trans_mat = trans_mat.pivot_table(values='signal',index=trans_mat.prediction,columns='next_oc')\n            transition = pd.DataFrame(columns = np.unique(temp_df.prediction),index = np.unique(temp_df.prediction))\n            transition.loc[:,:] = 0.5 #Smoothing Factor\n            transition += trans_mat\n            transition = transition.fillna(0.5)\n            transition = transition.div(transition.sum(axis=1),axis=0)\n            Transition_matrices[mini_batch] = transition","42d3f537":"window = 0.1\nsignal_lb = signal_stats[('signal', 'mean')] - window\nsignal_ub = signal_stats[('signal', 'mean')] + window\n\nfor oc in range(11): test_df.loc[(test_df.signal>signal_lb.loc[oc])&(test_df.signal<signal_ub.loc[oc]),'prediction']=oc\n    \n#Estimating Percentiles\ndistribution = test_df[['signal','mini_batch','prediction']].groupby(['mini_batch','prediction']).agg('count').reset_index()\ndistribution = distribution[distribution.signal>=10]\ndistribution['signal'] = distribution.groupby(['mini_batch'])['signal'].apply(lambda x: x.cumsum())\ndistribution = distribution.pivot_table(values='signal',index=distribution.mini_batch,columns='prediction')\ndistribution = distribution.div(distribution.max(axis=1),axis=0).fillna(-1)\ntest_distribution = distribution\n\nfor mini_batch in tqdm(range(50,70)):\n    temp_df = test_df[test_df.mini_batch==mini_batch]\n    oc=0\n    percentile = distribution.loc[mini_batch,oc]\n    temp_df['prediction'] = -1\n    while(percentile==-1):\n        oc+=1\n        percentile = distribution.loc[mini_batch,oc]\n    while(percentile!=-1 and oc<11):\n        threshold = temp_df.signal.quantile(percentile)\n        temp_df.loc[(temp_df.prediction==-1) & (temp_df.signal<=threshold),'prediction'] =oc\n        oc+=1\n        if oc<11: percentile = distribution.loc[mini_batch,oc]\n    temp_df['next_oc'] = temp_df.prediction.shift(-1)\n    trans_mat = temp_df[['prediction','next_oc','signal']].groupby(['prediction','next_oc']).agg('count').reset_index()\n    trans_mat = trans_mat.pivot_table(values='signal',index=trans_mat.prediction,columns='next_oc')\n    transition = pd.DataFrame(columns = np.unique(temp_df.prediction),index = np.unique(temp_df.prediction))\n    transition.loc[:,:] = 0.5 #Smoothing Factor\n    transition += trans_mat\n    transition = transition.fillna(0.5)\n    transition = transition.div(transition.sum(axis=1),axis=0)\n    Transition_matrices[mini_batch] = transition","90fee060":"test_distribution","c22f831e":"signal_means = train_df[['open_channels','signal']].groupby('open_channels').agg('mean')\nstandard_dev = {\n 0: 0.24069495895789328,\n 1: 0.24661370964721863,\n 2: 0.2462656210881039,\n 3: 0.24593499463670376,\n 4: 0.24285132812035926,\n 5: 0.26328499147888296,\n 6: 0.24325060224204165,\n 7: 0.24219770330946155,\n 8: 0.24284409806346507,\n 9: 0.24213353276403837,\n 10: 0.24582776879974166,\n 11: 0.24496232384251448,\n 12: 0.24419957371390066,\n 13: 0.24390344537922137,\n 14: 0.24401429614362458,\n 15: 0.2650094712461518,\n 16: 0.26515878374200286,\n 17: 0.26653346413097956,\n 18: 0.2666851765378241,\n 19: 0.2661300555645225,\n 20: 0.4069574034485397,\n 21: 0.4057626703610925,\n 22: 0.40661851101785595,\n 23: 0.4014537024692114,\n 24: 0.40342408294931176,\n 25: 0.2864659504038526,\n 26: 0.28792615084155104,\n 27: 0.2861063013595865,\n 28: 0.28894890652916927,\n 29: 0.28746591555904344,\n 30: 0.24588901544435604,\n 31: 0.24562918245064735,\n 32: 0.24454967653259863,\n 33: 0.24712026725094094,\n 34: 0.24671466539168418,\n 35: 0.2944529092246321,\n 36: 0.2897700004860426,\n 38: 0.29459674149215787,\n 39: 0.27525129540509075,\n 40: 0.2855328333419974,\n 41: 0.28533031164314704,\n 42: 0.28859497286113045,\n 43: 0.28256191424770405,\n 44: 0.2841569645597404,\n 45: 0.4066384374459793,\n 46: 0.4061604936865585,\n 47: 0.40514687628102375,\n 48: 0.4067380853754845,\n 49: 0.40637947598259294,\n 50: 0.24073643515935061,\n 51: 0.2754600438388737,\n 52: 0.29036115205832036,\n 53: 0.2410069541187777,\n 54: 0.2448870602255716,\n 55: 0.40433399257591063,\n 56: 0.2861781553817768,\n 57: 0.40588197314984553,\n 58: 0.24015921663211293,\n 59: 0.2724222128619417,\n 60: 0.2388213978815562,\n 61: 0.2409860881280551,\n 62: 0.2404194193333159,\n 63: 0.2454693052384903,\n 64: 0.24194863011797696,\n 65: 0.2413491063034735,\n 66: 0.24715158528432513,\n 67: 0.24293092821730067,\n 68: 0.24327976456590195,\n 69: 0.24585838370130864\n}","216ae8b9":"class ViterbiClassifier:\n    def __init__(self):\n        self._p_trans = None\n        self._p_signal = None\n    \n    def fit(self, mini_batch):\n        self._n_states = 11\n        self._states = list(range(self._n_states))\n        \n        self._p_trans = self.markov_p_trans(mini_batch)\n        \n        self._dists = []\n        for s in np.arange(0, 11):\n            self._dists.append((signal_means.loc[s,'signal'], standard_dev[mini_batch]))\n        \n        return self\n        \n    def predict(self, x, p_signal=None, proba=False):\n        if p_signal is None:\n            p_signal = self.markov_p_signal(x)\n\n        preds, probs = self.viterbi(self._p_trans, p_signal[self._states], x)\n        \n        \n        return preds, probs\n    \n    def markov_p_signal(self, signal):\n#         print(self._n_states, len(signal))\n        p_signal = np.zeros((self._n_states, len(signal)))\n        for k, dist in enumerate(self._dists):\n            p_signal[k, :] = norm.pdf(signal, *dist)\n            \n        return p_signal\n    \n    def markov_p_trans(self, mini_batch):\n        # https:\/\/www.kaggle.com\/friedchips\/the-viterbi-algorithm-a-complete-solution\n        transition = Transition_matrices[mini_batch]\n        TM = pd.DataFrame(index=range(11),columns=range(11))\n        TM.loc[transition.index,transition.columns] = transition\n        TM = TM.fillna(0)\n        return TM.values\n    \n    def viterbi(self, p_trans, p_signal, signal):\n        # https:\/\/www.kaggle.com\/friedchips\/the-viterbi-algorithm-a-complete-solution\n        offset = 10**(-20) # added to values to avoid problems with log2(0)\n\n        p_trans_tlog  = np.transpose(np.log2(p_trans  + offset)) # p_trans, logarithm + transposed\n        p_signal_tlog = np.transpose(np.log2(p_signal + offset)) # p_signal, logarithm + transposed\n        \n        T1 = np.zeros(p_signal.shape)\n        T2 = np.zeros(p_signal.shape)\n\n        T1[:, 0] = p_signal_tlog[0, :]\n        T2[:, 0] = 0\n\n        for j in range(1, p_signal.shape[1]):\n            for i in range(len(p_trans)):\n                T1[i, j] = np.max(T1[:, j - 1] + p_trans_tlog[:, i] + p_signal_tlog[j, i])\n                T2[i, j] = np.argmax(T1[:, j - 1] + p_trans_tlog[:, i] + p_signal_tlog[j, i])\n        \n        x = np.empty(p_signal.shape[1], 'B')\n        x[-1] = np.argmax(T1[:, p_signal.shape[1] - 1])\n        for i in reversed(range(1, p_signal.shape[1])):\n            x[i - 1] = T2[x[i], i]\n    \n        return x, T1\n    \nclass PosteriorDecoder:\n    def __init__(self):\n        self._p_trans = None\n        self._p_signal = None\n    \n    def fit(self, mini_batch):\n        self._n_states = 11\n        self._states = list(range(self._n_states))\n        \n        self._p_trans = self.markov_p_trans(mini_batch)\n        \n        self._dists = []\n        for s in np.arange(0, 11):\n            self._dists.append((signal_means.loc[s,'signal'], standard_dev[mini_batch]))\n        \n        return self\n        \n    def predict(self, x, p_signal=None, proba=False):\n        if p_signal is None:\n            p_signal = self.markov_p_signal(x)\n        preds,probs = self.posterior_decoding(self._p_trans, p_signal[self._states])\n        \n        return preds, probs\n    \n    def markov_p_signal(self, signal):\n        p_signal = np.zeros((self._n_states, len(signal)))\n        for k, dist in enumerate(self._dists):\n            p_signal[k, :] = norm.pdf(signal, *dist)\n            \n        return p_signal\n    \n    def markov_p_trans(self, mini_batch):\n        # https:\/\/www.kaggle.com\/friedchips\/the-viterbi-algorithm-a-complete-solution\n        transition = Transition_matrices[mini_batch]\n        TM = pd.DataFrame(index=range(11),columns=range(11))\n        TM.loc[transition.index,transition.columns] = transition\n        TM = TM.fillna(0)\n        return TM.values\n    \n    def forward(self, p_trans, p_signal):\n        \"\"\"Calculate the probability of being in state `k` at time `t`, \n           given all previous observations `x_1 ... x_t`\"\"\"\n        T1 = np.zeros(p_signal.shape)\n        T1[:, 0] = p_signal[:, 0]\n        T1[:, 0] \/= np.sum(T1[:, 0])\n\n        for j in range(1, p_signal.shape[1]):\n            for i in range(len(p_trans)):\n                T1[i, j] = p_signal[i, j] * np.sum(T1[:, j - 1] * p_trans[i, :])\n            T1[:, j] \/= np.sum(T1[:, j])\n\n        return T1\n\n    def backward(self, p_trans, p_signal):\n        \"\"\"Calculate the probability of observing `x_{t + 1} ... x_n` if we \n           start in state `k` at time `t`.\"\"\"\n        T1 = np.zeros(p_signal.shape)\n        T1[:, -1] = p_signal[:, -1]\n        T1[:, -1] \/= np.sum(T1[:, -1])\n\n        for j in range(p_signal.shape[1] - 2, -1, -1):\n            for i in range(len(p_trans)):\n                T1[i, j] = np.sum(T1[:, j + 1] * p_trans[:, i] * p_signal[:, j + 1])\n            T1[:, j] \/= np.sum(T1[:, j])\n\n        return T1\n    \n    def posterior_decoding(self, p_trans, p_signal):\n        fwd = self.forward(p_trans, p_signal)\n        bwd = self.backward(p_trans, p_signal)\n\n        preds = np.empty(p_signal.shape[1], 'B')\n        probs = np.zeros((11,fwd.shape[1]))\n        for i in range(p_signal.shape[1]):\n            preds[i] = np.argmax(fwd[:, i] * bwd[:, i])\n            probs[:,i] = fwd[:, i] * bwd[:, i]\n\n        return preds,probs","3b6a1a8b":"oof_probs = []\noof_predictions = []\nfor mini_batch in range(50):\n    if mini_batch in Transition_matrices:\n        signal = train_df[train_df.mini_batch==mini_batch].signal.values\n        open_channels = train_df[train_df.mini_batch==mini_batch].open_channels.values\n        viterbi = PosteriorDecoder().fit(mini_batch)\n        viterbi_predictions,viterbi_probabilities = viterbi.predict(signal)\n        print(mini_batch,accuracy_score(viterbi_predictions,open_channels))\n        train_df.loc[train_df.mini_batch==mini_batch,'prediction'] = viterbi_predictions\n        oof_probs.append(viterbi_probabilities)\n        oof_predictions.append(viterbi_predictions)\noof_probs = np.transpose(np.concatenate(oof_probs,axis=1))\noof_predictions = np.concatenate(oof_predictions)\nnp.save('oof_probs.npy', oof_probs)  \nnp.save('oof_predictions.npy', oof_predictions)  ","3459d0e7":"print(f1_score(train_df.open_channels,train_df.prediction,average='macro'))\npd.DataFrame(confusion_matrix(train_df.open_channels,train_df.prediction))","92b4378c":"test_probs = []\ntest_predictions = []\nfor mini_batch in tqdm(range(50,70)):\n    if mini_batch in Transition_matrices:\n        signal = test_df[test_df.mini_batch==mini_batch].signal.values\n        viterbi = PosteriorDecoder().fit(mini_batch)\n        viterbi_predictions,viterbi_probabilities = viterbi.predict(signal)\n        test_probs.append(viterbi_probabilities)\n        test_predictions.append(viterbi_predictions)\ntest_probs = np.transpose(np.concatenate(test_probs,axis=1))\nnp.save('test_probs.npy', test_probs)  \ntest_predictions = np.concatenate(test_predictions)\nnp.save('test_predictions.npy', test_predictions)  ","69d09b62":"submission_df = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/sample_submission.csv')\nsubmission_df['open_channels'] = test_predictions\nsubmission_df.to_csv(\"submission.csv\", float_format='%.4f', index=False)\nsubmission_df.open_channels.value_counts()","86b4d2f1":"for oc in range(11):\n    print(\"Open Channels\",oc)\n    print(f1_score((train_df.open_channels==oc).astype(int),(train_df.prediction==oc).astype(int)))","a14e3db3":"test_df['prediction'] = test_predictions\ndistribution = test_df[['signal','mini_batch','prediction']].groupby(['mini_batch','prediction']).agg('count').reset_index()\ndistribution = distribution.pivot_table(values='signal',index=distribution.mini_batch,columns='prediction')\ndistribution = distribution.div(distribution.sum(axis=1),axis=0).fillna(-1) \ndistribution","074ac437":"### This Kernel shows how to train HMM without any priors about groups. \n#### Basic Steps\n1. Estimated distribution using regions of high confidence intervals\n2. Estimated Transition Matrix using preliminary predictions\n3. Estimated Mean and Standard Deviations\n4. Trained the Posterior Decoder models\nThis Kernel beats all the public lb scores in the private lb. The findings from this kernel helped to build powerful random forest models. As this approach don't hold any prior, It's highly powerful with unseen data. ","cce03ab2":"Estimating Distribution & Transition Matrix","b8c2352b":"### Posterior Decoder from: https:\/\/www.kaggle.com\/group16\/lb-0-936-1-feature-forward-backward-vs-viterbi","165f95d1":"Idea to try: Use both Forward and backward prob as feature in any supervised model","7353999a":"Standard Deviation Estimated from: https:\/\/www.kaggle.com\/ks2019\/estimating-standard-deviations-on-new-data?scriptVersionId=34750556"}}