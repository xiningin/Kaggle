{"cell_type":{"d775fabe":"code","02a9e522":"code","9e710811":"code","f19de678":"code","0a150c63":"code","7f2e39f2":"code","e435f7ce":"code","669e65e2":"code","e0dd97d7":"code","17cb3a0a":"code","8de0f480":"code","3940571c":"code","0827121c":"code","e2d21ead":"code","7a5c9b49":"code","6179b6fd":"code","e28ee0b7":"code","0dbaaf56":"code","81672c9b":"code","9ef38060":"code","deef92dc":"code","8281326c":"code","954ab20f":"code","fcc5bd83":"code","c271bf3e":"code","a5a5a6a8":"code","bdc7f481":"code","fff8c690":"code","c3f60746":"code","2cfa5bba":"code","ab643a37":"code","7ce13c8e":"code","4116e703":"code","036eadc0":"code","c522dd9e":"code","ef11a005":"code","95dde494":"code","7a44dc5c":"code","bf22c9bd":"code","6fe07aa3":"code","396a7287":"code","5933c274":"code","224f4f0e":"code","c99c0958":"code","6ca992e4":"code","c98ef36e":"code","1aee2c10":"code","514860aa":"code","1150f065":"code","fdd891a0":"code","f35e05d6":"code","eb953e92":"code","a03b1a16":"code","59641e25":"code","8d02db2e":"code","15104086":"code","9b9cb7da":"code","183bb836":"code","68b6db1f":"markdown","791ae1ff":"markdown","2e32bc87":"markdown","b7581a56":"markdown","ed8667af":"markdown","abea82ed":"markdown","8b88f44c":"markdown","0df27727":"markdown","1680b0a6":"markdown","22ec73af":"markdown","de2c0ff0":"markdown","a80e902e":"markdown","62963b20":"markdown","b83bcf2c":"markdown","89e24658":"markdown","491a59e9":"markdown","43d186d9":"markdown","00dfbc9c":"markdown","1266e2c0":"markdown","ed9dd5bb":"markdown","266f182a":"markdown","986259d6":"markdown","51fa1c4c":"markdown","5e73269a":"markdown","4c01f956":"markdown","d4b62b90":"markdown","ee009fa2":"markdown","3c222973":"markdown","325dd1f3":"markdown","0d70b3d8":"markdown","51ff45ad":"markdown","807a0766":"markdown","4c5313c5":"markdown","ad0eb881":"markdown","259a35d8":"markdown","d59e2ca8":"markdown","5444b277":"markdown","5f205650":"markdown","69333ca3":"markdown","d9aab960":"markdown","a921a7dd":"markdown","bcdafbd6":"markdown","50a53c32":"markdown","98fe1fd0":"markdown"},"source":{"d775fabe":"!pip install fastai2 -q","02a9e522":"#Load the dependancies\nfrom fastai2.basics import *\nfrom fastai2.callback.all import *\nfrom fastai2.vision.all import *\nfrom fastai2.medical.imaging import *\n\nimport pydicom\nimport seaborn as sns\n\nimport numpy as np\nimport pandas as pd\nimport os","9e710811":"source = Path(\"..\/input\/siim-isic-melanoma-classification\")\nfiles = os.listdir(source)\nprint(files)","f19de678":"train = source\/'train'\ntrain_files = get_dicom_files(train)\ntrain_files","0a150c63":"patient1 = train_files[7]\ndimg = dcmread(patient1)","7f2e39f2":"dimg","e435f7ce":"def show_one_patient(file):\n    \"\"\" function to view patient image and choosen tags within the head of the DICOM\"\"\"\n    pat = dcmread(file)\n    print(f'patient Name: {pat.PatientName}')\n    print(f'Patient ID: {pat.PatientID}')\n    print(f'Patient age: {pat.PatientAge}')\n    print(f'Patient Sex: {pat.PatientSex}')\n    print(f'Body part: {pat.BodyPartExamined}')\n    trans = Transform(Resize(256))\n    dicom_create = PILDicom.create(file)\n    dicom_transform = trans(dicom_create)\n    return show_image(dicom_transform)","669e65e2":"show_one_patient(patient1)","e0dd97d7":"from pydicom.pixel_data_handlers.util import convert_color_space","17cb3a0a":"arr = dimg.pixel_array\nconvert = convert_color_space(arr, 'YBR_FULL_422', 'RGB')\nshow_image(convert)","8de0f480":"px = dimg.pixels.flatten()\nplt.hist(px, color='c')","3940571c":"df = pd.read_csv(source\/'train.csv')\ndf.head()","0827121c":"#Plot 3 comparisons\ndef plot_comparison3(df, feature, feature1, feature2):\n    \"Plot 3 comparisons from a dataframe\"\n    fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (16, 4))\n    s1 = sns.countplot(df[feature], ax=ax1)\n    s1.set_title(feature)\n    s2 = sns.countplot(df[feature1], ax=ax2)\n    s2.set_title(feature1)\n    s3 = sns.countplot(df[feature2], ax=ax3)\n    s3.set_title(feature2)\n    plt.show()","e2d21ead":"plot_comparison3(df, 'sex', 'age_approx', 'benign_malignant')","7a5c9b49":"#Plot 1 comparisons\ndef plot_comparison1(df, feature):\n    \"Plot 1 comparisons from a dataframe\"\n    fig, (ax1) = plt.subplots(1,1, figsize = (16, 4))\n    s1 = sns.countplot(df[feature], ax=ax1)\n    s1.set_title(feature)\n    plt.show()","6179b6fd":"plot_comparison1(df, 'diagnosis')","e28ee0b7":"plot_comparison1(df, 'target')","0dbaaf56":"plot_comparison1(df, 'anatom_site_general_challenge')","81672c9b":"eda_df = df[['sex','age_approx','anatom_site_general_challenge','diagnosis','target']]\neda_df.head()","9ef38060":"len(eda_df)","deef92dc":"sex_count = eda_df['sex'].isna().sum(); age_count = eda_df['age_approx'].isna().sum(); anatom_count = eda_df['anatom_site_general_challenge'].isna().sum()\nprint(f'Nan values in sex column: {sex_count}, age column: {age_count}, anatom count: {anatom_count}')","8281326c":"df_drop = eda_df.dropna()\nlen(df_drop)","954ab20f":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nedaa_df = eda_df.apply(lambda col: le.fit_transform(col.astype(str)), axis=0, result_type='expand')\nedaa_df.head()","fcc5bd83":"sns.set(style=\"whitegrid\")\nsns.set_context(\"paper\")","c271bf3e":"sns.pairplot(eda_df, hue=\"target\", height=5, aspect=2, palette='gist_rainbow_r')","a5a5a6a8":"sns.pairplot(eda_df, hue=\"age_approx\", height=6, aspect=3, diag_kws={'bw':'0.05'})","bdc7f481":"sns.set(style=\"whitegrid\")\nsns.set_context(\"poster\")\nsns.pairplot(edaa_df, hue=\"target\", height=6, palette='gist_rainbow', diag_kws={'bw':'0.05'})","fff8c690":"get_x = lambda x:source\/'train'\/f'{x[0]}.dcm'","c3f60746":"get_y=ColReader('target')","2cfa5bba":"batch_tfms = aug_transforms(flip_vert=True, max_lighting=0.1, max_zoom=1.05, max_warp=0.)","ab643a37":"class PILDicom2(PILBase):\n    _open_args,_tensor_cls,_show_args = {},TensorDicom,TensorDicom._show_args\n    @classmethod\n    def create(cls, fn:(Path,str,bytes), mode=None)->None:\n        \"Open a `DICOM file` from path `fn` or bytes `fn` and load it as a `PIL Image`\"\n        dimg = dcmread(fn)\n        arr = dimg.pixel_array; convert = convert_color_space(arr,'YBR_FULL_422', 'RGB')\n        im = Image.fromarray(convert)\n        im.load()\n        im = im._new(im.im)\n        return cls(im.convert(mode) if mode else im)","7ce13c8e":"blocks = (ImageBlock(cls=PILDicom2), CategoryBlock)","4116e703":"melanoma = DataBlock(blocks=blocks,\n                   get_x=get_x,\n                   splitter=RandomSplitter(),\n                   item_tfms=Resize(128),\n                   get_y=ColReader('target'),\n                   batch_tfms=batch_tfms)","036eadc0":"dls = melanoma.dataloaders(df.sample(100), bs=2)","c522dd9e":"dls = dls.cuda()","ef11a005":"dls.show_batch(max_n=12, nrows=2, ncols=6)","95dde494":"roc = RocAuc()\ndls.c","7a44dc5c":"model = xresnet18_deeper(n_out=dls.c)","bf22c9bd":"set_seed(77)\nlearn = Learner(dls, model, \n                opt_func=ranger,\n                loss_func=LabelSmoothingCrossEntropy(),\n                metrics=[accuracy, roc],\n                cbs = ShowGraphCallback())","6fe07aa3":"learn.freeze()\nlearn.fit_one_cycle(1, 5e-2)","396a7287":"learn.save('xresnet18_stg1')","5933c274":"learn.unfreeze()","224f4f0e":"learn.fit_flat_cos(2,slice(1e-6,1e-4))","c99c0958":"learn.save('xresnet18_stg2')","6ca992e4":"interp = Interpretation.from_learner(learn)","c98ef36e":"interp.plot_top_losses(12)","1aee2c10":"tst = source\/'test'\ntest_set = get_dicom_files(tst)\ntest_set","514860aa":"test_set = test_set[:100]\ntest_set","1150f065":"test_patient = test_set[1]\ntest_patient","fdd891a0":"learn.load('xresnet18_stg2')","f35e05d6":"_ = learn.predict(test_patient)\n_","eb953e92":"_ = learn.predict(test_patient)\nprint(_[2][1])","a03b1a16":"sample_sub = pd.read_csv(source\/'sample_submission.csv')\nsample_sub = sample_sub[:100]\nsample_sub","59641e25":"del sample_sub['target']","8d02db2e":"sample_list = []\nfor i in test_set:\n    pre = learn.predict(i)\n    l = float(pre[2][1])\n    sample_list.append(l)","15104086":"sample_list","9b9cb7da":"sub = sample_sub.assign(target=sample_list)\nsub.to_csv('submission.csv', index=False)","183bb836":"sub = pd.read_csv('submission.csv')\nsub","68b6db1f":"Lets create a dataframe with a few features we want to explore more","791ae1ff":"As the dataset is huge, we can test the model by just training with `100` samples from the `train` dataset","2e32bc87":"We now specify the 'y' or output using `ColReader` and specify the `target` column in the csv file which in this case `0` denotes benign and `1` denotes malignant.  ","b7581a56":"We can now easily collate all the data into a `DataBlock` and use `fastai2`s inbuilt `splitter` function that will split the data into `train` and `valid` sets.  `Resize` ensure all the images are the same size when we feed it to the model.","ed8667af":"# EDA","abea82ed":"# Fastai2 DICOM starter","8b88f44c":"The plot above shows the distribution between age and target, here it is clear to see the differences between age in patients with target `0` and target `1`","0df27727":"**But why does the image look so unnatural?**","1680b0a6":"Specify the evaluation metric and check how many labels there are","22ec73af":"# Loading the `test` set","de2c0ff0":"### Pixel Distribution","a80e902e":"Create a function that will display an image and show choosen tags within the head of the DICOM, in this case `PatientName`, `PatientID`, `PatientSex`, `BodyPartExamined` and we can use `Tranform` from `fastai2` that conveniently allows us to resize the image","62963b20":"Load in the csv file","b83bcf2c":"Specify a test patient ","89e24658":"There are a number of `Nan` values within each column that we want to get rid off","491a59e9":"We can now explore the distribution of the data","43d186d9":"`fastai2` provides a convenient way of using `blocks`, in this case because we are specifying an `x` and a `y` we can now specify that the `x` will be `PILDicom` image and the `y` will be a `CategoryBlock` because we want the target to be either benign, `0` or malignant `1`","00dfbc9c":"We can look at the top losses","1266e2c0":"You can now view all the information of the DICOM file. Explanation of each element is beyond the scope of this notebook but [this](http:\/\/dicom.nema.org\/medical\/dicom\/current\/output\/chtml\/part03\/sect_C.7.6.3.html#sect_C.7.6.3.1.4) site has some excellent information about each of the entries. Information is listed by the DICOM tag (eg: 0008, 0005) or DICOM keyword (eg: Specific Character Set)","ed9dd5bb":"For testing purposes we will only use the first 100 images in the test set","266f182a":"lets set some seaborn parameters","986259d6":"This is because these images are stored in `YBR_FULL_422` color space and this is stated in the following tag:\n\n`(0028, 0004) Photometric Interpretation CS: 'YBR_FULL_422'`\n\nTo view the images as they are intended the color space needs to be converted from `YBR_FULL_422` to `RGB`.  `Pydicom` provides a means of converting from one color space to another by using `convert_color_space` where it takes the (pixel array, current color space, desired color space) as attributes.  This is done by acessing the `pixel_array` and then converting to the desired color space","51fa1c4c":"In order to load DICOMs in fastai2 we need to all load the `fastai2.medical.imaging` module.  However we will not be able to use the full functionality of the medical imaging module because these DICOM images are saved as `XC` format which stands for `External-camera Photography` hence these images are restricted to pixel values between `0` and `255`.  This is way limited to say 16 bit DICOM images that could have values ranging from `-32768` to `32768`.\n\n`Pydicom` is a python package for parsing DICOM files and makes it easy to covert DICOM files into pythonic structures for easier manipulation. Files are opened using pydicom.dcmread\n\n","5e73269a":"Specify the source","4c01f956":"Specify the architecure to be used.  In this case we ensure that the the output of the model is either `0` or `1` or 2 classes.  `dls.c` is a convenient way to specify that the output of the model will be 2.","d4b62b90":"Load the `sample_submisson`","ee009fa2":"However before we can create the `DataBlock` so that the images look 'real' we need to create a new method so that `PILBase` takes into consideration the `Photometric Interpretation`.","3c222973":"## Next steps:\n\n- Experiment with various models and augmentations\n","325dd1f3":"Lets first specify the `x` or input.  In this case we can create a `lambda` function that will get the image files from the `train` folder","0d70b3d8":"Specify the folder that contains the training images `train` and use `fastai2`s method of accessing the DICOM files by using `get_dicom_files`","51ff45ad":"Viewing a batch","807a0766":"Lets see what information is contained within each DICOM file","4c5313c5":"[Fastai2](https:\/\/github.com\/fastai\/fastai2) starter code using DICOMs.  DICOM(Digital Imaging and COmmunications in Medicine) is the de-facto standard that establishes rules that allow medical images(X-Ray, MRI, CT) and associated information to be exchanged between imaging equipment from different vendors, computers, and hospitals.\n\nDICOM files typically have a `.dcm` extension and provides a means of storing data in separate 'tags' such as patient information as well as image\/pixel data. A DICOM file consists of a header and image data sets packed into a single file. The information within the header is organized as a constant and standardized series of tags. \n\nBy extracting data from these tags one can access important information regarding the patient demographics, study parameters, etc\n\n![Parts of a DICOM](https:\/\/asvcode.github.io\/MedicalImaging\/images\/copied_from_nb\/my_icons\/dicom_.PNG)\n\nYou can find out more about medical imaging by viewing this [blog](https:\/\/asvcode.github.io\/MedicalImaging\/)","ad0eb881":"The `predict` function displays the predicted class, in this case `0`, the tensor class `tensor(0)` and the probabilites of the each of the classes.  In this dataset there are 2 classes `0` and `1` and the the probabilites are predicted for each class so in this case the probablility that the `test_patient` is `benign` or class `0` is `0.9923` and the probability that the `test_patient` is `malignant` or `1` is `0.0317`","259a35d8":"That looks better!","d59e2ca8":"Lets look at a prediction for the `test_patient`","5444b277":"For some more EDA we need to convert the categorical features in the dataframe into numeric values. `LabelEncoder` encode labels with a value between 0 and n_classes-1 where `n` is the number of distinct labels","5f205650":"The evalution requirement in this competiton is that for each image_name in the test set, you must predict the probability (target) that the sample is malignant.  So we need to get the probability of class `1`\n\nWe can use the code below to get the probability of class `1`:","69333ca3":"Get the probabilites for the test set and create a list of the probabilites for each image in the test set and convert the probabilty to a float","d9aab960":"We can also view the pixel distribution of the image.  For this competition this is not really that important but can be as shown in this kernel [Understanding Dicoms](https:\/\/www.kaggle.com\/avirdee\/understanding-dicoms)","a921a7dd":"We can delete the `target` column as we will be populating this with the probabilites","bcdafbd6":"Getting some quick `batch_tfms`","50a53c32":"# Getting the data ready for training","98fe1fd0":"### Load the dependancies"}}