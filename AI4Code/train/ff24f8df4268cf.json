{"cell_type":{"56afa667":"code","28bcc2b6":"code","9ddca535":"code","90fa1e8c":"code","ad26e694":"code","8bda8f89":"code","95c010bd":"code","bf468ea9":"code","644e2383":"code","1a70d690":"code","52cab21d":"code","bcac7f4c":"code","950676a2":"code","7db60f5f":"code","11b26042":"markdown","6b8f5a57":"markdown","3c00eac5":"markdown","bc090dd4":"markdown","45a54b33":"markdown","59758d02":"markdown","76bfc47b":"markdown","fce25d1e":"markdown","5c6bd989":"markdown","df27c397":"markdown"},"source":{"56afa667":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","28bcc2b6":"import json","9ddca535":"# with open(\"..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json\",\"r\") as f:\n#     datastore = json.load(f)","90fa1e8c":"data = pd.read_json(\"..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json\",lines=True)\ndata.head()","ad26e694":"data.shape","8bda8f89":"# This will be treated as a training data.\nsentences = list(data['headline'])\nlabels = list(data['is_sarcastic'])\n\n# Will not use for this case.\nurls = list(data['article_link'])","95c010bd":"## Run this cell to check the sentences list:\n#sentences","bf468ea9":"print(len(sentences))\nprint(len(labels))","644e2383":"## Let's develop a word index using Tokenizer\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\ntokenizer = Tokenizer(oov_token= \"<OOV'>\")   \n## OOV stands for Out of Vocabulary\n## Used for labelling words which are not in Word-Index or Vocabulary of the model.\n\n## Fitting the tokenizer on the sentences:\ntokenizer.fit_on_texts(sentences)\n# Sentences here contains the headlines of sarcasm dataset.\n\n## Now we will create Vocabulary or Word_index.\n## This will be a dictionary.\n## With words as key and Index as values.\nword_index = tokenizer.word_index\n## This will numbered all the words present in the headlines\n## Also there will be no duplicacy in the Index.\n\n\nprint(\"Number of Unique Words in the Headlines: \",len(word_index))","1a70d690":"print(type(word_index))\n# word_index","52cab21d":"## Now let's develop the Sequences:\n\nsequences = tokenizer.texts_to_sequences(sentences)\nprint(\"The Number of Sequences is Equal to : \",len(sequences))\nprint()\nprint(\"First ten sequences are: \")\nprint()\nfor i in range(10):\n    print(sequences[i])\n    print()","bcac7f4c":"test=[\n    \"i love my country\"\n]\n\ntest_seq = tokenizer.texts_to_sequences(test)\nprint(\"Sequence for :\",test[0],test_seq)\n\n","950676a2":"print(word_index['i'])\nprint(word_index['love'])\nprint(word_index['my'])\nprint(word_index['country'])","7db60f5f":"from wordcloud import WordCloud, STOPWORDS \nimport matplotlib.pyplot as plt\n\nword = str(sentences[::])\n## As sentences here is a list with multiple strings(used above)\n## So wordcloud can only be fed strings and not list that have strings.\n## So every string in the sentences is put into a single variable word here\n## Which can be fed into the wordcloud.\n\n\nstopwords = set(STOPWORDS) \nwordcloud = WordCloud(width = 1600, height = 1600, \n                background_color ='black', \n                stopwords = stopwords).generate(word) \n  \n# plotting the Wordcloud Image                        \nplt.figure(figsize = (10, 10)) \nplt.imshow(wordcloud) \nplt.axis(\"off\")\n## Used to remove the axis. As by-default matplolib gives Axis in all it's Plots\n\n\nplt.title(\"HEADLINES\")\nplt.show()","11b26042":"- So, yeah, that was quite simple.\n- This also means that we have a lot in our Vocabulary.","6b8f5a57":"# WORDCLOUD VISUALIZATION\n\n- A tag cloud or Word cloud is a novelty **visual representation of text data**, typically used to depict keyword metadata on websites, or to visualize free form text.\n- Tags are usually single words, and the importance of each tag is shown with font size or color.\n\n#### Reference: \n\n- DOCS: https:\/\/amueller.github.io\/word_cloud\/generated\/wordcloud.WordCloud.html\n- Visualization Aid: https:\/\/www.kaggle.com\/venky73\/sarcasm","3c00eac5":"- Now as we have done the creation, let's test it on random sentence and see whether it can detect every word of that sentence or not.","bc090dd4":"### FIRST ATTEMPT TOWARDS NLP\n\n- Natural Language processing is considered a difficult problem in computer science. It's the nature of the human language that makes NLP difficult. While humans can easily master a language, the ambiguity and imprecise characteristics of the natural languages are what make NLP difficult for machines to implement.\n\n- Playing with words is difficult for Neural Networks as we can't feed the Words into the Neural Network, so there is a need to convert those words into Numbers for Perceptrons to understand","45a54b33":"# Loading the Data","59758d02":"# Casual Testing","76bfc47b":"- Let's verify it.","fce25d1e":"# Fitting the Data\n\n## Constructing a Tokenizer:","5c6bd989":"**What this plot Explains**\n\n- This plot shows the amount of a particular word occuring in headlines.\n\n- For eg: man and new occured the most as there size is the greatest and other words like: trump,one,say,report are next in line.","df27c397":"# NLP : Natural Language Processing \n \n\n- Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\n\nSOURCE: https:\/\/en.wikipedia.org\/wiki\/Natural_language_processing\n\n### RFERENCES:\n    \n    DATA: https:\/\/www.kaggle.com\/rmisra\/news-headlines-dataset-for-sarcasm-detection\n    \n    TUTORIAL : https:\/\/www.coursera.org\/learn\/natural-language-processing-tensorflow\/lecture\/uskDE\/notebook-for-lesson-3"}}