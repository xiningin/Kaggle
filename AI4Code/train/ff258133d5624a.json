{"cell_type":{"ca4398e4":"code","81fd1903":"code","901a4d2d":"code","cea7bf0e":"code","9c4a02bb":"code","f3211a4d":"code","12d05563":"code","b3d3b91b":"code","7a1b0d2a":"code","a14143b3":"code","bd828a48":"code","58ff1d1a":"code","11d417a3":"code","56a789cf":"code","4d86211c":"code","25242eab":"code","095c23d2":"code","a8d98428":"code","d498dec0":"code","8caadd24":"code","1856413f":"code","05e38701":"code","1c73a578":"code","57fe1bc6":"code","9f21f567":"code","f05be363":"code","fd3650ff":"code","dda34c73":"code","16ce5b2d":"code","86f307b1":"code","86c72643":"code","55c9bef6":"code","50f576ae":"code","a4707c27":"code","30a97b10":"code","2503f00b":"code","5e2a3fb2":"code","52f1f40e":"markdown","465125c6":"markdown","66eba492":"markdown","4b4cc7b4":"markdown","653ee6d5":"markdown","2c87653a":"markdown","a4476943":"markdown","6eb643da":"markdown","333ad57b":"markdown","94184892":"markdown","c08dcb10":"markdown","ea1ad9ed":"markdown","a6730511":"markdown","9dd67404":"markdown","b5d165eb":"markdown","b3a93521":"markdown","7c106699":"markdown","0e56421c":"markdown","fb9cb704":"markdown","9055b072":"markdown","bf68d13b":"markdown","385a6e09":"markdown","2e4ec98c":"markdown","53250d48":"markdown","0f609010":"markdown","c7698df4":"markdown","aaad98b1":"markdown","cdf1573a":"markdown","7577d9e3":"markdown","cba82c64":"markdown","99015e54":"markdown","e8959239":"markdown","944e0464":"markdown","98277f88":"markdown","9789bde1":"markdown","e3d40875":"markdown","695a8644":"markdown","1771b9d4":"markdown","03f825e9":"markdown","319580f2":"markdown","7255f215":"markdown","47e1fc35":"markdown","78de370e":"markdown","e878459b":"markdown","3ff8b34c":"markdown","4ad69716":"markdown","f8ae1df4":"markdown","99628f56":"markdown","5f49b549":"markdown"},"source":{"ca4398e4":"# packages we will be using\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model, metrics, model_selection\nimport numpy as np\nimport pandas as pd","81fd1903":"num_hours_studied = np.array([1, 3, 3, 4, 5, 6, 7, 7, 8, 8, 10])\nexam_score = np.array([18, 26, 31, 40, 55, 62, 71, 70, 75, 85, 97])\nplt.scatter(num_hours_studied, exam_score)\nplt.xlabel('num_hours_studied')\nplt.ylabel('exam_score')\nplt.show()","901a4d2d":"# Fit the model\nexam_model = linear_model.LinearRegression(normalize=True)\nx = np.expand_dims(num_hours_studied, 1)\ny = exam_score\nexam_model.fit(x, y)\na = exam_model.coef_\nb = exam_model.intercept_\nprint(exam_model.coef_)\nprint(exam_model.intercept_)","cea7bf0e":"# Visualize the results\nplt.scatter(num_hours_studied, exam_score)\nx = np.linspace(0, 10)\ny = a*x + b\nplt.plot(x, y, 'r')\nplt.xlabel('num_hours_studied')\nplt.ylabel('exam_score')\nplt.show()","9c4a02bb":"\"\"\"\nTask: Load the data with pandas\n\"\"\"\nfile_name = '..\/input\/prostate.csv'\ndata = pd.read_csv(file_name, sep='\\t', index_col=0)","f3211a4d":"assert len(data.columns) == 10\nassert len(data) == 97\nfor column_name in ['lcavol', 'lweight', 'age', 'lbph', 'svi', \n                    'lcp', 'gleason', 'pgg45', 'lpsa', 'train']:\n    assert column_name in data.columns\nprint('Success!')","12d05563":"data.head()","b3d3b91b":"# function to help us plot\ndef scatter(_data, x_name):\n    plt.scatter(_data[x_name], _data['lpsa'])\n    plt.xlabel(x_name)\n    plt.ylabel('lpsa')\n    plt.show()\n\nscatter(data, 'pgg45')","7a1b0d2a":"\"\"\"\nTask: Explore the relationships between the response and the other features\n\"\"\"\n","a14143b3":"\"\"\"\nTask: Split the data into train and test\n\"\"\"\ntrain = data[data['train'] == 'T'].drop(['train'], axis=1)\ntest = data[data['train'] == 'F'].drop(['train'], axis=1)","bd828a48":"assert len(train) == 67\nassert len(test) == 30\nassert 'train' not in train.columns\nassert 'train' not in test.columns\nassert len(train.columns) == 9\nassert len(test.columns) == 9\nprint('Success!')","58ff1d1a":"train.columns != 'lpsa'","11d417a3":"x_train = train.loc[:, train.columns != 'lpsa']\ny_train = train['lpsa']\nx_test = test.loc[:, test.columns != 'lpsa']\ny_test = test['lpsa']","56a789cf":"assert len(x_train.columns) == 8\nassert len(x_test.columns) == 8\nassert len(y_train) == 67\nassert len(y_test) == 30\nprint('Success!')","4d86211c":"model = linear_model.LinearRegression(normalize=True)\nmodel.fit(x_train, y_train)","25242eab":"train_pred = model.predict(x_train)\nmse_train = metrics.mean_squared_error(y_train, train_pred)\nprint(mse_train)","095c23d2":"np.sqrt(mse_train)","a8d98428":"print('lpsa min: %s' % data['lpsa'].min())\nprint('lpsa max: %s' % data['lpsa'].max())","d498dec0":"# get predictions on test set\ntest_pred = model.predict(x_test)\ntest_mse = metrics.mean_squared_error(y_test, test_pred)\nprint('test MSE: %s' % test_mse)\nprint('test RMSE: %s' % np.sqrt(test_mse))","8caadd24":"# this cell tells you what all the feature names are for convenience\nx_train.columns","1856413f":"# you can use this cell to visualize here for convenience\nscatter(data, 'lcavol')","05e38701":"wanted_features = ['lcavol', 'lweight']\nselect_features = [column in wanted_features for column in data.columns]\nx_train_fs = x_train.loc[:, select_features]\nx_train_fs.head()","1c73a578":"model_fs = linear_model.LinearRegression(normalize=True)\nmodel_fs.fit(x_train_fs, y_train)\ntrain_preds_fs = model_fs.predict(x_train_fs)\nmetrics.mean_squared_error(y_train, train_preds_fs)","57fe1bc6":"x_test_fs = x_test.loc[:, select_features]\ntest_preds_fs = model_fs.predict(x_test_fs)\nmetrics.mean_squared_error(y_test, test_preds_fs)","9f21f567":"def scorer(model, X, y):\n    preds = model.predict(X)\n    return metrics.mean_squared_error(y, preds)","f05be363":"alphas = np.linspace(start=0, stop=0.5, num=11)\nalphas","fd3650ff":"\"\"\"\nTask: Perform 10-fold cross validation on all values of alpha and save the mses.\n\"\"\"\nmses = []\nfor alpha in alphas:\n    ridge = linear_model.Ridge(alpha=alpha, normalize=True)\n    mse = model_selection.cross_val_score(ridge, x_train, y_train, cv=10, scoring=scorer)\n    mses.append(mse.mean())","dda34c73":"assert len(mses) == 11             # must have the same number of scores as we have alpha values\nassert isinstance(mses[0], float)  # i.e. not an array, i.e. mean() has been called\nprint('Success!')","16ce5b2d":"plt.plot(alphas, mses)\nplt.xlabel('alpha')\nplt.ylabel('mse')\nplt.show()","86f307b1":"best_alpha = alphas[np.argmin(mses)]\nbest_alpha","86c72643":"ridge = linear_model.Ridge(alpha=best_alpha, normalize=True)\nridge.fit(x_train, y_train)\ntrain_preds = ridge.predict(x_train)\ntest_preds = ridge.predict(x_test)\ntrain_mse = metrics.mean_squared_error(y_train, train_preds)\ntest_mse = metrics.mean_squared_error(y_test, test_preds)\nprint('Train MSE: %s' % train_mse)\nprint('Test MSE: %s' % test_mse)","55c9bef6":"for i in range(0, len(train.columns) - 1):\n    print('Coefficient for %s:%s\\t%s' %\n          (train.columns[i], \n           '\\t' if len(train.columns[i]) < 7 else '',\n           ridge.coef_[i]))","50f576ae":"\"\"\"\nTask: See if you can get a lower test MSE with ridge regression and a subset of features\n\"\"\"","a4707c27":"# step 1: feature selection\nwanted_features = ['lcavol', 'lweight', 'age', 'lbph', 'svi', \n                   'lcp', 'gleason', 'pgg45']  # CHANGE THIS!!!\nselect_features = [column in wanted_features for column in data.columns]\nx_train_fs = x_train.loc[:, select_features]","30a97b10":"# first fit that without ridge regression and check test set MSE\nno_rr_model = linear_model.LinearRegression(normalize=True)\nno_rr_model.fit(x_train_fs, y_train)\nx_test_fs = x_test.loc[:, select_features]\nno_rr_test_preds = no_rr_model.predict(x_test_fs)\nmetrics.mean_squared_error(y_test, no_rr_test_preds)","2503f00b":"# now add ridge regression - first find the best alpha\nmses = []\nalphas = np.linspace(start=0, stop=0.5, num=11)  # YOU CAN ALSO CHANGE THIS!\nfor alpha in alphas:\n    ridge = linear_model.Ridge(alpha=alpha, normalize=True)\n    mse = model_selection.cross_val_score(ridge, x_train_fs, y_train, cv=10, scoring=scorer)\n    mses.append(mse.mean())\nbest_alpha = alphas[np.argmin(mses)]\nprint(best_alpha)\nprint(min(mses))","5e2a3fb2":"# now train with ridge regression and evaluate on test set\nridge = linear_model.Ridge(alpha=best_alpha, normalize=True)\nridge.fit(x_train_fs, y_train)\ntest_preds = ridge.predict(x_test_fs)\nmetrics.mean_squared_error(y_test, test_preds)","52f1f40e":"In statistical learning we have two important measures to trade-off: bias and variance.\n\nSuppose the real function we are trying to model (not directly available to us) is $f$. Our hypothesis gives us an estimate of this function, $\\hat{f} = h_\\theta$. How far $f$ is from $\\hat{f}$ is called **bias**.\n\nBut since we don't have access to $f$ we use some data to make our estimate. But not all datasets will give us a clean view of $f$. There will be noise and incompleteness. This will introduce **variance** into our estimate: the amount to which we miss $f$ due to fitting noise in our dataset.\n\nThe following picture shows various bias-variance regimes, assuming our function is aiming for a bulls-eye:\n- top-left: high bias, low variance\n- top-right: high bias, high variance\n- bottom-left: low bias, low variance\n- bottom-right: low bias, high variance\n\n![bias_and_variance](https:\/\/qph.ec.quoracdn.net\/main-qimg-8871671ae520db34622d740497ca8d19.webp)\n\nThis is why in practice we often don't try to minimize the loss on the training set completely. We try to strike a balance. This is usually controlled in terms of \"model complexity\" which we will discuss shortly.\n\nThe following diagram shows the pattern of interest. As model complexity increases, error on the training set reduces and continues to do so. However after some point error on unseen test data start to increase, giving the characteristic U-shape of the total error. The best model lies at the bottom of that U, where bias and variance are best traded-off.\n\n![bias-variance_trade-off](https:\/\/www.researchgate.net\/profile\/Ljubomir_Jacic2\/post\/How_does_model_complexity_impact_the_bias-variance_tradeoff\/attachment\/59d6233579197b807798188f\/AS%3A306150770184192%401450003439733\/download\/biasvariance.png)\n\nWhen training loss is low relative to testing loss we are **overfitting** to the training data. When training loss is high relative to testing loss we are **underfitting**. Balancing both cases amounts to finding the best trade-off between bias and variance.","465125c6":"However, that is just on the training data. In fact, we don't really care about how well we can fit the training data. We mostly care about how well our model can predict unseen cases. This is traditionally what we use our held out test set for. Let's see how we did.","66eba492":"We haven't had time to cover everything important related to feature selection in this tutorial. For a further reference take a look at the SciKitLearn tutorial on feature selection here http:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html.\n\nFor example, there are some greedy algorithms for automatic feature selection. If you have the computational resources then I guess you could also try every single combination.","4b4cc7b4":"How did it do? Did it beat your feature selection approach?\n\nLet's see what features it selected. We can do this by examining the coefficients.","653ee6d5":"I'm using the shorthand `fs` to indicate this variable is my feature selection data.\n\nLet's see how this randomly chose subset performs","2c87653a":"Now is your turn. We need to define a variable `mses`, loop over the values of alpha in `alphas`, get the vector of mses for each fold using `cross_val_score`, take the mean, and append each average mse to the list.\n\nNote that the mses returned by `cross_val_score` is a vector, a numpy array, so you will need to call `mean()` on it to get the average MSE for all folds.","a4476943":"#### Normal Equations\n\nLinear regression actually has a closed-form solution - the normal equation. It is beyond our scope to show the derivation, but here it is:\n\n$$\\mathbf{a}^* = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{y}$$\n\nWe won't be implementing this equation, but you should know this is what `sklearn.linear_model.LinearRegression` is doing under the hood. We will talk more about optimization in later tutorials, where we have no closed-form solution.\n\nSometimes you might encounter an error that complains you have a singular design matrix. This usually occurs when you have redundant features. For example you might have two features for `distance`: one in metres, another in kilometres. These columns will therefore be linearly dependent - and the matrix therefore singular. In these cases you can simply remove the redundant feature column.","6eb643da":"We can see we have a boolean vector. This tells us which columns to select. They are ordered, so we can see that `'lpsa'` is the last column, where the `False` appears. We can confirm this by looking at the `data.head()` output above.\n\nNow you have the tools you need to complete the next task. The variables you need to create are\n- `x_train`\n- `y_train`\n- `x_test`\n- `y_test`","333ad57b":"The data comes in `.csv` format. We will use the `pandas` library (which we imported as `pd`) to load and manage the data. It is a very convenient library for handling data and conducting machine learning experiments.\n\nYour first task is to use the `pd.read_csv()` function to load the data. We have given you the file name to use as `file_name`. You can refer to the documentation here (https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.read_csv.html) for details on all the options. Pay particular attention to `sep` and `index_col` (you only need these two). This data is separated by the tab character `'\\t'` and the index column is the first one (remember python uses zero-based indexing). Take a look at the raw data (just load the file in a text editor) to see what you are doing and get a feel for this important step.\n\nMake sure you load the data into a variable named `data`. The following cell will help confirm you have completed this first step successfully.\n\nIf you want to see what you have loaded type `data.head()` as the bottom line in the cell to debug.","94184892":"That's a fair but worse than 0.4392 that we achieved with all the features.\n\nBut, let's check out the test set performance","c08dcb10":"#### Univariate Example\n\nLet's now see what our univariate example looks like","ea1ad9ed":"Next we need to prepare the design matrix for the learning algorithm, and the response vector. \n\nThe response vector can be recovered with simple selection (like we saw above in visualization - i.e. `data[column_name]`). \n\nA nice syntax for selecting all columns except one from a `DataFrame` is\n\n```\ndata.loc[:, data.columns != column_name]\n```\n\nIn pandas `loc` allows boolean indexing of rows and columns. So the above code can be read as \"select all rows and all columns except `column_name`. The `:` symbol means all. The first position inside the square brackets is the rows (axis 0) - so all rows. In the second position, columns (axis 1), we have something more going on. Let's investigate that first","a6730511":"# Linear Regression Tutorial","9dd67404":"## Practice #2\n\nThe goal of this practice is to let you use your own intuition to reduce the test set MSE by implementing regularized linear regression. The goals are as follows\n\n1. Implement regularization via feature selection\n2. Implement regularization by penalizing large parameter values (ridge regression)","b5d165eb":"### Visualize the Data\n\nIn order to assess the suitability of linear regression for each of the features, we should always try and plot the relationships in the data.\n\nScatter plots are useful. We can make one for each feature. We will use `matplotlib` to do this. The first one we will do for you, then you should visualize the rest, exploring the relationships in the data.","b3a93521":"As often happens in machine learning, our test error is worse than our training error. Our next job is to improve this.","7c106699":"Now, let's try it out on the full training set and get a final test set MSE.","0e56421c":"### Load the Data","fb9cb704":"We can see we have our eight feature columns and the response `lpsa` as well. We also have another column called `train` which is a boolean column. This tells us which data points to include in our training set, and which to hold our for testing.\n\nWe will discuss training versus testing further below.","9055b072":"We need to define a reasonable range of values for alpha to search over. We can use `np.linspace` to get a vector of evenly spaced numbers over an interval. Reference here https:\/\/docs.scipy.org\/doc\/numpy-1.14.0\/reference\/generated\/numpy.linspace.html. Let's try $(0, 3)$. We'll also choose the arguments so we get nice clean decimals.","bf68d13b":"The line fits pretty well using the eye, as it should, because the true function is linear and the data has just a little noise.\n\nBut we need a mathematical way to define a good fit in order to find the optimal parameters for our hypothesis.","385a6e09":"### Fit the Data","2e4ec98c":"### Normalization\n\nIt is a good idea to normalize all the values in the design matrix. This means all values should be in the range $(0, 1)$ and centered around zero.\n\n![normalization](http:\/\/cs231n.github.io\/assets\/nn2\/prepro1.jpeg)\n\n(Image taken from http:\/\/cs231n.github.io\/neural-networks-2\/)\n\nNormalization helps the learning algorithm perform better. A deeper discussion of this is beyond our current scope. Just think of it as a practical trick for now.\n\nFortunately, `sklearn.linear_model.LinearRegression` has an initialization parameter `normalize` that will take care of this for you. Beware that its default is `False` so you will need to set this yourself manually if you want it. Refer to http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html.","53250d48":"Something we haven't explored here is whether using ridge regression with a subset of features could improve the result. Try that out for yourself for a final challenge. Can you get your test MSE lower? You can first try by dropping features to which ridge regression has assigned low weights...","0f609010":"#### Design Matrix\n\nIn general with $n$ data points and $p$ features our design matrix will have $n$ rows and $p$ columns.\n\nReturning to our exam score regression example, let's add one more feature - number of hours slept the night before the exam. If we have 4 data points and 2 features, then our matrix will be of shape $4 \\times 3$ (remember we add a bias column). It might look like\n\n$$\n\\begin{bmatrix}\n    1 & 1 & 8 \\\\\n    1 & 5 & 6 \\\\\n    1 & 7 & 6 \\\\\n    1 & 8 & 4 \\\\\n\\end{bmatrix}\n$$\n\nNotice we do **not** include the response in the design matrix.","c7698df4":"There is a pretty clear linear relationship there. It is far from as clean as our toy example at the start of this tutorial. But this is more realistic data, so there will not be such easy relationships and such clean data.","aaad98b1":"That should show you a very clear u-shape and give you an optimal value for $\\alpha$.","cdf1573a":"### What is a Good Fit?\n\nTypically we use \"mean squared error\" to measure the goodness of fit in a regression problem.\n\n$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y^{(i)} - h_\\theta^{(i)})^2$$\n\nYou can see that this is measuring how far away each of the real data points are from our predicted point which makes good sense. Here is a visualization\n\n![MSE](http:\/\/www.statisticshowto.com\/wp-content\/uploads\/2015\/03\/residual.png)\n\n(Image taken from http:\/\/www.statisticshowto.com\/rmse\/)\n\nThis function is then taken to be our \"loss\" function - a measure of how badly we are doing. In general we want to minimize this.","7577d9e3":"### Dataset\n\nThe data for this example come from a study by Stamey et al. (1989). They examined the correlation between the level of prostate-specific antigen and a number of clinical measures in men who were about to receive a radical prostatectomy. The variables are \n- log cancer volume ( lcavol )\n- log prostate weight ( lweight )\n- age\n- log of the amount of benign prostatic hyperplasia ( lbph )\n- seminal vesicle invasion ( svi )\n- log of capsular penetration ( lcp )\n- Gleason score ( gleason )\n- percent of Gleason scores 4 or 5 ( pgg45 )\n\n(Taken from Elements of Statistical Learning, Hastie and Tibshirani).\n\nSo we have eight features (our $\\mathbf{X}$) to predict our response $y$, the level of prostate-specific antigen (`lpsa`).","cba82c64":"#### Assess the Goodness of Fit\n\nLet's now assess the performance of this fit using mean squared error. `sklearn` makes this easy to calculate, providing the `sklearn.metric.mean_squared_error` function - reference here http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.mean_squared_error.html.","99015e54":"## Practice #1\n\nIn this first practice our goals are:\n1. Load the data\n2. Visualize the relationships in the data\n3. Prepare the data for the learning algorithm\n4. Fit the data","e8959239":"### Feature Selection\n\nYou may wish to return to the place above where you were using your eye to examine the correlations between each feature and the response. Your goal is to select the optimal subset of features to reduce the **test set error**.\n\nYou can get a boolean vector of column selectors with the following syntax\n\n```\n[column in ['wanted_column_1', ..., 'wanted_column_n'] for column in data.columns]\n```\n\nFor example, if I only wanted lcavol and lweight I would use","944e0464":"#### Optimization Problem\n\nThe typical recipe for machine learning algorithms is to define a loss function of the parameters of a hypothesis, then to minimize the loss function. In our case we have the optimization problem\n\n$$\\min_{\\mathbf{a}} \\frac{1}{n} \\sum_{i=1}^n (y^{(i)} - \\mathbf{a}^\\top\\mathbf{X}^{(i)})^2$$\n\nNote that we have added the bias into the design matrix.\n\nThis happens to be a convex function. So the \"loss surface\" (i.e. the map of the loss values over all the parameter values) has a nice convex shape like so:\n\n![convex function](http:\/\/theinf2.informatik.uni-jena.de\/theinf2_multimedia\/bilder\/lectures\/convex-width-244-height-261.jpg)\n\n(Image taken from http:\/\/theinf2.informatik.uni-jena.de\/Lectures\/Convex+Optimization.html)\n\nThis has the nice property that regardless of where you are on the loss surface you always know how to move to reach the global minimum.","98277f88":"We can see the this is nearly a straight line. We suspect with such a high linear correlation that linear regression will be a successful technique for this task.\n\nWe will now build a linear model to fit this data.","9789bde1":"Let's now take a look at the data.","e3d40875":"### How to learn the parameters?","695a8644":"#### Learn the Parameters\n\nFitting the model really is the easy part, as far as coding is concerned. Data preparation is usually the most time consuming.\n\nThere is an example of how to fit the model at the top of the tutorial already. This is your next task.\n\nYou need to create a new instance of `sklearn.linear_model.LinearRegression` and use the `fit()` function to fit it to `x_train` using `y_train`. The documentation is here: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html.\n\n(Note that when you choose `normalize=True` the argument `fit_intercept` is ignored, since we have centered the data around zero)","1771b9d4":"### Ridge Regression\n\nThis time we will implement feature selection via ridge regression. To do this we can use the class `sklearn.linear_model.Ridge`. The reference is here at 1.1.2: http:\/\/scikit-learn.org\/stable\/modules\/linear_model.html. We will throw all features in, let ridge regression do selection, and see how our own feature sets compare.\n\nNotice we have an additional hyperparameter, $\\alpha$. This controls how heavy the penalty is for larger weights. How do we select the best value for this hyperparameter?\n\n#### Cross Validation\n\nTo perform hyperparameter tuning we can use a technique called **cross validation**. We take our test set, randomly divide into $k$ training and testing sets. Differnent samples will be randomly placed in each set, each with their own variance. We then average the results over the folds to estimate how well we will do on unseen test data. After splitting the data into $k$ folds (10 is a usual choice for $k$), we will try out different hyperparameter values and select the best one.\n\n![cross_validation](https:\/\/www.researchgate.net\/profile\/Kiret_Dhindsa\/publication\/323969239\/figure\/fig10\/AS:607404244873216@1521827865007\/The-K-fold-cross-validation-scheme-133-Each-of-the-K-partitions-is-used-as-a-test.ppm)\n\nNote that we **do not use the test set to choose hyperparameter values**. This would not give us a fair estimate of how well we do un unseen data. We use the training set to create the 10-folds.\n\nWe want to try out different values of $\\alpha$, get cross-validated MSE values, and then use those values to choose the best $\\alpha$ for our model. \n\nMuch of this process is already wrapped in `sklearn.model_selection.cross_val_score` for us. We just need to define a scoring function which we will do for you here","03f825e9":"You might be wondering, how good of a fit is this in absolute terms? How should I interpret this value?\n\nIf we take the square root of the MSE we get a value that relates directly to our response, $y$. So we can make a judgment in terms of its accuracy with respect to our quantity of interest.","319580f2":"### Prepare the Data\n\nWe noticed when we loaded the data that we had an extra column indicating whether the data point should be used to train the model parameters, or whether it should be held out for testing.\n\nWe now need to separate our data into train and test sets. This is your next task.\n\nYou will need to use the pandas selection syntax which for an \"equals\" relation is:\n\n```\nnew_data = data[data['column_name'] == desired_value]\n```\n\nYou must create two new variables, `train` and `test` including the correct data points. \n\nWe also don't want to keep those columns and mess up our design matrix, so we will use `data.drop()` to do that. Refer here (https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.drop.html) for reference on this function. Note that we need to pass a list of column names to drop, and specify the axis (where `1` is the column axis) like\n\n```\nnew_data = data.drop(['unwanted_column'], axis=1)\n```\n\nThe following cell will validate your work.","7255f215":"We can use `pandas` to get a quick and convenient range for our response.","47e1fc35":"We will now plot alpha versus MSE and make a choice of our value for $\\alpha$.","78de370e":"That is indeed lower than our previous test MSE of 0.5212. We have traded off some bias for more variance and improved our performance.\n\nThat was just arbitrarily selecting the first two features from the list. I'm sure you can do better!","e878459b":"## The Bias-Variance Trade-Off","3ff8b34c":"## What is Linear Regression?\n\nFinding a straight line of best fit through the data. This works well when the true underlying function is linear.\n\n### Example\n\nWe use features $\\mathbf{x}$ to predict a \"response\" $y$. For example we might want to regress `num_hours_studied` onto `exam_score` - in other words we predict exam score from number of hours studied.\n\nLet's generate some example data for this case and examine the relationship between $\\mathbf{x}$ and $y$.","4ad69716":"Use the cell below (or add more) to play with our `scatter` function and investigate the relationships between the response and the other features.","f8ae1df4":"### Linear Model\n\n#### Hypothesis\n\nA linear model makes a \"hypothesis\" about the true nature of the underlying function - that it is linear. We express this hypothesis in the univariate case as\n\n$$h_\\theta(x) = ax + b$$\n\nOur simple example above was an example of \"univariate regression\" - i.e. just one variable (or \"feature\") - number of hours studied. Below we will have more than one feature (\"multivariate regression\") which is given by\n\n$$h_\\theta(\\mathbf{x}) = \\mathbf{a}^\\top \\mathbf{X}$$\n\nHere $\\mathbf{a}$ is a vector of learned parameters, and $\\mathbf{X}$ is the \"design matrix\" with all the data points. In this formulation the intercept term has been added to the design matrix as the first column (of all ones).","99628f56":"So we can say that the range of our response is $(-0.43, 5.58)$ and our mean error is $0.66$.","5f49b549":"## Controlling Model Complexity for Linear Regression\n\nOne direct measure of model complexity is simple: how many features you are using. In our model above we used **all** the features. But maybe some of them aren't *really* associated with the response. Maybe their small correlations represent noise and not the real function $f$.\n\nAnother way to control model complexity is to limit the size of the coefficients. We can do this by adding a **penalty term** to our loss function that increases loss for larger parameter values. Our optimization objective then becomes\n\n$$\\min_{\\mathbf{a}} (y - h_\\theta(x))^2 + \\alpha \\lVert \\mathbf{a} \\rVert^2$$\n\nSo the larger the norm of our parameter vector grows, the greater the loss, controlled by an extra hyperparameter $\\alpha$. This is technically called **ridge regression**.\n\nTo get an intuitive feel for how smaller parameters equates to reducing model complexity, consider what happens to a hypothesis with three features when you reduce one of the coefficients say $w_3$) to nearly zero\n\n$$h_\\theta = w_1x_1 + w_2x_2 + w_3x_3 \\approx w_1x_1 + w_2x_2$$\n\nWe are essentially performing feature selection here. Looking at it another way, telling the model that it only has a limited parameter budget, we force it to spend that budget on the most predictive combination of features."}}