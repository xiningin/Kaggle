{"cell_type":{"b1d0745e":"code","1270ba3c":"code","37e6c98a":"code","d5c602d9":"code","fd930cd7":"code","261015f7":"code","5197aa36":"code","c6af01ef":"code","cae3eb4d":"code","4916efc3":"code","5bc1ca76":"code","eac21961":"code","e97d87e0":"code","676a0281":"code","37a1aad2":"code","568aecd2":"code","949c7f92":"code","ae63ddcc":"code","78cc824a":"code","ce0f7985":"code","d15551ae":"code","9a55443b":"code","9d5c65ad":"code","cf9ccb2f":"code","27446ad2":"markdown","41600032":"markdown","387a9b4b":"markdown","ebf40901":"markdown","05f2bf1f":"markdown","3df4a652":"markdown"},"source":{"b1d0745e":"import os\nimport gc\nimport re\nimport nltk\nimport json\nimport spacy\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom scipy import spatial\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom gensim.models.word2vec import Word2Vec\nfrom scipy.spatial.distance import jensenshannon\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n\ntqdm.pandas()\n# nltk.download('stopwords')\nwarnings.filterwarnings(\"ignore\")","1270ba3c":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","37e6c98a":"BASEDIR = '\/kaggle\/input\/CORD-19-research-challenge\/'","d5c602d9":"#1.read csv\ndf = pd.read_csv(BASEDIR+'metadata.csv')","fd930cd7":"JSON_DIR = BASEDIR + 'document_parses\/'","261015f7":"#2.find path\n# path_list_pdf = ['comm_use_subset\/comm_use_subset\/pdf_json\/',\n#              'noncomm_use_subset\/noncomm_use_subset\/pdf_json\/',\n#              'custom_license\/custom_license\/pdf_json\/',\n#              'biorxiv_medrxiv\/biorxiv_medrxiv\/pdf_json\/',\n#              'arxiv\/arxiv\/pdf_json\/']\n# path_list_pmc = ['comm_use_subset\/comm_use_subset\/pmc_json\/',\n#              'noncomm_use_subset\/noncomm_use_subset\/pmc_json\/',\n#              'custom_license\/custom_license\/pmc_json\/',\n#              'biorxiv_medrxiv\/biorxiv_medrxiv\/pmc_json\/']\n\n# def find_path_sha(filename):\n#     for i in path_list_pdf:\n#         path = BASEDIR+i+'{}.json'.format(filename)\n#         if os.path.exists(path):\n#             return path\n#     return None\n\n# def find_path_pmc(filename):\n#     for i in path_list_pmc:\n#         path = BASEDIR+i+'{}.xml.json'.format(filename)\n#         if os.path.exists(path):\n#             return path\n#     return None\ndef find_path_sha(filename):\n    path = JSON_DIR+'pdf_json\/'+'{}.json'.format(filename)\n    if os.path.exists(path):\n        return path\n    return None\n\ndef find_path_pmc(filename):\n    path = JSON_DIR+'pmc_json\/'+'{}.xml.json'.format(filename)\n    if os.path.exists(path):\n        return path\n    return None\n","5197aa36":"#3.delete rows without json file\ndf['sha'] = df['sha'].fillna('')\ndf['pmcid'] = df['pmcid'].fillna('')\ndf['file_path'] = df['sha'].progress_apply(find_path_sha)\ndf['file_path_2'] = df['pmcid'].progress_apply(find_path_pmc)\ndf['file_path'] = df['file_path'].fillna(df['file_path_2'])\ndf.drop('file_path_2',axis=1,inplace=True)\ndf.dropna(subset=['file_path'],inplace=True)\ndf.index = range(0,len(df))","c6af01ef":"#4.get year\ndf['year'] = df['publish_time'].map(lambda x: int(re.findall(r'\\d{4}',x)[0]))","cae3eb4d":"#5.add details from json files to csv\n\n# define stopwords\nnlp = spacy.load('en_core_web_sm')\nstopword_list = list(spacy.lang.en.stop_words.STOP_WORDS)\n\n#add the reference's titles to df\ndef read_ref(path):\n    with open(path,'r') as f:\n        jsonfile = json.load(f)\n    reference = ''\n    for i in jsonfile['bib_entries'].keys():\n        try:\n            reference += jsonfile['bib_entries'][i]['title'].lower()\n            reference += '|'\n        except:\n            pass\n    reference = reference.split(' ')\n    reference = [i for i in reference if not i in stopword_list]\n    reference = ' '.join(reference)\n    return reference\n\n\n#add the authors' country to df\ndef read_country(path):\n    with open(path,'r') as f:\n        jsonfile = json.load(f)\n    try:\n        return jsonfile['metadata']['authors'][0]['affiliation']['location']['country'].lower()\n    except:\n        return None\n\n    \n#add contents from json to df\ndef read_content(path):\n    with open(path,'r') as f:\n        jsonfile = json.load(f)\n    try:\n        text = ''\n        for i in jsonfile['body_text']:\n            text = text + (i['text'].lower()) + '\\n\\n'\n        text = text.split(' ')\n        text = [i for i in text if not i in stopword_list]\n        text = ' '.join(text)\n        text = text.strip()\n        return text\n    except:\n        return None","4916efc3":"df['country']    = df['file_path'].progress_apply(read_country)\ndf['ref_titles'] = df['file_path'].progress_apply(read_ref)\ndf['content']    = df['file_path'].progress_apply(read_content)","5bc1ca76":"#6.remove irregular words\nfor i in ['title', 'abstract' , 'ref_titles' , 'content']:\n    df[i] = df[i].fillna('')\n    df[i] = df[i].str.lower()\n    df[i] = df[i].progress_apply(lambda x : re.sub(r\"[^a-z0-9\\-\\' ]\",' ',x))\n    df[i] = df[i].progress_apply(lambda x : ' '.join([i for i in x.split() if not i in stopword_list]))\n    df[i] = df[i].progress_apply(lambda x : ' '.join([i for i in x.split() if len(i)>2]))","eac21961":"#7.merge text\nfor i in ['title', 'abstract' , 'ref_titles']:\n    df['content'] = df['content'] + ' ' + df[i]","e97d87e0":"#1.create Vectorizer\nc = CountVectorizer()\nvectorizer = c.fit_transform(df['content'])","676a0281":"#2.create lda model\nlda = LatentDirichletAllocation(n_components=50, random_state=0)\ndf_topic = lda.fit_transform(vectorizer)","37a1aad2":"#3.find related articles\ndef finding_related_articles_lda(keyword,df,fromYear,toYear,topn):\n    df = df.loc[(df['year']>=fromYear) & (df['year']<=toYear)]\n    search_vec = c.transform([keyword])\n    search_topic = lda.transform(search_vec)\n    df['distances'] = pd.DataFrame(df_topic).iloc[df.index].apply(lambda x: jensenshannon(x, search_topic[0]), axis=1)\n    return df.sort_values('distances').iloc[0:topn,:]","568aecd2":"finding_related_articles_lda('What do we know about non-pharmaceutical interventions',df,2019,2020,10)","949c7f92":"#1.create w2v_model\nw2v_model = Word2Vec(df['content'].apply(lambda x : x.split()),min_count=1)","ae63ddcc":"#2.find related articles\ndef finding_related_articles_w2v(keyword,df,fromYear,toYear,topn,keynum):\n    df = df.loc[(df['year']>=fromYear) & (df['year']<=toYear)]\n    \n    \n    keyword_filtered = keyword.split('')\n    keyword_filtered = [i for i in keyword_filtered if not i in stopword_list]\n    keyword_filtered = w2v_model.most_similar(positive=keyword_filtered,topn=keynum)\n    keyword_filtered = list(np.array(keyword_filtered)[:,0])\n    \n    def containKey(string):\n        string = string.split()\n        for i in keyword_filtered:\n            if i in string:\n                return True\n        return False\n    df['hasKey'] = df['content'].progress_apply(containKey)\n    \n    df = df[df['hasKey'] == True]\n    df = df.drop('hasKey',axis=1)\n    \n    df['distances'] = df['content'].progress_apply(lambda x : w2v_model.wv.wmdistance(keyword.split(),str(x).split()))\n    return df.sort_values('distances').iloc[0:topn,:]","78cc824a":"finding_related_articles_w2v('What do we know about non-pharmaceutical interventions',df,2019,2020,10)","ce0f7985":"#create TFIDF Vectorizer\nT = TfidfVectorizer()\ntdidf = T.fit_transform(df['content'])\n\n#dict{number:word}\ntfidf_dict = pd.Series(data=pd.Series(T.vocabulary_).index,index=pd.Series(T.vocabulary_).values)\n\n#dict{word : number}\ntf_idf_dict = pd.Series(T.vocabulary_)\n\n#delete words not in w2v_model\ntf_idf_dict_filtered = tf_idf_dict[w2v_model.wv.index2word]\ntf_idf_dict_filtered.dropna(inplace=True)\ntf_idf_dict_filtered = tf_idf_dict_filtered.astype('int')\n\n#dict{word : number} => dict{number : word}\ntf_idf_dict_filtered = pd.Series(data=tf_idf_dict_filtered.index,index=tf_idf_dict_filtered.data)","d15551ae":"# transfer words to tfidf vector\ndef tfidf_calc(string):\n    tfidf_vec = T.transform([string]).toarray()[0]\n    return tfidf_vec\n\n# transfer tfidf vector to word2vec vector(Weighted average)\ndef vector_calc(tfidf_vec):\n    #remove 0s in vector(words not in content)\n    tfidf_index = np.concatenate(np.argwhere(tfidf_vec))\n    tfidf_index_filtered = tf_idf_dict_filtered[tfidf_index].dropna()\n    \n    #words after filtered\n    tfidf_words = tfidf_index_filtered.values\n    #tfidf values after filtered\n    tfidf_value = tfidf_vec[tfidf_index_filtered.index]\n    \n    #tfidf values * word2vec values (shape=(x,100))\n    mix_array = w2v_model[tfidf_words]*tfidf_value.reshape(-1,1)\n    \n    # mean values of weight average values(shape=(100,)) \n    vector_final = np.mean(mix_array,axis=0)\n    \n    return vector_final\n\n# mix of two function above\ndef stringToArray(string):\n    return list(vector_calc(tfidf_calc(string)))","9a55443b":"#.find related articles\ndef finding_related_articles_w2v_TFIDF(keyword,df,fromYear,toYear,topn):\n    keyword = keyword.lower()\n    keyword = stringToArray(keyword)\n    df = df.loc[(df['year']>=fromYear) & (df['year']<=toYear)]\n    df['vector'] = df['content'].progress_apply(stringToArray)\n    df['distance'] = df['vector'].progress_apply(lambda x : np.abs(spatial.distance.cosine(x, keyword)))\n    return df.sort_values('distances').iloc[0:topn,:]","9d5c65ad":"finding_related_articles_w2v_TFIDF('What do we know about non-pharmaceutical interventions',df,2019,2020,10)","cf9ccb2f":"#11.plot\ndef plot_wc(df_plot):\n    plt.figure(figsize=(10,5))\n    for i in range(6):\n        plt.subplot(2,3,i+1)\n        wc = WordCloud().generate(df_rank['content'].iloc[i])\n        plt.imshow(wc, interpolation='bilinear')\n        plt.title('paper{}'.format(i))\n        plt.axis(\"off\")","27446ad2":"> # 3.plot","41600032":"> # 1. Parse json to csv","387a9b4b":"> # Model training(Word2Vec+tfidf+cosine_distance)","ebf40901":"> # Model training(CountVectorizer+LDA+jensenshannon)","05f2bf1f":"> # 2.Preproccessing","3df4a652":"> # Model training(Word2Vec+wvdistance)"}}