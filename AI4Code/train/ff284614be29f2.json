{"cell_type":{"fee75ec8":"code","284f2507":"code","0f81cbfd":"code","03e934ac":"code","e9e4fa14":"code","e8cf7f93":"code","01dc9478":"code","b3b7f14a":"code","6a5ae3b6":"code","8ddcad02":"code","553c38cc":"code","b842c24f":"code","78264f0f":"code","a1116fd4":"code","dac45b71":"code","58df1210":"code","7f08f4f8":"code","bf8cdde4":"code","02d9d173":"code","1b69f8ba":"code","fabc60ea":"code","9a9b850e":"code","45ddf1e9":"code","eb385506":"code","1b99898f":"code","a8d0aa93":"code","6b0b34e5":"code","205bf18f":"code","be6fe138":"code","dd13d46e":"code","407f212f":"code","5dfe6838":"code","c291f800":"code","a83d3cb0":"code","be0f3e3c":"code","74d53483":"markdown","8781f109":"markdown","54050ba7":"markdown","8b45c07f":"markdown","11292749":"markdown","e90c5aca":"markdown","ae5f0b79":"markdown"},"source":{"fee75ec8":"# load packages\nfrom sklearn import metrics,preprocessing,model_selection\nfrom sklearn.metrics import accuracy_score\nimport keras\nfrom keras.layers import Input, Lambda, Dense\nfrom keras.models import Model\nimport keras.backend as K\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport string\nimport pandas as pd\nimport re\nimport spacy\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\nfrom spacy.lang.en import English\nspacy.load('en')\nparser = English()","284f2507":"# get elmo from tensorflow hub\n\nimport tensorflow_hub as hub\nimport tensorflow as tf\n\nembed = hub.Module(\"https:\/\/tfhub.dev\/google\/elmo\/2\", trainable=True)\n\n# ELMo Embedding\ndef ELMoEmbedding(x):\n    return embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]","0f81cbfd":"# Stop words and special characters \nSTOPLIST = set(stopwords.words('english') + list(ENGLISH_STOP_WORDS)) \nSYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-\", \"...\", \"\u201d\", \"\u201d\",\"''\"]","03e934ac":"# Data Cleaner and tokenizer\ndef tokenizeText(text):\n    \n    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n    text = text.lower()\n    \n    tokens = parser(text)\n    \n    # lemmatization\n    lemmas = []\n    for tok in tokens:\n        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n    tokens = lemmas\n    \n    # reomve stop words and special charaters\n    tokens = [tok for tok in tokens if tok.lower() not in STOPLIST]\n    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n    \n    tokens = [tok for tok in tokens if len(tok) >= 3]\n    \n    # remove remaining tokens that are not alphabetic\n    tokens = [tok for tok in tokens if tok.isalpha()]\n    \n    tokens = list(set(tokens))\n    \n    return ' '.join(tokens[:])","e9e4fa14":"def encode(le_enc, labels):\n    enc = le_enc.transform(labels)\n    return keras.utils.to_categorical(enc)\n\ndef decode(le_enc, one_hot):\n    dec = np.argmax(one_hot, axis=1)\n    return le_enc.inverse_transform(dec)","e8cf7f93":"# load the dataset\ntrainDF_Sheet_1 = pd.read_csv('..\/input\/Sheet_1.csv',usecols=['response_id','class','response_text'],encoding='latin-1')","01dc9478":"trainDF_Sheet_1.head(10)","b3b7f14a":"trainDF_Sheet_1.shape","6a5ae3b6":"trainDF_Sheet_1['class'].unique()","8ddcad02":"trainDF_Sheet_1['class'].value_counts()","553c38cc":"sns.set(rc={'figure.figsize':(8,8)})\nsns.countplot(trainDF_Sheet_1['class'])","b842c24f":"# Data cleaning\ntrainDF_Sheet_1['response_text'] = trainDF_Sheet_1['response_text'].apply(lambda x:tokenizeText(x))","78264f0f":"# Data preparation\nX = trainDF_Sheet_1['response_text'].tolist()\ny = trainDF_Sheet_1['class'].tolist()\n\n# Lebel encoding\nle_enc = preprocessing.LabelEncoder()\nle_enc.fit(y)\n\ny_en = encode(le_enc, y)","a1116fd4":"# split the dataset into training and testing datasets\nx_train, x_test, y_train, y_test = model_selection.train_test_split(np.asarray(X), np.asarray(y_en), test_size=0.2, random_state=42)","dac45b71":"x_train.shape","58df1210":"# Build Model\ninput_text = Input(shape=(1,), dtype=tf.string)\nembedding = Lambda(ELMoEmbedding, output_shape=(1024, ))(input_text)\ndense = Dense(256, activation='relu')(embedding)\npred = Dense(2, activation='softmax')(dense)\nmodel = Model(inputs=[input_text], outputs=pred)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nwith tf.Session() as session:\n    K.set_session(session)\n    session.run(tf.global_variables_initializer())  \n    session.run(tf.tables_initializer())\n    history = model.fit(x_train, y_train, epochs=20, batch_size=16)\n    model.save_weights('.\/response-elmo-model.h5')\n\nwith tf.Session() as session:\n    K.set_session(session)\n    session.run(tf.global_variables_initializer())\n    session.run(tf.tables_initializer())\n    model.load_weights('.\/response-elmo-model.h5')  \n    predicts = model.predict(x_test, batch_size=16)","7f08f4f8":"# decode test labels\ny_test = decode(le_enc, y_test)\n# decode predicted labels\ny_preds = decode(le_enc, predicts)","bf8cdde4":"print(metrics.confusion_matrix(y_test, y_preds))","02d9d173":"print(metrics.classification_report(y_test, y_preds))","1b69f8ba":"print(\"Accuracy of ELMO is:\",accuracy_score(y_test,y_preds))","fabc60ea":"# load the dataset\ntrainDF_Sheet_2 = pd.read_csv('..\/input\/Sheet_2.csv',encoding='latin-1')","9a9b850e":"trainDF_Sheet_2.head(10)","45ddf1e9":"trainDF_Sheet_2.shape","eb385506":"trainDF_Sheet_1['class'].unique()","1b99898f":"trainDF_Sheet_2['class'].value_counts()","a8d0aa93":"sns.countplot(trainDF_Sheet_2['class'])","6b0b34e5":"# Data cleaning\ntrainDF_Sheet_2['resume_text'] = trainDF_Sheet_2['resume_text'].apply(lambda x:tokenizeText(x))","205bf18f":"# Data preparation\nX = trainDF_Sheet_2['resume_text'].tolist()\ny = trainDF_Sheet_2['class'].tolist()\n\n# Lebel encoding\nle_enc = preprocessing.LabelEncoder()\nle_enc.fit(y)\n\ny_en = encode(le_enc, y)","be6fe138":"# split the dataset into training and testing datasets\nx_train, x_test, y_train, y_test = model_selection.train_test_split(np.asarray(X), np.asarray(y_en), test_size=0.2, random_state=42)","dd13d46e":"x_train.shape","407f212f":"# Build Model\ninput_text = Input(shape=(1,), dtype=tf.string)\nembedding = Lambda(ELMoEmbedding, output_shape=(1024, ))(input_text)\ndense = Dense(256, activation='relu')(embedding)\npred = Dense(2, activation='softmax')(dense)\nmodel = Model(inputs=[input_text], outputs=pred)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nwith tf.Session() as session:\n    K.set_session(session)\n    session.run(tf.global_variables_initializer())  \n    session.run(tf.tables_initializer())\n    history = model.fit(x_train, y_train, epochs=20, batch_size=16)\n    model.save_weights('.\/resume-elmo-model.h5')\n\nwith tf.Session() as session:\n    K.set_session(session)\n    session.run(tf.global_variables_initializer())\n    session.run(tf.tables_initializer())\n    model.load_weights('.\/resume-elmo-model.h5')  \n    predicts = model.predict(x_test, batch_size=16)","5dfe6838":"# decode test labels\ny_test = decode(le_enc, y_test)\n# decode predicted labels\ny_preds = decode(le_enc, predicts)","c291f800":"print(metrics.confusion_matrix(y_test, y_preds))","a83d3cb0":"print(metrics.classification_report(y_test, y_preds))","be0f3e3c":"print(\"Accuracy of ELMO is:\",accuracy_score(y_test,y_preds))","74d53483":"## Resumes Dataset","8781f109":"# Text Classification - ELMo","54050ba7":"### Load ELMo ","8b45c07f":"### Label Encoding","11292749":"### Data Cleaning","e90c5aca":"## User Responses Dataset","ae5f0b79":"ELMo is a deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). These word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. They can be easily added to existing models and significantly improve the state of the art across a broad range of challenging NLP problems, including question answering, textual entailment and sentiment analysis."}}