{"cell_type":{"eb343ba0":"code","a76bcc10":"code","70a2fcdf":"code","b9bdf7ce":"code","e8e4d8fe":"code","fbb743fa":"code","cbf93f07":"code","60940337":"code","aeb98e7d":"code","4d0f0d12":"code","25aae2a2":"code","54366200":"code","677e0e91":"code","890ca8b5":"code","f7a6f34b":"code","3e857acd":"code","0fe93772":"code","d3a798d2":"code","c44b516b":"code","457783c0":"code","b3406097":"code","678a6366":"code","66736fc2":"code","c583c394":"code","e6a346ca":"code","4c18abf8":"code","56f32d3a":"code","3fb3516e":"code","e92e2cb1":"code","55c1c687":"code","3bbe8e5f":"markdown","10d52192":"markdown"},"source":{"eb343ba0":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nfrom sklearn import metrics\nfrom sklearn import datasets\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier","a76bcc10":"df=pd.read_csv(\"C:\\\\Users\\\\diya achaya\\\\Desktop\\\\dataset\\\\heart.csv\")\ndf.head()","70a2fcdf":"duplicate_rows = df[df.duplicated()]\nprint(\"Number of duplicate rows :: \", duplicate_rows.shape)\ndf = df.drop_duplicates()\nduplicate_rows = df[df.duplicated()]\nprint(\"Number of duplicate rows :: \", duplicate_rows.shape)","b9bdf7ce":"df.isnull().sum()","e8e4d8fe":"df.describe()","fbb743fa":"plt.figure(figsize= (15,6))\nsns.set_style(\"dark\")\nsns.heatmap(df.corr(),annot= True)\nplt.show()","cbf93f07":"sns.countplot('output',data=df,palette='magma')","60940337":"df.output.mean()*100","aeb98e7d":"sns.FacetGrid(df,col='output',height=7).map(plt.hist,'age')","4d0f0d12":"sns.FacetGrid(df,col='output',height=7).map(sns.countplot,'sex')","25aae2a2":"sns.FacetGrid(df,col='output',height=7).map(sns.countplot,'cp')","54366200":"\nfor i in range(0,13):\n    \n    grid=sns.boxplot(y=df.iloc[:,i])\n    plt.show()\n    ","677e0e91":"#Removing outliers using Z-score\nz = np.abs(stats.zscore(df))\ndf2 = df[(z<3).all(axis=1)]\ndf2.shape","890ca8b5":"fig = plt.subplots(figsize=(14,8))\nsns.heatmap(df2.corr(method=\"pearson\"),annot= True,linewidth=0.1)","f7a6f34b":"fig = plt.subplots(figsize=(14,8))\nsns.heatmap(df.corr(method=\"spearman\"),annot= True,linewidth=0.1)","3e857acd":"df2['age_range'] = pd.qcut(df2['age'], 3)\n\nprint(df2[[\"age_range\", \"output\"]].groupby([\"age_range\"], as_index = False).mean().sort_values(by = \"age_range\", ascending = True))\ndf2=df2.drop(['age_range'], axis=1)\n","0fe93772":"dfa= [df2]  \nfor dataset in dfa:\n    dataset.loc[(dataset[\"age\"] <= 51, \"age_cat\")] = 0\n    dataset.loc[(dataset[\"age\"] >51) & (dataset[\"age\"] <= 59), \"age_cat\"] = 1\n    dataset.loc[(dataset[\"age\"] > 59) , \"age_cat\"] = 2\ndf2=df2.drop(['age'],axis=1)    \ndf2\nsns.countplot(x='age_cat',data=df2,hue='output')    ","d3a798d2":"df2['trtbps_range'] = pd.qcut(df2['trtbps'], 3)\n\n\n\nprint(df2[[\"trtbps_range\", \"output\"]].groupby([\"trtbps_range\"], as_index = False).mean().sort_values(by = \"trtbps_range\", ascending = True))\ndf2=df2.drop(['trtbps_range'], axis=1)","c44b516b":"dfa= [df2]  \nfor dataset in dfa:\n    dataset.loc[(dataset[\"trtbps\"] <= 122, \"trtbps_cat\")] = 0\n    dataset.loc[(dataset[\"trtbps\"] > 122 ) & (dataset[\"trtbps\"] <= 138), \"trtbps_cat\"] = 1\n    dataset.loc[(dataset[\"trtbps\"] > 138), \"trtbps_cat\"] = 2\ndf2\nsns.countplot(x='trtbps_cat',data=df2,hue='output')  \ndf2=df2.drop(['trtbps'],axis=1)","457783c0":"df2['chol_range'] = pd.qcut(df2['chol'], 4)\n\n\n\nprint(df2[[\"chol_range\", \"output\"]].groupby([\"chol_range\"], as_index = False).mean().sort_values(by = \"chol_range\", ascending = True))\ndf2=df2.drop(['chol_range'], axis=1)","b3406097":"dfa= [df2]  \nfor dataset in dfa:\n    dataset.loc[(dataset[\"chol\"] <= 211, \"chol_cat\")] = 0\n    dataset.loc[(dataset[\"chol\"] > 211) & (dataset[\"chol\"] <= 241), \"chol_cat\"] = 1\n    dataset.loc[(dataset[\"chol\"] > 241) & (dataset[\"chol\"] <= 274), \"chol_cat\"] = 2\n    dataset.loc[(dataset[\"chol\"] > 274), \"chol_cat\"] = 3\ndf2=df2.drop(['chol'], axis=1)\nsns.countplot(x='chol_cat',data=df2,hue='output')  ","678a6366":"df2['thalach_range'] = pd.qcut(df2['thalachh'], 3)\n\n\n\nprint(df2[[\"thalach_range\", \"output\"]].groupby([\"thalach_range\"], as_index = False).mean().sort_values(by = \"thalach_range\", ascending = True))\ndf2=df2.drop(['thalach_range'], axis=1)","66736fc2":"dfa= [df2]  \nfor dataset in dfa:\n    dataset.loc[(dataset[\"thalachh\"] <= 142, \"thalach_cat\")] = 0\n    dataset.loc[(dataset[\"thalachh\"] > 142) & (dataset[\"thalachh\"] <= 162), \"thalach_cat\"] = 1\n    dataset.loc[(dataset[\"thalachh\"] > 162), \"thalach_cat\"] = 2\n    \ndf2=df2.drop(['thalachh'], axis=1)\nsns.countplot(x='thalach_cat',data=df2,hue='output')  \ndf2","c583c394":"df2['oldpeak_range'] = pd.qcut(df2['oldpeak'], 3)\n\n\n\nprint(df2[[\"oldpeak_range\", \"output\"]].groupby([\"oldpeak_range\"], as_index = False).mean().sort_values(by = \"oldpeak_range\", ascending = True))\ndf2=df2.drop(['oldpeak_range'], axis=1)","e6a346ca":"dfa= [df2]  \nfor dataset in dfa:\n    dataset.loc[(dataset[\"oldpeak\"] <= 0.1, \"oldpeak_cat\")] = 2\n    dataset.loc[(dataset[\"oldpeak\"] > 0.1) & (dataset[\"oldpeak\"] <= 1.4), \"oldpeak_cat\"] = 1\n    dataset.loc[(dataset[\"oldpeak\"] > 1.4), \"oldpeak_cat\"] = 2\n    \ndf2=df2.drop(['oldpeak'], axis=1)\nsns.countplot(x='oldpeak_cat',data=df2,hue='output') ","4c18abf8":"df2","56f32d3a":"x = df2.drop(\"output\", axis=1)\ny = df2[\"output\"]\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.1,random_state=0)","3fb3516e":"# Using Random forest classifier\nrf = RandomForestClassifier(n_estimators=120)\nrf.fit(x_train,y_train)\ny_pred_rf = rf.predict(x_test)\nprint(\"Accuracy of Random Forest Classifier :: \", metrics.accuracy_score(y_test, y_pred_rf))\n#Find the score of each feature in model and drop the features with low scores\nx_train1=x_train.drop(['cp','exng','thalach_cat'],axis=1)\nx_test1=x_test.drop(['cp','exng','thalach_cat'],axis=1)\nrf2 = RandomForestClassifier(n_estimators=120)\nrf2.fit(x_train1,y_train)\ny_pred_rf2 = rf2.predict(x_test1)\nprint(\"Accuracy of Random Forest Classifier :: \", metrics.accuracy_score(y_test, y_pred_rf2))","e92e2cb1":"logReg = LogisticRegression(random_state=0, solver='liblinear')\nlogReg.fit(x_train, y_train)\n#Check accuracy of Logistic Regression\ny_pred_logReg = logReg.predict(x_test)\n#Model Accuracy\nprint(\"Accuracy of logistic regression classifier :: \" ,metrics.accuracy_score(y_test,y_pred_logReg))\nlogReg1 = LogisticRegression(random_state=0, solver='liblinear')\nlogReg1.fit(x_train1, y_train)\n#Check accuracy of Logistic Regression\ny_pred_logReg1 = logReg1.predict(x_test1)\n#Model Accuracy\nprint(\"Accuracy of logistic regression classifier :: \" ,metrics.accuracy_score(y_test,y_pred_logReg))","55c1c687":"\nfor n in range (1, 20, 2):\n    knn = KNeighborsClassifier (n_neighbors = n)\n    knn.fit(x_train, y_train)\n    y_pred = knn.predict(x_test)\n    acc_knn = round(knn.score(x_train, y_train) * 100, 2)\n    print(\"The accuracy with\", n, \"neihbors is:\", acc_knn)","3bbe8e5f":"<pre>\n           The variable description \n\nAge: age in years\nGender: gender (1 = male; 0 = female)\nCp: chest pain type\nValue 1: typical angina\nValue 2: atypical angina\nValue 3: non-anginal pain\nValue 4: asymptomatic\nTrestbps: resting blood pressure (in mm Hg on admission to the hospital)\nChol: serum cholesterol in mg\/dl\nFbs: fasting blood sugar > 120 mg\/dl (1 = true; 0 = false)\nRestecg: resting electrocardiographic results\nValue 0: normal\nValue 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 005 mV)\nValue 2: showing probable or definite left ventricular hypertropy by Estes criteria\nThalach: maximum heart rate achieved in beats per minute (bpm)\nExang: exercise induced angina (1 = yes; 0 = no)\nOldpeak: ST depression induced by exercise relative to rest\nSlope: the slope of the peak exercise ST segment\nValue 1: upsloping\nValue 2: flat\nValue 3: down-sloping\nCa: number of major vessels (0-3) colored by fluoroscopy\nThal: 3 = normal; 6 = fixed defect; 7 = reversible defect\nThe target feature has two classes and hence it is a binary classification problem. To reiterate, the goal is to predict whether a person has heart disease.\n<\/pre>","10d52192":"# HEART ATTACK ANALYSIS AND PREDICTION"}}