{"cell_type":{"fac28859":"code","ea750361":"code","24df5b36":"code","8c85bc67":"code","de571eb7":"code","471b9d38":"code","22abc836":"code","1bd9b1f8":"code","ff2eb759":"markdown","68097d32":"markdown","0952b3ce":"markdown","471904b8":"markdown","bb3c742b":"markdown","b5959248":"markdown","3da88d5e":"markdown","1b69091b":"markdown"},"source":{"fac28859":"import random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nrandom.seed(2016) # define the seed (important to reproduce the results)\n\ndata = pd.read_csv('..\/input\/usp-pj01\/train_Iris.csv', header=(0))\n\ndata = data.dropna(axis='rows') #remove NaN\n\n# armazena os nomes das classes\nclasses = np.array(pd.unique(data[data.columns[-1]])) \n\n\nnrow, ncol = data.shape\n\n\nattributes = list(data.columns)\n","ea750361":"data = data.to_numpy()\nnrow,ncol = data.shape\ny = data[:,-1]\nX = data[:,0:ncol-1]","24df5b36":"scaler = StandardScaler().fit(X)\nX = scaler.transform(X)","8c85bc67":"from sklearn.model_selection import train_test_split\np = 0.75 # fracao de elementos no conjunto de treinamento\nx_train, x_test, y_train, y_test = train_test_split(X, y, train_size = p, random_state = 42)\nprint(y_test)","de571eb7":"from scipy.stats import multivariate_normal\n\n#matrix to store the probabilities\nP = pd.DataFrame(data=np.zeros((x_test.shape[0], len(classes))), columns = classes) \nPc = np.zeros(len(classes)) #fraction of elements in each class\nfor i in np.arange(0, len(classes)):\n    elements = tuple(np.where(y_train == classes[i] ))\n    elements = elements[0]\n    Pc[i] = len(elements)\/len(y_train)\n    #print(\"\\n Pc[i]: \",Pc[i])\n    #print(\"\\n Tamanho:\",len(elements))\n    Z = x_train[elements,:]\n    #print(\"\\n Z:\",Z)\n    m = np.mean(Z, axis = 0)\n    cv = np.cov(np.transpose(Z))\n    \n    for j in np.arange(0,x_test.shape[0]):\n        x = x_test[j,:]\n        #print(x,\"\\n\")\n        pj = multivariate_normal.pdf(x, mean=m, cov=cv, allow_singular=True)\n        P[classes[i]][j] = pj*Pc[i]","471b9d38":"y_pred = []\n\nfor i in np.arange(0, x_test.shape[0]):\n    c = np.argmax(np.array(P.iloc[[i]]))\n    y_pred.append(classes[c])\ny_pred = np.array(y_pred)\n#testa-se a Acuracia\nfrom sklearn.metrics import accuracy_score\nscore = accuracy_score(y_pred, y_test)\nprint('Acuracia:', score)","22abc836":"from sklearn.neighbors import KernelDensity\nfrom sklearn.metrics import accuracy_score\n\n# Matriz que armazena as probabilidades para cada classe\nP = pd.DataFrame(data=np.zeros((x_test.shape[0], len(classes))), columns = classes) \nPc = np.zeros(len(classes)) # Armaze a fracao de elementos em cada classe\nh = 2\nfor i in np.arange(0, len(classes)): # Para cada classe\n    elements = tuple(np.where(y_train == classes[i])) # elmentos na classe i\n    elements =elements[0]\n    Pc[i] = len(elements)\/len(y_train) # Probabilidade pertencer a classe i\n    Z = x_train[elements,:] # Elementos no conjunto de treinamento\n    kde = KernelDensity(kernel='gaussian', bandwidth=h).fit(Z)\n    for j in np.arange(0,x_test.shape[0]): # para cada observacao no conjunto de teste\n        x = x_test[j,:]\n        x = x.reshape((1,len(x)))\n        # calcula a probabilidade pertencer a cada classe\n        pj = np.exp(kde.score_samples(x)) \n        P[classes[i]][j] = pj*Pc[i]\n        \ny_pred = [] # Vetor com as classes preditas\nfor i in np.arange(0, x_test.shape[0]):\n    c = np.argmax(np.array(P.iloc[[i]]))\n    y_pred.append(classes[c])\ny_pred = np.array(y_pred)\n# calcula a acuracia\nscore = accuracy_score(y_pred, y_test)\nprint('Acuracia:', score)","1bd9b1f8":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn import metrics\n\nmodel = GaussianNB()\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_test)\nscore = accuracy_score(y_pred, y_test)\nprint('Accuracy:', score)","ff2eb759":"Agora faremos a sele\u00e7\u00e3o para qual classe pertence e logo em seguida testaremos a Acuracia","68097d32":"###Metodo Naive-Beyes\n\u00c9 tentado ainda o metodo Naives-Beyes do Sklearn","0952b3ce":"###Classificador Bayesiano\nJ\u00e1 selecionamos os conjuntos de treinamento e teste anteriormente. No conjunto de treinamento, vamos calcular a m\u00e9dia e desvio padr\u00e3o de cada atributo para cada classe. A seguir, reaizamos a classifica\u00e7\u00e3o, dos dados usando a teoria da decis\u00e3o Bayesiana, isto \u00e9: $X \\in C_i$ se, e somente se, $P(C_i|X) = \\max P(C_j|X)$ para todo $j$.","471904b8":"Agora para treinar o classificador \u00e9 separado uma por\u00e7\u00e3o para treina-lo","bb3c742b":"# Task 1 - Iris dataset\nPrimeira parte do C\u00f3digo, carregando as bibliotecas e base de dados\n","b5959248":"Vamos construir as vari\u00e1veis $x$ e $y$, sendo que o processo classifica\u00e7\u00e3o se resume em estimar a fun\u00e7\u00e3o $f$ na rela\u00e7\u00e3o $y = f(x) + \\epsilon$, onde $\\epsilon$ \u00e9 o erro, que tem distribui\u00e7\u00e3o normal com m\u00e9dia igual a zero e vari\u00e2ncia $\\sigma^2$.\n\nConvertemos os dados para o formato Numpy para facilitar a sua manipula\u00e7\u00e3o.","3da88d5e":"### Baysiano N\u00e3o-Parametrico\nTreinaremos o conjunto tambem com Baysiano n\u00e3o parametrico","1b69091b":"Agora os dados s\u00e3o normalizados, de modo a evitar o efeito da escala dos atributos."}}