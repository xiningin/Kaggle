{"cell_type":{"88096bac":"code","61f2e9db":"code","7a3c14f6":"code","a6a55cf8":"code","bc2f6747":"code","1fb5a428":"code","eddb7be6":"code","70f45084":"code","7cb1f56d":"code","3c388eba":"code","4fafbc37":"code","651523ea":"code","9a01e800":"code","d855c26b":"code","d9e68856":"code","98ececcf":"code","7f4423b5":"code","79a97e69":"code","e2bcec16":"code","f4ea3b26":"code","b5226615":"code","6725e563":"code","82214a6c":"code","0c458166":"code","9f33f927":"code","54e7b8ca":"code","cba480a7":"code","676dd413":"code","f1d538a7":"code","2685677d":"code","2c6e2148":"code","a06b7a1c":"code","4d97d8eb":"code","a71f0cb1":"code","6f1d16dc":"code","44e86258":"code","3c405fb9":"code","65ceecb6":"code","394dc8b4":"code","93e7cc9b":"code","76cce0f8":"code","c14cb820":"code","b6acf93c":"code","f63dfbcf":"code","e900b1ca":"code","c5bfbe04":"code","f1604962":"code","2b40f0ef":"code","790c055e":"markdown","ed5d7cc6":"markdown","3153c814":"markdown","3f9d45e7":"markdown","61528ece":"markdown","87a007ff":"markdown","2f9b4286":"markdown","7c754b9b":"markdown","08949c28":"markdown","600ddf2f":"markdown","9cef3b3e":"markdown","5ea81e3d":"markdown","0921642d":"markdown","9dc5ca21":"markdown","32d41bb1":"markdown","53d9179b":"markdown","c4340a12":"markdown","fc075c22":"markdown","4e559337":"markdown","d04e790a":"markdown","834286ea":"markdown","2d2def36":"markdown","f5ff420e":"markdown","d7e537c1":"markdown","10807169":"markdown","da049941":"markdown","4877eccb":"markdown","ce925399":"markdown","e4343505":"markdown","9341be76":"markdown","6a8890bb":"markdown","d22e89f0":"markdown","71b3d8aa":"markdown","bac33b19":"markdown","c5045146":"markdown","53ef7d34":"markdown","1638d91a":"markdown","86ca734e":"markdown","9ef9208f":"markdown","1f013b7e":"markdown","91e252dd":"markdown","6d46ba05":"markdown","c85ea872":"markdown","19113424":"markdown","b8439b7d":"markdown","387a99de":"markdown","27cef41a":"markdown","a1ad6262":"markdown","84f9f6fd":"markdown","3aa03a17":"markdown","d5accab3":"markdown","783eb155":"markdown"},"source":{"88096bac":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport category_encoders as ce\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nfrom sklearn import preprocessing, metrics\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import f1_score\nfrom sklearn.feature_selection import SelectFromModel, SelectKBest, chi2, mutual_info_classif\nfrom sklearn.decomposition import FactorAnalysis\nimport seaborn as sns\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nimport os","61f2e9db":"def get_data_splits(dataframe, valid_fraction=0.1):\n    \"\"\" Splits a dataframe into train, validation, and test sets. First, orders by \n        the column 'click_time'. Set the size of the validation and test sets with\n        the valid_fraction keyword argument.\n    \"\"\"\n    valid_rows = int(len(dataframe) * valid_fraction)\n    train = dataframe[:-valid_rows * 2]\n    valid = dataframe[-valid_rows * 2:]\n    \n    return train, valid","7a3c14f6":"#    A     B              A     B   A_missing\n# 0  4.5   ...        0  4.5   ...  FALSE\n# 1  NaN   ...   =>   1  NaN   ...  TRUE\n# 2  6.7   ...        2  6.7   ...  FALSE","a6a55cf8":"#     A         B            A         B \n#  0  female   ...        0  female   ...\n#  1  NaN      ...   =>   1  female   ...\n#  2  male     ...        2  male     ...\n# 'Female' is most frequent value","bc2f6747":"#     A     B              A     B\n#  0  4.5   ...        0  4.5   ...\n#  1  NaN   ...   =>   1  5.6   ...\n#  2  6.7   ...        2  6.7   ...\n# 5.6 is a mean of column A","1fb5a428":"#     A          B              A        B   A_count\n# 0  female    ...        0  female     ...    2\n# 1  male      ...   =>   1  male       ...    1\n# 2  female    ...        2  female     ...    2","eddb7be6":"from sklearn.pipeline import Pipeline\n\nclass CustomTransformBase:\n    def __init__(self, **kwargs):\n        if 'is_verbose' in kwargs.keys():\n            self.is_verbose = kwargs['is_verbose']\n        else:\n            self.is_verbose = False\n        if self.is_verbose:\n            print(self.__class__.__name__)\n            print(kwargs['columns'])\n        self.cols = kwargs['columns']\n    def get_cols(self):\n        if self.is_verbose:\n            print(self.__class__.__name__, ' get_cols')\n        return self.cols\n    def get_params(self, deep):\n        return self.cols\n    def fit_transform(self, X, y=None):\n        if self.is_verbose:\n            print(self.__class__.__name__, ' fit_transform')\n        self.fit(X, y)\n        return self.transform(X, y)\n\nclass ColumnsDropper(CustomTransformBase):\n    def __init__(self, **kwargs):\n        CustomTransformBase.__init__(self, **kwargs)\n    def fit(self, X, y=None):\n        if self.is_verbose:\n            print(self.__class__.__name__,' fit')\n        return self\n    def transform(self, X, y=None):\n        if self.is_verbose:\n            print(self.__class__.__name__,' transform')\n        for col in self.get_cols():\n            X.drop(col, axis=1, inplace=True)\n        if self.is_verbose:\n            print(X)\n        return X\n\nclass MissingValuesColumnCreator(CustomTransformBase):\n    def __init__(self, **kwargs):\n        CustomTransformBase.__init__(self, **kwargs)\n    def fit(self, X, y=None):\n        self.cols_with_missing = [col for col in X.columns if X[col].isnull().any()]\n        if self.is_verbose:\n            print(self.__class__.__name__,' columns with missing')\n            print(self.cols_with_missing)\n        return self\n    def transform(self, X, y=None):\n        if self.cols_with_missing is None:\n            self.fit(X, y)\n        add_for_missing = pd.DataFrame()\n        for col in self.cols_with_missing:\n            add_for_missing[col + '_was_missing'] = X[col].isnull()\n        add_for_missing.index = pd.RangeIndex(X.index.start, X.index.stop, X.index.step)\n        return pd.concat([X, add_for_missing], axis=1)\n\nclass CatLabeler(CustomTransformBase):\n    def __init__(self, **kwargs):\n        CustomTransformBase.__init__(self, **kwargs)\n        self.cat_labeler = LabelEncoder()\n    def fit(self, X, y=None):\n        if self.is_verbose:\n            print(self.__class__.__name__,' fit')\n        return self\n    def transform(self, X, y=None):\n        if self.is_verbose:\n            print(self.__class__.__name__,' transform')\n        for col in self.get_cols():\n            X[col] = self.cat_labeler.fit_transform(X[col])\n        if self.is_verbose:\n            print(X)\n        return X\n    \nclass CustomImputer(CustomTransformBase):\n    def __init__(self, **kwargs):\n        CustomTransformBase.__init__(self, **kwargs)\n    def fit(self, X, y=None):\n        if self.is_verbose:\n            print(self.__class__.__name__,' fit')\n        self.imputer.fit(X[self.get_cols()])\n        return self\n    def transform(self, X, y=None):\n        if self.is_verbose:\n            print(self.__class__.__name__,' transform')\n        X[self.get_cols()] = pd.DataFrame(self.imputer.transform(X[self.get_cols()]))\n        X[self.get_cols()].columns = self.get_cols()\n        if self.is_verbose:\n            print(X)\n        return X\n    \nclass CatImputer(CustomImputer):\n    def __init__(self, **kwargs):\n        CustomImputer.__init__(self, **kwargs)\n        self.imputer = SimpleImputer(strategy='most_frequent', copy=False)\n    def fit(self, X, y=None):\n        if self.is_verbose:\n            print(self.__class__.__name__,' fit')\n        self.imputer.fit(X[self.get_cols()])\n        X[self.get_cols()].columns = self.get_cols()\n        return self\n    def transform(self, X, y=None):\n        if self.is_verbose:\n            print(self.__class__.__name__,' transform')\n        X[self.get_cols()] = pd.DataFrame(self.imputer.transform(X[self.get_cols()]))\n        X[self.get_cols()].columns = self.get_cols()\n        return X\n        \nclass NumImputer(CustomImputer):\n    def __init__(self, **kwargs):\n        CustomImputer.__init__(self, **kwargs)\n        self.imputer = SimpleImputer(strategy='median', copy=False)\n    def fit(self, X, y=None):\n        if self.is_verbose:\n            print(self.__class__.__name__,' fit')\n        self.imputer.fit(X[self.get_cols()])\n        X[self.get_cols()].columns = self.get_cols()\n        return self\n    def transform(self, X, y=None):\n        if self.is_verbose:\n            print(self.__class__.__name__,' transform')\n        X[self.get_cols()] = self.imputer.transform(X[self.get_cols()])\n        X[self.get_cols()].columns = self.get_cols()\n        return X\n    \nclass CountColumnsCreator(CustomTransformBase):\n    def __init__(self, **kwargs):\n        CustomTransformBase.__init__(self, **kwargs)\n        self.countEncoder = ce.CountEncoder(cols=kwargs['columns'])\n    def fit(self, X, y=None):\n        if self.is_verbose:\n            print(self.__class__.__name__,' fit')\n        self.countEncoder.fit(X[self.get_cols()])\n        return self\n    def transform(self, X, y=None):\n        if self.is_verbose:\n            print(self.__class__.__name__,' transform')\n        X_ = self.countEncoder.transform(X[self.get_cols()])\n        #in case unseen categories encountered\n        X_.fillna(value=0,inplace=True)\n        return X.join(X_.add_suffix(\"_count\"))\n\nclass Discretizer(CustomTransformBase):\n    def __init__(self, **kwargs):\n        CustomTransformBase.__init__(self, **kwargs)\n        self.nbins = kwargs['bins']\n        self.discretizer = KBinsDiscretizer(n_bins=self.nbins)\n    def fit(self, X, y=None):\n        if self.is_verbose:\n            print(self.__class__.__name__,' fit')\n        for col in self.get_cols():\n            self.discretizer.fit(X[col].values.reshape(-1,1))\n        return self\n    def transform(self, X, y=None):\n        if self.is_verbose:\n            print(self.__class__.__name__,' transform')\n        for col in self.get_cols():\n            encoded = self.discretizer.transform(X[col].values.reshape(-1,1))\n            binned_cols = []\n            imputed_df = pd.DataFrame(encoded.toarray())\n            for i in range(0,len(imputed_df.columns)):\n                binned_cols.append(col+str(i))\n            imputed_df.columns = binned_cols\n            imputed_df.index = pd.RangeIndex(X.index.start, X.index.stop, X.index.step)\n            X = pd.concat([X, imputed_df], axis=1)\n            X.drop(col, axis=1, inplace=True)\n        return X\n    \nclass CustomSelector(CustomTransformBase):\n    def __init__(self, **kwargs):\n        CustomTransformBase.__init__(self, **kwargs)\n        if 'selector' in kwargs.keys():\n            self.selector = kwargs['selector']\n        else:\n            self.selector = None\n        self.selected_columns = None\n    def fit (self, X, y=None):\n        if self.selector is None:\n            return self\n        X_new = self.selector.fit_transform(X,y)\n        #print(X_new)\n        selected_features = pd.DataFrame(self.selector.inverse_transform(X_new),\n                                         index=X.index,\n                                         columns=X.columns)\n\n        self.selected_columns = selected_features.columns[selected_features.var() != 0]\n    def transform (self, X, y=None):\n        if self.selected_columns is None:\n            return X\n        return X[self.selected_columns]","70f45084":"class DataPreprocessor:\n    def __init__(self,cols_to_drop,cat_features,numerical_cols,create_features=False, selector=None):\n        dropper = ColumnsDropper(columns=cols_to_drop,is_verbose=False)\n        catImputer = CatImputer(columns=cat_features,is_verbose=False)\n        catLabeler = CatLabeler(columns=cat_features,is_verbose=False)\n        num_imputer = NumImputer(columns=numerical_cols,is_verbose=False)\n        discretizer = Discretizer(columns=numerical_cols, bins=4,is_verbose=False)\n        customSelector = CustomSelector(columns=[],is_verbose=False,selector=selector)\n        steps = []\n        steps.append(('dropper', dropper))\n        if create_features:\n            missing = MissingValuesColumnCreator(columns=[],is_verbose=False)\n            steps.append(('missing', missing))\n        steps.append(('cat_imputer', catImputer))\n        steps.append(('cat_labeler',catLabeler))\n        steps.append(('num_imputer', num_imputer))\n        if create_features:\n            countColumnsCreator = CountColumnsCreator(columns=cat_features,is_verbose=False)\n            steps.append(('count_col_creation', countColumnsCreator))\n        steps.append(('discretizer', discretizer))\n        steps.append(('selector', customSelector))\n        self.preprocessor = Pipeline(steps=steps,verbose=True)\n\n    def preprocess_data(self, df, y=None, is_train=False):\n        if is_train:\n            return pd.DataFrame(self.preprocessor.fit_transform(df, y))\n        return pd.DataFrame(self.preprocessor.transform(df))\n\n    def separate_data_to_features_and_label(train, label_name, validation=None, separate=False):\n        train = train.copy()\n        if validation is None and separate:\n            train, validation = get_data_splits(train)\n        train_y = train[label_name]\n        if validation is not None:\n            val_y = validation['Survived']\n        else:\n            val_y = None\n        train.drop('Survived', axis=1, inplace=True)\n        if validation is not None:\n            validation.drop('Survived', axis=1, inplace=True)\n        return train, train_y, validation, val_y\n    def addCustomColumn(miniproc, dataset, **kwargs):\n        col = miniproc(dataset, **kwargs)\n        return pd.concat([dataset,col],axis=1)","7cb1f56d":"def get_number_of_components(fa_train_x, fa_train_y, fa_val_x, fa_val_y):\n    best_fa_score = None\n    best_fa_ncomp = None\n    best_fa = None\n    for i in range(2, len(fa_train_x.columns), 2):\n        fa = FactorAnalysis(n_components=i)\n        fa = fa.fit(fa_train_x)\n        score = fa.score(fa_val_x)\n        if best_fa_score is None or score > best_fa_score:\n            best_fa_score = score\n            best_fa_ncomp = i\n            best_fa = fa\n    print('FA best_score ', best_fa_score, ' with ', best_fa_ncomp, ' components')\n    best_fa = FactorAnalysis(n_components=best_fa_ncomp)\n    best_fa = best_fa.fit(fa_train_x)\n    loading_matrix = pd.DataFrame(best_fa.components_,columns=fa_train_x.columns)\n    loading_matrix = np.transpose(loading_matrix)\n    print(loading_matrix)\n    return best_fa","3c388eba":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","4fafbc37":"n_estimators = [2000, 1600, 1400, 1000, 400]\nmax_features = ['auto', 'sqrt']\nmax_depth = [10, 30, 60, 90]\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2]\nbootstrap = [True, False]\ntuned_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","651523ea":"def evaluate_rf_with_select(X, y, val_X, val_y, class_weights, preprocessor):\n    rfModel = RandomForestClassifier(class_weight=class_weights)\n        \n    if type(X) == np.ndarray:\n        X_cv = np.concatenate((X, val_X))\n    else:\n        X_cv = pd.concat([X, val_X], axis=0)\n    if type(y) == np.ndarray:\n        y_cv = np.concatenate((y, val_y))\n    else:\n        y_cv = pd.concat([y, val_y], axis=0)\n\n    #choose one of the following\n    #RandomizedSearchCV\n    rf_random = RandomizedSearchCV(estimator = rfModel, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n    #GridSearch\n    #rf_random = GridSearchCV(estimator = rfModel, param_grid = tuned_grid, \n    #                      cv = 3, n_jobs = -1, verbose = 2)\n    # Fit the random search model\n    rf_random.fit(X_cv, y_cv)\n    print(rf_random.best_params_)\n    model = SelectFromModel(rf_random.best_estimator_, prefit=True)\n    X_new = model.transform(X_cv)\n    selected_features = pd.DataFrame(model.inverse_transform(X_new),\n                                      index=X_cv.index,\n                                      columns=X_cv.columns)\n\n    selected_columns = selected_features.columns[selected_features.var() != 0]\n    rf_random.best_estimator_.fit(selected_features[selected_columns], y_cv)\n    preds_val = rf_random.best_estimator_.predict(val_X[selected_columns])\n    f1score = f1_score(val_y, preds_val)\n    print(\"RF RandomCV F1 score: \", f1score)\n    print(\"RF RandomCV Training AUC score: \", metrics.roc_auc_score(val_y, preds_val))\n    return f1score, rf_random.best_estimator_, selected_columns, None","9a01e800":"from sklearn.model_selection import RandomizedSearchCV\ndef evaluate_rf_with_search_cv(X, y, val_X, val_y, class_weights, preprocessor):\n    if type(X) == np.ndarray:\n        X_cv = np.concatenate((X, val_X))\n    else:\n        X_cv = pd.concat([X, val_X], axis=0)\n    if type(y) == np.ndarray:\n        y_cv = np.concatenate((y, val_y))\n    else:\n        y_cv = pd.concat([y, val_y], axis=0)\n\n    rfModel = RandomForestClassifier(class_weight=class_weights)\n    #choose one of the following\n    #RandomizedSearchCV\n    rf_random = RandomizedSearchCV(estimator = rfModel, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n    #GridSearch\n    #rf_random = GridSearchCV(estimator = rfModel, param_grid = tuned_grid, \n    #                      cv = 3, n_jobs = -1, verbose = 2)\n    # Fit the random search model\n    rf_random.fit(X_cv, y_cv)\n    print(rf_random.best_params_)\n    preds_val = rf_random.best_estimator_.predict(val_X)\n    f1score = f1_score(val_y, preds_val)\n    print(\"RF RandomCV F1 score: \", f1score)\n    print(\"RF RandomCV Training AUC score: \", metrics.roc_auc_score(val_y, preds_val))\n    return f1score, rf_random.best_estimator_, None, None","d855c26b":"def evaluate_rf_with_fa(X, y, val_X, val_y, class_weights, preprocessor):\n    fa = get_number_of_components(X, y, val_X, val_y)\n    X = fa.transform(X)\n    val_X = fa.transform(val_X)\n    #f1score,clf, selected_columns, _ = evaluate_rf(X, y, val_X, val_y, class_weights, preprocessor)\n    f1score,clf, selected_columns, _ = evaluate_rf_with_search_cv(X, y, val_X, val_y, class_weights, preprocessor)\n    return f1score,clf, selected_columns,fa ","d9e68856":"def evaluate_lgbm(X, y, val_X, val_y, class_weights, preprocessor):\n    lgbmClf = lgb.LGBMClassifier()\n    lgbmClf.fit(X, y)\n    preds_val = lgbmClf.predict(val_X)\n    f1score = f1_score(val_y, preds_val)\n    print(\"LGBM F1 score: \", f1score)\n    print(\"LGBM Training AUC score: \", metrics.roc_auc_score(val_y, preds_val))\n    return f1score, lgbmClf, None, None","98ececcf":"from sklearn.ensemble import GradientBoostingClassifier\ndef evaluate_GradientBoost(X, y, val_X, val_y, class_weights, preprocessor):\n    gbClf = GradientBoostingClassifier()\n    gbClf.fit(X, y)\n    preds_val = gbClf.predict(val_X)\n    f1score = f1_score(val_y, preds_val)\n    print(\"GradientBoost F1 score: \", f1score)\n    print(\"GradientBoost Training AUC score: \", metrics.roc_auc_score(val_y, preds_val))\n    return f1score, gbClf, None, None","7f4423b5":"import catboost\nfrom catboost import CatBoostClassifier\ndef evaluate_catboost(X, y, val_X, val_y, class_weights, preprocessor):\n    catboost = CatBoostClassifier(verbose=0, n_estimators=100, class_weights=[class_weights[1],class_weights[0]])\n    catboost.fit(X, y)\n    preds_val = catboost.predict(val_X)\n    f1score = f1_score(val_y, preds_val)\n    print(\"CatBoost F1 score: \", f1score)\n    print(\"CatBoost Training AUC score: \", metrics.roc_auc_score(val_y, preds_val))\n    return f1score, catboost, None, None","79a97e69":"import xgboost\nfrom xgboost import XGBClassifier\ndef evaluate_xgb(X, y, val_X, val_y, class_weights, preprocessor):\n    xgbClf = XGBClassifier()\n    xgbClf.fit(X, y)\n    preds_val = xgbClf.predict(val_X)\n    f1score = f1_score(val_y, preds_val)\n    print(\"XGB F1 score: \", f1score)\n    print(\"XGB Training AUC score: \", metrics.roc_auc_score(val_y, preds_val))\n    return f1score, xgbClf, None, None","e2bcec16":"classifiers = [(evaluate_rf_with_select, 'RandomForest_with_SelectFromModel'),\n               (evaluate_rf_with_fa, 'RandomForest_with_FactorAnalysis'),\n            #commented this to shorten train time. Feel free to uncomment and try\n              # (evaluate_lgbm,'LGBM'),\n              # (evaluate_GradientBoost, 'GradientBoost'),\n              # (evaluate_catboost, 'CatBoost'),\n              # (evaluate_xgb, 'XGB'),\n              # (evaluate_rf_with_search_cv, 'RandomForestRandomCV')\n              ]","f4ea3b26":"def train_predict_and_save(X, y, valX, val_y, test, preprocessor, extension, class_weights=None):\n    best_f1_score = None\n    selected_clf = None\n    best_selected_columns = None\n    best_fa = None\n    best_clf_name = None\n    print('Train set')\n    print(X)\n    print('Val set')\n    print(valX)\n    for get_clf in classifiers:     \n        f1score,clf, selected_columns, fa = get_clf[0](X, y, valX, val_y, class_weights, preprocessor)\n        print('f1 score ', f1score, ' ', best_f1_score)\n        if best_f1_score is None or best_f1_score < f1score:\n            best_f1_score = f1score\n            selected_clf = clf\n            best_clf_name = get_clf[1]\n            best_selected_columns = selected_columns\n            best_fa = fa\n    passenger_id = test['PassengerId']\n    X_test = preprocessor.preprocess_data(test,is_train=False)\n    if best_selected_columns is not None:\n        X_test = X_test[best_selected_columns]\n    if best_fa is not None:\n        X_test = best_fa.transform(X_test)\n    print('Test set')\n    print(X_test)\n    print('Selected model ', best_clf_name, ' ', selected_clf)\n    print('Selected mode F1 scores ', best_f1_score)\n    predictions = selected_clf.predict(X_test)\n    data_to_output = pd.DataFrame({'PassengerId': passenger_id, 'Survived':predictions})\n    data_to_output.to_csv('predict_rf_'+extension+best_clf_name+'.csv', index=False)","b5226615":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n","6725e563":"explore_set = train.copy()\nexplore_set.head(10)","82214a6c":"print('Survived ',explore_set.loc[explore_set['Survived'] == 1]['Survived'].count())\nprint('Died ',explore_set.loc[explore_set['Survived'] == 0]['Survived'].count())","0c458166":"sns.countplot(explore_set['Survived'])","9f33f927":"sns.countplot(explore_set['Sex'])","54e7b8ca":"sns.countplot(explore_set['Pclass'])","cba480a7":"g = sns.catplot(x=\"Pclass\", hue=\"Sex\", col=\"Survived\",\n                data=explore_set, kind=\"count\",\n                height=4, aspect=.7);","676dd413":"explore_set.count()","f1d538a7":"class_weighted_train = train.copy()\ny = class_weighted_train['Survived']\nweight_for_0 = (1 \/ (class_weighted_train['Survived'].map(lambda is_survived: is_survived == 0)).sum())*(y.count())\/2.0 \nweight_for_1 = (1 \/ (class_weighted_train['Survived'].map(lambda is_survived: is_survived == 1)).sum())*(y.count())\/2.0\n\nclass_weights = {0: weight_for_0, 1: weight_for_1}\n\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))","2685677d":"class_weighted_train, train_y, class_weighted_valid, val_y = \\\nDataPreprocessor.separate_data_to_features_and_label(train,  'Survived', separate=True)","2c6e2148":"cols_to_drop = ['Cabin', 'Ticket', 'Name', 'PassengerId']\ncat_features = ['Sex', 'Pclass', 'Embarked', 'SibSp', 'Parch']\nnumerical_cols = ['Age','Fare']","a06b7a1c":"preprocessor = DataPreprocessor(cols_to_drop, cat_features, numerical_cols, create_features=False)\ntrain_X = preprocessor.preprocess_data(class_weighted_train,is_train=True)\nval_X = preprocessor.preprocess_data(class_weighted_valid,is_train=False)\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain_predict_and_save(train_X, train_y, val_X, val_y, test, preprocessor, 'class_weighted', class_weights=class_weights)","4d97d8eb":"class_weighted_train, train_y, class_weighted_valid, val_y = \\\n    DataPreprocessor.separate_data_to_features_and_label(train,  \n                                                        'Survived', separate=True)\npreprocessor = DataPreprocessor(cols_to_drop, cat_features, numerical_cols,create_features=True)\ntrain_X = preprocessor.preprocess_data(class_weighted_train,is_train=True)\nval_X = preprocessor.preprocess_data(class_weighted_valid,is_train=False)\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain_predict_and_save(train_X, train_y, val_X,val_y, test, preprocessor, 'class_weighted_more_features', class_weights=class_weights)","a71f0cb1":"cols_to_drop = ['Cabin', 'Ticket', 'Name', 'PassengerId', 'SibSp', 'Parch']\ncat_features = ['Sex', 'Pclass', 'Embarked']\nnumerical_cols = ['Age','Fare']\nclass_weighted_train, train_y, class_weighted_valid, val_y = \\\nDataPreprocessor.separate_data_to_features_and_label(train,  'Survived', separate=True)\npreprocessor = DataPreprocessor(cols_to_drop, cat_features, numerical_cols,create_features=True)\ntrain_X = preprocessor.preprocess_data(class_weighted_train,is_train=True)\nval_X = preprocessor.preprocess_data(class_weighted_valid,is_train=False)\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain_predict_and_save(train_X, train_y, val_X, val_y, test, preprocessor, 'class_weighted_more_features2', class_weights=class_weights)","6f1d16dc":"cols_to_drop = ['Cabin', 'Ticket', 'Name', 'PassengerId']\ncat_features = ['Sex', 'Pclass', 'Embarked', 'SibSp', 'Parch']\nnumerical_cols = ['Age','Fare']","44e86258":"from sklearn.decomposition import FactorAnalysis\n\ntrain_X, train_y, val_X, val_y = \\\nDataPreprocessor.separate_data_to_features_and_label(train,  'Survived', separate=True)\npreprocessor = DataPreprocessor(cols_to_drop, cat_features, numerical_cols,create_features=False)\ntrain_X = preprocessor.preprocess_data(train_X,is_train=True)\nval_X = preprocessor.preprocess_data(val_X,is_train=False)\nfa = get_number_of_components(train_X, train_y, val_X, val_y)","3c405fb9":"selector = SelectKBest(score_func=chi2, k='all')\nselect_k_best_train, train_y, _, _ = \\\nDataPreprocessor.separate_data_to_features_and_label(train,  'Survived')\n\npreprocessor = DataPreprocessor(cols_to_drop, cat_features, numerical_cols)\ntrain_X = preprocessor.preprocess_data(select_k_best_train,is_train=True)\nfs = selector.fit(train_X, train_y)\nfor i in range(len(fs.scores_)):\n    print('Feature %d: %f' % (i, fs.scores_[i]))\n# plot the scores\nplt.bar([i for i in range(len(fs.scores_))], fs.scores_)\nplt.show()","65ceecb6":"selector = SelectKBest(chi2, k=4)\nselect_k_best_train, train_y, val_X, val_y = \\\nDataPreprocessor.separate_data_to_features_and_label(train,  'Survived',separate=True)\n\npreprocessor = DataPreprocessor(cols_to_drop, cat_features, numerical_cols,selector=selector)\ntrain_X = preprocessor.preprocess_data(select_k_best_train,y=train_y,is_train=True)\nval_X = preprocessor.preprocess_data(val_X,y=val_y,is_train=False)\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain_predict_and_save(train_X, train_y, val_X, val_y, test, preprocessor, 'selectk_chi2', class_weights=class_weights)","394dc8b4":"selector = SelectKBest(score_func=mutual_info_classif, k='all')\nselect_k_best_train, train_y, _, _ = \\\nDataPreprocessor.separate_data_to_features_and_label(train,  'Survived')\npreprocessor = DataPreprocessor(cols_to_drop, cat_features, numerical_cols)\ntrain_X = preprocessor.preprocess_data(select_k_best_train,y=train_y, is_train=True)\nfs = selector.fit(train_X, train_y)\nfor i in range(len(fs.scores_)):\n    print('Feature %d: %f' % (i, fs.scores_[i]))\n# plot the scores\nplt.bar([i for i in range(len(fs.scores_))], fs.scores_)\nplt.show()","93e7cc9b":"selector = SelectKBest(mutual_info_classif, k=4)\nselect_k_best_train, train_y, val_X, val_y  = \\\nDataPreprocessor.separate_data_to_features_and_label(train,  'Survived', separate=True)\n\npreprocessor = DataPreprocessor(cols_to_drop, cat_features, numerical_cols,selector=selector)\ntrain_X = preprocessor.preprocess_data(select_k_best_train, y=train_y, is_train=True)\nval_X = preprocessor.preprocess_data(val_X, y=train_y, is_train=False)\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain_predict_and_save(train_X, train_y, val_X, val_y, test, preprocessor, 'selectk_mutualinfo', class_weights=class_weights)","76cce0f8":"def oversample_dataset(train, column, minor_criteria, major_criteria):\n    oversampled_train = train.copy()\n    minor_features = oversampled_train.loc[oversampled_train[column] == minor_criteria]\n    major_features = oversampled_train.loc[oversampled_train[column] == major_criteria]\n    minor_labels = oversampled_train.loc[oversampled_train[column] == minor_criteria]\n    major_labels = oversampled_train.loc[oversampled_train[column] == major_criteria]\n    minor_labels = minor_labels[column]\n    major_labels = major_labels[column]\n    minor_features.drop(column, axis=1, inplace=True)\n    major_features.drop(column, axis=1, inplace=True)\n    #generate row indexes for minor class\n    ids = np.arange(len(minor_features))\n    #choose row indexes, amount equals major class size\n    choices = np.random.choice(ids, len(major_features))\n    #create resampled minor those size equals major size\n    res_minor_features = minor_features.iloc[choices]\n    res_minor_labels = minor_labels.iloc[choices]\n    #concatenate minor and major\n    resampled_features = pd.concat([res_minor_features, major_features])\n    resampled_labels = pd.concat([res_minor_labels, major_labels])\n    #create random order\n    order = np.arange(len(resampled_labels))\n    np.random.shuffle(order)\n    #apply random order\n    resampled_features = resampled_features.iloc[order]\n    resampled_labels = resampled_labels.iloc[order]\n    resampled_features.reset_index(inplace=True)\n    resampled_features.drop('index', axis=1,inplace=True)\n    resampled_labels = resampled_labels.reset_index()\n    return resampled_features, resampled_labels","c14cb820":"resampled_features, resampled_labels = oversample_dataset(train, 'Survived', 1.0, 0.0)","b6acf93c":"print(resampled_features)\nprint(resampled_labels)\noversampled_set = pd.concat([resampled_features, resampled_labels],axis=1)\noversampled_set.drop('index', axis=1,inplace=True)\ntrain_X, train_y, val_X, val_y  = \\\nDataPreprocessor.separate_data_to_features_and_label(oversampled_set,  'Survived', separate=True)\npreprocessor = DataPreprocessor(cols_to_drop, cat_features, numerical_cols)\ntrain_X = preprocessor.preprocess_data(train_X,is_train=True)\nval_X = preprocessor.preprocess_data(val_X,is_train=False)\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain_predict_and_save(train_X, train_y, val_X, val_y, test, preprocessor, 'oversampled', {0:1, 1:1})","f63dfbcf":"resampled_features, resampled_labels = oversample_dataset(train, 'Survived', 1.0, 0.0)\noversampled_set = pd.concat([resampled_features, resampled_labels],axis=1)\noversampled_set.drop('index', axis=1,inplace=True)\ntrain_X, train_y, val_X, val_y  = \\\nDataPreprocessor.separate_data_to_features_and_label(oversampled_set,  'Survived', separate=True)\nselector = SelectKBest(chi2, k=4)\npreprocessor = DataPreprocessor(cols_to_drop, cat_features, numerical_cols,selector=selector)\ntrain_X = preprocessor.preprocess_data(train_X,y=train_y,is_train=True)\nval_X = preprocessor.preprocess_data(val_X,y=val_y,is_train=False)\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain_predict_and_save(train_X, train_y, val_X, val_y, test, preprocessor, 'resampled_selectk_chi2', class_weights={0:1, 1:1})","e900b1ca":"familySizeColCreator = lambda dataset, **kwargs: pd.DataFrame(dataset[kwargs['col1']] + dataset[kwargs['col2']], columns=[kwargs['col3']])","c5bfbe04":"extended_train = train.copy()\nextended_train = DataPreprocessor.addCustomColumn(familySizeColCreator,extended_train, col1='SibSp', col2='Parch',col3='FamilySize')\nprint(extended_train)\nresampled_features, resampled_labels = oversample_dataset(extended_train, 'Survived', 1.0, 0.0)\noversampled_set = pd.concat([resampled_features, resampled_labels],axis=1)\noversampled_set.drop('index', axis=1,inplace=True)\ntrain_X, train_y, val_X, val_y  = \\\nDataPreprocessor.separate_data_to_features_and_label(oversampled_set,  'Survived', separate=True)\ncols_to_drop = ['Cabin', 'Ticket', 'Name', 'PassengerId', 'SibSp', 'Parch']\ncat_features = ['Sex', 'Pclass', 'Embarked', 'FamilySize']\nnumerical_cols = ['Age','Fare']\npreprocessor = DataPreprocessor(cols_to_drop, cat_features, numerical_cols)\ntrain_X = preprocessor.preprocess_data(train_X,is_train=True)\nval_X = preprocessor.preprocess_data(val_X,is_train=False)\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nfamily_size_column = pd.DataFrame(test['SibSp'] + test['Parch'], columns=['FamilySize'])\ntest = pd.concat([test,family_size_column],axis=1)\ntrain_predict_and_save(train_X, train_y, val_X, val_y, test, preprocessor, 'resampled_selectk_chi2', class_weights={0:1, 1:1})","f1604962":"travelAloneColCreator = lambda dataset, **kwargs: pd.DataFrame((dataset[kwargs['col1']] + dataset[kwargs['col2']])>0, columns=[kwargs['col3']])","2b40f0ef":"travel_alone_train = train.copy()\ntravel_alone_train = DataPreprocessor.addCustomColumn(travelAloneColCreator,travel_alone_train, col1='SibSp', col2='Parch',col3='TravelAlone')\nprint(travel_alone_train)\nresampled_features, resampled_labels = oversample_dataset(travel_alone_train, 'Survived', 1.0, 0.0)\noversampled_set = pd.concat([resampled_features, resampled_labels],axis=1)\noversampled_set.drop('index', axis=1,inplace=True)\ntrain_X, train_y, val_X, val_y  = \\\nDataPreprocessor.separate_data_to_features_and_label(oversampled_set,  'Survived', separate=True)\ncols_to_drop = ['Cabin', 'Ticket', 'Name', 'PassengerId', 'SibSp', 'Parch']\ncat_features = ['Sex', 'Pclass', 'Embarked', 'TravelAlone']\nnumerical_cols = ['Age','Fare']\npreprocessor = DataPreprocessor(cols_to_drop, cat_features, numerical_cols)\ntrain_X = preprocessor.preprocess_data(train_X,is_train=True)\nval_X = preprocessor.preprocess_data(val_X,is_train=False)\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntravel_alone_column = pd.DataFrame((test['SibSp'] + test['Parch'])>0, columns=['TravelAlone'])\ntest = pd.concat([test,travel_alone_column],axis=1)\ntrain_predict_and_save(train_X, train_y, val_X, val_y, test, preprocessor, 'travel_alone', class_weights={0:1, 1:1})","790c055e":"First look at the dataset","ed5d7cc6":"And finally a function called for each dataset we want to train and evaluate the models, select the best model, make predictions and save the results","3153c814":"# Split data helper function","3f9d45e7":"The targer values are in 'Survived' column. We have 11 more columns.\nIt looks like there are few columns which are unlikely to help but create noise:\n- PassengerId\n- Name\n- Ticket","61528ece":"GradientBoost classifier train and evaluate","87a007ff":"For RandomizedSearchCV:","2f9b4286":"Now we check how many non-NA entries are in each column:","7c754b9b":"A function to train and evaluate RandomForest with SelectFromModel","08949c28":"Now we're going to create another column, FamilySize. Below is the lambda function for this:","600ddf2f":"We have 342 survived passenger agains 549 who died. We can later try to:\n- Apply class weights\n- Resample the minor (survived positives) class\nWe continue to explore the dataset. \nWe look at distributions of various features:","9cef3b3e":"we can see that FactorAnalysis proposes the dimension reduction to 8 features","5ea81e3d":"**Load Titanic dataset**","0921642d":"And another attempt with chi2 squared selection with the oversampled dataset:","9dc5ca21":"CatBoost classifier train and evaluate","32d41bb1":"Create a dataset with TravelAlone feature and select a model:","53d9179b":"Redefine category features to include SibSp and Parch","c4340a12":"Now this is the time to prepare our data. We've already said we want to drop some columns and impute missing values. Some columns contain numerical data, however the numbers actually represent different categories. For example, Pclass columns has values 1, 2 and 3. These numbers correspond different cabin classes. We're going to label such columns to make it usable by the RandomForest algorithm.\nHowever, one columns with continuous numerical data - this is the Age and Fare columns. To make it usable for RandomForest, we're going to discretize the values in the Age and Fare columns by binning","fc075c22":"# Titanic exercise - trying to apply some knowledge from micro-courses","4e559337":"This function will apply FactorAnalysis before evaluating RandomForest","d04e790a":"Since we use categorical features, I try FactorAnalysis in attempt to reduce dimensions rather than PCA","834286ea":"There are two sets from which the parameters for RandomForest model are selected. Each member of a set contains a number of possible values. One set is used for RandomizedSerachCV and another is for GridSearchCV","2d2def36":"Until now, we've identified the following steps to preprocess our data:\n- drop PassengerId, Name, Ticket and Cabin columns\n- Impute Age and Embarked columns\n- Discretize the Age column","f5ff420e":"We can see while some columns don't have NA entries at all (those which count 891) some columns have few NAs (Age, Embarked), one column has only 204 non-NA of 891. We are going to ignore the column with the very high number of NAs and impute the columns where we have few NAs.","d7e537c1":"It looks we have up to 5 features to select. This time we're going to tell SelectKBest to select 4 features, then train and predict:","10807169":"* Count feature creator\n> This will create an additional column indicating a number of values in corresponding column:","da049941":"Since we have imbalanced classes, calculate class weights:","4877eccb":"Now we try to apply Factor Analysis to try to select the most important features","ce925399":"We repeat the feature selection this time using mutual information classification","e4343505":"Now we're going to remove SibSp and Parch features and repeat:","9341be76":"Now it is time to try oversampling (the dataset is imbalanced with higher precense of negative cases so we're going to make increase a number of positive ones):","6a8890bb":"XGB classifier train and evaluate","d22e89f0":"For GridSearchCV:","71b3d8aa":"Try to select a model with resampled set:","bac33b19":"Now we're going to create some features (missing values and count columns, we set create_features=True for that) and repeat the steps:","c5045146":"* Numerical features imputer\n> For missing values in the columns with numerical data, this will impute some value","53ef7d34":"* Categorical features imputer\n> For missing values in the columns which correspond categorical features, this will impute some value","1638d91a":"Here is a list of classifiers to evaluate","86ca734e":"Now we're going to create another column, TravelAlone. Below is the lambda function for this:","9ef9208f":"# Several classes for data preprocessing\n* Column dropper\nTo drop some columns\n* Missing values indicator features creator\nThis will do the following add A_missing such as:","1f013b7e":"# Data preprocessor\nThis uses a Pipeline to define a sequence of steps to pre-process the data\n\nIf create_features is True, additional features created:\n* Missing values columns\n* Count columns","91e252dd":"Create a dataset with FamilySize feature and select a model:","6d46ba05":"This time we're going to do chi-squared feature selection. We tell SlectKBest to use all the features, then we print the scores","c85ea872":"# I start from defining some functions and classes which will serve to train, evaluate and select the best model while experimenting with different compositions of features in the dataset","19113424":"A function to train and evaluate RandomForest","b8439b7d":"Create labels, drop labels from train and validation sets:","387a99de":"This function is used to train and evaluate LightGMB classifier","27cef41a":"Looks not more than 4 features is worth to try to select","a1ad6262":"Now we are going to explore some dependencies","84f9f6fd":"Preprocess the data, choose the best number of estimators and max_depth (of the trees). \nThen train on whole train set, predict and save results:","3aa03a17":"Now we check how imbalanced our classes are:","d5accab3":"# Setup","783eb155":"* Discretizer\n> This will create categorical data from continuous. For example, an age range 0-80 may be binned into 4 bins: 0-20, 21-40, 41-60, 61-80\n* Custom Selector\n> This will apply feature selection using provided selector such as SelectKBest or SelectFromModel"}}