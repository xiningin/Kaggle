{"cell_type":{"59228298":"code","15a03169":"code","7fec403d":"code","fb91918e":"code","eeff9312":"code","b0ac212c":"code","cea64866":"code","fd48a0a6":"code","ab2dc10a":"code","5ab8f34b":"code","54712bfe":"code","db3a17e2":"code","9f2ee180":"code","66409a5f":"code","bab6f320":"code","ca6d061c":"code","210e02b9":"code","824b2f3f":"code","06edafe3":"code","de1fb71d":"code","5bf3d83f":"code","3485a4bf":"code","4052af64":"code","2397ad7d":"code","64237be3":"code","809e5706":"code","61417f19":"code","63a25096":"code","e923e1b1":"code","348c34e0":"code","8387aee4":"code","c5386be4":"code","a72da5e5":"code","d7b9b331":"code","f98b910d":"code","20223b1e":"code","d12e1fcc":"code","817544b2":"code","d82c8920":"code","a99351e0":"code","2b28e28d":"code","9c89b454":"code","12ca7feb":"code","2d551a30":"code","77cb8d55":"markdown","489d4f2a":"markdown","9e2ca3a8":"markdown","fab14b81":"markdown","e3cbd689":"markdown","5f03fad0":"markdown","fdfc08a8":"markdown","27e0deff":"markdown","37228146":"markdown","e89bab62":"markdown","ab100c30":"markdown","b57fc169":"markdown","e4312957":"markdown","c11c1d42":"markdown","4244d4e4":"markdown","d0e6a71e":"markdown","dd207c7f":"markdown","fe786268":"markdown","4c33c0bc":"markdown","13d20d1c":"markdown","c8d1d26c":"markdown","0982aaf5":"markdown"},"source":{"59228298":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport plotly.offline as pyo\npyo.init_notebook_mode()\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport string\nimport re\nfrom wordcloud import WordCloud,STOPWORDS\nimport nltk\nfrom sklearn.metrics import mean_squared_error\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer as CVTZ\nfrom sklearn.linear_model import LinearRegression\nfrom keras import Sequential,Model\nfrom keras.layers import Dense\nfrom keras.layers.merge import concatenate\nfrom sklearn.model_selection import train_test_split\n\n\nimport tensorflow as tf\nimport collections\ndef set_seed(seed=31415):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed()\n\ndef RMSE(Y,YHAT):\n    return np.sqrt(mean_squared_error(Y,YHAT))\n\nplt.rc('figure',figsize=(20,11))","15a03169":"n_data = pd.read_csv('\/kaggle\/input\/internet-articles-data-with-users-engagement\/articles_data.csv',usecols=['source_name','author',\n                                                                                                            'title','description','published_at',\n                                                                                                            'top_article','engagement_reaction_count',\n                                                                                                            'engagement_comment_count',\n                                                                                                            'engagement_share_count'])\nn_data.head(5)","7fec403d":"fig = make_subplots(\n    rows=2, cols=2,\n    shared_xaxes=True,\n    vertical_spacing=0.03,\n    specs=[[{\"type\": \"heatmap\",'rowspan':2},{\"type\": \"table\",'rowspan':2}],\n           [None             ,None],\n          ]\n)\n\nfig.add_trace(\n    go.Heatmap(\n        z=n_data.isna().T.astype(int),\n        x=np.arange(0,len(n_data)),\n        y=n_data.columns\n    ),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Table(\n    header=dict(values=list(['Feature Name','Number Of Missing']),\n                fill_color='royalblue',\n                font_color='white',\n                font_size=13,\n                align='left'),\n    cells=dict(values=[n_data.columns,n_data.isna().sum().to_frame()[0]],\n               fill_color='azure',\n               font_size=14,\n               align='left')),\n    row=1, col=2\n)\n\n\n\n\nfig.update_layout(\n    height=800,\n    showlegend=False,\n    title_text=\"Proportion Of Missing Values\",\n)\nfig.update_yaxes(title_text=\"Sentiment Strength\")\nfig.show()","fb91918e":"n_data.published_at = pd.to_datetime(n_data.published_at)\n\nn_data['Day_Of_Week'] = n_data.published_at.apply(lambda x: x.dayofweek)\nn_data['Month'] = n_data.published_at.apply(lambda x: x.month)\nn_data['Year'] = n_data.published_at.apply(lambda x: x.year)\n\n","eeff9312":"plt.subplot(3,1,1)\nplt.title(\"Distribution Of Engagement Reaction Counts Before Log Transformation\")\nsns.kdeplot(n_data.engagement_reaction_count)\nplt.subplot(3,1,2)\nplt.title(\"Distribution Of Engagement Reaction Counts After Log Transformation\")\nsns.kdeplot(np.log(n_data.engagement_reaction_count+0.00001))\nn_data.engagement_reaction_count = np.log(n_data.engagement_reaction_count+0.00001)\nplt.subplot(3,1,3)\nsns.boxplot(n_data.engagement_reaction_count)\nplt.show()","b0ac212c":"plt.subplot(3,1,1)\nplt.title(\"Distribution Of Engagement Comment Counts Before Log Transformation\")\nsns.kdeplot(n_data.engagement_comment_count)\nplt.subplot(3,1,2)\nplt.title(\"Distribution Of Engagement Comment Counts After Log Transformation\")\nsns.kdeplot(np.log(n_data.engagement_comment_count+0.00001))\nn_data.engagement_comment_count = np.log(n_data.engagement_comment_count+0.00001)\nplt.subplot(3,1,3)\nsns.boxplot(n_data.engagement_comment_count)\nplt.show()","cea64866":"plt.subplot(3,1,1)\nplt.title(\"Distribution Of Engagement Share Counts Before Log Transformation\")\nsns.kdeplot(n_data.engagement_share_count)\nplt.subplot(3,1,2)\nplt.title(\"Distribution Of Engagement Share Counts After Log Transformation\")\nsns.kdeplot(np.log(n_data.engagement_share_count+0.00001))\nn_data.engagement_share_count = np.log(n_data.engagement_share_count+0.00001)\nplt.subplot(3,1,3)\nsns.boxplot(n_data.engagement_share_count)\nplt.show()","fd48a0a6":"n_data.title = n_data.title.str.lower()\nn_data = n_data[~n_data.title.isna()]\nn_data.title = n_data.title.apply(lambda x: ' '.join(re.findall(r'\\w+', x)) )\nn_data.title = n_data.title.apply(lambda x:x.strip())\n\nn_data.description = n_data.description.str.lower()\nn_data = n_data[~n_data.description.isna()]\nn_data.description = n_data.description.apply(lambda x: ' '.join(re.findall(r'\\w+', x)) )\nn_data.description = n_data.description.apply(lambda x:x.strip())","ab2dc10a":"sid = SIA()\nn_data['Sentiment']            = n_data.description.apply(lambda x:sid.polarity_scores(x))\nn_data['DESC_Positive Sentiment']   = n_data.Sentiment.apply(lambda x: x['pos'])\nn_data['DESC_Neutral Sentiment']    = n_data.Sentiment.apply(lambda x: x['neu'])\nn_data['DESC_Negative Sentiment']   = n_data.Sentiment.apply(lambda x: x['neg'])\n\nn_data.drop(columns=['Sentiment'],inplace=True)","5ab8f34b":"info = n_data.describe()\ninfo.loc['Skewness'] = n_data.skew()\ninfo.loc['Kurtosis'] = n_data.kurt()\ninfo.loc['Median'] = n_data.median()\ninfo","54712bfe":"plt.subplot(1,3,1)\nplt.title('Engagement Reaction Of Each Month At A Particular Day')\nsns.heatmap(n_data.pivot_table(columns='Day_Of_Week',index='Month',values='engagement_reaction_count'),cbar=False,cmap='coolwarm')\nplt.subplot(1,3,2)\nplt.title('Engagement Comment Of Each Month At A Particular Day')\nsns.heatmap(n_data.pivot_table(columns='Day_Of_Week',index='Month',values='engagement_comment_count'),cbar=False,cmap='coolwarm')\nplt.subplot(1,3,3)\nplt.title('Engagement Share Of Each Month At A Particular Day')\nsns.heatmap(n_data.pivot_table(columns='Day_Of_Week',index='Month',values='engagement_share_count'),cmap='coolwarm')\nplt.show()","db3a17e2":"ex.pie(n_data,names='top_article',title='Proportion Of Atricles Marked As \"TOP ARTICLE\" ')","9f2ee180":"plt.title(\"Distribution Of Different Sources In Our Dataset\",fontsize=20)\nsns.barplot(x=n_data.source_name.value_counts().values,y=n_data.source_name.value_counts().index)\nplt.xlabel('Number Of Articles')\nplt.show()","66409a5f":"plt.title(\"Top 10 Authors In Our Dataset\",fontsize=20)\nsns.barplot(x=n_data.author.value_counts()[:10].values,y=n_data.author.value_counts()[:10].index)\nplt.xlabel('Number Of Articles')\nplt.show()","bab6f320":"wc = WordCloud(width=700,height=400,stopwords=STOPWORDS).generate(' '.join(n_data.title))\nplt.imshow(wc)\nplt.axis('off')\nplt.title(\"Most Used Words In Article Titles\",fontsize=20)\nplt.show()","ca6d061c":"wc = WordCloud(width=700,height=400,stopwords=STOPWORDS).generate(' '.join(n_data.description))\nplt.imshow(wc)\nplt.axis('off')\nplt.title(\"Most Used Words In Article Descriptions\",fontsize=20)\nplt.show()","210e02b9":"plt.subplot(2,1,1)\nplt.title(\"Distirubtion Of Negative Sentiment Scores For Different Soruces\")\nax = sns.boxplot(n_data['source_name'],n_data['DESC_Negative Sentiment'])\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.show()\nplt.subplot(2,1,2)\nplt.title(\"Distirubtion Of Positive Sentiment Scores For Different Soruces\")\nax = sns.boxplot(n_data['source_name'],n_data['DESC_Positive Sentiment'])\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.show()","824b2f3f":"fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Perason Correaltion',  'Spearman Correaltion'))\ncolorscale=[[0.0, \"rgb(165,0,38)\"],\n                [0.1111111111111111, \"rgb(215,48,39)\"],\n                [0.2222222222222222, \"rgb(244,109,67)\"],\n                [0.3333333333333333, \"rgb(253,174,97)\"],\n                [0.4444444444444444, \"rgb(254,224,144)\"],\n                [0.5555555555555556, \"rgb(224,243,248)\"],\n                [0.6666666666666666, \"rgb(171,217,233)\"],\n                [0.7777777777777778, \"rgb(116,173,209)\"],\n                [0.8888888888888888, \"rgb(69,117,180)\"],\n                [1.0, \"rgb(49,54,149)\"]]\n\ns_val =n_data.drop(columns=['Year']).corr('pearson')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,name='pearson',showscale=False,xgap=1,ygap=1,colorscale=colorscale),\n    row=1, col=1\n)\n\n\ns_val =n_data.drop(columns=['Year']).corr('spearman')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,xgap=1,ygap=1,colorscale=colorscale),\n    row=2, col=1\n)\n\nfig.update_layout(height=700, width=900, title_text=\"Locations That Contribute The Most To Our Cut-Offs\")\nfig.show()","06edafe3":"fig = make_subplots(rows=3, cols=1, subplot_titles=('Engagement Reaction Count','Engagement Comment Count','Engagement Share Count'))\n\nb_date_mean = n_data.copy()\nb_date_mean.published_at = pd.to_datetime(b_date_mean.published_at).dt.normalize()\nb_date_mean = b_date_mean.groupby(by='published_at').mean().reset_index()\n\nfig.add_trace(\ngo.Scatter(x=b_date_mean.published_at, y=b_date_mean.engagement_reaction_count,name='engagement_reaction_count'),\nrow=1, col=1)\n\nfig.add_trace(\ngo.Scatter(x=b_date_mean.published_at, y=b_date_mean.engagement_comment_count,name='engagement_comment_count'),\nrow=2, col=1)\n\nfig.add_trace(\ngo.Scatter(x=b_date_mean.published_at, y=b_date_mean.engagement_share_count,name='engagement_share_count'),\nrow=3, col=1)\n\nfig.update_layout(height=600, width=900, title_text=\"Behavior Of Different Engagement Attributes Over Time\")\nfig.show()","de1fb71d":"tf_model = CVTZ()\nN_COMPONENTS = 900\n\nsvd_model = TruncatedSVD(n_components = N_COMPONENTS)\ndesc_matrix = tf_model.fit_transform(n_data.description)\ntrunc_matrix = svd_model.fit_transform(desc_matrix)\n\nevr = svd_model.explained_variance_ratio_\nevr_cs = np.cumsum(evr)\ntr1 = go.Scatter(x=np.arange(0,len(evr_cs)),y=evr_cs,name='Explained Variance Cumulative')\ntr2 = go.Scatter(x=np.arange(0,len(evr_cs)),y=evr,name='Explained Variance')\n\nfig = go.Figure(data=[tr1,tr2],layout=dict(title='Explained Variance Ratio Using {} Components'.format(N_COMPONENTS),\n                                          xaxis_title='Number Of Components',yaxis_title='Explained Variance Ratio'))\n\nfig.show()\n","5bf3d83f":"n_data = n_data[~n_data.engagement_reaction_count.isna()]\ndescs_clean = n_data['description'].apply(lambda x: ' '.join([word for word in x.split() if word not in list(STOPWORDS) and len(word) > 1]))\nvocab = collections.Counter(' '.join(descs_clean).split(' '))\n\n\nMAX_LENGTH = max(descs_clean.apply(lambda x: len(x)))\nVOCAB_SIZE = len(vocab.keys())\nVECTOR_SPACE = 100\n\nencoded_docs = [tf.keras.preprocessing.text.one_hot(d,VOCAB_SIZE) for d in descs_clean]\n\npadded_docs = tf.keras.preprocessing.sequence.pad_sequences(encoded_docs,maxlen=MAX_LENGTH,padding='post')\n\npadded_docs_eval = padded_docs[0:1000]\npadded_docs = padded_docs[1000:]\n\nY_1 = n_data.engagement_reaction_count[1000:]\nY_eval_1 = n_data.engagement_reaction_count[:1000]\n\nY_2 = n_data.engagement_share_count[1000:]\nY_eval_2 = n_data.engagement_share_count[:1000]\n\nY_3 = n_data.engagement_comment_count[1000:]\nY_eval_3 = n_data.engagement_comment_count[:1000]","3485a4bf":"FCNN_MODEL = Sequential([\n    tf.keras.layers.Embedding(VOCAB_SIZE,VECTOR_SPACE,input_length=MAX_LENGTH),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dropout(0.5),\n    Dense(activation='linear',units=13),\n    Dense(activation='linear',units=1)\n    \n])\n\nFCNN_MODEL.compile(optimizer='adam', loss='mse', metrics=['mae'])","4052af64":"tf.keras.utils.plot_model(FCNN_MODEL,show_shapes=True)\n","2397ad7d":"history = FCNN_MODEL.fit(padded_docs, Y_1,validation_data=(padded_docs_eval,Y_eval_1),epochs=10,batch_size=150,verbose=False)","64237be3":"predictions = FCNN_MODEL.predict(padded_docs)\npredictions = predictions.reshape(-1)","809e5706":"h = pd.DataFrame(history.history)[['loss','mae']]\nsns.lineplot(x = np.arange(0,len(h)),y=h['loss'],label='loss')\nsns.lineplot(x = np.arange(0,len(h)),y=h['mae'],label='mae')\nplt.title('Model Preformence')\nplt.xlabel('EPOCH #')\nplt.xticks(np.arange(0,len(h)))\nplt.show()","61417f19":"print('ROOT MEAN SQUARED ERROR: %f' % (RMSE(predictions,Y_1)))","63a25096":"pd.DataFrame({\"Actual Engagement Reaction Count\":(Y_1.values),'Prediction':predictions}).head(10)","e923e1b1":"print('ROOT MEAN SQUARED ERROR: %f' % (RMSE(FCNN_MODEL.predict(padded_docs_eval),Y_eval_1)))","348c34e0":"results = pd.DataFrame({\"Actual Engagement Reaction Count\":(Y_1.values),'Prediction':predictions})\nresults.to_csv('Predicted_Engagement.csv',index=False)\nsns.residplot(x=results['Actual Engagement Reaction Count'],y=results['Prediction'])","8387aee4":"FCNN_MODEL = Sequential([\n    tf.keras.layers.Embedding(VOCAB_SIZE,VECTOR_SPACE,input_length=MAX_LENGTH),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dropout(0.5),\n    Dense(activation='linear',units=13),\n    Dense(activation='linear',units=1)\n    \n])\n\nFCNN_MODEL.compile(optimizer='adam', loss='mse', metrics=['mae'])","c5386be4":"history = FCNN_MODEL.fit(padded_docs, Y_2,validation_data=(padded_docs_eval,Y_eval_2),epochs=10,batch_size=150,verbose=False)","a72da5e5":"predictions = FCNN_MODEL.predict(padded_docs)\npredictions = predictions.reshape(-1)","d7b9b331":"h = pd.DataFrame(history.history)[['loss','mae']]\nsns.lineplot(x = np.arange(0,len(h)),y=h['loss'],label='loss')\nsns.lineplot(x = np.arange(0,len(h)),y=h['mae'],label='mae')\nplt.title('Model Preformence')\nplt.xlabel('EPOCH #')\nplt.xticks(np.arange(0,len(h)))\nplt.show()","f98b910d":"print('ROOT MEAN SQUARED ERROR: %f' % (RMSE(predictions,Y_2)))","20223b1e":"pd.DataFrame({\"Actual Engagement Share Count\":(Y_2.values),'Prediction':predictions}).head(10)","d12e1fcc":"results = pd.DataFrame({\"Actual Engagement Share Count\":(Y_2.values),'Prediction':predictions})\nresults.to_csv('Predicted_Share.csv',index=False)\nsns.residplot(x=results['Actual Engagement Share Count'],y=results['Prediction'])","817544b2":"FCNN_MODEL = Sequential([\n    tf.keras.layers.Embedding(VOCAB_SIZE,VECTOR_SPACE,input_length=MAX_LENGTH),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dropout(0.5),\n    Dense(activation='linear',units=13),\n    Dense(activation='linear',units=1)\n    \n])\n\nFCNN_MODEL.compile(optimizer='adam', loss='mse', metrics=['mae'])","d82c8920":"history = FCNN_MODEL.fit(padded_docs, Y_3,validation_data=(padded_docs_eval,Y_eval_3),epochs=10,batch_size=150,verbose=False)","a99351e0":"predictions = FCNN_MODEL.predict(padded_docs)\npredictions = predictions.reshape(-1)","2b28e28d":"h = pd.DataFrame(history.history)[['loss','mae']]\nsns.lineplot(x = np.arange(0,len(h)),y=h['loss'],label='loss')\nsns.lineplot(x = np.arange(0,len(h)),y=h['mae'],label='mae')\nplt.title('Model Preformence')\nplt.xlabel('EPOCH #')\nplt.xticks(np.arange(0,len(h)))\nplt.show()","9c89b454":"print('ROOT MEAN SQUARED ERROR: %f' % (RMSE(predictions,Y_3)))","12ca7feb":"pd.DataFrame({\"Actual Engagement Comment Count\":(Y_3.values),'Prediction':predictions}).head(10)","2d551a30":"results = pd.DataFrame({\"Actual Engagement Comment Count\":(Y_3.values),'Prediction':predictions})\nresults.to_csv('Predicted_Comment.csv',index=False)\nsns.residplot(x=results['Actual Engagement Comment Count'],y=results['Prediction'])","77cb8d55":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>The sentiments of the description were extracted for us to determine if it has any correlation with the &quot;count&quot; features.<\/span><\/p>","489d4f2a":"<a id=\"3.4\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Model Evaluation<\/h3>","9e2ca3a8":"<a id=\"3.2\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Sequantial Model Assembling<\/h3>\n","fab14b81":"<a id=\"3.4\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Model Evaluation<\/h3>","e3cbd689":"<p style=\"text-align: left;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Overall the model performance is moderate, we had a fairly low RMSE on the data itself (probably due to some degree of overfitting even though a dropout layer was used)&nbsp;<\/span><\/p>\n<p style=\"text-align: left;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">but when evaluating new data yet to be seen by the model before the performance drastically decreased.<\/span><\/span><\/p>\n<p style=\"text-align: left;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br><\/span><\/span><\/p>\n<p style=\"text-align: left;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span style=\"font-size: 24px;\">Feature work should include embedding of other text features<\/span>&nbsp;<\/span><\/p>","5f03fad0":"<a id=\"3.4\"><\/a>\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Engagement Share Model<\/h1>","fdfc08a8":"<a id=\"3.4\"><\/a>\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Engagement Comment Model<\/h1>","27e0deff":"<a id=\"1.2\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Data Importation And Missing Value Assessment<\/h3>\n","37228146":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>The initial approach I hopped will fit this dataset is using vectorization of the description and using the vectorized matrix as a predictor for the reaction count feature, unfortunately, both count vectorization and tf-idf vectorization had required a vary significant amount of components in order to explain a relatively small amount of the variance in the data.<\/span><\/p>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br><\/span><\/span><\/p>\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>The alternative approach we will implement next will be using a dense neutral network with a text embedding layer.<\/span><\/p>\n<p><br><\/p>","e89bab62":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Exploratory Data Analysis<\/h1>\n","ab100c30":"<a id=\"3.1\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Vocabulary Extraction And Preprocessing<\/h3>\n","b57fc169":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Data Preprocessing and Feature Engineering<\/h1>\n","e4312957":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>We can see that we have only 2 unique months in our dataset and that September Saturdays had a higher activity. <\/span><\/p>","c11c1d42":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>We can see that as far as positive and negative sentiment most of our sources have fairly similar distributions and mean values but at the same time we can observe that some of our sources have noticeable range differences !<\/span><\/p>\n<p><br><\/p>","4244d4e4":"<a id=\"3.4\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Conclusion<\/h3>","d0e6a71e":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Time Based Analysis<\/h1>\n","dd207c7f":"<a id=\"3.4\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Model Evaluation<\/h3>","fe786268":"<p style=\"text-align: center;\"><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'>Interestingly we see that the Engagement Reaction Count feature distribution is very rigged and has large outliers but after we performed a log transformation we got a smooth bimodal distribution.<\/span><\/p>\n<p style=\"text-align: center;\"><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'>What is really beautiful is that all the count features provided in the dataset follow the same effect and turn into bimodal distributions after being log normalized.<\/span><\/p>\n<p><br><\/p>","4c33c0bc":"<a id=\"1.1\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Libraries And Utilities<\/h3>\n","13d20d1c":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Text Based Modeling<\/h1>\n","c8d1d26c":"<p style=\"text-align: center;\"><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'>It is important to note that there were some redundant features in my opinion that were ignored during the data loading stage together with features that had a high missing value count.<\/span><\/p>\n<p><br><\/p>","0982aaf5":"<a id=\"3.4\"><\/a>\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Engagement Reaction Model<\/h1>"}}