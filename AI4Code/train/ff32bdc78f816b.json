{"cell_type":{"46fd4b36":"code","b94fd017":"code","85d3443b":"code","0a5a873c":"code","ecce8f58":"code","c1fdda78":"code","fbb950fd":"code","d08c4ae6":"code","a5c8672d":"code","361f4bd4":"code","28410640":"code","9028eeff":"code","b118a9f8":"code","26976044":"code","7a697e4c":"code","49a7e87a":"code","6fe03e09":"code","6d5566ea":"code","28ea4524":"code","230f7119":"code","a94654cf":"code","0f8a2202":"code","20af992d":"code","f28d45cc":"code","5d9185a9":"code","1a4720f1":"code","d6bbd5fc":"code","42b99e28":"code","a7f8d9d1":"code","7617a9b5":"code","1a92e6d8":"code","5be9853f":"code","f1f7ff96":"code","727f71fe":"code","d401189e":"code","c7296081":"code","77bd125d":"code","bfc43f0f":"code","ff13bb5f":"code","5b72ef5c":"code","dd313774":"code","c99c5b7d":"code","b1bed718":"code","531a0cbf":"code","d1d077a6":"code","5eb4a97f":"code","00ddfd31":"code","531332e3":"code","efc6fc3d":"code","0821d3ad":"code","2486d208":"code","d19a823c":"code","b9750173":"code","a07983f7":"code","1f4413ea":"code","f28b6423":"code","5c9cc0e4":"code","274a562a":"code","00a9c55c":"code","6e8d0fc1":"code","086c42db":"code","34d005fd":"code","f4643207":"code","bfa4ae16":"markdown","2b6ac2e2":"markdown","02698f55":"markdown","377be10b":"markdown"},"source":{"46fd4b36":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd \nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport missingno as msno \nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport re\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nimport nltk\nfrom nltk.tokenize import word_tokenize,RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer\nimport tensorflow as tf\n# data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b94fd017":"train_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntest_df.head()\ntest_df","85d3443b":"def explore_data(df):\n    print(\"-\"*50)\n    print('Shape of the dataframe:',df.shape)\n    print(\"Number of records in train data set:\",df.shape[0])\n    print(\"Information of the dataset:\")\n    df.info()\n    print(\"-\"*50)\n    print(\"First 5 records of the dataset:\")\n    return df.head()\n    print(\"-\"*50)","0a5a873c":"explore_data(train_df)\n","ecce8f58":"def missing_values(df):    \n    print(\"Number of records with missing location:\",df.location.isnull().sum())\n    print(\"Number of records with missing keywords:\",df.keyword.isnull().sum())\n    print('{}% of location values are missing from Total Number of Records.'.format(round((df.location.isnull().sum())\/(df.shape[0])*100),2))\n    print('{}% of keywords values are missing from Total Number of Records.'.format(round((df.keyword.isnull().sum())\/(df.shape[0])*100),2))\n#     msno.matrix(df);","c1fdda78":"missing_values(train_df)","fbb950fd":"\n# Drop the column 'location' from the training dataset\ntrain_df=train_df.drop(['location'],axis=1)","d08c4ae6":"train_df['text_length'] = train_df['text'].apply(lambda x : len(x))\ntrain_df.head(4)","a5c8672d":"def clean_text(text):\n\n    '''\n    Input- 'text' to be cleaned\n       \n       Output- Convert input 'text' to lowercase,remove square brackets,links,punctuation\n       and words containing numbers. Return clean text.\n    \n    '''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","361f4bd4":"train_df1=train_df.copy()\ntest_df1=test_df.copy()\ntrain_df1['text'] = train_df1['text'].apply(lambda x: clean_text(x))\ntest_df1['text'] = test_df1['text'].apply(lambda x: clean_text(x))\ntest_df.shape[0]","28410640":"def text_after_preprocess(before_text,after_text):\n    \n    '''\n    Input- before_text=text column before cleanup\n              after_text= text column after cleanup\n       Output- print before and after text to compare how it looks after cleanup\n       \n    '''\n    print('-'*60)\n    print('Text before cleanup')\n    print('-'*60)\n    print(before_text.head(5))\n    print('-'*60)\n    print('Text after cleanup')\n    print('-'*60)\n    print(after_text.head(5))","9028eeff":"text_after_preprocess(train_df.text,train_df1.text)\n","b118a9f8":"tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntrain_df1['text'] = train_df1['text'].apply(lambda x: tokenizer.tokenize(x))\ntest_df1['text'] = test_df1['text'].apply(lambda x: tokenizer.tokenize(x))","26976044":"train_df1['text'].head()\n# test_df.shape[0]","7a697e4c":"def remove_stopwords(text):\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words","49a7e87a":"train_df1['text'] = train_df1['text'].apply(lambda x : remove_stopwords(x))\ntest_df1['text'] = test_df1['text'].apply(lambda x : remove_stopwords(x))\ntest_df.shape[0]","6fe03e09":"train_df1.text.head()\n","6d5566ea":"def combine_text(text):\n    \n    '''\n    Input-text= list cleand and tokenized text\n    Output- Takes a list of text and returns combined one large chunk of text.\n    \n    '''\n    all_text = ' '.join(text)\n    return all_text","28ea4524":"train_df1['text'] = train_df1['text'].apply(lambda x : combine_text(x))\ntest_df1['text'] = test_df1['text'].apply(lambda x : combine_text(x))\ntrain_df1.head()","230f7119":"def pre_process_text_combined(text):\n    \n    \"\"\"\n    Input- text= text to be pre-processed\n    \n    Oputput- return cleaned and combined text to be vectrorized for Machine learning.\n\n    \"\"\"\n    #Initiate a tokenizer\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    # Clean the text using clean_text function\n    cleaned_txt = clean_text(text)\n    tokenized_text = tokenizer.tokenize(cleaned_txt)\n    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(remove_stopwords)\n    return  combined_text\ntest_df.shape[0]","a94654cf":"def pre_process_text(text):\n    \"\"\"\n    Input- text= text to be pre-processed\n    \n    Oputput- return cleaned text to be vectrorized for Machine learning.\n\n    \"\"\"\n    #Initiate a tokenizer\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    # Clean the text using clean_text function\n    cleaned_txt = clean_text(text)\n    tokenized_text = tokenizer.tokenize(cleaned_txt)\n    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    return remove_stopwords","0f8a2202":"train_df2=train_df.copy()\ntrain_df2['text'] = train_df2['text'].apply(lambda x : pre_process_text_combined(x))\ntrain_df2.head()","20af992d":"test_df2=test_df.copy()\ntest_df2['text'] = test_df2['text'].apply(lambda x : pre_process_text_combined(x))\ntest_df.head()","f28d45cc":"train_df3=train_df.copy()\ntrain_df3['text'] = train_df3['text'].apply(lambda x : pre_process_text(x))\ntrain_df3.head()","5d9185a9":"test_df3=test_df.copy()\ntest_df3['text'] = test_df3['text'].apply(lambda x : pre_process_text(x))\ntest_df3.head()","1a4720f1":"from wordcloud import WordCloud\nfig, ax = plt.subplots(figsize=[10, 6])\nwordcloud = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(train_df2.text))\nax.imshow(wordcloud)\nax.axis('off')\nax.set_title('Disaster Tweets',fontsize=40);","d6bbd5fc":"import tensorflow as tf\np = tf.config.experimental.list_physical_devices('GPU')\ntf.config.experimental.set_visible_devices(p[0], 'GPU')","42b99e28":"\nfrom collections import Counter, namedtuple\ndef word_counter(text):  \n    \n    count = Counter()\n    for i in text.values:\n        for word in i.split():\n            count[word] += 1\n    return count    \n\ntext = train_df2['text']\ncounter = word_counter(text)\n\nvocab_size = len(counter)","a7f8d9d1":"from nltk.corpus import stopwords\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score, accuracy_score\n\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM, Embedding,GlobalMaxPool1D,Bidirectional\nfrom keras.optimizers import Nadam\n\n\n\nskf = StratifiedKFold(n_splits=5)\nX = train_df2['text']\ny = train_df2['target']\n\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 200, input_length = 20))\nmodel.add(Bidirectional(LSTM(128,return_sequences = True)))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(64,activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nnadam = Nadam(learning_rate=0.0001)\n\nmodel.compile(loss = 'binary_crossentropy', optimizer=nadam, metrics=['accuracy'])","7617a9b5":"model.summary()","1a92e6d8":"t = Tokenizer(num_words = vocab_size)\nt.fit_on_texts(X)\n# train model on 5 folds\naccuracy = []\nfor train_index, test_index in skf.split(X, y):\n    \n    train_x, test_x = X[train_index], X[test_index]\n    train_y, test_y = y[train_index], y[test_index]\n    print(\"Tweet before tokenization: \", train_x.iloc[0])\n    \n    #Tokenize the tweets using tokenizer.\n    train_tweets = t.texts_to_sequences(train_x)\n    test_tweets = t.texts_to_sequences(test_x)\n    print(\"Tweet after tokenization: \", train_tweets[0])\n    \n    #pad the tokenized tweet data\n    train_tweets_padded = pad_sequences(train_tweets, maxlen=20, padding='post', truncating='post')\n    test_tweets_padded = pad_sequences(test_tweets, maxlen=20, padding='post', truncating='post')\n    print('Tweet after padding: ', train_tweets_padded[0])\n    \n    #train model on processed tweets\n    history = model.fit(train_tweets_padded, train_y, epochs=5, validation_data = (test_tweets_padded,test_y))\n    \n    #make predictions\n    pred_y = model.predict_classes(test_tweets_padded)\n    print(\"Validation accuracy : \",accuracy_score(pred_y, test_y))\n    \n    #store validation accuracy\n    accuracy.append(accuracy_score(pred_y, test_y))","5be9853f":"\ntokenized_tweets = t.texts_to_sequences(test_df2['text'])\npadded_tweets = pad_sequences(tokenized_tweets, maxlen=20, padding='post', truncating='post')\npred_y = model.predict_classes(padded_tweets)\npred_y = pred_y.flatten()\npred_y","f1f7ff96":"sample = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\n\nsubmissions = pd.DataFrame({'id':sample.id,\n                      'target':pred_y})\nsubmissions.to_csv(\"Submit3.csv\",index=False,header = True)\nsubmissions.head()","727f71fe":"count_vectorizer = CountVectorizer()\ntrain_cv = count_vectorizer.fit_transform(train_df2['text'])\ntest_cv = count_vectorizer.transform(test_df2[\"text\"])\ntest_cv.shape[0]","d401189e":"X_train_cv, X_test_cv, y_train_cv, y_test_cv =train_test_split(train_cv,train_df.target,test_size=0.2,random_state=2020)\n","c7296081":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n\nmodels=[]\nmodels.append((\"logreg\",LogisticRegression()))\nmodels.append((\"tree\",DecisionTreeClassifier()))\nmodels.append((\"lda\",LinearDiscriminantAnalysis()))\nmodels.append((\"svc\",SVC()))\nmodels.append((\"knn\",KNeighborsClassifier()))\nmodels.append((\"nb\",GaussianNB()))\n\nseed=7\nscoring='accuracy'\n\n\nfrom sklearn.model_selection import KFold \nfrom sklearn.model_selection import cross_val_score\nresult=[]\nnames=[]\n\nfor name,model in models:\n    #print(model)\n    kfold=KFold(n_splits=10,random_state=seed)\n    cv_result=cross_val_score(model,X_train_cv,y_train_cv,cv=kfold,scoring=scoring)\n    result.append(cv_result)\n    names.append(name)\n    print(\"%s %f %f\" % (name,cv_result.mean(),cv_result.std()))","77bd125d":"from sklearn import ensemble\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn import decomposition\nfrom sklearn import pipeline\n\n\n# classifier = SVC()\n\n# param_grid = {\n#     \"C\":[1,1.2,1.3,1.4,1.5],\n#     \"kernel\":['rbf','linear'],\n#     \"degree\":[2,3,4]\n# }\n# model = model_selection.GridSearchCV(\n#     estimator = classifier,\n#     param_grid  = param_grid,\n#     scoring = \"accuracy\",\n#     verbose = 10,\n#     n_jobs = 1,\n#     cv=5\n# )\n\n# model.fit(X_train_cv,y_train_cv)\n# print(model.best_score_)\n# print(model.best_estimator_.get_params())","bfc43f0f":"model = SVC(C=1.4,\n           kernel = 'rbf',\n           degree = 2,\n           gamma = 'scale',\n           max_iter=-1,\n           decision_function_shape=\"ovr\"\n           )\nmodel.fit(X_train_cv,y_train_cv)","ff13bb5f":"kfold=KFold(n_splits=10,random_state=seed)\ncv_result = cross_val_score(model,X_train_cv,y_train_cv,cv=kfold,scoring=scoring)\ncv_result.mean()","5b72ef5c":"sample = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\nsample\npredictions = model.predict(test_cv)\nsubmissions = pd.DataFrame({'id':sample.id,\n                      'target':predictions})\nsubmissions\nsubmissions.to_csv(\"Submit9.csv\",index=False,header = True)","dd313774":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","c99c5b7d":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nimport tokenization","b1bed718":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","531a0cbf":"random_state_split = 2\nDropout_num = 0\nlearning_rate = 6e-6\nvalid = 0.15\nepochs_num = 3\nbatch_size_num = 16\ntarget_corrected = False\ntarget_big_corrected = False","d1d077a6":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    \n    if Dropout_num == 0:\n        # Without Dropout\n        out = Dense(1, activation='sigmoid')(clf_output)\n    else:\n        # With Dropout(Dropout_num), Dropout_num > 0\n        x = Dropout(Dropout_num)(clf_output)\n        out = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","5eb4a97f":"def clean_tweets(tweet):\n    \"\"\"Removes links and non-ASCII characters\"\"\"\n    \n    tweet = ''.join([x for x in tweet if x in string.printable])\n    \n    # Removing URLs\n    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n    \n    return tweet","00ddfd31":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","531332e3":"def remove_punctuations(text):\n    punctuations = '@#!?+&*[]-%.:\/();$=><|{}^' + \"'`\"\n    \n    for p in punctuations:\n        text = text.replace(p, f' {p} ')\n\n    text = text.replace('...', ' ... ')\n    \n    if '...' not in text:\n        text = text.replace('..', ' ... ')\n    \n    return text","efc6fc3d":"abbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}","0821d3ad":"def convert_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word","2486d208":"def convert_abbrev_in_text(text):\n    tokens = word_tokenize(text)\n    tokens = [convert_abbrev(word) for word in tokens]\n    text = ' '.join(tokens)\n    return text","d19a823c":"module_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","b9750173":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","a07983f7":"\ntrain[\"text\"] = train[\"text\"].apply(lambda x: clean_tweets(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x: clean_tweets(x))\n\ntrain[\"text\"] = train[\"text\"].apply(lambda x: remove_emoji(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x: remove_emoji(x))\n\ntrain[\"text\"] = train[\"text\"].apply(lambda x: remove_punctuations(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x: remove_punctuations(x))\n\ntrain[\"text\"] = train[\"text\"].apply(lambda x: convert_abbrev_in_text(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x: convert_abbrev_in_text(x))","1f4413ea":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","f28b6423":"# Encode the text into tokens, masks, and segment flags\ntrain_input = bert_encode(train.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = train.target.values","5c9cc0e4":"model_BERT = build_model(bert_layer, max_len=160)\nmodel_BERT.summary()","274a562a":"checkpoint = ModelCheckpoint('model_BERT.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model_BERT.fit(\n    train_input, train_labels,\n    validation_split = valid,\n    epochs = epochs_num, # recomended 3-5 epochs\n    callbacks=[checkpoint],\n    batch_size = batch_size_num\n)","00a9c55c":"model_BERT.load_weights('model_BERT.h5')\ntest_pred_BERT = model_BERT.predict(test_input)\ntest_pred_BERT_int = test_pred_BERT.round().astype('int')","6e8d0fc1":"train_pred_BERT = model_BERT.predict(train_input)\ntrain_pred_BERT_int = train_pred_BERT.round().astype('int')","086c42db":"test_pred_BERT_int = test_pred_BERT_int.flatten()","34d005fd":"sub = pd.DataFrame({\"id\":test.id,\n                   \"target\":test_pred_BERT_int})\nsub.head()","f4643207":"sub.to_csv(\"sub1.csv\",index=False)","bfa4ae16":"# Neural Networks Implementation","2b6ac2e2":"**BERT using TFHUB**","02698f55":"* Thanks to very good kernel https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub\n* Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\n\n\n","377be10b":"# Conventional Approach"}}