{"cell_type":{"fe224020":"code","28fa1737":"code","9a1e8d31":"code","bb833818":"code","f95629c5":"code","425b8a25":"code","22da9f4c":"code","68b1842e":"code","2c2b8624":"code","7c079b3a":"code","7929fcef":"code","67ad4d24":"code","db33ef6e":"code","e5da1356":"code","3f592372":"code","256512f2":"code","4204f71a":"code","e8996b51":"code","805c64d1":"code","2f4468a1":"code","f75fd780":"code","7c5bf74b":"code","f2928d1b":"code","4b48c591":"code","65e50636":"code","dd9c94ab":"code","cfebf4f1":"markdown","874831c5":"markdown","9a07ddcf":"markdown","70140703":"markdown","e8a27a61":"markdown","4c7dd4fa":"markdown","34f17ef1":"markdown","d08d4cc8":"markdown","d4ff9359":"markdown","0abe1485":"markdown","d7ea0a1e":"markdown","83bd43cd":"markdown","04de8e40":"markdown","d34f0455":"markdown"},"source":{"fe224020":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.options.display.max_columns = 9999 #Making sure all columns appear\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","28fa1737":"FILEPATH_TRAIN = '\/kaggle\/input\/santander-value-prediction-challenge\/train.csv'\nFILEPATH_TEST = '\/kaggle\/input\/santander-value-prediction-challenge\/test.csv'","9a1e8d31":"train = pd.read_csv(FILEPATH_TRAIN, sep=',', engine='c') #Specify sep when using C engine\ntest = pd.read_csv(FILEPATH_TEST, sep=',', engine='c')\nprint(\"Shape of train:\",train.shape, '\\n',\"Shape of test:\",test.shape)","bb833818":"train.head()","f95629c5":"train.describe()","425b8a25":"import matplotlib.pyplot as plt\nimport seaborn as sns","22da9f4c":"plt.figure(figsize=(9, 7))\nplt.scatter(range(train.shape[0]), np.sort(train['target'].values))\nplt.xlabel('Index', fontsize=14)\nplt.ylabel('Target', fontsize=14)\nplt.title(\"Target Distribution\", fontsize=14)\nplt.show()","68b1842e":"plt.figure(figsize=(10,8))\nsns.distplot(train['target'].values, bins=50, kde=False)\nplt.xlabel('Target', fontsize=11)\nplt.title('Target Histogram', fontsize=11)\nplt.show()","2c2b8624":"plt.figure(figsize=(10,8))\nsns.distplot(np.log1p(train['target'].values), bins=50, kde=False)\nplt.xlabel('Target', fontsize=11)\nplt.title('Target Histogram', fontsize=11)\nplt.show()","7c079b3a":"train.isnull().sum()","7929fcef":"train.isnull().sum().any()","67ad4d24":"unique_vals = train.nunique().reset_index() #This drops NaN values by default\nunique_vals.columns = [\"Name\", \"Uniqueness\"]\nconst_d = unique_vals[unique_vals[\"Uniqueness\"]==1]\nconst_d.shape","db33ef6e":"str(const_d.Name.tolist())","e5da1356":"#Ignore any warnings that arise\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom scipy.stats import spearmanr","3f592372":"labels = []\nvalues = []\n\nfor col in train.columns:\n    if col not in [\"ID\", \"target\"]:\n        labels.append(col)\n        values.append(spearmanr(train[col].values, train['target'].values)[0])\n\ncorrelation_df = pd.DataFrame({'column_label':labels, 'correlation_val':values})        \ncorrelation_df = correlation_df.sort_values(by='correlation_val')\n\ncorrelation_df = correlation_df[(correlation_df['correlation_val']>0.1) | (correlation_df['correlation_val']<-0.1)]","256512f2":"index = np.arange(correlation_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(10,25))\nrec = ax.barh(index, np.array(correlation_df.correlation_val.values), color='r')\nax.set_yticks(index) #Set Y to index value of the df\nax.set_yticklabels(correlation_df.column_label.values, rotation='horizontal') #Define horizontal bar graph\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation coefficient of the variables\")\nplt.show()","4204f71a":"import seaborn as sns\n\ncolumns = correlation_df[(correlation_df['correlation_val']>0.11) | (correlation_df['correlation_val']<-0.11)].column_label.tolist()\n\ntmp = train[columns]\ncomat = tmp.corr(method='spearman') #Since we used spearman coefficient\nfig, ax = plt.subplots(figsize=(30,30))\n\nsns.heatmap(comat, square=True, cmap=\"RdYlGn\", annot=True)\nplt.title(\"Correlation Heatmap\", fontsize=18)\nplt.show()","e8996b51":"tr_x = train.drop(const_d.Name.tolist()+ [\"ID\", \"target\"], axis=1)\nte_x = test.drop(const_d.Name.tolist()+[\"ID\"], axis=1)\ntr_y = np.log1p(train['target'].values)","805c64d1":"from sklearn import ensemble\nmodel = ensemble.ExtraTreesRegressor(n_estimators=200, max_depth=20, max_features=0.5, n_jobs=-1, random_state=0)\nmodel.fit(tr_x, tr_y)","2f4468a1":"#Plot Importance factor\nfeatures = tr_x.columns.values\nimportance = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\nindices = np.argsort(importance)[::-1][:20]\n\nplt.figure(figsize=(14,14))\nplt.title(\"Feature Importances\")\nplt.bar(range(len(indices)), importance[indices], color=\"b\", yerr=std[indices])\nplt.xticks(range(len(indices)), features[indices], rotation='vertical')\nplt.xlim([-1, len(indices)])\nplt.show()","f75fd780":"import lightgbm as lgb","7c5bf74b":"def run_lgb(train_x, train_y, val_x, val_y, test_x):\n    parameters = {\n        'objective': 'regression',\n        'metric': 'rmse',\n        'num_leaves': 30,\n        'learning_rate': 0.01,\n        'bagging_fraction': 0.7,\n        'feature_fraction': 0.7,\n        'bagging_frequency': 5,\n        'bagging_seed': 2018,\n        'verbosity': -1\n    }\n    \n    lgtrain = lgb.Dataset(train_x, label=train_y)\n    lgval = lgb.Dataset(val_x, label=val_y)\n    evals_result = {}\n    model = lgb.train(parameters, lgtrain, 1000, valid_sets=[lgval], early_stopping_rounds=100, verbose_eval=200, evals_result=evals_result)\n    \n    pred_test_y = model.predict(test_x, num_iteration=model.best_iteration)\n    \n    return pred_test_y, model, evals_result","f2928d1b":"from sklearn.model_selection import KFold","4b48c591":"k_fold = KFold(n_splits=5, shuffle=True, random_state=2020)\npred_test_final = 0\n\nfor d_ind, v_ind in k_fold.split(tr_x):\n    \n    d_x, v_x = tr_x.loc[d_ind, :], tr_x.loc[v_ind, :]\n    d_y, v_y = tr_y[d_ind], tr_y[v_ind]\n    pred_test, model, evals_result = run_lgb(d_x, d_y, v_x, v_y, te_x)\n    pred_test_final += pred_test\n    \npred_test_final \/= 5\npred_test_final = np.expm1(pred_test_final)","65e50636":"final_df = pd.DataFrame({\"ID\":test[\"ID\"].values, \"target\":pred_test_final})\nfinal_df.to_csv(\"submission.csv\", index=False)","dd9c94ab":"#Feature importance for LightGBM\nfig, ax = plt.subplots(figsize=(14,20))\nlgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n#ax.grid(False)\nplt.title(\"LightGBM - Feature Importance\", fontsize=16)\nplt.show()","cfebf4f1":"**Plotting of the target variable**","874831c5":"**Sources:**\n\nhttps:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-baseline-santander-value\n\nhttps:\/\/www.kaggle.com\/c\/santander-value-prediction-challenge\/discussion\/59128\n\nhttps:\/\/en.wikipedia.org\/wiki\/Ordinal_data\n\nhttps:\/\/support.minitab.com\/en-us\/minitab-express\/1\/help-and-how-to\/modeling-statistics\/regression\/supporting-topics\/basics\/a-comparison-of-the-pearson-and-spearman-correlation-methods\/#:~:text=The%20Pearson%20correlation%20evaluates%20the%20linear%20relationship%20between%20two%20continuous%20variables.&text=The%20Spearman%20correlation%20coefficient%20is,evaluate%20relationships%20involving%20ordinal%20variables.\n\nhttps:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.nunique.html\n\nhttps:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.spearmanr.html\n\nhttps:\/\/seaborn.pydata.org\/generated\/seaborn.heatmap.html\n\nhttps:\/\/matplotlib.org\/3.1.0\/tutorials\/colors\/colormaps.html\n\nhttps:\/\/www.geeksforgeeks.org\/matplotlib-pyplot-xlim-in-python\/#:~:text=The%20xlim()%20function%20in,limits%20of%20the%20current%20axes.&text=Parameters%3A%20This%20method%20accept%20the,set%20the%20xlim%20to%20right.\n\nhttps:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.expm1.html","9a07ddcf":"**No null values. Always a good sign**","70140703":"TODO: Hyperparameter Tuning","e8a27a61":"256 Unique columns are present","4c7dd4fa":"**K fold cross validation for predictions in test set**","34f17ef1":"**Random values are the column names which mean the columns are anonymous**","d08d4cc8":"**Best way to display a right skewed distribution is to use a log scale**","d4ff9359":"**Spearman correlation is better to use due to the fact that it is computed based on ranks and this data is not linear where we can use Pearson correlation**","0abe1485":"**Creating Submission file**","d7ea0a1e":"**The Heatmap of Correlation**","83bd43cd":"**Baseline Light GBM** (TODO:Tune it)","04de8e40":"**Print the anonymised columns**","d34f0455":"**Much better**"}}