{"cell_type":{"53e5734e":"code","f80496ed":"code","d286b3f9":"code","29128fd9":"code","2de1758c":"code","48096f4f":"code","34721996":"code","a0de5102":"code","b2b06230":"code","463a51e3":"code","8e87699b":"code","a1249d75":"code","61298370":"code","ff6e2d87":"code","aaabf095":"code","127dd639":"code","a9bac67c":"code","a5d118b7":"code","b96e9553":"code","162667fc":"code","89a31b0f":"code","e84a95ff":"code","6f319580":"code","836c437f":"code","5e2fd5e8":"code","ab6501c3":"code","d37da431":"code","8f1b630d":"code","b2f61db1":"code","0102ff27":"code","1cf73c92":"code","4937124a":"code","7bad2cd8":"code","35cf9061":"code","799468a8":"code","2200cf0c":"code","9bc2879d":"code","88532289":"code","d9196439":"code","fdec4d1e":"code","b8d05d0f":"code","eb8908a4":"code","d23aaa1d":"code","8abe2de3":"code","a912eaae":"code","37f33b54":"code","f3eda686":"code","78be52c8":"code","63bb9928":"code","7baec68a":"code","ed251c16":"code","a9ef5f99":"code","922d6068":"code","abb64d66":"code","c1d5a883":"code","dfd5e08c":"code","12e1d703":"code","7003ba95":"code","56eb0af1":"code","29931424":"code","2d7571b6":"code","41fa399d":"code","25b47965":"code","ceab1b5b":"code","2b819d59":"code","8d400e74":"code","098d9b81":"code","db1cc70b":"code","8ab7a830":"code","cd7fcbad":"code","ca9c7f24":"code","335c6c46":"code","af5a93c2":"code","0a1e3126":"code","487c7e30":"code","0d04b863":"code","a4d1f401":"code","3734d911":"code","201ee429":"code","9cf6d985":"code","f24f1c3d":"code","4405a23a":"code","5bccc0fd":"code","8f6a3e72":"code","8e0687fc":"code","1b76e6ce":"code","64e5dbc3":"code","aedc893c":"code","e0aef359":"code","235b461b":"code","2b1e80dc":"code","1167d78d":"code","482f37e3":"code","b416418c":"code","409a73d2":"code","fa75fb6c":"code","ab00f76d":"code","1aa9145b":"code","38b9672a":"code","d42c2fd1":"code","f3bde0cf":"code","e40ec672":"code","1ee4596a":"code","43628d4c":"code","c1a747c9":"code","627f2f23":"code","fa94380a":"code","47c307f0":"code","160a733f":"code","ac821583":"code","b8fdbd69":"code","4fc71260":"code","b7fb20aa":"markdown","be3e1e76":"markdown","9b9e5f62":"markdown","2503d8cd":"markdown","a91386cb":"markdown","619d63bf":"markdown","e888cb4a":"markdown","79575805":"markdown","1d79e0f4":"markdown","a7299e57":"markdown","8564e122":"markdown","95280b04":"markdown","0cb49bcb":"markdown","abb5b786":"markdown","69b514f7":"markdown","9d602963":"markdown","8e6e495f":"markdown","3e6224e6":"markdown","00e8dd45":"markdown"},"source":{"53e5734e":"import numpy as np\nimport pandas as pd\n\ntrain = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/train.csv', index_col = 'PassengerId')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/test.csv', index_col = 'PassengerId')","f80496ed":"train.shape","d286b3f9":"test.shape","29128fd9":"train.tail()","2de1758c":"test.head()","48096f4f":"train.isna().sum()","34721996":"test.isna().sum()","a0de5102":"train.dtypes.unique()","b2b06230":"test.dtypes.unique()","463a51e3":"train.select_dtypes(include = ['object']).describe()","8e87699b":"train.drop('Survived', axis = 1).select_dtypes(exclude = ['object']).describe()","a1249d75":"target = train.Survived.copy()\ntarget","61298370":"target.isna().any()","ff6e2d87":"target.loc[target == 1].size \/ target.size","aaabf095":"target.describe()","127dd639":"train.drop('Survived', axis = 1).columns.equals(test.columns)","a9bac67c":"pd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nsns.set_style('whitegrid')","a5d118b7":"plt.figure(figsize = (16, 6))\nsns.countplot(x = train.Survived, palette = 'Purples_r')","b96e9553":"def plot_grid(data, fig_size, grid_size, plot_type, target = ''):\n    \"\"\"\n    Custom function for plotting grid of plots.\n    It takes: DataFrame of data, size of a grid, type of plots, string name of target variable;\n    And it outputs: grid of plots.\n    \"\"\"\n    fig = plt.figure(figsize = fig_size)\n    if plot_type == 'histplot':\n        for i, column_name in enumerate(data.select_dtypes(exclude = 'object').columns):\n            fig.add_subplot(grid_size[0], grid_size[1], i + 1)\n            plot = sns.histplot(data[column_name], kde = True, color = 'blueviolet', stat = 'count')\n    if plot_type == 'boxplot':\n        for i, column_name in enumerate(data.select_dtypes(exclude = 'object').columns):\n            fig.add_subplot(grid_size[0], grid_size[1], i + 1)\n            plot = sns.boxplot(x = data[column_name], color = 'blueviolet')\n    if plot_type == 'countplot':\n        target = data[target]\n        for i, column_name in enumerate(data.drop(target.name, axis = 1).columns):\n            fig.add_subplot(grid_size[0], grid_size[1], i + 1)\n            plot = sns.countplot(x = data[column_name], hue = target, palette = 'Purples_r')\n            plot.legend(loc = 'upper right', title = target.name)\n    plt.tight_layout()","162667fc":"plot_grid(train.drop('Survived', axis = 1), (16, 6), (2,3), 'histplot')","89a31b0f":"pd.pivot_table(train, index = 'Survived', values = ['Age', 'SibSp', 'Parch', 'Fare', 'Pclass'], aggfunc = 'mean')","e84a95ff":"plot_grid(train.select_dtypes(exclude = 'object').drop(['Fare', 'Age'], axis = 1), (16, 6), (1, 3), 'countplot', 'Survived')","6f319580":"print(f\"{pd.pivot_table(train, index = 'Survived', columns = 'Pclass', values = 'Name', aggfunc ='count')} \\n\\n\" +\n      f\"{pd.pivot_table(train, index = 'Survived', columns = 'SibSp', values = 'Name', aggfunc ='count')} \\n\\n\" +\n      f\"{pd.pivot_table(train, index = 'Survived', columns = 'Parch', values = 'Name', aggfunc ='count')}\")","836c437f":"plt.figure(figsize = (16, 6))\nsns.heatmap(train.corr(), \n            annot = True,\n            fmt = '.2f',\n            square = True,\n            cmap = \"Purples_r\", \n            mask = np.triu(train.corr()))","5e2fd5e8":"plot_grid(train.drop('Survived', axis = 1), (16, 6), (2,3), 'boxplot')","ab6501c3":"plot_grid(pd.concat([train.select_dtypes(include = 'object').drop(['Name', 'Ticket', 'Cabin'], axis = 1), target], axis = 1), (16, 6), (2,1), 'countplot', 'Survived')","d37da431":"print(f\"{pd.pivot_table(train, index = 'Survived', columns = 'Sex', values = 'Name', aggfunc ='count')} \\n\\n\" +\n      f\"{pd.pivot_table(train, index = 'Survived', columns = 'Embarked', values = 'Name', aggfunc ='count')}\")","8f1b630d":"train.select_dtypes(include = 'object').nunique().sort_values(ascending = False)","b2f61db1":"train_test = pd.concat([train.drop('Survived', axis = 1), test], keys = ['train', 'test'], axis = 0)\nmissing_values = pd.concat([train_test.isna().sum(),\n                            (train_test.isna().sum() \/ train_test.shape[0]) * 100], axis = 1, \n                            keys = ['Values missing', 'Percent of missing'])\nmissing_values.loc[missing_values['Percent of missing'] > 0].sort_values(ascending = False, by = 'Percent of missing').style.background_gradient('Purples')","0102ff27":"train_cleaning = train.drop('Survived', axis = 1).copy()\ntest_cleaning = test.copy()\n\ntrain_cleaning['Cabin'].fillna('none', inplace = True)\ntest_cleaning['Cabin'].fillna('none', inplace = True)\n\ntrain_cleaning['Ticket'].fillna('none', inplace = True)\ntest_cleaning['Ticket'].fillna('none', inplace = True)\n\ntrain_cleaning.loc[train_cleaning.Sex == 'male', 'Age'] = train_cleaning.loc[train_cleaning.Sex == 'male', 'Age'].fillna(train_cleaning.loc[train_cleaning.Sex == 'male', 'Age'].median())\ntrain_cleaning.loc[train_cleaning.Sex == 'female', 'Age'] = train_cleaning.loc[train_cleaning.Sex == 'female', 'Age'].fillna(train_cleaning.loc[train_cleaning.Sex == 'female', 'Age'].median())\ntest_cleaning.loc[test_cleaning.Sex == 'male', 'Age'] = test_cleaning.loc[test_cleaning.Sex == 'male', 'Age'].fillna(train_cleaning.loc[train_cleaning.Sex == 'male', 'Age'].median())\ntest_cleaning.loc[test_cleaning.Sex == 'female', 'Age'] = test_cleaning.loc[test_cleaning.Sex == 'female', 'Age'].fillna(train_cleaning.loc[train_cleaning.Sex == 'female', 'Age'].median())\n\ntrain_cleaning.loc[train_cleaning.Sex == 'male', 'Embarked'] = train_cleaning.loc[train_cleaning.Sex == 'male'].groupby('Pclass').Embarked.apply(lambda x: x.fillna(x.mode()[0]))\ntrain_cleaning.loc[train_cleaning.Sex == 'female', 'Embarked'] = train_cleaning.loc[train_cleaning.Sex == 'female'].groupby('Pclass').Embarked.apply(lambda x: x.fillna(x.mode()[0]))\n\ntrain_cleaning.loc[train_cleaning.Sex == 'male', 'Fare'] = train_cleaning.loc[train_cleaning.Sex == 'male'].groupby('Pclass').Fare.apply(lambda x: x.fillna(x.median()))\ntrain_cleaning.loc[train_cleaning.Sex == 'female', 'Fare'] = train_cleaning.loc[train_cleaning.Sex == 'female'].groupby('Pclass').Fare.apply(lambda x: x.fillna(x.median()))\nfor i in train_cleaning.Pclass.unique():\n    test_cleaning.loc[(test_cleaning.Pclass == i) & (test_cleaning.Sex == 'male'), 'Embarked'] = test_cleaning.loc[(test_cleaning.Pclass == i) & (test_cleaning.Sex == 'male'), 'Embarked'].fillna(train_cleaning.loc[(train_cleaning.Pclass == i) & (train_cleaning.Sex == 'male')].Embarked.mode()[0])\n    test_cleaning.loc[(test_cleaning.Pclass == i) & (test_cleaning.Sex == 'female'), 'Embarked'] = test_cleaning.loc[(test_cleaning.Pclass == i) & (test_cleaning.Sex == 'female'), 'Embarked'].fillna(train_cleaning.loc[(train_cleaning.Pclass == i) & (train_cleaning.Sex == 'female')].Embarked.mode()[0])\n    test_cleaning.loc[(test_cleaning.Pclass == i) & (test_cleaning.Sex == 'male'), 'Fare'] = test_cleaning.loc[(test_cleaning.Pclass == i) & (test_cleaning.Sex == 'male'), 'Fare'].fillna(train_cleaning.loc[(train_cleaning.Pclass == i) & (train_cleaning.Sex == 'male')].Fare.mode()[0])\n    test_cleaning.loc[(test_cleaning.Pclass == i) & (test_cleaning.Sex == 'female'), 'Fare'] = test_cleaning.loc[(test_cleaning.Pclass == i) & (test_cleaning.Sex == 'female'), 'Fare'].fillna(train_cleaning.loc[(train_cleaning.Pclass == i) & (train_cleaning.Sex == 'female')].Fare.mode()[0])\n    \n\n# train_cleaning['Embarked'].fillna('none', inplace = True)\n# test_cleaning['Embarked'].fillna('none', inplace = True)","1cf73c92":"train_cleaning.isnull().sum().max() + test_cleaning.isnull().sum().max()","4937124a":"train_test_cleaning = pd.concat([train_cleaning, test_cleaning], keys = ['train', 'test'], axis = 0)\ntrain_test_cleaning","7bad2cd8":"train_test_cleaning['CabinLetter'] = train_test_cleaning.Cabin.str.split().apply(lambda x: x[-1][0].strip().lower() if x[0] != 'none' else np.nan)\ntrain_test_cleaning['TicketLetters'] = train_test_cleaning.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.', '').replace('\/', '').lower() \n                                                                        if len(x.split(' ')[:-1]) > 0 else np.nan)\n# train_test_cleaning['CabinIsNull'] = train_test_cleaning.Cabin.apply(lambda x: 1 if x == 'none' else 0)\n# train_test_cleaning['TicketIsNull'] = train_test_cleaning.Ticket.apply(lambda x: 1 if x == 'none' else 0)\n# train_test_cleaning['EmbarkedIsNull'] = train_test_cleaning.Embarked.apply(lambda x: 1 if x == 'none' else 0)","35cf9061":"train_cleaning_new = train_test_cleaning.xs('train').copy()\ntest_cleaning_new = train_test_cleaning.xs('test').copy()\n\ntrain_cleaning_new.loc[train_cleaning_new.Sex == 'male'].groupby('Pclass').CabinLetter.apply(lambda x: x.value_counts().index[0])","799468a8":"train_cleaning_new.loc[train_cleaning_new.Sex == 'female'].groupby('Pclass').CabinLetter.apply(lambda x: x.value_counts().index[0])","2200cf0c":"train_cleaning_new.loc[train_cleaning_new.Sex == 'male'].groupby('Pclass').TicketLetters.apply(lambda x: x.value_counts().index[0])","9bc2879d":"train_cleaning_new.loc[train_cleaning_new.Sex == 'female'].groupby('Pclass').TicketLetters.apply(lambda x: x.value_counts().index[0])","88532289":"# train_cleaning_new['CabinLetter'] = train_cleaning_new.groupby('Pclass')['CabinLetter'].apply(lambda x: x.fillna(x.mode()[0]))\n\ntrain_cleaning_new.loc[train_cleaning_new.Sex == 'male', 'CabinLetter'] = train_cleaning_new.loc[train_cleaning_new.Sex == 'male'].groupby('Pclass')['CabinLetter'].apply(lambda x: x.fillna(x.mode()[0]))\ntrain_cleaning_new.loc[train_cleaning_new.Sex == 'female', 'CabinLetter'] = train_cleaning_new.loc[train_cleaning_new.Sex == 'female'].groupby('Pclass')['CabinLetter'].apply(lambda x: x.fillna(x.mode()[0]))\n\ntrain_cleaning_new.loc[train_cleaning_new.Sex == 'male', 'TicketLetters'] = train_cleaning_new.loc[train_cleaning_new.Sex == 'male'].groupby('Pclass')['TicketLetters'].apply(lambda x: x.fillna(x.mode()[0]))\ntrain_cleaning_new.loc[train_cleaning_new.Sex == 'female', 'TicketLetters'] = train_cleaning_new.loc[train_cleaning_new.Sex == 'female'].groupby('Pclass')['TicketLetters'].apply(lambda x: x.fillna(x.mode()[0]))\n\nfor i in train_cleaning_new.Pclass.unique():\n    test_cleaning_new.loc[(test_cleaning_new.Pclass == i) & (test_cleaning_new.Sex == 'male'), 'CabinLetter'] = test_cleaning_new.loc[(test_cleaning_new.Pclass == i) & (test_cleaning_new.Sex == 'male'), 'CabinLetter'].fillna(train_cleaning_new.loc[(train_cleaning_new.Pclass == i) & (train_cleaning_new.Sex == 'male')].CabinLetter.mode()[0])\n    test_cleaning_new.loc[(test_cleaning_new.Pclass == i) & (test_cleaning_new.Sex == 'female'), 'CabinLetter'] = test_cleaning_new.loc[(test_cleaning_new.Pclass == i) & (test_cleaning_new.Sex == 'female'), 'CabinLetter'].fillna(train_cleaning_new.loc[(train_cleaning_new.Pclass == i) & (train_cleaning_new.Sex == 'female')].CabinLetter.mode()[0])\n    \n    test_cleaning_new.loc[(test_cleaning_new.Pclass == i) & (test_cleaning_new.Sex == 'male'), 'TicketLetters'] = test_cleaning_new.loc[(test_cleaning_new.Pclass == i) & (test_cleaning_new.Sex == 'male'), 'TicketLetters'].fillna(train_cleaning_new.loc[(train_cleaning_new.Pclass == i) & (train_cleaning_new.Sex == 'male')].TicketLetters.mode()[0])\n    test_cleaning_new.loc[(test_cleaning_new.Pclass == i) & (test_cleaning_new.Sex == 'female'), 'TicketLetters'] = test_cleaning_new.loc[(test_cleaning_new.Pclass == i) & (test_cleaning_new.Sex == 'female'), 'TicketLetters'].fillna(train_cleaning_new.loc[(train_cleaning_new.Pclass == i) & (train_cleaning_new.Sex == 'female')].TicketLetters.mode()[0])\n\n    \ntrain_test_cleaning = pd.concat([train_cleaning_new, test_cleaning_new], keys = ['train', 'test'], axis = 0)","d9196439":"train.loc[:, ['Fare', 'Age']].select_dtypes(exclude = ['object']).describe()","fdec4d1e":"train_test_cleaning['CabinNumbers'] = train_test_cleaning.Cabin.apply(lambda x: int(x[1:]) if x != 'none' else 0)\n\ntrain_test_cleaning['TicketNumbers'] = train_test_cleaning.Ticket.apply(lambda x: int(x) if x.isnumeric() else 0 if x == 'none'\n                                                                        else int(x.split(' ')[-1]) if (x.split(' ')[-1]).isnumeric() else 0)\ntrain_test_cleaning['TicketNumbersGroup'] = train_test_cleaning['TicketNumbers'].apply(lambda x: 0 if (x == 0)\n                                                                                       else 1 if (x > 0 and x <= 100000)\n                                                                                       else 2 if (x > 100000 and x <= 260000)                                                                    \n                                                                                       else 3 if (x > 260000 and x <= 380000)\n                                                                                       else 4 if (x > 380000 and x <= 538000)\n                                                                                       else 5)\n\ntrain_test_cleaning['TicketIsNumeric'] = train_test_cleaning.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\n\ntrain_test_cleaning['FamilySize'] = train_test_cleaning.SibSp + train_test_cleaning.Parch + 1\ntrain_test_cleaning['FamilySize'] = train_test_cleaning['FamilySize'].apply(lambda x: 0 if (x == 1)\n                                                                            else 1 if (x == 2 or x == 3)\n                                                                            else 2)\ntrain_test_cleaning['IsAlone'] = train_test_cleaning['FamilySize'].apply(lambda x: 1 if (x == 1) else 0)\n\n# train_test_cleaning['AgeGroup'] = train_test_cleaning['Age'].apply(lambda x: 0 if (x < 25) \n#                                                                    else 1 if (x >= 25 and x < 39)                                                                    \n#                                                                    else 2 if (x >= 39 and x < 53)\n#                                                                    else 3)\n\ntrain_test_cleaning['AgeGroup'] = train_test_cleaning['Age'].apply(lambda x: 0 if (x < 10) \n                                                                   else 1 if (x >= 10 and x < 20)                                                                    \n                                                                   else 2 if (x >= 20 and x < 30)\n                                                                   else 3 if (x >= 30 and x < 40)\n                                                                   else 4 if (x >= 40 and x < 50)\n                                                                   else 5 if (x >= 50 and x < 60)\n                                                                   else 6 if (x >= 60 and x < 70)\n                                                                   else 7 if (x >= 70 and x < 80)\n                                                                   else 8)\n\ntrain_test_cleaning['FareGroup'] = train_test_cleaning['Fare'].apply(lambda x: 0 if (x < 10.04) \n                                                                     else 1 if (x >= 10.04 and x < 24.46)  \n                                                                     else 2 if (x >= 24.46 and x < 33.5)                                                            \n                                                                     else 3)\n\ntrain_test_cleaning['TicketLettersGroup'] = train_test_cleaning.TicketLetters.apply(lambda x: 0 if x == 'pc' \n                                                                                    else 3 if x in ['stono', 'stono2', 'sotono2', 'stonoq', 'aq3']\n                                                                                    else 2 if x in ['sotonoq', 'fa', 'a5', 'ca', 'fcc', 'scow', 'casoton', 'a4', 'wc', 'swpp', 'c']                                                                \n                                                                                    else 1)\n\n# train_test_cleaning['Surname'] = train_test_cleaning['Name'].apply(lambda x: x.split(',')[0].lower())\n\ntrain_test_cleaning['Embarked'] = train_test_cleaning['Embarked'].str.lower()","b8d05d0f":"from scipy.stats import skew, boxcox_normmax\nfrom scipy.special import boxcox1p\n\nlamb = boxcox_normmax(train_test_cleaning.loc['train', 'Fare'] + 1)\ntrain_test_cleaning.loc['train', 'Fare'] = boxcox1p(train_test_cleaning.loc['train', 'Fare'], lamb).values\ntrain_test_cleaning.loc['test', 'Fare'] = boxcox1p(train_test_cleaning.loc['test', 'Fare'], lamb).values\n\nlamb = boxcox_normmax(train_test_cleaning.loc['train', 'CabinNumbers'] + 1)\ntrain_test_cleaning.loc['train', 'CabinNumbers'] = boxcox1p(train_test_cleaning.loc['train', 'CabinNumbers'], lamb).values\ntrain_test_cleaning.loc['test', 'CabinNumbers'] = boxcox1p(train_test_cleaning.loc['test', 'CabinNumbers'], lamb).values\n\nlamb = boxcox_normmax(train_test_cleaning.loc['train', 'TicketNumbers'] + 1)\ntrain_test_cleaning.loc['train', 'TicketNumbers'] = boxcox1p(train_test_cleaning.loc['train', 'TicketNumbers'], lamb).values\ntrain_test_cleaning.loc['test', 'TicketNumbers'] = boxcox1p(train_test_cleaning.loc['test', 'TicketNumbers'], lamb).values","eb8908a4":"train_test_cleaning","d23aaa1d":"train_cleaning_target_cleaned = pd.concat([train_test_cleaning.xs('train'), target], axis = 1)\ntrain_cleaning_target_cleaned","8abe2de3":"print(f\"{pd.pivot_table(train_cleaning_target_cleaned, index = 'Survived', columns = 'CabinLetter', values = 'Name', aggfunc ='count')} \\n\\n\" +\n      f\"{pd.pivot_table(train_cleaning_target_cleaned, index = 'Survived', values = 'TicketNumbers', aggfunc = (lambda x: x.mode()[0]))} \\n\\n\" +\n      f\"{pd.pivot_table(train_cleaning_target_cleaned, index = 'Survived', columns = 'TicketIsNumeric', values = 'Name', aggfunc ='count')} \\n\\n\" +\n      \n      f\"{pd.pivot_table(train_cleaning_target_cleaned, index = 'Survived', columns = 'AgeGroup', values = 'Name', aggfunc ='count')} \\n\\n\" +\n      f\"{pd.pivot_table(train_cleaning_target_cleaned, index = 'Survived', columns = 'FareGroup', values = 'Name', aggfunc ='count')} \\n\\n\" +\n      f\"{pd.pivot_table(train_cleaning_target_cleaned, index = 'Survived', columns = 'TicketLettersGroup', values = 'Name', aggfunc ='count')} \\n\\n\" +\n      f\"{pd.pivot_table(train_cleaning_target_cleaned, index = 'Survived', columns = 'TicketNumbersGroup', values = 'Name', aggfunc ='count')} \\n\\n\" +\n      f\"{pd.pivot_table(train_cleaning_target_cleaned, index = 'Survived', columns = 'IsAlone', values = 'Name', aggfunc ='count')} \\n\\n\" +\n      \n      f\"{pd.pivot_table(train_cleaning_target_cleaned, index = 'Survived', columns = 'FamilySize', values = 'Name', aggfunc ='count')}\")","a912eaae":"pd.pivot_table(train_cleaning_target_cleaned, index = 'Survived', columns = 'TicketLetters', values = 'Name', aggfunc = 'count')","37f33b54":"train_cleaning_target_cleaned.select_dtypes(include = 'object').nunique().sort_values(ascending = False)","f3eda686":"plot_grid(train_cleaning_target_cleaned.loc[:,['Age', 'Fare', 'TicketNumbers', 'CabinNumbers']], (16, 6), (2, 3), 'histplot')","78be52c8":"plot_grid(train_cleaning_target_cleaned.drop(['Name', 'Ticket', 'Cabin', 'Age', 'Fare', 'TicketNumbers', 'TicketLetters', 'CabinNumbers'],\n                                             axis = 1), (16, 10), (5, 3), 'countplot', 'Survived')","63bb9928":"pd.crosstab(index = train_cleaning_target_cleaned.TicketLetters , columns= train_cleaning_target_cleaned.Survived, normalize = 'index' ). \\\nsort_values(by = 1).plot.bar(figsize = (15, 7), stacked = True, color = {0: 'grey', \n                                                                         1: 'purple'})\nplt.axhline(y = 0.8, color = 'r', linestyle = '-')\nplt.axhline(y = 0.65, color = 'g', linestyle = '-')","7baec68a":"from matplotlib import ticker\n# 'Age', 'Fare', 'TicketNumbers', 'CabinNumbers'\nfig, axs = plt.subplots(4, 1, figsize = (16, 16))\nsns.histplot(hue = train_cleaning_target_cleaned.Survived, x = train_cleaning_target_cleaned.Age, palette = {0 : 'black', 1 : 'purple'}, ax = axs[0])\naxs[0].set_title('Age distribution')\nsns.histplot(hue = train_cleaning_target_cleaned.Survived, x = train_cleaning_target_cleaned.Fare, palette = {0 : 'black', 1 : 'purple'}, ax = axs[1])\naxs[1].xaxis.set_major_locator(ticker.MultipleLocator(25))\naxs[1].xaxis.set_major_formatter(ticker.ScalarFormatter())\naxs[1].set_title('Fare distribution')\nsns.histplot(hue = train_cleaning_target_cleaned.Survived, x = train_cleaning_target_cleaned.TicketNumbers, palette = {0 : 'black', 1 : 'purple'}, ax = axs[2])\naxs[2].set_title('TicketNumbers distribution')\nsns.histplot(hue = train_cleaning_target_cleaned.Survived, x = train_cleaning_target_cleaned.CabinNumbers, palette = {0 : 'black', 1 : 'purple'}, ax = axs[3])\naxs[3].set_title('CabinNumbers distribution')\nplt.tight_layout()","ed251c16":"plt.figure(figsize = (16,10))\nsns.heatmap(train_cleaning_target_cleaned.corr(),\n            annot = True,\n            annot_kws = {\"size\": 13},\n            fmt = '.2f',\n            square = True,\n            cmap = \"Purples_r\",\n            mask = np.triu(train_cleaning_target_cleaned.corr()))","a9ef5f99":"to_drop = ['Name',\n           'Ticket',\n           'Cabin']\n\ntrain_test_cleaned = train_test_cleaning.drop(to_drop, axis = 1).copy()\ntrain_test_cleaned","922d6068":"label_cols = ['AgeGroup', 'FamilySize', 'TicketLettersGroup', 'Pclass', 'IsAlone', 'TicketIsNumeric', 'Sex']\nonehot_cols = ['CabinLetter', 'Embarked']\nnumerical_cols = ['SibSp', 'Parch', 'Fare', 'CabinNumbers', 'TicketNumbers']","abb64d66":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n# One-hot encoding\ntrain_test_onehot = pd.get_dummies(train_test_cleaned[onehot_cols])\nX_train_full_onehot, X_test_onehot = train_test_onehot.xs('train').reset_index(), train_test_onehot.xs('test').reset_index()\n\nX_train_full, X_test = train_test_cleaned.xs('train'), train_test_cleaned.xs('test')\n# Label encoding\nX_train_full_labeled = pd.DataFrame()\nX_test_labeled = pd.DataFrame()\nfor col in label_cols:\n    encoder = LabelEncoder()\n    encoder.fit(X_train_full[col])\n    \n    encoded_train = pd.Series(encoder.transform(X_train_full[col]), name = col)\n    X_train_full_labeled = pd.concat([X_train_full_labeled, encoded_train], axis = 1)\n    \n    encoded_test = pd.Series(encoder.transform(X_test[col]), name = col)\n    X_test_labeled = pd.concat([X_test_labeled, encoded_test], axis = 1)\n# Numerical features scaling\nscaler = StandardScaler()\nscaler.fit(X_train_full[numerical_cols])\nX_train_full_scaled = pd.DataFrame(scaler.transform(X_train_full[numerical_cols]), columns = numerical_cols)\nX_test_scaled = pd.DataFrame(scaler.transform(X_test[numerical_cols]), columns = numerical_cols)\n# Concatenating it all together\nX_train_full = pd.concat([X_train_full_onehot, \n                          X_train_full_labeled, \n                          X_train_full_scaled], axis = 1)\nX_train_full.set_index('PassengerId', inplace = True)\nX_test = pd.concat([X_test_onehot, \n                    X_test_labeled, \n                    X_test_scaled], axis = 1)\nX_test.set_index('PassengerId', inplace = True)\nX_train_full","c1d5a883":"X_test","dfd5e08c":"y_train_full = target\ny_train_full","12e1d703":"# train_test_cleaned_male = train_test_cleaned.loc[train_test_cleaned.Sex == 'male'].copy()\n# train_test_cleaned_female = train_test_cleaned.loc[train_test_cleaned.Sex == 'female'].copy()","7003ba95":"# from sklearn.preprocessing import LabelEncoder\n# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n# # Male\n# # One-hot encoding\n# train_test_onehot = pd.get_dummies(train_test_cleaned_male[onehot_cols])\n# X_train_full_onehot, X_test_onehot = train_test_onehot.xs('train').reset_index(), train_test_onehot.xs('test').reset_index()\n\n# X_train_full, X_test = train_test_cleaned_male.xs('train'), train_test_cleaned_male.xs('test')\n# # Label encoding\n# X_train_full_labeled = pd.DataFrame()\n# X_test_labeled = pd.DataFrame()\n# for col in label_cols:\n#     encoder = LabelEncoder()\n#     encoder.fit(X_train_full[col])\n    \n#     encoded_train = pd.Series(encoder.transform(X_train_full[col]), name = col)\n#     X_train_full_labeled = pd.concat([X_train_full_labeled, encoded_train], axis = 1)\n    \n#     encoded_test = pd.Series(encoder.transform(X_test[col]), name = col)\n#     X_test_labeled = pd.concat([X_test_labeled, encoded_test], axis = 1)\n# # Numerical features scaling\n# scaler = StandardScaler()\n# scaler.fit(X_train_full[numerical_cols])\n# X_train_full_scaled = pd.DataFrame(scaler.transform(X_train_full[numerical_cols]), columns = numerical_cols)\n# X_test_scaled = pd.DataFrame(scaler.transform(X_test[numerical_cols]), columns = numerical_cols)\n# # Concatenating it all together\n# X_train_full_male = pd.concat([X_train_full_onehot, \n#                           X_train_full_labeled, \n#                           X_train_full_scaled], axis = 1)\n# X_train_full_male.set_index('PassengerId', inplace = True)\n# X_test_male = pd.concat([X_test_onehot, \n#                     X_test_labeled, \n#                     X_test_scaled], axis = 1)\n# X_test_male.set_index('PassengerId', inplace = True)\n# X_train_full_male","56eb0af1":"# y_train_full_male = target.loc[target.index.isin(X_train_full_male.index)].copy()\n# y_train_full_male","29931424":"# # Female\n# # One-hot encoding\n# train_test_onehot = pd.get_dummies(train_test_cleaned_female[onehot_cols])\n# X_train_full_onehot, X_test_onehot = train_test_onehot.xs('train').reset_index(), train_test_onehot.xs('test').reset_index()\n\n# X_train_full, X_test = train_test_cleaned_female.xs('train'), train_test_cleaned_female.xs('test')\n# # Label encoding\n# X_train_full_labeled = pd.DataFrame()\n# X_test_labeled = pd.DataFrame()\n# for col in label_cols:\n#     encoder = LabelEncoder()\n#     encoder.fit(X_train_full[col])\n    \n#     encoded_train = pd.Series(encoder.transform(X_train_full[col]), name = col)\n#     X_train_full_labeled = pd.concat([X_train_full_labeled, encoded_train], axis = 1)\n    \n#     encoded_test = pd.Series(encoder.transform(X_test[col]), name = col)\n#     X_test_labeled = pd.concat([X_test_labeled, encoded_test], axis = 1)\n# # Numerical features scaling\n# scaler = StandardScaler()\n# scaler.fit(X_train_full[numerical_cols])\n# X_train_full_scaled = pd.DataFrame(scaler.transform(X_train_full[numerical_cols]), columns = numerical_cols)\n# X_test_scaled = pd.DataFrame(scaler.transform(X_test[numerical_cols]), columns = numerical_cols)\n# # Concatenating it all together\n# X_train_full_female = pd.concat([X_train_full_onehot, \n#                           X_train_full_labeled, \n#                           X_train_full_scaled], axis = 1)\n# X_train_full_female.set_index('PassengerId', inplace = True)\n# X_test_female = pd.concat([X_test_onehot, \n#                     X_test_labeled, \n#                     X_test_scaled], axis = 1)\n# X_test_female.set_index('PassengerId', inplace = True)\n# X_train_full_female","2d7571b6":"# y_train_full_female = target.loc[target.index.isin(X_train_full_female.index)].copy()\n# y_train_full_female","41fa399d":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\n\ntf.random.set_seed(1)","25b47965":"early_stopping = keras.callbacks.EarlyStopping(\n    patience = 10,#100 80 40 20 10\n    min_delta = 0.001,\n    restore_best_weights = True,\n)\n\nk = 5\nkf = StratifiedKFold(n_splits = k, shuffle = True, random_state = 1)\n\nhistory = pd.DataFrame(columns = ['ValAccuracy', 'TrainAccuracy', 'StoppedEpoch'], index = range(k))\n\nfor i, (train_idx, test_idx) in enumerate(kf.split(X_train_full, y_train_full)):\n    X_train = X_train_full.iloc[train_idx]\n    y_train = y_train_full.iloc[train_idx]\n    X_valid = X_train_full.iloc[test_idx]\n    y_valid = y_train_full.iloc[test_idx]\n    \n    model = keras.Sequential([layers.BatchNormalization(input_shape = [X_train.shape[1]]),\n                              layers.Dense(units = 16, activation = 'relu'),\n                              layers.Dropout(rate = 0.1),\n                              \n                              layers.BatchNormalization(),\n                              layers.Dense(units = 16, activation = 'relu'),\n                              layers.Dropout(rate = 0.1),\n                              \n                              layers.BatchNormalization(),\n                              layers.Dense(units = 1, activation = 'sigmoid')])\n    \n    model.compile(optimizer = 'adam',\n                  loss = 'binary_crossentropy',\n                  metrics = ['binary_accuracy'])\n    \n    model.fit(X_train, y_train,\n              validation_data = (X_valid, y_valid),\n              batch_size = 512,\n              epochs = 1000,\n              callbacks = [early_stopping],\n              verbose = 0,)\n    \n    history.loc[i, 'ValAccuracy'] = model.history.history['val_binary_accuracy']\n    history.loc[i, 'TrainAccuracy'] = model.history.history['binary_accuracy']\n    history.loc[i, 'StoppedEpoch'] = early_stopping.stopped_epoch","ceab1b5b":"fig, axs = plt.subplots(k, figsize = (16, 32))\nfig.suptitle(f'Train and validation accuracy for {k}-fold validation\\n\\n', fontsize = 16)\nfor i in range(k):\n    sns.lineplot(data = history.loc[i, 'ValAccuracy'], ax = axs[i], color = 'red')\n    sns.lineplot(data = history.loc[i, 'TrainAccuracy'], ax = axs[i], color = 'blue')\n    axs[i].set_title(f'{i+1} fold')\n    axs[i].legend(['Validation', 'Train'])\n    axs[i].set_ylabel('Accuracy')\n    axs[i].set_xlabel('Epochs')\n    \nplt.tight_layout()","2b819d59":"round(history.StoppedEpoch.mean())","8d400e74":"model = keras.Sequential([layers.BatchNormalization(input_shape = [X_train_full.shape[1]]),\n                              layers.Dense(units = 16, activation = 'relu'),\n                              layers.Dropout(rate = 0.1),\n                              \n                              layers.BatchNormalization(),\n                              layers.Dense(units = 16, activation = 'relu'),\n                              layers.Dropout(rate = 0.1),\n                              \n                              layers.BatchNormalization(),\n                              layers.Dense(units = 1, activation = 'sigmoid')])\n\nmodel.compile(optimizer = 'adam',\n              loss = 'binary_crossentropy',\n              metrics = ['binary_accuracy'])\n\nhistory = model.fit(X_train_full, y_train_full,\n                    batch_size = 512,\n                    epochs = round(history.StoppedEpoch.mean()),\n                    verbose = 0)","098d9b81":"print(f\"Train mean: {np.mean(history.history['binary_accuracy'])}\"+\"\\n\"+\n      f\"Train std: {np.std(history.history['binary_accuracy'])}\")","db1cc70b":"predictions_nn = model.predict(X_test)","8ab7a830":"predictions_nn[predictions_nn > 0.5] = 1\npredictions_nn[predictions_nn <= 0.5] = 0","cd7fcbad":"predictions_nn[predictions_nn == 1].size","ca9c7f24":"predictions_nn[predictions_nn == 0].size","335c6c46":"predictions_nn.flatten().astype('int64')","af5a93c2":"from sklearn.model_selection import cross_val_score, cross_validate\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier","0a1e3126":"def test_estimators(X, y, estimators, labels, cv):\n    ''' \n    A function for testing multiple estimators.\n    It takes: full train data and target, list of estimators, \n              list of labels or names of estimators,\n              cross validation splitting strategy;\n    And it returns: a DataFrame of table with results of tests\n    '''\n    result_table = pd.DataFrame()\n\n    row_index = 0\n    for est, label in zip(estimators, labels):\n\n        est_name = label\n        result_table.loc[row_index, 'Model Name'] = est_name\n\n        cv_results = cross_validate(est,\n                                    X,\n                                    y,\n                                    cv = cv,\n                                    n_jobs = -1)\n\n        result_table.loc[row_index, 'Test accuracy'] = cv_results['test_score'].mean()\n        result_table.loc[row_index, 'Test Std'] = cv_results['test_score'].std()\n        result_table.loc[row_index, 'Fit Time'] = cv_results['fit_time'].mean()\n\n        row_index += 1\n\n    result_table.sort_values(by=['Test accuracy'], ascending = False, inplace = True)\n\n    return result_table","487c7e30":"X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, \n                                                      y_train_full, \n                                                      stratify = y_train_full,\n                                                      train_size = 0.1,\n                                                      random_state = 1)\ny_train","0d04b863":"logreg = LogisticRegression()\ndt = DecisionTreeClassifier(random_state = 1)\nrf = RandomForestClassifier()\nxgb = XGBClassifier()\nlgbm = LGBMClassifier()\ncb = CatBoostClassifier(allow_writing_files = False, logging_level = 'Silent')\nlsvc = LinearSVC()\n\nestimators = [logreg,\n              dt,\n              rf,\n              lgbm, \n              cb,\n              lsvc,\n              xgb]\n\nlabels = ['LogRegression',\n          'DecisionTree',\n          'RandomForest',\n          'LGBM',\n          'CatBoost',\n          'LSVC',\n          'XGB']\n\nresults = test_estimators(X_train_full, y_train_full, estimators, labels, cv = 5)\nresults.style.background_gradient(cmap = 'Purples')","a4d1f401":"import optuna\nfrom optuna.trial import TrialState\n\nimport keras.optimizers\n\nfrom sklearn.metrics import accuracy_score\n\nfrom xgboost import DMatrix, cv\n\ndef define_nn(trial):    \n    model = keras.Sequential()\n    \n    n_layers = trial.suggest_int('n_layers', 1, 3)\n    \n    for i in range(n_layers):\n        units = trial.suggest_categorical(f'units_{i}', [8, 16, 32, 64, 128])\n        rate = trial.suggest_float(f'rate_{i}', 0.2, 0.5)\n        if (i == 0):\n            model.add(layers.BatchNormalization(input_shape = [X_train_full.shape[1]]))\n            model.add(layers.Dense(units = units, activation = 'relu'))\n            model.add(layers.Dropout(rate = rate))\n        else:\n            model.add(layers.BatchNormalization())\n            model.add(layers.Dense(units = units, activation = 'relu'))\n            model.add(layers.Dropout(rate = rate))\n    \n    model.add(layers.BatchNormalization())\n    model.add(layers.Dense(units = 1, activation = 'sigmoid'))\n    \n    return model\n\ndef define_cb(trial):    \n    params = {\n        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"bootstrap_type\": trial.suggest_categorical(\n            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n        ),\n        \n        'n_estimators': trial.suggest_int('n_estimators', 100, 2000, 50),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-1, log = True),\n        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-5, 1e-1, log = True),\n        'min_child_samples': trial.suggest_int('min_child_samples', 2, 20),\n        'random_strength': trial.suggest_float('random_strength', 0.05, 1, log = True)\n    }\n\n    if params[\"bootstrap_type\"] == \"Bayesian\":\n        params[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif params[\"bootstrap_type\"] == \"Bernoulli\":\n        params[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n\n    model = CatBoostClassifier(**params)\n    \n    return model\n\ndef define_lgbm(trial):    \n    params = {\n        \"objective\": \"binary\",\n        \"metric\": \"binary_logloss\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n        \n        'max_depth': trial.suggest_int('max_depth', 2, 12),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 2000, 50),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-1, log = True),\n    }\n\n    model = LGBMClassifier(**params)\n    \n    return model\n\ndef define_logreg(trial):    \n    params = {\n        'verbose': 0,\n        'solver': 'saga',\n        'tol': trial.suggest_float('tol', 1e-5, 1e-1, log = True),\n        'C': trial.suggest_float('C', 1e-10, 1e10, log = True),\n        'fit_intercept': trial.suggest_categorical('fit_intercept', [True, False]),\n        'max_iter': trial.suggest_int('max_iter', 50, 2000, 50),\n        'penalty': trial.suggest_categorical('penalty', ['l1', 'l2', 'elasticnet', 'none']),\n        'l1_ratio': trial.suggest_float('l1_ratio', 1e-5, 1, log = True),\n    }\n\n    model = LogisticRegression(**params)\n    \n    return model\n\ndef define_lsvc(trial):    \n    params = {\n        'verbose': 0,\n        'tol': trial.suggest_float('tol', 1e-5, 1e-1, log = True),\n        'C': trial.suggest_float('C', 1e-10, 1e10, log = True),\n        'fit_intercept': trial.suggest_categorical('fit_intercept', [True, False]),\n        'max_iter': trial.suggest_int('max_iter', 1000, 3000, 50),\n    }\n\n    model = LinearSVC(**params)\n    \n    return model\n\ndef define_rf(trial):    \n    params = {\n        'max_depth': trial.suggest_int('max_depth', 5, 20),\n        'max_features': trial.suggest_int('max_features', 5, 15),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n        'n_estimators': trial.suggest_int('n_estimators', 50, 1000, 25),\n    }\n\n    model = RandomForestClassifier(**params)\n    \n    return model\n\ndef objective(trial, model, X_train_full, y_train_full):\n    if (model == 'nn'):\n        \n#       Straight forward option\n\n        X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, stratify = y_train_full, train_size = 0.25)\n    \n        model = define_nn(trial)\n        optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD'])\n        lr = trial.suggest_float('lr', 1e-5, 1e-1, log = True)\n        optimizer = getattr(keras.optimizers, optimizer_name)(model.optimizer, lr = lr)\n\n        model.compile(optimizer = optimizer,\n                      loss = 'binary_crossentropy',\n                      metrics = ['binary_accuracy'])\n\n        epochs = trial.suggest_int('epochs', 10, 1000)\n        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256, 512, 1024])\n        model.fit(X_train, y_train,\n                  batch_size = batch_size,\n                  epochs = epochs,\n                  verbose = 0)\n        results = model.evaluate(X_valid, \n                                 y_valid,\n                                 batch_size = batch_size,)\n        \n        return results[1]\n\n#       Cross-validation option\n\n#         k = 5\n#         kf = StratifiedKFold(n_splits = k, shuffle = True, random_state = 1)\n#         history = []\n#         for i, (train_idx, test_idx) in enumerate(kf.split(X_train_full, y_train_full)):\n#             X_train = X_train_full.iloc[train_idx]\n#             y_train = y_train_full.iloc[train_idx]\n#             X_valid = X_train_full.iloc[test_idx]\n#             y_valid = y_train_full.iloc[test_idx]\n            \n#             model = define_nn(trial)\n#             optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD'])\n#             lr = trial.suggest_float('lr', 1e-5, 1e-1, log = True)\n#             optimizer = getattr(keras.optimizers, optimizer_name)(model.optimizer, lr = lr)\n        \n#             model.compile(optimizer = optimizer,\n#                           loss = 'binary_crossentropy',\n#                           metrics = ['binary_accuracy'])\n\n#             epochs = trial.suggest_int('epochs', 10, 1000)\n#             batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256, 512, 1024])\n#             model.fit(X_train, y_train,\n#                       batch_size = batch_size,\n#                       epochs = epochs,\n#                       verbose = 0)\n#             results = model.evaluate(X_valid, \n#                                      y_valid,\n#                                      batch_size = batch_size,)\n#             history.append(results[1])\n\n#             trial.report(np.mean(history), i)\n#             # Handle pruning based on the intermediate value.\n#             if trial.should_prune():\n#                 raise optuna.exceptions.TrialPruned()\n\n#         return np.mean(history)\n    \n    elif (model == 'cb'):\n        \n#       Straight forward option\n        \n#         X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, stratify = y_train_full, train_size = 0.25)\n        \n#         model = define_cb(trial)\n\n#         model.fit(X_train, y_train,\n#                   verbose = 0)\n\n#         preds = model.predict(X_valid)\n#         pred_labels = np.rint(preds)\n#         accuracy = accuracy_score(y_valid, pred_labels)\n\n#         trial.report(accuracy, i)\n#         # Handle pruning based on the intermediate value.\n#         if trial.should_prune():\n#             raise optuna.exceptions.TrialPruned()\n\n#         return accuracy\n\n#       Cross-validation option\n\n        k = 5\n        kf = StratifiedKFold(n_splits = k, shuffle = True, random_state = 1)\n\n        history = []\n        for i, (train_idx, test_idx) in enumerate(kf.split(X_train_full, y_train_full)):\n            X_train = X_train_full.iloc[train_idx]\n            y_train = y_train_full.iloc[train_idx]\n            X_valid = X_train_full.iloc[test_idx]\n            y_valid = y_train_full.iloc[test_idx]\n            \n            model = define_cb(trial)\n            \n            model.fit(X_train, y_train,\n                      verbose = 0)\n\n            preds = model.predict(X_valid)\n            pred_labels = np.rint(preds)\n            accuracy = accuracy_score(y_valid, pred_labels)\n            \n            history.append(accuracy)\n\n            trial.report(np.mean(history), i)\n            # Handle pruning based on the intermediate value.\n            if trial.should_prune():\n                raise optuna.exceptions.TrialPruned()\n\n        return np.mean(history)\n    \n    elif (model == 'lgbm'):\n        \n#       Straight forward option\n        \n#         X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, stratify = y_train_full, train_size = 0.25)\n        \n#         model = define_lgbm(trial)\n\n#         model.fit(X_train, y_train,\n#                   verbose = 0)\n\n#         preds = model.predict(X_valid)\n#         pred_labels = np.rint(preds)\n#         accuracy = accuracy_score(y_valid, pred_labels)\n\n#         trial.report(accuracy, i)\n#         # Handle pruning based on the intermediate value.\n#         if trial.should_prune():\n#             raise optuna.exceptions.TrialPruned()\n\n#         return accuracy\n\n#       Cross-validation option\n\n        k = 5\n        kf = StratifiedKFold(n_splits = k, shuffle = True, random_state = 1)\n\n        history = []\n        for i, (train_idx, test_idx) in enumerate(kf.split(X_train_full, y_train_full)):\n            X_train = X_train_full.iloc[train_idx]\n            y_train = y_train_full.iloc[train_idx]\n            X_valid = X_train_full.iloc[test_idx]\n            y_valid = y_train_full.iloc[test_idx]\n            \n            model = define_lgbm(trial)\n            \n            model.fit(X_train, y_train,\n                      verbose = 0)\n\n            preds = model.predict(X_valid)\n            pred_labels = np.rint(preds)\n            accuracy = accuracy_score(y_valid, pred_labels)\n            \n            history.append(accuracy)\n\n            trial.report(np.mean(history), i)\n            # Handle pruning based on the intermediate value.\n            if trial.should_prune():\n                raise optuna.exceptions.TrialPruned()\n\n        return np.mean(history)\n    \n    elif (model == 'logreg'):\n        \n#       Straight forward option\n        \n#         X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, stratify = y_train_full, train_size = 0.25)\n        \n#         model = define_logreg(trial)\n\n#         model.fit(X_train, y_train,)\n\n#         preds = model.predict(X_valid)\n#         pred_labels = np.rint(preds)\n#         accuracy = accuracy_score(y_valid, pred_labels)\n\n#         trial.report(accuracy, i)\n#         # Handle pruning based on the intermediate value.\n#         if trial.should_prune():\n#             raise optuna.exceptions.TrialPruned()\n\n#         return accuracy\n\n#       Cross-validation option\n\n        k = 5\n        kf = StratifiedKFold(n_splits = k, shuffle = True, random_state = 1)\n\n        history = []\n        for i, (train_idx, test_idx) in enumerate(kf.split(X_train_full, y_train_full)):\n            X_train = X_train_full.iloc[train_idx]\n            y_train = y_train_full.iloc[train_idx]\n            X_valid = X_train_full.iloc[test_idx]\n            y_valid = y_train_full.iloc[test_idx]\n            \n            model = define_logreg(trial)\n            \n            model.fit(X_train, y_train,)\n\n            preds = model.predict(X_valid)\n            pred_labels = np.rint(preds)\n            accuracy = accuracy_score(y_valid, pred_labels)\n            \n            history.append(accuracy)\n\n            trial.report(np.mean(history), i)\n            # Handle pruning based on the intermediate value.\n            if trial.should_prune():\n                raise optuna.exceptions.TrialPruned()\n\n        return np.mean(history)\n\n    elif (model == 'lsvc'):\n        \n#       Straight forward option\n        \n#         X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, stratify = y_train_full, train_size = 0.25)\n        \n#         model = define_lsvc(trial)\n\n#         model.fit(X_train, y_train,)\n\n#         preds = model.predict(X_valid)\n#         pred_labels = np.rint(preds)\n#         accuracy = accuracy_score(y_valid, pred_labels)\n\n#         trial.report(accuracy, i)\n#         # Handle pruning based on the intermediate value.\n#         if trial.should_prune():\n#             raise optuna.exceptions.TrialPruned()\n\n#         return accuracy\n\n#       Cross-validation option\n\n        k = 5\n        kf = StratifiedKFold(n_splits = k, shuffle = True, random_state = 1)\n\n        history = []\n        for i, (train_idx, test_idx) in enumerate(kf.split(X_train_full, y_train_full)):\n            X_train = X_train_full.iloc[train_idx]\n            y_train = y_train_full.iloc[train_idx]\n            X_valid = X_train_full.iloc[test_idx]\n            y_valid = y_train_full.iloc[test_idx]\n            \n            model = define_lsvc(trial)\n            \n            model.fit(X_train, y_train,)\n\n            preds = model.predict(X_valid)\n            pred_labels = np.rint(preds)\n            accuracy = accuracy_score(y_valid, pred_labels)\n            \n            history.append(accuracy)\n\n            trial.report(np.mean(history), i)\n            # Handle pruning based on the intermediate value.\n            if trial.should_prune():\n                raise optuna.exceptions.TrialPruned()\n\n        return np.mean(history)\n\n    elif (model == 'rf'):\n        \n#       Straight forward option\n        \n#         X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, stratify = y_train_full, train_size = 0.25)\n        \n#         model = define_rf(trial)\n\n#         model.fit(X_train, y_train,)\n\n#         preds = model.predict(X_valid)\n#         pred_labels = np.rint(preds)\n#         accuracy = accuracy_score(y_valid, pred_labels)\n\n#         trial.report(accuracy, i)\n#         # Handle pruning based on the intermediate value.\n#         if trial.should_prune():\n#             raise optuna.exceptions.TrialPruned()\n\n#         return accuracy\n\n#       Cross-validation option\n\n        k = 5\n        kf = StratifiedKFold(n_splits = k, shuffle = True, random_state = 1)\n\n        history = []\n        for i, (train_idx, test_idx) in enumerate(kf.split(X_train_full, y_train_full)):\n            X_train = X_train_full.iloc[train_idx]\n            y_train = y_train_full.iloc[train_idx]\n            X_valid = X_train_full.iloc[test_idx]\n            y_valid = y_train_full.iloc[test_idx]\n            \n            model = define_rf(trial)\n            \n            model.fit(X_train, y_train,)\n\n            preds = model.predict(X_valid)\n            pred_labels = np.rint(preds)\n            accuracy = accuracy_score(y_valid, pred_labels)\n            \n            history.append(accuracy)\n\n            trial.report(np.mean(history), i)\n            # Handle pruning based on the intermediate value.\n            if trial.should_prune():\n                raise optuna.exceptions.TrialPruned()\n\n        return np.mean(history)\n    \n    elif (model == 'xgb'):\n\n#       Cross-validation option\n        dtrain = DMatrix(X_train_full, label = y_train_full)\n    \n        param = {\n            \"verbosity\": 0,\n            \"objective\": \"binary:logistic\",\n            \"eval_metric\": \"auc\",\n            \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n            \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n            \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n            # sampling ratio for training data.\n            \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n            # sampling according to each tree.\n            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n            \n            'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-1, log = True),\n        }\n\n        if param[\"booster\"] == \"gbtree\" or param[\"booster\"] == \"dart\":\n            param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)\n            # minimum child weight, larger the term more conservative the tree.\n            param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n            param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n            param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n            param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n\n        if param[\"booster\"] == \"dart\":\n            param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n            param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n            param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n            param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n\n        k = 5\n        xgb_cv_results = cv(\n            params = param,\n            dtrain = dtrain,\n            num_boost_round = 10000,\n            nfold = k,\n            stratified = True,\n            early_stopping_rounds = 100,\n            verbose_eval = False,\n        )\n\n        # Set n_estimators as a trial attribute; Accessible via study.trials_dataframe().\n        trial.set_user_attr(\"n_estimators\", len(xgb_cv_results))\n\n        # Extract the best score.\n        best_score = xgb_cv_results[\"test-auc-mean\"].values[-1]\n        return best_score","3734d911":"study_nn = optuna.create_study(direction = 'maximize')\nstudy_nn.optimize(lambda trial: objective(trial, 'nn', X_train_full, y_train_full), n_trials = 100, timeout = 600)","201ee429":"pruned_trials = study_nn.get_trials(deepcopy=False, states=[TrialState.PRUNED])\ncomplete_trials = study_nn.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n\nprint(\"Keras NN study statistics: \")\nprint(\"  Number of finished trials: \", len(study_nn.trials))\nprint(\"  Number of pruned trials: \", len(pruned_trials))\nprint(\"  Number of complete trials: \", len(complete_trials))\n\nprint(\"Best trial:\")\ntrial = study_nn.best_trial\n\nprint(\"  Value: \", trial.value)\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","9cf6d985":"study_cb = optuna.create_study(direction = 'maximize')\nstudy_cb.optimize(lambda trial: objective(trial, 'cb', X_train_full, y_train_full), n_trials = 500, timeout = 600)","f24f1c3d":"pruned_trials = study_cb.get_trials(deepcopy=False, states=[TrialState.PRUNED])\ncomplete_trials = study_cb.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n\nprint(\"CatBoost study statistics: \")\nprint(\"  Number of finished trials: \", len(study_cb.trials))\nprint(\"  Number of pruned trials: \", len(pruned_trials))\nprint(\"  Number of complete trials: \", len(complete_trials))\n\nprint(\"Best trial:\")\ntrial = study_cb.best_trial\n\nprint(\"  Value: \", trial.value)\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","4405a23a":"study_lgbm = optuna.create_study(direction = 'maximize')\nstudy_lgbm.optimize(lambda trial: objective(trial, 'lgbm', X_train_full, y_train_full), n_trials = 500, timeout = 600)","5bccc0fd":"pruned_trials = study_lgbm.get_trials(deepcopy=False, states=[TrialState.PRUNED])\ncomplete_trials = study_lgbm.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n\nprint(\"LightGBM study statistics: \")\nprint(\"  Number of finished trials: \", len(study_lgbm.trials))\nprint(\"  Number of pruned trials: \", len(pruned_trials))\nprint(\"  Number of complete trials: \", len(complete_trials))\n\nprint(\"Best trial:\")\ntrial = study_lgbm.best_trial\n\nprint(\"  Value: \", trial.value)\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","8f6a3e72":"study_logreg = optuna.create_study(direction = 'maximize')\nstudy_logreg.optimize(lambda trial: objective(trial, 'logreg', X_train_full, y_train_full), n_trials = 100, timeout = 600)","8e0687fc":"pruned_trials = study_logreg.get_trials(deepcopy=False, states=[TrialState.PRUNED])\ncomplete_trials = study_logreg.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n\nprint(\"LogRegression study statistics: \")\nprint(\"  Number of finished trials: \", len(study_logreg.trials))\nprint(\"  Number of pruned trials: \", len(pruned_trials))\nprint(\"  Number of complete trials: \", len(complete_trials))\n\nprint(\"Best trial:\")\ntrial = study_logreg.best_trial\n\nprint(\"  Value: \", trial.value)\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","1b76e6ce":"study_lsvc = optuna.create_study(direction = 'maximize')\nstudy_lsvc.optimize(lambda trial: objective(trial, 'lsvc', X_train_full, y_train_full), n_trials = 100, timeout = 600)","64e5dbc3":"pruned_trials = study_lsvc.get_trials(deepcopy=False, states=[TrialState.PRUNED])\ncomplete_trials = study_lsvc.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n\nprint(\"LinearSVC study statistics: \")\nprint(\"  Number of finished trials: \", len(study_lsvc.trials))\nprint(\"  Number of pruned trials: \", len(pruned_trials))\nprint(\"  Number of complete trials: \", len(complete_trials))\n\nprint(\"Best trial:\")\ntrial = study_lsvc.best_trial\n\nprint(\"  Value: \", trial.value)\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","aedc893c":"study_rf = optuna.create_study(direction = 'maximize')\nstudy_rf.optimize(lambda trial: objective(trial, 'rf', X_train_full, y_train_full), n_trials = 100, timeout = 600)","e0aef359":"pruned_trials = study_rf.get_trials(deepcopy=False, states=[TrialState.PRUNED])\ncomplete_trials = study_rf.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n\nprint(\"RandomForest study statistics: \")\nprint(\"  Number of finished trials: \", len(study_rf.trials))\nprint(\"  Number of pruned trials: \", len(pruned_trials))\nprint(\"  Number of complete trials: \", len(complete_trials))\n\nprint(\"Best trial:\")\ntrial = study_rf.best_trial\n\nprint(\"  Value: \", trial.value)\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","235b461b":"study_xgb = optuna.create_study(direction = 'maximize')\nstudy_xgb.optimize(lambda trial: objective(trial, 'xgb', X_train_full, y_train_full), n_trials = 20, timeout = 600)","2b1e80dc":"print(\"Number of finished trials: \", len(study_xgb.trials))\nprint(\"Best trial:\")\ntrial = study_xgb.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n\nprint(\"  Number of estimators: {}\".format(trial.user_attrs[\"n_estimators\"]))","1167d78d":"# study_lgbm_male = optuna.create_study(direction = 'maximize')\n# study_lgbm_male.optimize(lambda trial: objective(trial, 'lgbm', X_train_full_male, y_train_full_male), n_trials = 500, timeout = 600)","482f37e3":"# pruned_trials = study_lgbm_male.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n# complete_trials = study_lgbm_male.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n\n# print(\"LightGBM male study statistics: \")\n# print(\"  Number of finished trials: \", len(study_lgbm_male.trials))\n# print(\"  Number of pruned trials: \", len(pruned_trials))\n# print(\"  Number of complete trials: \", len(complete_trials))\n\n# print(\"Best trial:\")\n# trial = study_lgbm_male.best_trial\n\n# print(\"  Value: \", trial.value)\n\n# print(\"  Params: \")\n# for key, value in trial.params.items():\n#     print(\"    {}: {}\".format(key, value))","b416418c":"# study_lgbm_female = optuna.create_study(direction = 'maximize')\n# study_lgbm_female.optimize(lambda trial: objective(trial, 'lgbm', X_train_full_female, y_train_full_female), n_trials = 500, timeout = 600)","409a73d2":"# study_lgbm_female = optuna.create_study(direction = 'maximize')\n# study_lgbm_female.optimize(lambda trial: objective(trial, 'lgbm', X_train_full_female, y_train_full_female), n_trials = 500, timeout = 600)pruned_trials = study_lgbm_female.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n# complete_trials = study_lgbm_female.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n\n# print(\"LightGBM female study statistics: \")\n# print(\"  Number of finished trials: \", len(study_lgbm_female.trials))\n# print(\"  Number of pruned trials: \", len(pruned_trials))\n# print(\"  Number of complete trials: \", len(complete_trials))\n\n# print(\"Best trial:\")\n# trial = study_lgbm_female.best_trial\n\n# print(\"  Value: \", trial.value)\n\n# print(\"  Params: \")\n# for key, value in trial.params.items():\n#     print(\"    {}: {}\".format(key, value))","fa75fb6c":"study_nn.best_params","ab00f76d":"nn = keras.Sequential()\n\nfor i in range(study_nn.best_params['n_layers']):\n    if (i == 0):\n        nn.add(layers.BatchNormalization(input_shape = [X_train_full.shape[1]]))\n        nn.add(layers.Dense(units = study_nn.best_params['units_0'], activation = 'relu'))\n        nn.add(layers.Dropout(rate = study_nn.best_params['rate_0']))\n    else:\n        str_units = 'units_' + str(i)\n        str_rate = 'rate_' + str(i)\n        nn.add(layers.BatchNormalization())\n        nn.add(layers.Dense(units = study_nn.best_params[str_units], activation = 'relu'))\n        nn.add(layers.Dropout(rate = study_nn.best_params[str_rate]))\n\nnn.add(layers.BatchNormalization())\nnn.add(layers.Dense(units = 1, activation = 'sigmoid'))\n\nopt = getattr(keras.optimizers, study_nn.best_params['optimizer'])(nn.optimizer, lr = study_nn.best_params['lr'])\n\nnn.compile(optimizer = opt, \n           loss = 'binary_crossentropy', \n           metrics = ['binary_accuracy'])\n\nnn.fit(X_train_full, y_train_full,\n       batch_size = study_nn.best_params['batch_size'],\n       epochs = study_nn.best_params['epochs'],\n       verbose = 0,)","1aa9145b":"lgbm = LGBMClassifier(**study_lgbm.best_params)\ncb = CatBoostClassifier(allow_writing_files = False, logging_level = 'Silent', **study_cb.best_params)\nlsvc = LinearSVC(**study_lsvc.best_params)\nlogreg = LogisticRegression(solver = 'saga', **study_logreg.best_params)\nrf = RandomForestClassifier(**study_rf.best_params)\nxgb = XGBClassifier(n_estimators = trial.user_attrs[\"n_estimators\"], **study_xgb.best_params)","38b9672a":"cb.fit(X_train_full, y_train_full)\nlgbm.fit(X_train_full, y_train_full)\nlsvc.fit(X_train_full, y_train_full)\nlogreg.fit(X_train_full, y_train_full)\nrf.fit(X_train_full, y_train_full)\nxgb.fit(X_train_full, y_train_full)","d42c2fd1":"predictions_cb = cb.predict(X_test)\npredictions_lgbm = lgbm.predict(X_test)\npredictions_lsvc = lsvc.predict(X_test)\npredictions_logreg = logreg.predict(X_test)\npredictions_rf = rf.predict(X_test)\npredictions_xgb = xgb.predict(X_test)","f3bde0cf":"predictions_nn = nn.predict(X_test)\npredictions_nn[predictions_nn > 0.5] = 1\npredictions_nn[predictions_nn <= 0.5] = 0","e40ec672":"submission = pd.DataFrame()","1ee4596a":"submission['PassengerId'] = X_test.index\nsubmission['pr_nn'] = predictions_nn.flatten().astype('int64')\nsubmission['pr_cb'] = predictions_cb\nsubmission['pr_lgbm'] = predictions_lgbm\nsubmission['pr_lsvc'] = predictions_lsvc\nsubmission['pr_logreg'] = predictions_logreg\nsubmission['pr_rf'] = predictions_rf\nsubmission['pr_xgb'] = predictions_xgb","43628d4c":"submission[[col for col in submission.columns if col.startswith('pr_')]].sum(axis = 1).value_counts()","c1a747c9":"submission['Survived'] = (submission[[col for col in submission.columns if col.startswith('pr_')]].sum(axis=1) >= 4).astype(int)\nsubmission","627f2f23":"submission[['PassengerId', 'Survived']].to_csv('submission_voting.csv', index = False)","fa94380a":"submission_svc = pd.DataFrame({'PassengerId': X_test.index,\n                               'Survived': submission.pr_lsvc})\nsubmission_svc.to_csv('submission_svc.csv', index = False)","47c307f0":"submission_cat = pd.DataFrame({'PassengerId': X_test.index,\n                               'Survived': submission.pr_cb})\nsubmission_cat.to_csv('submission_cat.csv', index = False)","160a733f":"submission_nn = pd.DataFrame({'PassengerId': X_test.index,\n                               'Survived': submission.pr_nn})\nsubmission_nn.to_csv('submission_nn.csv', index = False)","ac821583":"submission_lgbm = pd.DataFrame({'PassengerId': X_test.index,\n                                'Survived': submission.pr_lgbm})\nsubmission_lgbm.to_csv('submission_lgbm.csv', index = False)","b8fdbd69":"# from sklearn.ensemble import StackingClassifier, VotingClassifier\n# from keras.wrappers.scikit_learn import KerasClassifier\n\n# def NeuralNetwork():\n#     nn = keras.Sequential()\n\n#     for i in range(study_nn.best_params['n_layers']):\n#         if (i == 0):\n#             nn.add(layers.BatchNormalization(input_shape = [X_train_full.shape[1]]))\n#             nn.add(layers.Dense(units = study_nn.best_params['units_0'], activation = 'relu'))\n#             nn.add(layers.Dropout(rate = study_nn.best_params['rate_0']))\n#         else:\n#             str_units = 'units_' + str(i)\n#             str_rate = 'rate_' + str(i)\n#             nn.add(layers.BatchNormalization())\n#             nn.add(layers.Dense(units = study_nn.best_params[str_units], activation = 'relu'))\n#             nn.add(layers.Dropout(rate = study_nn.best_params[str_rate]))\n\n#     nn.add(layers.BatchNormalization())\n#     nn.add(layers.Dense(units = 1, activation = 'sigmoid'))\n\n#     opt = getattr(keras.optimizers, study_nn.best_params['optimizer'])(nn.optimizer, lr = study_nn.best_params['lr'])\n\n#     nn.compile(optimizer = opt, \n#                loss = 'binary_crossentropy', \n#                metrics = ['binary_accuracy'])\n    \n#     return nn\n\n# nn = KerasClassifier(build_fn = NeuralNetwork, \n#                      epochs = study_nn.best_params['epochs'],\n#                      batch_size = study_nn.best_params['batch_size'], \n#                      verbose = 0)\n\n# nn._estimator_type = 'classifier'\n\n# lgbm = LGBMClassifier(**study_lgbm.best_params)\n# cb = CatBoostClassifier(allow_writing_files = False, logging_level = 'Silent', **study_cb.best_params)\n# lsvc = LinearSVC(**study_lsvc.best_params)\n# logreg = LogisticRegression(solver = 'saga', **study_logreg.best_params)\n# rf = RandomForestClassifier(**study_rf.best_params)\n# xgb = XGBClassifier(n_estimators = trial.user_attrs[\"n_estimators\"], **study_xgb.best_params)\n\n# estimators = [\n#     ('0', nn),\n#     ('1', cb),\n#     ('2', lgbm),\n#     ('3', lsvc),\n#     ('4', logreg),\n#     ('5', rf),\n#     ('6', xgb),\n# ]\n\n# stacked = StackingClassifier(estimators = estimators, final_estimator = logreg, \n#                              verbose = 0, cv = 5)\n# stacked.fit(X_train_full, y_train_full)\n\n# predictions = stacked.predict(X_test)","4fc71260":"# submission_stacked = pd.DataFrame({'PassengerId': X_test.index, \n#                                    'Survived': predictions})\n# submission_stacked.to_csv('submission_stacked.csv', index = False)","b7fb20aa":"It didn't improve accuracy, left it just as a reference.","be3e1e76":"# 2. Visualization and data analysis","9b9e5f62":"# 5.6 Stacking ensemble","2503d8cd":"Taking a sample to save some time.","a91386cb":"# 4.2 Separating male and female sets","619d63bf":"# 5.2 Other models","e888cb4a":"# 5. Creating and evaluating models","79575805":"# 4. Feature engineering and encoding","1d79e0f4":"# 5.4 Creating tuned models","a7299e57":"# Introduction\nHello!\n\nIn this kernel you will find my full data science workflow of \"Tabular Playground Series - Apr 2021\" competition.\nI'm here to learn and improve, so by all means feel free to criticize or suggest anything in a comments section down below, I would really appreciate it! :)\n\nAlso, I would like to recommend [this amazing notebook](https:\/\/www.kaggle.com\/jitendramanwani\/tps-april-2021-eda-viz-insights-model-end-2-end) to you.","8564e122":"Trying LGBMClassifier with separated male\/female data sets","95280b04":"# 3. Data cleaning","0cb49bcb":"Decided not to use it, because it takes too much time to run and doesn't improve accuracy.","abb5b786":"# Table of contents:\n\n1. Meeting our data\n\n2. Visualization and data analysis\n\n3. Data cleaning\n\n4. Feature engineering and encoding\n\n    4.1 Full data set\n\n    4.2 Separating male and female sets\n\n5. Creating and evaluating models\n\n    5.1 Neural network\n\n    5.2 Other models\n    \n    5.3 Parameter tuning with Optuna\n    \n    5.4 Creating tuned models\n    \n    5.5 Voting ensemble\n    \n    5.6 Stacking ensemble","69b514f7":"# 5.3 Parameter tuning with Optuna","9d602963":"# 5.1 Neural network","8e6e495f":"# 4.1 Full data set","3e6224e6":"# 1. Meeting our data","00e8dd45":"# 5.5 Voting ensemble"}}