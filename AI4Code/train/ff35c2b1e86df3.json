{"cell_type":{"73bb8fc3":"code","d8161154":"code","4c35c342":"code","30ad239c":"code","8d7109ee":"code","65108d8b":"code","0d5ea628":"code","0b1e9777":"code","b937e099":"code","41358049":"code","b4aefb23":"code","c3282ea0":"code","9ef71278":"code","a680ed3b":"code","a118623e":"code","87201f76":"code","e40d1b92":"code","182de85d":"code","a0a92b86":"code","a676513a":"code","cffe6ae2":"code","64ef1012":"code","7303d29b":"code","5f945129":"code","efd07598":"code","d85ed78d":"code","518abdde":"code","0ab00907":"markdown","eedf4d15":"markdown","275baffa":"markdown","7624d1c4":"markdown","cb5e78d2":"markdown","860d948c":"markdown","9e52935f":"markdown","7475724b":"markdown","f59ddca9":"markdown"},"source":{"73bb8fc3":"! pip install -qU scikit-learn","d8161154":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import precision_score, recall_score, f1_score,\\\n                            accuracy_score, classification_report,\\\n                            plot_confusion_matrix, confusion_matrix\nfrom sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\n\nimport os\nfrom collections import Counter\n\nnp.random.seed(34)\npath = '\/kaggle\/input\/creditcardfraud\/'","4c35c342":"# reading data\ndf = pd.read_csv(f'{path}creditcard.csv')\ndf.drop(\"Time\", 1, inplace=True)\nprint(df.shape)\ndf.head()","30ad239c":"# High class imbalance\ndf['Class'].value_counts(normalize=True)*100","8d7109ee":"# Checking for Null values\nprint(f\"Number of Null values: {df.isnull().any().sum()}\")","65108d8b":"# checking for duplicate values\nprint(f\"Dataset has {df.duplicated().sum()} duplicate rows\")\n# dropping duplicate rows\ndf.drop_duplicates(inplace=True)","0d5ea628":"# high skweness in Amount feature\nplt.figure(figsize=(14,4))\ndf['Amount'].value_counts().head(50).plot(kind='bar')\nplt.show()","0b1e9777":"# checking skewness of other columns\ndf.drop('Class',1).skew()","b937e099":"# taking log transform of high positively skewed features\nskew_cols = df.drop('Class', 1).skew().loc[lambda x: x>2].index\nfor col in skew_cols:\n    lower_lim = abs(df[col].min())\n    normal_col = df[col].apply(lambda x: np.log10(x+lower_lim+1))\n    print(f\"Skew value of {col} after log transform: {normal_col.skew()}\")","41358049":"# Only applying log transform to Amount feature\ndf['Amount'] = df['Amount'].apply(lambda x: np.log10(x+1))","b4aefb23":"scaler = StandardScaler()\n#scaler = MinMaxScaler()\nX = scaler.fit_transform(df.drop('Class', 1))\ny = df['Class'].values\nprint(X.shape, y.shape)","c3282ea0":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)","9ef71278":"# simple linear regression\nlinear_model = LogisticRegression()\nlinear_model.fit(X_train, y_train)\n\ny_pred = linear_model.predict(X_test)\n\n# evaluation\nprint(classification_report(y_test, y_pred))\nplot_confusion_matrix(linear_model, X_test, y_test)\nplt.show()","a680ed3b":"weights = np.linspace(0.05, 0.95, 15)\n\ngscv = GridSearchCV(\n    estimator=LogisticRegression(),\n    param_grid={\n        'class_weight': [{0: x, 1: 1.0-x} for x in weights]\n    },\n    scoring='f1',\n    cv=3\n)\ngrid_res = gscv.fit(X, y)\n\nprint(\"Best parameters : %s\" % grid_res.best_params_)","a118623e":"# plotting F1 scores \nplt.plot(weights, grid_res.cv_results_['mean_test_score'], marker='o')\nplt.grid()\nplt.show()","87201f76":"# training with best weights\nwlr = LogisticRegression(**grid_res.best_params_)\nwlr.fit(X_train, y_train)\n\ny_pred = wlr.predict(X_test)\n\n# evaluation\nprint(classification_report(y_test, y_pred))\nplot_confusion_matrix(wlr, X_test, y_test)\nplt.show()","e40d1b92":"# constructing pipeline\npipe = Pipeline([\n        ('smote', SMOTE()),\n        ('lr', LogisticRegression())\n])\n# training model with smote samples\npipe.fit(X_train, y_train)\n\ny_pred = pipe.predict(X_test)\n\n# evaluation\nprint(classification_report(y_test, y_pred))\nsns.heatmap(confusion_matrix(y_test, y_pred),annot=True)\nplt.show()","182de85d":"pipe = Pipeline([\n        ('smote', SMOTE()),\n        ('lr', LogisticRegression())\n])\nsm_ratio = np.linspace(0.2, 0.8, 10)\nlr_weights = np.linspace(0.05, 0.95, 10)\n\ngscv = GridSearchCV(\n    estimator=pipe,\n    param_grid={\n        'smote__sampling_strategy': sm_ratio,\n        'lr__class_weight': [{0: x, 1: 1.0-x} for x in lr_weights]\n    },\n    scoring='f1',\n    cv=3\n)\ngrid_result = gscv.fit(X, y)\n\nprint(\"Best parameters : %s\" % grid_result.best_params_)","a0a92b86":"df_gs = pd.DataFrame(data=grid_result.cv_results_['mean_test_score'].reshape(10,10),\n                     index=np.around(sm_ratio[::-1], 2), \n                     columns=np.around(lr_weights[::-1], 2))\nplt.figure(figsize=(8,8))\nsns.heatmap(df_gs,\n            annot=True,\n            linewidths=.5)\nplt.show()","a676513a":"# training with best weights\npipe = Pipeline([\n        ('smote', SMOTE(sampling_strategy= 0.2)),\n        ('lr', LogisticRegression(class_weight={0: 0.95, 1: 0.05}))\n])\npipe.fit(X_train, y_train)\n\ny_pred = pipe.predict(X_test)\n\n# evaluation\nprint(classification_report(y_test, y_pred))\nsns.heatmap(confusion_matrix(y_test, y_pred),annot=True)\nplt.show()","cffe6ae2":"from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, Concatenate\nfrom tensorflow.keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.initializers import RandomNormal\nimport tensorflow.keras.backend as K\nfrom sklearn.utils import shuffle","64ef1012":"class cGAN():\n    def __init__(self):\n        self.latent_dim = 32\n        self.out_shape = 29\n        self.num_classes = 2\n        self.clip_value = 0.01\n        optimizer = Adam(0.0002, 0.5)\n        #optimizer = RMSprop(lr=0.00005)\n\n        # build discriminator\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss=['binary_crossentropy'],\n                                   optimizer=optimizer,\n                                   metrics=['accuracy'])\n\n        # build generator\n        self.generator = self.build_generator()\n\n        # generating new data samples\n        noise = Input(shape=(self.latent_dim,))\n        label = Input(shape=(1,))\n        gen_samples = self.generator([noise, label])\n\n        self.discriminator.trainable = False\n\n        # passing gen samples through disc. \n        valid = self.discriminator([gen_samples, label])\n\n        # combining both models\n        self.combined = Model([noise, label], valid)\n        self.combined.compile(loss=['binary_crossentropy'],\n                              optimizer=optimizer,\n                             metrics=['accuracy'])\n        self.combined.summary()\n\n    def wasserstein_loss(self, y_true, y_pred):\n        return K.mean(y_true * y_pred)\n\n    def build_generator(self):\n        init = RandomNormal(mean=0.0, stddev=0.02)\n        model = Sequential()\n\n        model.add(Dense(128, input_dim=self.latent_dim))\n        #model.add(Dropout(0.2))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n\n        model.add(Dense(256))\n        #model.add(Dropout(0.2))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n\n        model.add(Dense(512))\n        #model.add(Dropout(0.2))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n\n        model.add(Dense(self.out_shape, activation='tanh'))\n        model.summary()\n\n        noise = Input(shape=(self.latent_dim,))\n        label = Input(shape=(1,), dtype='int32')\n        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n        \n        model_input = multiply([noise, label_embedding])\n        gen_sample = model(model_input)\n\n        return Model([noise, label], gen_sample, name=\"Generator\")\n\n    \n    def build_discriminator(self):\n        init = RandomNormal(mean=0.0, stddev=0.02)\n        model = Sequential()\n\n        model.add(Dense(512, input_dim=self.out_shape, kernel_initializer=init))\n        model.add(LeakyReLU(alpha=0.2))\n        \n        model.add(Dense(256, kernel_initializer=init))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.4))\n        \n        model.add(Dense(128, kernel_initializer=init))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.4))\n        \n        model.add(Dense(1, activation='sigmoid'))\n        model.summary()\n        \n        gen_sample = Input(shape=(self.out_shape,))\n        label = Input(shape=(1,), dtype='int32')\n        label_embedding = Flatten()(Embedding(self.num_classes, self.out_shape)(label))\n\n        model_input = multiply([gen_sample, label_embedding])\n        validity = model(model_input)\n\n        return Model(inputs=[gen_sample, label], outputs=validity, name=\"Discriminator\")\n\n\n    def train(self, X_train, y_train, pos_index, neg_index, epochs, batch_size=32, sample_interval=50):\n\n        # Adversarial ground truths\n        valid = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n\n        for epoch in range(epochs):\n            \n            #  Train Discriminator with 8 sample from postivite class and rest with negative class\n            idx1 = np.random.choice(pos_index, 8)\n            idx0 = np.random.choice(neg_index, batch_size-8)\n            idx = np.concatenate((idx1, idx0))\n            samples, labels = X_train[idx], y_train[idx]\n            samples, labels = shuffle(samples, labels)\n            # Sample noise as generator input\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n\n            # Generate a half batch of new images\n            gen_samples = self.generator.predict([noise, labels])\n\n            # label smoothing\n            if epoch < epochs\/\/1.5:\n                valid_smooth = (valid+0.1)-(np.random.random(valid.shape)*0.1)\n                fake_smooth = (fake-0.1)+(np.random.random(fake.shape)*0.1)\n            else:\n                valid_smooth = valid \n                fake_smooth = fake\n                \n            # Train the discriminator\n            self.discriminator.trainable = True\n            d_loss_real = self.discriminator.train_on_batch([samples, labels], valid_smooth)\n            d_loss_fake = self.discriminator.train_on_batch([gen_samples, labels], fake_smooth)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n            # Train Generator\n            # Condition on labels\n            self.discriminator.trainable = False\n            sampled_labels = np.random.randint(0, 2, batch_size).reshape(-1, 1)\n            # Train the generator\n            g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)\n\n            # Plot the progress\n            if (epoch+1)%sample_interval==0:\n                print (f\"{epoch} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]}] [G loss: {g_loss}]\")","7303d29b":"cgan = cGAN()","5f945129":"y_train = y_train.reshape(-1,1)\npos_index = np.where(y_train==1)[0]\nneg_index = np.where(y_train==0)[0]\ncgan.train(X_train, y_train, pos_index, neg_index, epochs=2000)","efd07598":"# generating new samples\nnoise = np.random.normal(0, 1, (400, 32))\nsampled_labels = np.ones(400).reshape(-1, 1)\n\ngen_samples = cgan.generator.predict([noise, sampled_labels])\ngen_samples = scaler.inverse_transform(gen_samples)\nprint(gen_samples.shape)","d85ed78d":"gen_df = pd.DataFrame(data = gen_samples,\n                      columns = df.drop('Class',1).columns)\ngen_df.head()","518abdde":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\nax[0].scatter(df[df['Class']==1]['Amount'], df[df['Class']==1]['V1'])\nax[1].scatter(gen_df['Amount'], gen_df['V1'])\nplt.show()","0ab00907":"# Using weighted regression to improve accuracy","eedf4d15":"# Grid Search on SMOTE and Regression","275baffa":"# Using GANs to generate new data","7624d1c4":"# Data Exploration and Cleaning","cb5e78d2":"**Slight improvement when using weighted regression**","860d948c":"# Using SMOTE for upsampling","9e52935f":"**Using SMOTE with weighted regression improves results**","7475724b":"# Training a Baseline Model","f59ddca9":"**Poor F1 score even when using SMOTE**"}}