{"cell_type":{"2b2f4e29":"code","dcdf4707":"code","7c346f0d":"code","8ef5118e":"code","551fc146":"code","5ce77287":"code","a99a02dd":"code","4cc9a5b1":"code","dde6831a":"code","621f070f":"code","147eea4d":"code","2a4bcea2":"code","4c7efa24":"code","07fc5cad":"code","36be7bb0":"code","551c7adb":"code","82dfe7b9":"code","cfcbed2f":"code","2558f5ad":"code","cd9228f2":"code","5de534fd":"markdown","7b16b37d":"markdown","14aa5f19":"markdown","b9883aea":"markdown","1df7341a":"markdown","a7f11df6":"markdown","74166e78":"markdown","0f08e2cb":"markdown"},"source":{"2b2f4e29":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport time\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","dcdf4707":"train_df = pd.read_csv('..\/input\/train.csv')\nprint('Rows: ',train_df.shape[0],'Columns: ',train_df.shape[1])\ntrain_df.info()","7c346f0d":"train_df.head()","8ef5118e":"train_df['target'].value_counts()","551fc146":"sns.countplot(train_df['target'])\nsns.set_style('whitegrid')","5ce77287":"test_df = pd.read_csv('..\/input\/test.csv')","a99a02dd":"X_test = test_df.drop('ID_code',axis=1)","4cc9a5b1":"X = train_df.drop(['ID_code','target'],axis=1)\ny = train_df['target']","dde6831a":"n_fold = 5\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)","621f070f":"params = {'num_leaves': 8,\n         'min_data_in_leaf': 42,\n         'objective': 'binary',\n         'max_depth': 16,\n         'learning_rate': 0.0123,\n         'boosting': 'gbdt',\n         'bagging_freq': 5,\n         'bagging_fraction': 0.8,\n         'feature_fraction': 0.8201,\n         'bagging_seed': 11,\n         'reg_alpha': 1.728910519108444,\n         'reg_lambda': 4.9847051755586085,\n         'random_state': 42,\n         'metric': 'auc',\n         'verbosity': -1,\n         'subsample': 0.81,\n         'min_gain_to_split': 0.01077313523861969,\n         'min_child_weight': 19.428902804238373,\n         'num_threads': 4}","147eea4d":"prediction = np.zeros(len(X_test))\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X,y)):\n    print('Fold', fold_n, 'started at', time.ctime())\n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    train_data = lgb.Dataset(X_train, label=y_train)\n    valid_data = lgb.Dataset(X_valid, label=y_valid)\n        \n    model = lgb.train(params,train_data,num_boost_round=20000,\n                    valid_sets = [train_data, valid_data],verbose_eval=300,early_stopping_rounds = 200)\n            \n    #y_pred_valid = model.predict(X_valid)\n    prediction += model.predict(X_test, num_iteration=model.best_iteration)\/5","2a4bcea2":"from catboost import CatBoostClassifier,Pool\nprediction1 = np.zeros(len(X_test))\nm = CatBoostClassifier(loss_function=\"Logloss\",eval_metric=\"AUC\",\n                       boosting_type = 'Ordered')\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X,y)):\n    print('Fold', fold_n, 'started at', time.ctime())\n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n\n    train_data = Pool(X_train, label=y_train)\n    valid_data = Pool(X_valid, label=y_valid)\n\n    model1 = m.fit(train_data,eval_set=valid_data,use_best_model=True,verbose=300)\n    \n    prediction1 += model1.predict(X_test)\/5","4c7efa24":"mod = xgb.XGBClassifier(max_depth=4,n_estimators=999999, colsample_bytree=0.7,subsample = 0.7, \n                              min_child_weight = 50, eval_metric = \"auc\",gamma = 5,alpha = 0,\n                               booster = \"gbtree\",colsample_bylevel = 0.7, learning_rate=0.1,\n                              objective='binary:logistic', n_jobs=-1)\n\nprediction2 = np.zeros(len(X_test))\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X,y)):\n    print('Fold', fold_n, 'started at', time.ctime())\n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    #evallist = [(valid_data, 'eval'), (train_data, 'train')]\n    model2 = mod.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],verbose=200, eval_metric='auc',\n                        early_stopping_rounds=200)\n    \n    prediction2 += model2.predict(X_test, ntree_limit=model2.best_ntree_limit)\/5","07fc5cad":"prediction","36be7bb0":"sub = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\nsub[\"target\"] = prediction\nsub.to_csv(\"submission.csv\", index=False)","551c7adb":"sub[\"target\"] = prediction1\nsub.to_csv(\"submission1.csv\", index=False)","82dfe7b9":"sub[\"target\"] = (prediction + prediction1)\/2\nsub.to_csv(\"submission2.csv\", index=False)","cfcbed2f":"sub[\"target\"] = prediction2\nsub.to_csv(\"submission3.csv\", index=False)","2558f5ad":"sub[\"target\"] = (prediction + prediction2)\/2 \nsub.to_csv(\"submission4.csv\", index=False)","cd9228f2":"sub[\"target\"] = (prediction + prediction1 + prediction2)\/3 \nsub.to_csv(\"submission5.csv\", index=False)","5de534fd":"## **CatBoost Classifier**","7b16b37d":"## Train the model","14aa5f19":"<div style=\"background: linear-gradient(to bottom, #200122, #6f0000); border: 2px; box-radius: 20px\"><h1 style=\"color: white; text-align: center\"><br> <center>Santander Customer Transaction Prediction<center><br><\/h1><\/div>","b9883aea":"## **Submission**","1df7341a":"## **LGBM**","a7f11df6":"## **XGBoost**","74166e78":"## **Load the Data**","0f08e2cb":"- The Dataset containing 200 numeric feature variables from var_0 to var_199 and a target value."}}