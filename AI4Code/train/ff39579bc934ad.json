{"cell_type":{"f0123855":"code","926e6a41":"code","2d6350dd":"code","6acca040":"code","38f73141":"code","cb144f0b":"code","aad08c8b":"code","7478ffdb":"code","9d16fda0":"code","d7206590":"code","1ca318e4":"code","555e6ecb":"code","7468310a":"code","90888aa2":"code","c219420e":"code","c6f419df":"code","29ec569e":"code","7570bce7":"code","7face20b":"code","08b134a9":"code","38159882":"code","b402662e":"code","f6e7e56a":"code","430c9b79":"code","ac2d3160":"code","28cc7b40":"code","84fdbcaa":"markdown","c1d1da2a":"markdown","5c86733a":"markdown","75178656":"markdown","d701cf5e":"markdown","62a6de72":"markdown","06c9b29d":"markdown","23586d6b":"markdown","b2a0313a":"markdown","724597c6":"markdown","d5469bf2":"markdown","f1a739f0":"markdown","f6bd6a32":"markdown","b74c66ad":"markdown","66002768":"markdown","7967cef4":"markdown","4d69a84d":"markdown","8a36c6b6":"markdown","4eea6e48":"markdown","7d1f6008":"markdown","4d391e39":"markdown","2392de5d":"markdown"},"source":{"f0123855":"import os\nfrom pathlib import Path\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.api.types import CategoricalDtype\n\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom xgboost import XGBRegressor\n\n# import warnings\n# warnings.filterwarnings('ignore')\n","926e6a41":"df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\", index_col=\"Id\")\ndf.head()","2d6350dd":"X = df.copy()\ny = X.pop('SalePrice')\nX.shape","6acca040":"sns.histplot(y,stat='probability',kde=True);\nprint(\"Skewness: %f\" % y.skew())\nprint(\"Kurtosis: %f\" % y.kurt())","38f73141":"y_log = np.log(y)\ny_log.rename('SalePrice_log',inplace=True)\nsns.histplot(y_log,stat='probability',kde=True);\nprint(\"Skewness: %f\" % y_log.skew())\nprint(\"Kurtosis: %f\" % y_log.kurt())","cb144f0b":"n = (X.dtypes != 'object')\nnum_cols = list(n[n].index)\nprint(f\"There are {len(num_cols)} mumerical inputs, they are: {', '.join(num_cols)}\")","aad08c8b":"num_cols = list(set(num_cols)-set(['MSSubClass']))","7478ffdb":"f = pd.melt(X, value_vars=num_cols)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=4, sharex=False, sharey=False)\ng.map(lambda _x, **kwargs: sns.histplot(_x,stat='probability',kde=True), 'value');","9d16fda0":"to_transform = ['LotFrontage','1stFlrSF','GrLivArea','LotArea']\nfor col in to_transform:\n    X[col+'_log']=np.log(X[col])\n    fig,axes = plt.subplots(1, 2, sharey=True)\n    sns.histplot(X[col],stat='probability',kde=True,ax=axes[0])\n    axes[0].set_title(f\"Skewness: {'{:.2f}'.format(X[col].skew())},\\n Kurtosis: {'{:.2f}'.format(X[col].kurt())}\")\n    sns.histplot(X[col+'_log'],stat='probability',kde=True,ax=axes[1]);\n    axes[1].set_title(f\"Skewness: {'{:.2f}'.format(X[col+'_log'].skew())},\\n Kurtosis: {'{:.2f}'.format(X[col+'_log'].kurt())}\")","d7206590":"# Replace the skewed feature with their log-transformed counterparts\nX.drop(columns=to_transform,inplace=True)\nn = (X.dtypes != 'object')\nnum_cols = list(set(n[n].index)-set(['MSSubClass']))","1ca318e4":"# Get list of categorical variables\ncat_cols = list(set(X.columns)-set(num_cols))\nprint(f\"There are {len(cat_cols)} categorical inputs\")\n# Plot the cardinality of categorical variables\nX[cat_cols].nunique().sort_values().plot.barh(figsize=(6,12));","555e6ecb":"set(X.Exterior2nd.unique()).union(set(X.Exterior1st.unique()))","7468310a":"X['Exterior2nd'] = X.Exterior2nd.replace({'Brk Cmn':'BrkComm','CmentBd':'CemntBd','Wd Shng':'Wd Sdng'})","90888aa2":"ord_cols = ['LotShape','LandSlope','ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','HeatingQC',\n              'KitchenQual','Functional','FireplaceQu','GarageFinish','GarageQual','GarageCond']\nnom_cols = list(set(cat_cols)-set(ord_cols))","c219420e":"print(\"Unique values in ordinal columns\")\nfor col in ord_cols:\n    print(f\"{col}: {X[col].unique()}\")","c6f419df":"five_levels = ['Po','Fa','TA','Gd','Ex']\nlevels = {'LotShape':['Reg', 'IR1' ,'IR2', 'IR3'],\n         'LandSlope':['Gtl', 'Mod', 'Sev'],\n         'ExterQual':five_levels,\n         'ExterCond':five_levels,\n         'BsmtQual':five_levels,\n         'BsmtCond':five_levels,\n         'BsmtExposure':['No','Mn','Av','Gd'],\n         'BsmtFinType1':['Unf','LwQ','Rec','BLQ','ALQ','GLQ'],\n         'BsmtFinType2':['Unf','LwQ','Rec','BLQ','ALQ','GLQ'],\n         'HeatingQC':five_levels,\n         'KitchenQual': five_levels,\n         'Functional':['Sal','Sev','Maj2','Maj1','Mod','Min2','Min1','Typ'],\n         'FireplaceQu':five_levels,\n         'GarageFinish':['Unf','RFn','Fin'],\n         'GarageQual':five_levels,\n         'GarageCond':five_levels}","29ec569e":"# Encode categorical features\nfor col in nom_cols:\n    X[col]=X[col].astype('category')\n    \nfor col in ord_cols:\n    X[col] = X[col].astype(CategoricalDtype(levels[col],ordered=True))","7570bce7":"mis_ratio = X.isna().sum()\/len(X)\nmis_cols = mis_ratio[mis_ratio>0].sort_values(ascending=True)\nmis_cols.plot.barh(figsize=(8,10)).set_title('Ratio of missing values for features with missing values');","7face20b":"for col in num_cols:\n    X[col]=X[col].fillna(0)\nfor col in cat_cols:\n    if 'NA' not in X[col].cat.categories:\n        X[col]=X[col].cat.add_categories(\"NA\").fillna(\"NA\")\n    else:\n        print(col)","08b134a9":"for col in cat_cols:\n    X[col] = X[col].cat.codes","38159882":"def score_dataset(X, y, model):\n    score = cross_val_score(\n        model, X, y, cv=5, scoring=\"neg_mean_squared_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score","b402662e":"# Creating baseline\nbaseline_score = score_dataset(X, y_log,XGBRegressor())\nprint(f\"Baseline score: {baseline_score:.5f} RMSLE\")","f6e7e56a":"def make_mi_scores(X, y):\n    mi_scores = mutual_info_regression(X, y, discrete_features=[X[col].dtype =='int8' for col in X.columns])\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=True)\n    return mi_scores","430c9b79":"mi_scores = make_mi_scores(X, y_log)\nmi_scores.plot.barh(figsize=(10,18)).set_title('Mutual Information Scores');\n# print(mi_scores.map('{:,.5f}'.format).to_string())","ac2d3160":"X_drop= X.loc[:,mi_scores > 0]","28cc7b40":"drop_uninformative_score = score_dataset(X_drop, y_log,XGBRegressor())\nprint(f\"Score after dropping uninformative features: {drop_uninformative_score:.5f} RMSE\")\nprint(f\"Improvement compares to baseline: {(baseline_score-drop_uninformative_score)\/baseline_score:.2%}\")","84fdbcaa":"### Check distribution of SalePrice <a class=\"anchor\" id=\"step1a\"><\/a>","c1d1da2a":"The training dataset contains contains 1460 rows. Not many data points. We'll use cross validation to calculate the score. But 79 columns! That is intimidating.","5c86733a":"### Addressing numerical features <a class=\"anchor\" id=\"step1b\"><\/a>\nBe careful of features of integer type. They might be categorical instead of numerical.","75178656":"#### Encode ordinal and nominal features\nI identify ordinal and nominal features by checking *data_description.txt*. Ordinal features have a clear ordered level, e.g. from bad to good.","d701cf5e":"`MSSubClass` sounds more like a classficiation instead of a continuous variable. After checking *data_description.txt*, it turns out to be a nominal categorical feature (without orders) indeed. Let's drop it from the list of numerical features.","62a6de72":"#### Check distribution of numerical features","06c9b29d":"### Addressing Categorical features ### <a class=\"anchor\" id=\"step1c\"><\/a>\n**Check cardinality of categorical features**","23586d6b":"\nIt is not surprising that `OverallQual` has the highest MI score. It is interesting to see `Neighborhood` beeing the second highest ranking feature in MI score. So there are indeed expensive and cheap neighborhoods. Features related to the size of a house like `GrLivArea_log` and `GarageCars` all have high MI scores, as bigger houses tend to be more expensive.\\\nThere are also features with 0 MI score, like `MoSold` and `3SsnPorch`. The 0 MI score of `MoSold` shows that there is no seasonality in house prices, and features that only apply to very few houses like three-season porch area are not very informative.\n**Let's Drop these uninformative features with 0 MI score**\n### Drop uninformative features","b2a0313a":"#### Let's look at the inputs with high cardinality\nIt is normal that `Neighbourhood` has high cardinality, as there are many different neighbourhoods. However, the high cardinality of `Exterior2nd` and `Exterior1s` feels fishy. Let's check for redundant values caused by typos.","724597c6":"## Step 1: EDA <a class=\"anchor\" id=\"step1\"><\/a>\n#### Load data and have a peek","d5469bf2":"### Feature selection using mutual information <a class=\"anchor\" id=\"step2b\"><\/a>\nUse **mutual information** to compute a utility score for each feature","f1a739f0":"After checking the data_description_txt, we found out that `Exterior2nd` contains typos like *Brk Cmn*,*CmentBd* and *Wd Shng*. Let's correct them ","f6bd6a32":"# Introduction #\nMy take on [House Prices - Advanced Regression Techniques](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques) competition\n## Table of contents\n* [Step 1: EDA](#step1)\n    * [Investigating the target](#step1a)\n    * [Addressing numerical features](#step1b)\n    * [Addressing categorical features](#step1c)\n    * [Address missing values](#step1d)\n* [Step 2: Feature selection](#step2)\n    * [Establishing baseline](#step2a)\n    * [Feature selection using mutual information](#step2b)\n* [Conclusions and future work](#step3)","b74c66ad":"None of the numerical features follows a normal distribution. Some features like `GrLivArea` and `1stFlrSF` are good candidates for log transform. Let's do that.","66002768":"#### Check unique values in ordinal values\nNoteice that the same 5-level evaluation critera are shared by multiple ordinal features","7967cef4":"## Conclusions and future work <a class=\"anchor\" id=\"step3\"><\/a>\n1. A comprehensive EDA has been performed on the housing price dataset with 79 features.\n2. Droping uninformative features based on mutual information score has a sligth improvement (ca.1%) compared with the baseline, which uses all features\n3. Further improvement can be made by investigating highly-correlated features, or using PCA to create new features","4d69a84d":"## Step 2: Feature Selection <a class=\"anchor\" id=\"step2\"><\/a>\n### Create a baseline <a class=\"anchor\" id=\"step2a\"><\/a>\n* Use only pre-processed features (cleaned, inmputed, and encoded)\n* Calculate the score using cross validation as the training dataset is relatively small","8a36c6b6":"### Missing values <a class=\"anchor\" id=\"step1d\"><\/a>","4eea6e48":"`SalePrice` after log transform looks much more \"normal now\". Moreover, according to the competetion evaluation, \"Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price\"\n**From now on `SalePrice_log` will be the new target.**","7d1f6008":"Some inputs (e.g. `PoolQC` and `MiscFeature`) has more than 90% values missing. Understandable, as only luxurious houses have pools, and miscellaneous features are usually only applicable to very few houses. For imputation let's use the simplest strategy:\n* Fill 0 for missing values in numerical features\n* Fill \"NA\" for missing values in categorical features. **Not using \"None\" because features like `MasVnrType` have a category \"None\" and NaN values**","4d391e39":"`SalePrice` does not follow a normal distribution, and is skewed to the right. Log transform can help.","2392de5d":"#### Label encoding for categorical values\nUse **label encoding** because of the high cardinality of categorical features, and it is usually sufficient for XGBoost and RandomForest"}}