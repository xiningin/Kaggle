{"cell_type":{"f8fbb0f1":"code","0f12fa1c":"code","def02960":"code","f460cc91":"code","df7d250a":"code","7bd97049":"code","944b6af5":"code","b92adc2e":"code","5ad133bd":"code","b61a92ad":"code","d1d07216":"code","64725c26":"code","882d80e0":"code","e20ee455":"code","7ed0f98d":"code","db133c96":"code","28de1b4a":"code","36445e14":"code","3e9da618":"code","2bb1fec2":"code","af791c27":"code","acee503b":"code","c971a87f":"code","e00f375a":"code","000f0c62":"code","e846f906":"code","ab4e4618":"code","682f3bb3":"code","8e081934":"code","4c82cf4d":"code","0898a71f":"code","953f0803":"code","2ff2d549":"code","50e68f96":"code","fc5c943a":"code","0d4dd55c":"code","b9eab0dc":"code","87b72588":"code","72086189":"code","c10a42da":"code","529787f7":"code","6aed7e9a":"code","533adc5c":"code","c858bc87":"code","baccd666":"code","f089339a":"code","f2322be3":"code","cdf192e5":"code","1633dcab":"code","70e0b0db":"code","b258f699":"code","a06d9db8":"code","4fdc2962":"code","936cca5b":"code","a4bf0adf":"code","ab29c738":"code","591a56a8":"code","b39dc580":"code","3bee227a":"code","4c3344fd":"code","46fbcecd":"code","d8991169":"code","1d3f5d16":"code","453b0afb":"code","628bb277":"code","8560b2b5":"code","835a43a0":"code","a6b82087":"code","4cb62759":"code","8bef992e":"code","f0cc9e32":"code","214f533d":"code","43bcd2a2":"code","b35c9e1c":"code","898d0b81":"code","c3352091":"code","99720902":"code","aac1861c":"code","62705500":"code","58610c0d":"code","34dd3543":"code","0e5b17b8":"code","61c787af":"code","6c596264":"code","16a11b16":"code","b3048b55":"code","d8e8189c":"code","3206f593":"code","5cf55748":"code","7bccce70":"code","60a1f72b":"code","f2707e4c":"code","d9a97a9a":"code","ed7c4789":"code","a8484af5":"code","df508fd2":"code","2d6cf4a9":"code","b6d6e22f":"code","a89a0cee":"code","38fa7da0":"code","6b0ff42a":"code","485211f1":"code","c033efa6":"code","ac646c41":"code","bdb5c1aa":"code","6fe972f4":"code","7d912ff6":"code","8018268a":"code","9c38021c":"code","91913be9":"code","a8f0d27d":"code","723f8dea":"code","e4e76ec9":"code","c82c917c":"code","398d881b":"code","fe533c9d":"code","87a51b45":"code","000ee252":"code","0a897073":"code","0d095817":"code","17d245d7":"code","1d458407":"code","e107d757":"code","f0ce4134":"code","bcdcd6b6":"code","e45bef25":"code","29c06c8c":"code","3fbfc06a":"code","709d1922":"code","82693c4c":"code","8bb8115b":"code","08ec8d3f":"code","adc7bde9":"code","56bb264f":"code","1a8b8b58":"code","6f5c324c":"code","9fb809cc":"code","810c715b":"code","cb64f66a":"code","b4c64a73":"code","7b245120":"code","d0e448f9":"code","1e00cc45":"code","84892ea9":"code","66032b0e":"code","21307ba4":"code","3fc735b8":"code","10679093":"code","61086031":"code","707eb618":"code","c60b283a":"code","4d45a87b":"code","ff30bd64":"code","7446170a":"code","943fc8a7":"code","8c8a3f38":"markdown","bb469e23":"markdown","8feb96f0":"markdown","9ac00c10":"markdown","d46c886e":"markdown","530e9404":"markdown","b648e46d":"markdown","832eca52":"markdown","9d7e6eb4":"markdown","4de48cd1":"markdown","7bcea1f0":"markdown","2545f25b":"markdown","588b4c10":"markdown","a7ad0874":"markdown","260b51ce":"markdown","24857293":"markdown","9a8b0524":"markdown","92e76f47":"markdown","3048815d":"markdown","74de9a44":"markdown","b703f804":"markdown","0b5d6b88":"markdown","5f1a71a8":"markdown","cd92b2c8":"markdown","126b3ac2":"markdown","c0271efe":"markdown","5cd28da7":"markdown","61af7c27":"markdown","985a2b2b":"markdown","370ee580":"markdown","5b70817a":"markdown","a54f6997":"markdown","cbd1ca90":"markdown","26b1da92":"markdown","43c7935e":"markdown","fb048917":"markdown","7e44cf79":"markdown","77cc79a3":"markdown","fca009af":"markdown","fc69a0ca":"markdown","25596f2f":"markdown","fde522c6":"markdown","f8190459":"markdown","b782f136":"markdown","58922971":"markdown","7d851bc8":"markdown","38d0732a":"markdown","7aacf961":"markdown","0fb7405e":"markdown","32db210c":"markdown","9a4a59bc":"markdown","f5568fa9":"markdown","9f8ea301":"markdown","498e7831":"markdown","fd8c6673":"markdown","7c656442":"markdown","d3318768":"markdown","ce0e72f4":"markdown","6d4f1324":"markdown","6b20bea1":"markdown","fae01ed4":"markdown","a0eab284":"markdown","ca609242":"markdown","9dabd7d4":"markdown","59e69be5":"markdown","66eef5ce":"markdown","1cdc378d":"markdown","deb140f0":"markdown","de3b8f18":"markdown","cfa60839":"markdown","1205e7b1":"markdown","2d261015":"markdown","d1e602aa":"markdown","9373ee11":"markdown","6276eeab":"markdown","d43d71f8":"markdown","956fe4d6":"markdown","e062e623":"markdown","2311a2f6":"markdown","05760fed":"markdown","4b53e197":"markdown","bd92dde6":"markdown","fd344cda":"markdown","43c6a57f":"markdown","28d99274":"markdown","a0649c58":"markdown","c6e82887":"markdown","7b97e2d9":"markdown","6fe2a888":"markdown","14214818":"markdown","8424b0d7":"markdown","05010b49":"markdown","d3593d39":"markdown","42d8536b":"markdown","1d845d93":"markdown","99773822":"markdown","4d2c63da":"markdown","c1aa6657":"markdown"},"source":{"f8fbb0f1":"# Standard\nimport os\nimport gc\nimport re\nimport pandas as pd\nimport numpy as np\nimport random\nimport time\nimport seaborn as sns\nsns.set(font_scale=1.4)\nsns.set_style('darkgrid')\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom scipy.stats import pearsonr\nfrom numpy.random import seed\nimport scipy as sc\n\nfrom tqdm import tqdm \nfrom joblib import parallel_backend, Parallel, delayed\n\n# Modelling\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, KFold\nfrom skmultilearn.problem_transform import BinaryRelevance\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\n\n# Saving\nimport pickle\nimport joblib\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_colwidth', 150)\n\ndef set_seed(seeed):\n    seed(seeed)\n    os.environ['PYTHONHASHSEED'] = str(seeed)\n    np.random.seed(seeed)\n    random.seed(seeed)\n    \nset_seed(42)","0f12fa1c":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    columns = list(df.columns)\n\n    for col in columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n#         else:\n#             df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","def02960":"root = '..\/input\/zindi-user-behaviour\/zindi-user-behaviour-birthday-challenge\/'\ninfo = pd.read_csv(root+'VariableDefinitions.csv').drop_duplicates()\ntrain = pd.read_csv(root+'Train.csv').drop_duplicates()\ntest = pd.read_csv(root+'Test.csv').drop_duplicates()\nsub = pd.read_csv(root+'SampleSubmission.csv').drop_duplicates()","f460cc91":"train.rename(columns={'User_ID':'UserID'}, inplace=True)\ndisplay(train.head(), train.shape)","df7d250a":"test.rename(columns={'User_ID':'UserID'}, inplace=True)\ndisplay(test.head(), test.shape)","7bd97049":"train[train['UserID']=='ID_HF9PSCY3'].sort_values(['year','month'])","944b6af5":"test[test['UserID']=='ID_HF9PSCY3'].sort_values(['year','month'])","b92adc2e":"test_vc = test['UserID'].value_counts().value_counts().to_frame()\ntest_vc = test_vc.reset_index()\ntest_vc.columns = ['Number of Months to Predict', 'Count Users']\ntest_vc.set_index('Number of Months to Predict', inplace=True)\nax = test_vc[['Count Users']].plot(kind='pie', legend=True, subplots=True)","5ad133bd":"display(sub.head(), sub.shape)","b61a92ad":"discussions = pd.read_csv(root+'Discussions.csv').drop_duplicates()\nsubmissions = pd.read_csv(root+'Submissions.csv').drop_duplicates()\ncp = pd.read_csv(root+'CompetitionPartipation.csv').drop_duplicates()\ncomments = pd.read_csv(root+'Comments.csv').drop_duplicates()\ncomps = pd.read_csv(root+'Competitions.csv').drop_duplicates()\nusers = pd.read_csv(root+'Users.csv').drop_duplicates()","d1d07216":"info = info[~info['VariableDefinition'].isna()]\ndisplay(info)","64725c26":"k = 0\nfor i in info['VariableDefinition'].values:\n    if 'merge' in i:\n        print(info.iloc[k]['VariableName'],':',i)\n    k+=1","882d80e0":"display(print('DISCUSSIONS'),discussions.head(), discussions.shape) #1\ndisplay(print('COMMENTS'), comments.head(), comments.shape)         #1\ndisplay(print('USERS'), users.head(), users.shape)                  #1\ndisplay(print('COMPETITION PARTICIPATION'), cp.head(), cp.shape)    #2\ndisplay(print('COMPETITIONS'), comps.head(), comps.shape)           #2\ndisplay(print('SUBMISSIONS'), submissions.head(), submissions.shape)#3","e20ee455":"# CLEAN THE COLUMN NAMES\ndiscussions.columns = discussions.columns.str.replace(' ', '_')\ncomments.columns = comments.columns.str.replace(' ', '_')\nusers.columns = users.columns.str.replace(' ', '_')\ncp.columns = cp.columns.str.replace(' ', '_')\ncomps.columns = comps.columns.str.replace(' ', '_')\nsubmissions.columns = submissions.columns.str.replace(' ', '_')\n\n# 1 HOT ENCODE CATORGORICAL COLUMNS FROM USER AND COMPS DF\nusers = pd.get_dummies(users, columns=['FeatureY', 'Points'], drop_first=False)\ncomps = pd.get_dummies(comps, columns=['Country','FeatureD', 'FeatureC'], drop_first=False)\n\n# CLEAN COMPETITION PARTICIPATION COLUMNS AND 1 HOT ENCODE\ncp['PublicRank'] = cp['PublicRank'].str.replace('rank ', '')\ncp['Successful_Submission_Count'] = cp['Successful_Submission_Count'].str.replace('count ', '')\ncp = pd.get_dummies(cp, columns = ['PublicRank', 'Successful_Submission_Count'], drop_first=False)\n\n# SOME COMPS ARE NEVER ENDING\ncomps['CompEndTime_Year'] = comps['CompEndTime_Year'].replace('not mapped', np.NaN).astype('float32')\n\n# THE DATA IN FEATURE COLUMNS IN OUR COMPS TABLE ARE IN LISTS, INDICATING WHICH CATORGORIES THE COMP FALLS INTO\n## LETS CREATE SOME NEW FEATURES FROM EACH FEATURE COLUMNS VALUES IN THE LISTS\n# FIRST REMOVE LIST BRACKETS SO WE ARE LEFT WITH COMMA SEPARATED VALUES\ncomps['FeatureA'] =  comps['FeatureA'].apply(lambda x:re.sub(r'\\[|\\]', '', x))\ncomps['FeatureB'] =  comps['FeatureB'].apply(lambda x:re.sub(r'\\[|\\]', '', x))\ncomps['FeatureE'] =  comps['FeatureE'].apply(lambda x:re.sub(r'\\[|\\]', '', x))\n\n# CREATE DATAFRAME OF COLUMNS INDICATING WHETHER A VALUE IS PRESENT IN THE FEATURE COLUMN FOR EACH COMPID\nfeature_A = comps.set_index('CompID').FeatureA.str.split(', ') \\\n            .apply(pd.value_counts).fillna(0).astype(int).reset_index(drop=True)\nfeature_B = comps.set_index('CompID').FeatureB.str.split(', ') \\\n            .apply(pd.value_counts).fillna(0).astype(int).reset_index(drop=True)\nfeature_E = comps.set_index('CompID').FeatureE.str.split(', ') \\\n            .apply(pd.value_counts).fillna(0).astype(int).reset_index(drop=True)\n\n# RENAME THE COLUMNS OF CREATED DATAFRAMES\nfor col in feature_A.columns:\n    feature_A.rename(columns={col:'feature_A_'+col}, inplace=True)\nfor col in feature_B.columns:\n    feature_B.rename(columns={col:'feature_B_'+col}, inplace=True)\nfor col in feature_E.columns:\n    feature_E.rename(columns={col:'feature_E_'+col}, inplace=True)\n\n# CONCAT THE CLEAN DATAFRAMES ON COMPS TABLE AND DROP THE ORIGINAL FEATURE\ncomps = pd.concat([comps, feature_A, feature_B, feature_E], axis=1).drop(['FeatureA', 'FeatureB', 'FeatureE'],1)","7ed0f98d":"display(print('DISCUSSIONS'),discussions.head(), discussions.shape) #1\ndisplay(print('COMMENTS'), comments.head(), comments.shape)         #1\ndisplay(print('USERS'), users.head(), users.shape)                  #1\ndisplay(print('COMPETITION PARTICIPATION'), cp.head(), cp.shape)    #2\ndisplay(print('COMPETITIONS'), comps.head(), comps.shape)           #2\ndisplay(print('SUBMISSIONS'), submissions.head(), submissions.shape)#3","db133c96":"def fix_incorrect_target_labels(train):\n\n    check1 = train[train['Comment']==1]['UserID'].unique()\n    print(len(check1))\n    check2 = comments[comments['UserID'].isin(check1)]['UserID'].unique()\n    print(len(check2))\n    \n    # TO BE CORRECTED USERIDS FOR COMMENTS TABLE\n    tbc = list(set(check1) - set(check2))\n    print(len(tbc))\n\n    for userid in tbc:\n        train.loc[train['UserID']==userid, 'Comment'] = 0\n\n    check1 = train[train['Comment']==1]['UserID'].unique()\n    print(len(check1))\n    check2 = comments[comments['UserID'].isin(check1)]['UserID'].unique()\n    print(len(check2))\n    \n    # SHOULD BE CORRECTED NOW: LETS CHECK\n    tbc = list(set(check1) - set(check2))\n    print(len(tbc))\n    \n    check1 = train[train['CompPart']==1]['UserID'].unique()\n    print(len(check1))\n    check2 = cp[cp['UserID'].isin(check1)]['UserID'].unique()\n    print(len(check2))\n    tbc = list(set(check1) - set(check2))\n    print(len(tbc))\n\n    for userid in tbc:\n        train.loc[train['UserID']==userid, 'CompPart'] = 0\n\n    check1 = train[train['CompPart']==1]['UserID'].unique()\n    print(len(check1))\n    check2 = cp[cp['UserID'].isin(check1)]['UserID'].unique()\n    print(len(check2))\n    tbc = list(set(check1) - set(check2))\n    print(len(tbc))\n\n    train['Target'] = train['CompPart'] + train['Comment'] + train['Sub'] + train['Disc']\n    train.loc[train['Target']>0, 'Target'] = 1\n    return train\n\ntrain = fix_incorrect_target_labels(train)","28de1b4a":"user_comps = cp.merge(comps, on='CompID', how='left')\ndisplay(user_comps.head(), user_comps.shape)","36445e14":"# CREATE EMPTY DATAFRAME TO STORE AGGREGATIONS\nuser_comps_agg = pd.DataFrame()\n# CREATE GROUBY OBJECT FOR THE MONTH AND YEAR THE A USER PARTICIPATED IN A COMP\nuc_groupby = user_comps.groupby(['UserID', 'CompPartCreated_Year', 'CompPartCreated_Month'])\n\nfor col in user_comps.columns:\n    for i in ['Rank', 'Submission_Count', 'feature', 'Country', 'FeatureD', 'Points_Reward',\n             'SubmissionLimitPerDay']:\n        if i in col:\n            # SUM ALL THESE FEATURES FOR A USER BY MONTH YEAR AND ADD TO AGG DF\n            user_comps_agg[col + '_sum'] = uc_groupby.agg({col:np.sum})\n            \nfor col in user_comps.columns:\n    for i in ['CompPartCreated_Day_of_week','Kind', 'SecretCode', 'CompStartTime_Year',\n              'CompStartTime_Month', 'CompStartTime_Day_of_week', 'CompEndTime_Month',\n              'CompEndTime_Year']:\n        if i in col:\n            # GET MEAN OF ALL THESE FEATURES FOR A USER BY MONTH YEAR AND ADD TO AGG DF\n            user_comps_agg[col + '_mean'] = uc_groupby.agg({col:np.mean})\n\n# FIND OUT HOW MANY COMPS A USER JOINED DURING EACH MONTH\nuser_comps_agg['count_comps'] = uc_groupby.agg({'UserID':pd.Series.count})\nuser_comps_agg = user_comps_agg.reset_index()\n# RENAME COLUMNS FOR EASY MERGE WITH TRAIN COLUMNS\nuser_comps_agg.rename(columns={'CompPartCreated_Month':'month', 'CompPartCreated_Year':'year'}, inplace=True)\nuser_comps_agg.set_index(['UserID', 'month', 'year'], inplace=True)\ndisplay(user_comps_agg.head(), user_comps_agg.shape)","3e9da618":"display(submissions)","2bb1fec2":"def diff(df):\n    \"\"\"\n    Function to calculate circular difference between two days of the week where each day is\n    consecutive and labeled from 1 - 7.\n    \n    eg. The mathematical difference between 7 and 1 is 6 but the the circular difference \n        between day 7 and day 1 is 1. \n    \n    input:   df[['day of week', 'day of week shitfed']]\n    returns: series with circular differenece along axis 1.\n    \"\"\"\n    min_ = df.min(axis=1).reset_index(drop=True)\n    max_ = df.max(axis=1).reset_index(drop=True)\n    diff = max_ - min_\n    d = diff.where(diff<=4, abs(6 - max_))\n    return d.reset_index(drop=True)","af791c27":"%%time\n# SORT SUBMISSIONS\nsubmissions = submissions.sort_values(['SubDate_Year', 'SubDate_Month', 'SubDate_Day_of_week'])\n\n# APPLY DIFF FUNCTION\ngroups = []\nfor [uid, cid, y, m], df in tqdm(submissions.groupby(['UserID', 'CompID', 'SubDate_Year','SubDate_Month'])):\n    df.reset_index(drop=True, inplace=True)\n    df['SubDate_Day_of_week_shift'] = df['SubDate_Day_of_week'].shift()\n    df['sub_time_diff'] = diff(df[['SubDate_Day_of_week','SubDate_Day_of_week_shift']])\n    groups.append(df)\n    \nsubmissions_df = pd.concat(groups, axis=0)\n\nsubmissions_df.drop(['SubDate_Day_of_week_shift'],1,inplace=True)\nsubmissions = submissions_df.copy()\ndel submissions_df\ngc.collect()","acee503b":"# CREATE EMPTY DATARFRAME TO STORE AGGREGATED DATA\nsubmissions_agg = pd.DataFrame()\n# CREATE GROUBY OBJECT FOR THE MONTH AND YEAR A USER SUMMITED TO A COMP\nsubs_groupby = submissions.groupby(['UserID', 'CompID', 'SubDate_Year','SubDate_Month'])\n            \nfor col in submissions.columns:\n    for i in ['SubDate_Day_of_week','FeatureG', 'sub_time_diff']:\n        if i in col:\n            # MEAN VALUES FOR COLUMNS PER COMP IN A MONTH\n            submissions_agg[col + '_mean'] = subs_groupby.agg({col:np.mean})\n\n# NUMBER OF SUBS TO COMP IN A MONTH PER USER\nsubmissions_agg['count_subs'] = subs_groupby.agg({'UserID':pd.Series.count})\n\nsubmissions_agg = submissions_agg.reset_index()\n\n# NUMBER OF COMPS A USER MADE SUBMISSIONS TO\nsubmissions_agg['count_comps_sub'] = submissions_agg.groupby(['UserID', \n                                                              'SubDate_Year',\n                                                              'SubDate_Month'])['UserID'].transform('count')\n# TOTAL NUMBER OF SUBS MADE TO COMPS IN A MONTH\nsubmissions_agg['count_subs'] = submissions_agg.groupby(['UserID', \n                                                         'SubDate_Year',\n                                                         'SubDate_Month'])['count_subs'].transform('sum')\n\n# MEAN VALUES FOR COLUMNS IN A MONTH (COMBINE INFO FROM COMPS SUBBED TO)\nfor col in ['FeatureG_mean', 'SubDate_Day_of_week_mean', 'sub_time_diff_mean']:\n    submissions_agg[col] = submissions_agg.groupby(['UserID', \n                                                    'SubDate_Year',\n                                                    'SubDate_Month'])[col].transform('mean')\n\n# RENAME COLUMNS FOR EASY MERGE WITH TRAIN DF\nsubmissions_agg.rename(columns={'SubDate_Month':'month', 'SubDate_Year':'year'}, inplace=True)\n\n# DROP COMPID AS IT IS NO LONGER REQUIRED\nsubmissions_agg = submissions_agg.drop(['CompID'],1).drop_duplicates().set_index(['UserID', \n                                                                                  'month',\n                                                                                  'year'])\n\ndisplay(submissions_agg)","c971a87f":"train = pd.read_csv(root+'Train.csv').drop_duplicates()\ntrain.rename(columns={'User_ID':'UserID'}, inplace=True)\ntrain = fix_incorrect_target_labels(train)\ntrain = train.set_index(['UserID', 'month', 'year'])\nprint(train.shape)","e00f375a":"train = train.merge(submissions_agg, left_index=True, right_index=True, how='left')\ntrain = train.merge(user_comps_agg, left_index=True, right_index=True, how='left')\n\ntrain = train.reset_index()\ntrain = train.sort_values(['year','month'])\n\nprint('\\n\\n NEW TRAIN DATAFRAME: \\n')\ndisplay(train)","000f0c62":"# RENAME COLUMNS FOR EASY MERGE WITH TRAIN DF\ncomments.rename(columns={'User_ID':'UserID', 'CommentDate_Year': 'year', \n                         'CommentDate_Month': 'month'}, inplace=True)","e846f906":"# FIND NUMBER OF COMMENTS MADE BY USERS IN A MONTH\ncomments['count_comments'] = comments.groupby(['UserID', 'year', 'month'])['UserID'].transform('count')\n# GET THE MEAN DAY OF WEEK EACH USER MADE A COMMENT ON\ncomments['CommentDate_Day_of_week'] = comments.groupby(['UserID', 'year', 'month'])['CommentDate_Day_of_week'].\\\n                                                                                     transform('mean')\ncomments = comments.drop_duplicates()\ndisplay(comments)","ab4e4618":"train = train.merge(comments, on = ['UserID', 'year', 'month'], how='left')\ndisplay(train)","682f3bb3":"train = train.merge(users, on='UserID', how='left')\ndisplay(train)","8e081934":"train[['count_subs', 'count_comps_sub', 'count_comps', 'count_comments']] = \\\ntrain[['count_subs', 'count_comps_sub', 'count_comps', 'count_comments']].fillna(0)\n\n# 1 HOT ENCODE THE CATOROGORICAL COUNTY COLUMN\ntrain = pd.get_dummies(train, columns=['Country'])\n\n# SORT\ntrain = train.sort_values(['UserID','year','month'])","4c82cf4d":"# SUBSET THE COMPETETION PARTICIPATION DATAFRAME FOR THE LAST COMP A USER JOINED PER MONTH\ncp = cp.sort_values(['UserID', 'CompPartCreated_Year', \n                     'CompPartCreated_Month', \n                     'CompPartCreated_Day_of_week']).drop_duplicates(subset=\n                                                                     ['UserID', 'CompPartCreated_Year', \n                                                                      'CompPartCreated_Month'], keep='last')\ndisplay(cp)","0898a71f":"# MERGE WITH COMPS INFO TABLE\nlatest_cp = pd.merge(cp,comps, on='CompID', how='left')\n# 1 HOT ENCODE THE COMP_ID COLUMN\nlatest_cp = pd.get_dummies(latest_cp, columns=['CompID'])\n\n# DO VERY MINIMAL FEATURE SELECTION \ncols = ['UserID'] + [c for c in latest_cp.columns if 'Comp' in c] + \\\n       [c for c in latest_cp.columns if 'Feature' in c] + [c for c in latest_cp.columns if 'feature' in c]\nlatest_cp = latest_cp[cols]\n\nlatest_cp.rename(columns={'CompPartCreated_Month':'month', 'CompPartCreated_Year':'year'}, inplace=True)\ndisplay(latest_cp.head(), latest_cp.shape)","953f0803":"# FOR MERGING ONTO THE TRAIN DF WE WILL CREATE A DATE COLUMN BY MAPPING A MONTH AND YEAR NAME TO EACH MONTH,YEAR\nyear_dict = {1:2015,2:2016,3:2017,4:2018}\nmonth_dict = {1:'January',2:'February',3:'March',4:'April',5:'May',6:'June', 7:'July',8:'August',9:'September', 10:'October', 11:'November', 12:'December'}\ntrain['date_year'] = train['year'].map(year_dict)\ntrain['date_month'] = train['month'].map(month_dict)\n\nlatest_cp['date_year'] = latest_cp['year'].map(year_dict)\nlatest_cp['date_month'] = latest_cp['month'].map(month_dict)","2ff2d549":"# CONVERT THE DATE COLUMN TO DATETIME\ntrain['date'] = pd.to_datetime(train['date_year'].astype(str)  + train['date_month'], format='%Y%B')\nlatest_cp['date'] = pd.to_datetime(latest_cp['date_year'].astype(str)  + latest_cp['date_month'], format='%Y%B')\nlatest_cp = latest_cp.drop(['year', 'month', 'date_year', 'date_month'],1)\n\n# SORT BY DATE\nlatest_cp = latest_cp.sort_values('date')\ntrain = train.sort_values('date')","50e68f96":"train = pd.merge_asof(train, latest_cp, by='UserID', on='date')\ntrain = train.drop(['date', 'date_year', 'date_month'],1)\ntrain = train.sort_values(['UserID','year','month'])","fc5c943a":"# CREATE EMPTY DATAFRAME\nnunique_df = pd.DataFrame()\n\n# FILL THE DATAFRAME WITH NUMBER OF UNIQUE FEATURES FOR EACH COLUMN IN TRAIN DF\nfeatures = [c for c in train.columns if c not in ['UserID', 'month', 'year']]\nfor col in features:\n    nunique_df[col] = [train[col].nunique()]\n\nnunique_df = nunique_df.T.reset_index()\nnunique_df.rename(columns={0:'n_unique', 'index':'feature'}, inplace=True)\nnunique_df = nunique_df.sort_values('n_unique')\ndisplay(nunique_df)","0d4dd55c":"%%time\n# SELECT COLUMNS\nccols = nunique_df[nunique_df['n_unique']>=18]['feature'].tolist()+['CompPart', 'Comment', 'Sub', 'Disc','Target']\n\n# CREATE CUMSUM FEATURES FROM SELECTED COLUMNS\nfor col in tqdm(ccols):\n    train[f'{col}_cumsum'] = train.groupby('UserID')[col].cumsum()","b9eab0dc":"%%time\n# REPLACE INFINITE VALUES\ntrain = train.replace([np.inf, -np.inf], np.nan)\n# FILL NaNs BY FORWARD FILLING COLUMNS\ntrain = train.groupby('UserID').apply(lambda x: pd.DataFrame.ffill(x))","87b72588":"for col in tqdm(['CompPart', 'Comment', 'Sub', 'Disc','Target']):\n    for shift in [1,2,3]:\n        train[col+f'_shift{shift}'] = train.groupby('UserID')[col].shift(shift)","72086189":"train['cumcount'] = train.groupby(['UserID']).cumcount()\ntrain['cumcount'] = train['cumcount']+1","c10a42da":"ccols = ['SubmissionLimitPerDay_sum_cumsum', 'CompPartCreated_Day_of_week_mean_cumsum', 'CompStartTime_Month_mean_cumsum', \n         'Points_Reward_sum_cumsum', 'CompStartTime_Day_of_week_mean_cumsum', 'CompEndTime_Month_mean_cumsum']\nfor col in tqdm(ccols):\n    train[f'{col}_div_cumcount'] = train[col]\/train['cumcount']\n\ndel train['cumcount']\ngc.collect()","529787f7":"display(train.head(10))","6aed7e9a":"PREDICT_CP = False    \nLOAD_SAVED_CP = False \nINSPECT_CP = True # YOU CAN LEAVE THIS AS TRUE TO SEE WHAT I MEANT ABOVE","533adc5c":"if PREDICT_CP or LOAD_SAVED_CP or INSPECT_CP:\n    \n    # LOAD AND PREPROCESS THE COMP PARTICIPATION TABLE AGAIN\n    cp_ = pd.read_csv(root+'CompetitionPartipation.csv').drop_duplicates()\n    cp_.columns = cp_.columns.str.replace(' ', '_')\n    cp_['PublicRank'] = cp_['PublicRank'].str.replace('rank ', '')\n    cp_['Successful_Submission_Count'] = cp_['Successful_Submission_Count'].str.replace('count ', '')\n    cp_ = pd.get_dummies(cp_, columns = ['PublicRank', 'Successful_Submission_Count'], drop_first=False)\n    display(cp_)","c858bc87":"if PREDICT_CP or LOAD_SAVED_CP or INSPECT_CP:\n    \n    # CREATE NEW DF FROM SORTED COMPS INFO TABLE\n    comps_df = comps.sort_values(['CompStartTime_Year', 'CompStartTime_Month', 'Points_Reward'])\n    # SORT CP DF BY DATE JOINED COMP\n    cp_ = cp_.sort_values(['CompPartCreated_Year', 'CompPartCreated_Month'])\n    # CREATE MONTH YEAR IDENTIFIER\n    comps_df['start_my'] = comps_df['CompStartTime_Month'].astype(str) +'_'+comps_df['CompStartTime_Year'].astype(str) \n    cp_['start_my'] = cp_['CompPartCreated_Month'].astype(str) +'_'+cp_['CompPartCreated_Year'].astype(str)\n    # ADD YEAR 4 MONTHS TO INDENTIFIERS\n    my = list(cp_['start_my'].unique()) + ['1_4', '2_4', '3_4']\n    print(my)\n    \n    # CREATE DICT TO ORDINALLY ENCODE THE MY VALUES \n    mydict = {}\n    for k, v in zip(my,np.arange(len(my))):\n        mydict[k] = v\n        \n    # ENCODE\n    comps_df['start_my'] = comps_df['start_my'].map(mydict)\n    cp_['start_my'] = cp_['start_my'].map(mydict)\n    print(comps_df['start_my'].unique())\n    display(comps_df)","baccd666":"def get_cp_prediction(userid, user_cp_preds1, user_cp_preds2, user_cp_preds3):\n    \"\"\"\n    Funtion to train a model to predict the probability or a user joining a competiton scheduled to start\n    1 month ahead, 2 months ahead and 3 months ahead of the current month, respectively.\n    \n    userid:           str  - The user identifier\n    user_cp_preds1:   dict - The dictionary predictions for 1 month ahead for all users will be stored in \n    user_cp_preds2:   dict - The dictionary predictions for 2 months ahead for all users will be stored in \n    user_cp_preds3:   dict - The dictionary predictions for 3 months ahead for all users will be stored in \n    \n    returns:\n    NONE - probabilities are stored in the dictionaries given as input\n    \n    \"\"\"\n    \n    # FIND COMPETITIONS THE USER JOINED\n    ucomps = cp_[cp_['UserID']==userid]['CompID'].values\n    \n    # LABEL THOSE COMPS AS OUR TARGET POSITIVE CLASS\n    comps_df['target'] = 0\n    comps_df.loc[comps_df['CompID'].isin(ucomps), 'target'] = 1\n    \n    # GET THE FIRST DATE A USER JOINED A COMP\n    unique_my = cp_[cp_['UserID']==userid]['start_my'].unique()\n    \n    # CREATE DICTS INSIDE THE INPUT DICTS TO HOLD OUR PREDICTIONS FOR EACH MONTH\n    user_cp_preds1[userid]={}\n    user_cp_preds2[userid]={}\n    user_cp_preds3[userid]={}\n    \n    # FOR EACH DATE SINCE A USER JOINED A COMP TILL THE LAST DATE OF THE TRAIN DATAFRAME: DO THIS->\n    for my in list(range(unique_my[0],33)):\n        # FOR EACH DICT AND MONTH AHEAD DO THIS ->\n        for pred_dict, offset in zip([user_cp_preds1, user_cp_preds2, user_cp_preds3],[1,2,3]):\n            \n            # OUR TRAINING SAMPLES ARE ALL COMPS BEFORE AND EQUAL TO THE THE DATE THE USE JOINED A COMP\n            X = comps_df[comps_df['start_my']<=my].drop(['CompID',\n                                                         'target']+drop,1).fillna(-9999)\n            \n            y = comps_df[comps_df['start_my']<=my]['target']\n            \n            # OUR TEST SAMPLES ARE THE ONES WE NEED PREDICTIONS FOR AND ARE THOSE COMPS\n            # WHICH START THE FOLLING MONTH OR THE NEXT MONTH OR THE NEXT NEXT MONTH\n            test_c = comps_df[comps_df['start_my']==my+offset].drop(['CompID',\n                                                                     'target']+drop,1).fillna(-9999)\n            \n            # IF WE DONT HAVE ANY COMPS STARTING THE NEXT MONTH WE CANT PREDICT AND CONTINUE THE LOOP\n            if len(test_c) == 0:\n                pred_dict[userid][my] = np.nan\n                \n            # OTHERWISE WE GET STRAIGHT TO THE TRAINING\n            else:\n                # SCALE\n                scaler = MinMaxScaler()\n                X = scaler.fit_transform(X)\n                test_c = scaler.transform(test_c)\n                \n                # I USE A LIGHT GBM WITH DEFAULT PARAMS BUT WITH N_JOBS = 1 BECUASE I ONLY WANT TO USE \n                # ONE PROCESSOR TO FIT THE MODEL AS IT DOESN'T REQUIRE MORE BECUASE THE DATA IS SMALL.\n                ## MAINLY I SET THIS PARAMETER BECAUSE I WOULD LIKE TO RUN THIS FUNCTION ON MULTIPLE \n                ## USERS AT THE SAME TIME USING MULTIPROCESSING SINCE THE NUMBER OF USERS ARE LARGE. \n                ## AND I'LL GET A GOOD SPEED UP. \n                model = LGBMClassifier(max_depth=3,n_jobs=1)\n                \n                # TRAIN THE MODEL\n                model.fit(X, y)\n                # PREDICT ON TEST SET\n                preds = model.predict_proba(test_c)[:,1]\n                \n                # WE WILL HAVE MULTIPLE PREDICTIONS FOR DIFFERENT COMPS BUT ONLY NEED ONE PROBABILTY OF \n                # A USER JOINING A COMP AND THEREFORE GET THE MEAN OF ALL PREDICTIONS,\n                \n                # THE PROBABILITY IS ADDED TO THE PREDICTION DICT WITH DATE BEING THE KEY\n                pred_dict[userid][my] = preds.mean()","f089339a":"%%time\nif PREDICT_CP or LOAD_SAVED_CP:\n    # CREATE DICTS TO STORE PREDICTIONS\n    user_cp_preds1, user_cp_preds2, user_cp_preds3 = {},{},{}\n    \n    # SELECT ONLY USERS WHO JOINED 2 OR MORE COMPS\n    cp_vc = cp_['UserID'].value_counts().to_frame()\n    cp_vc = cp_vc[cp_vc['UserID']>9]\n    cp_users = cp_vc.index\n    print(len(cp_users))\n    \n    \n    \n    # USE PARALLEL PROCESSING TO RUN THE CP PREDICTION ON MULTIPLE USERS AT ONCE. THIS SPEEDS UP COMPUTATION TIME.\n    \n    with parallel_backend('threading', n_jobs=2):\n        Parallel()(delayed(get_cp_prediction)(userid, user_cp_preds1, user_cp_preds2, user_cp_preds3) for userid in  cp_users[:20].values)\n    \n    # CREATE DATAFRAME FROM THE DICT OF OUR PROBABILITY PREDICTIONS FOR 1 MONTH AHEAD\n    cp_udf = pd.DataFrame(user_cp_preds1)\n    cp_udf = cp_udf.sort_index()\n    display(cp_udf)","f2322be3":"%%time\nif PREDICT_CP:\n    user_cp_preds1, user_cp_preds2, user_cp_preds3 = {},{},{}\n\n    cp_vc = cp_['UserID'].value_counts().to_frame()\n    cp_vc = cp_vc[cp_vc['UserID']>9]\n    cp_users = cp_vc.index\n    print(len(cp_users))\n\n    with parallel_backend('threading', n_jobs=2):\n        Parallel()(delayed(get_cp_prediction)(userid, user_cp_preds1, user_cp_preds2, user_cp_preds3) for userid in tqdm(cp_users.values))\n\n    cp_udf = pd.DataFrame(user_cp_preds1)\n    cp_udf = cp_udf.sort_index()\n    display(cp_udf)\n    \n    \n    # SAVE DICTIONARIES USING PICKLE.DUMP\n    with open('user_cp_preds1.pkl', 'wb') as f:\n        pickle.dump(user_cp_preds1,f)\n\n    with open('user_cp_preds2.pkl', 'wb') as f:\n        pickle.dump(user_cp_preds2,f)\n\n    with open('user_cp_preds3.pkl', 'wb') as f:\n        pickle.dump(user_cp_preds3,f)","cdf192e5":"if LOAD_SAVED_CP:\n    \n    path_to_dicts = '..\/input\/user-activity-cp-preds\/'\n    \n    with open(path_to_dicts+'user_cp_preds1.pkl','rb') as f:\n        user_cp_preds1 = pickle.load(f)\n        \n    with open(path_to_dicts+'user_cp_preds2.pkl','rb') as f:\n        user_cp_preds2 = pickle.load(f)\n        \n    with open(path_to_dicts+'user_cp_preds3.pkl','rb') as f:\n        user_cp_preds3 = pickle.load(f)","1633dcab":"if PREDICT_CP or LOAD_SAVED_CP:\n    \n    cp_udf1 = pd.DataFrame(user_cp_preds1)\n    cp_udf1 = cp_udf1.sort_index()\n    # STACK IT AND RESET INDEX TO USERID AS A COLUMN FOR QUICK MERGE ONTO TRAIN DF\n    cp_udf1 = cp_udf1.stack().to_frame().reset_index()\n    cp_udf1.columns = ['my', 'UserID', 'cp_prob1']\n    display(cp_udf1.head())\n\n    cp_udf2 = pd.DataFrame(user_cp_preds2)\n    cp_udf2 = cp_udf2.sort_index()\n    cp_udf2 = cp_udf2.stack().to_frame().reset_index()\n    cp_udf2.columns = ['my', 'UserID', 'cp_prob2']\n    display(cp_udf2.head())\n\n    cp_udf3 = pd.DataFrame(user_cp_preds3)\n    cp_udf3 = cp_udf3.sort_index()\n    cp_udf3 = cp_udf3.stack().to_frame().reset_index()\n    cp_udf3.columns = ['my', 'UserID', 'cp_prob3']\n    display(cp_udf3.head())","70e0b0db":"%%time\nif PREDICT_CP or LOAD_SAVED_CP:\n    \n    # CREATE COLUMN IN TRAIN DF WITH SAME MAPPING TO MERGE ON\n    train['my'] = train['month'].astype(str)+\"_\"+train['year'].astype(str)\n    train['my'] = train['my'].map(mydict)\n\n    # MERGE\n    train = train.merge(cp_udf1, on=['UserID', 'my'], how='left')\n    train = train.merge(cp_udf2, on=['UserID', 'my'], how='left')\n    train = train.merge(cp_udf3, on=['UserID', 'my'], how='left')\n\n    del cp_udf1, cp_udf2, cp_udf3, train['my']","b258f699":"if PREDICT_CP or LOAD_SAVED_CP:\n    display(train[train['UserID'].isin(user_cp_preds1.keys())][['UserID', 'month', 'year', 'cp_prob1', 'cp_prob2', 'cp_prob3']])","a06d9db8":"# USING THE LATEST JOINED COMP INFO AS FEATURES MEANS WE NOW HAVE COLUMNS WE CAN CALCULATE TIME DIFFERENCE ON IN OUR TRAIN DF\n\nfeatures = ['UserID','month','year','CompStartTime_Month','CompStartTime_Year','CompEndTime_Month','CompEndTime_Year']\n\n# SUBSET TRAIN DF FOR USERS WHO TOOK PART IN A COMPETITION\ncomps_ = train[train['CompPart']==1][features].sort_values(['CompStartTime_Year','CompStartTime_Month'])\ncomps_","4fdc2962":"print(comps_.info('deep'))","936cca5b":"# FILLING NaN ENDTIME_MONTH AND ENDTIME_YEAR \nend_month = comps_[comps_['CompEndTime_Year']==comps_['CompEndTime_Year'].max()]['CompEndTime_Month'].max()\ncomps_['CompEndTime_Month'] = comps_['CompEndTime_Month'].fillna(end_month)\ncomps_['CompEndTime_Year'] = comps_['CompEndTime_Year'].fillna(comps_['CompEndTime_Year'].max())\n\n# CAST COLUMNS TO INT DTYPE \nc = [c for c in comps_.columns if c not in ['UserID']]\ncomps_[c] = comps_[c].astype(int)","a4bf0adf":"# CREATE DATE IDENTIFIER \ncomps_['my'] = comps_['month'].astype(str) +'_'+comps_['year'].astype(str)\ncomps_['m_y'] = comps_['CompStartTime_Month'].astype(str) +'_'+comps_['CompStartTime_Year'].astype(str)\ncomps_['end_m_y'] = comps_['CompEndTime_Month'].astype(str) +'_'+comps_['CompEndTime_Year'].astype(str)\ncomps_ = comps_.sort_values(['year','month'])\nmy = list(comps_['my'].unique())\ncomps_ = comps_.sort_values(['CompStartTime_Year','CompStartTime_Month'])\nm_y = list(comps_['m_y'].unique())\ncomps_ = comps_.sort_values(['CompEndTime_Year','CompEndTime_Month'])\nend_m_y = list(comps_['end_m_y'].unique())\ncomps_ = comps_.sort_values(['CompStartTime_Year','CompStartTime_Month']).reset_index(drop=True)\nprint(m_y)\nprint(end_m_y)\ndisplay(comps_)","ab29c738":"# COMBINE ALL DATE IDENTIFIERS AND SORT THEM \ndates = my+m_y+end_m_y\ndates = sorted(dates, key = lambda x: (int(x.split('_')[1]), int(x.split('_')[0])))\n\n# GET ONLY THE UNIQUE DATES BY USING THE FACT THAT DICTIONARIES CAN'T HAVE MULTIPLE KEYS WITH SAME NAME.\n# i.e. CREATE DICT AND CONVERT TO LIST\ndates = list(dict.fromkeys(dates))\nprint(dates)","591a56a8":"# ORDINALLY ENCODE DATAES AND SAVE IN DICT\nm_ydict = {}\nfor k, v in zip(dates,np.arange(len(dates))):\n    m_ydict[k] = v\n\n# MAP DICT OF DATES ONTO COMP_DF DATE IDENTIFIER\ncomps_['my'] = comps_['my'].map(m_ydict)\ncomps_['m_y'] = comps_['m_y'].map(m_ydict)\ncomps_['end_m_y'] = comps_['end_m_y'].map(m_ydict)\n\n# CALCULATE COMP DURATION AS END TIME - START TIME\ncomps_['comp_duration'] = comps_['end_m_y'] - comps_['m_y']\n\n# INITIATE CALCULATION OF MONTHS LEFT AS END TIME - CURRENT TIME\ncomps_['months_left'] = comps_['end_m_y'] - comps_['my']\ndisplay(comps_)","b39dc580":"display(comps_[comps_['UserID']=='ID_4S9JJCZA'])","3bee227a":"train = train.merge(comps_[['UserID', 'year', 'month', 'comp_duration', 'months_left']], on = ['UserID', 'year', 'month'], how='left')\n\n# FILL NaN END TIME MONTH\/YEAR IN TRAIN DF\nend_month = train[train['CompEndTime_Year']==train['CompEndTime_Year'].max()]['CompEndTime_Month'].max()\ntrain['CompEndTime_Month'] = train['CompEndTime_Month'].fillna(end_month)\ntrain['CompEndTime_Year'] = train['CompEndTime_Year'].fillna(comps_['CompEndTime_Year'].max())\n\n# FILL NaN COMP DURATION BY FORWARD FILLING WITH VALUE OF LAST COMP JOINED\ntrain['comp_duration'] = train.groupby('UserID')['comp_duration'].ffill()\ndisplay(train)","4c3344fd":"cols = ['UserID','month','year', 'CompPart']+['CompStartTime_Month','CompStartTime_Year','CompEndTime_Month',\n                                              'CompEndTime_Year', 'comp_duration', 'months_left'] \ndf = train[train['UserID']=='ID_4S9JJCZA'][cols]\ndf","46fbcecd":"mask = df['months_left'].isna()\ncumSum = mask.cumsum()\nposCumSum = (cumSum - cumSum.where(~mask).ffill().fillna(0).astype(int))\nposCumSum","d8991169":"df['months_left'] = df['months_left'].ffill() - posCumSum\ndf","1d3f5d16":"cpuids = train[train['CompPart']==1]['UserID'].unique()","453b0afb":"%%time\ndef cal_months_left(df):\n    \"\"\"\n    Function to fill NaN months left positions appropriately. \n    \"\"\"\n    # ONLY USE USERS WHO TOOK PART IN A COMP\n    if df['UserID'].values[0] in cpuids:\n        positions = df['months_left'].isna().astype(int)\n        mask = df['months_left'].isna()\n        cumSum = mask.cumsum()\n        posCumSum = (cumSum - cumSum.where(~mask).ffill().fillna(0).astype(int))\n        df['months_left'] = df['months_left'].ffill() - posCumSum\n        return df\n    else:\n        return df\n\ntrain = train.groupby('UserID').apply(cal_months_left)","628bb277":"%%time\ndef last_active(df):\n    df = df.reset_index(drop=True).reset_index()\n    df['months_since_Disc'] = df['index']-df['index'].where(df[\"Disc\"]==1).ffill()\n    df['months_since_Comment'] = df['index']-df['index'].where(df[\"Comment\"]==1).ffill()\n    df['months_since_CompPart'] = df['index']-df['index'].where(df[\"CompPart\"]==1).ffill()\n    df['months_since_Sub'] = df['index']-df['index'].where(df[\"Sub\"]==1).ffill()\n    df['months_since_Target'] = df['index']-df['index'].where(df[\"Target\"]==1).ffill()\n    df.drop('index',1, inplace=True)\n    return df\n\ntrain = train.groupby('UserID').apply(last_active)\ntrain = train.reset_index(drop=True)","8560b2b5":"train = train.fillna(-9999)\ndisplay(train)","835a43a0":"# EXTRACTING THE TESTSET\nfor_test = train[(train['year']==3)&(train['month']==12)]\n\n# REDUCE MEMORY USAGE OF DATAFRAME\ntrain = reduce_mem_usage(train)\n# COPY THE TRAIN DATAFRAME INTO NEW VARIABLE FOR USE LATER\ntrain_3 = train.copy()","a6b82087":"display(for_test.head(), for_test.shape)","4cb62759":"train[train['UserID']=='ID_5DHW13OH']","8bef992e":"test[test['UserID']=='ID_5DHW13OH']","f0cc9e32":"# SAVE TRAIN FOR USE LATER\ntrain.reset_index(drop=True).to_feather('train.feather')","214f533d":"# CREATING DATAFRAME FOR PREDICTING 1 MONTH AHEAD\n## GETTING THE VALUE COUNTS\ntrain_vc = train['UserID'].value_counts().to_frame().reset_index()\n## SELECTING USERS WITH VALUE COUNTS == 1\ntrain_new_users = train_vc[train_vc['UserID'].isin([1])]['index'].values\n## EXCLUDING THOSE USERS FROM FINAL DATAFRAME\ntrain_1 = train_3[~train_3['UserID'].isin(train_new_users)]\ndisplay(train_1)\n\n# CREATING DATAFRAME FOR PREDICTING 2 MONTHS AHEAD\n## GETTING THE VALUE COUNTS\ntrain_vc = train['UserID'].value_counts().to_frame().reset_index()\n## SELECTING USERS WITH VALUE COUNTS == 1 AND 2\ntrain_new_users = train_vc[train_vc['UserID'].isin([1,2])]['index'].values\n## EXCLUDING THOSE USERS FROM FINAL DATAFRAME\ntrain_2 = train_3[~train_3['UserID'].isin(train_new_users)]\ndisplay(train_2)\n\n# CREATING DATAFRAME FOR PREDICTING 3 MONTHS AHEAD\n## GETTING THE VALUE COUNTS\ntrain_vc = train['UserID'].value_counts().to_frame().reset_index()\n## SELECTING USERS WITH VALUE COUNTS == 1 ,2 AND 3\ntrain_new_users = train_vc[train_vc['UserID'].isin([1,2,3])]['index'].values\n## EXCLUDING THOSE USERS FROM FINAL DATAFRAME\ntrain_3 = train_3[~train_3['UserID'].isin(train_new_users)]\ndisplay(train_3)","43bcd2a2":"%%time\n# TARGET FOR PREDICTING 1 MONTH AHEAD ADDED TO TRAIN 1\ntrain_1['target1'] = train_1.groupby('UserID')['Target'].shift(-1)\n## DROP MONTHS FOR WHICH WE CANNOT CALCULATE THE TARGET\ntrain_1 = train_1.dropna(axis = 0, subset=['target1'])\ntrain_1 = train_1.reset_index(drop=True)\ntrain_1.reset_index(drop=True).to_feather('train_1.feather')\ndisplay(train_1)\n    \n# TARGET FOR PREDICTING 2 MONTH AHEAD ADDED TO TRAIN 2\ntrain_2['target2'] = train_2.groupby('UserID')['Target'].shift(-2)\n## DROP MONTHS FOR WHICH WE CANNOT CALCULATE THE TARGET\ntrain_2 = train_2.dropna(axis = 0, subset=['target2'])\ntrain_2 = train_2.sort_values(['year', 'month'])\ntrain_2 = train_2.reset_index(drop=True)\ntrain_2.reset_index(drop=True).to_feather('train_2.feather')\ndisplay(train_2)\n    \n# TARGET FOR PREDICTING 3 MONTH AHEAD ADDED TO TRAIN 3  \ntrain_3['target3'] = train_3.groupby('UserID')['Target'].shift(-3)\n## DROP MONTHS FOR WHICH WE CANNOT CALCULATE THE TARGET\ntrain_3 = train_3.dropna(axis = 0, subset=['target3'])\ntrain_3 = train_3.sort_values(['year', 'month'])\ntrain_3 = train_3.reset_index(drop=True)\ntrain_3.reset_index(drop=True).to_feather('train_3.feather')\ndisplay(train_3)","b35c9e1c":"fig, ax = plt.subplots(1,3, figsize=(9,3))\nfor i,column in zip([0,1,2], [train_1['target1'], train_2['target2'],train_3['target3']]):\n    column.value_counts(normalize=True).plot(kind='bar', ax=ax[i], title=column.name,ylabel='counts')","898d0b81":"%%time\nCORR=True\ntargets = ['target1', 'target2', 'target3']\ndataframes = [train_1, train_2, train_3]\n\nif CORR:\n    for df, target in zip(dataframes, targets):\n        print('\\n\\t', target.upper())\n        features = [c for c in df.columns if c not in ['UserID', target]]\n        corrs = {}\n        for c in tqdm(features):  \n            try:\n                corrs[c] = np.corrcoef(df[c], df[target])[0][1]\n            except:\n                pass\n\n        corr_df = pd.DataFrame(corrs, index = [0]).melt().rename(columns={'variable':'FEATURE', 'value': 'CORRELATION'}).sort_values('CORRELATION', ascending=False).dropna()\n        display(corr_df.head(20),corr_df.tail(4))","c3352091":"# USERS WHO JOINED BEFORE YEAR 4\ndf = train[['UserID','month', 'year']]\ndf['count_users'] = df.groupby(['month', 'year'])['UserID'].transform('count')\ndf = df.drop(['UserID'],1).drop_duplicates()\n\ndf = df.sort_values(['year', 'month'])\ndf['my'] = df['month'].astype(str) +'_'+df['year'].astype(str)\n# MAP PREVIOUSLY CREATED DICT TO TRANSFORM MY\ndf['my'] = df['my'].map(m_ydict)\n\n# USERS WHO JOINED IN YEAR 4 -- Note: We'll need predictions for these users as well\ndf2 = users[users['UserDate_Year']==4][['UserID', 'UserDate_Year', 'UserDate_Month']]\ndf2['count_users'] = df2.groupby(['UserDate_Month', 'UserDate_Year'])['UserID'].transform('count')\ndf2 = df2.drop(['UserID'],1).drop_duplicates()\n\ndf2 = df2.sort_values(['UserDate_Year', 'UserDate_Month'])\ndf2['my'] = df2['UserDate_Month'].astype(str) +'_'+df2['UserDate_Year'].astype(str)\n    \ndf2['my'] = df2['my'].map(m_ydict)\ndf2.rename(columns={'UserDate_Month':'month', 'UserDate_Year':'year'}, inplace=True)\n\n# APPEND NEW USER COUNTS DF TO END OF OLD USER COUNTS DF\ndf = df.append(df2)\ndisplay(df.tail())\ndf.loc[df['my']==33, 'count_users'] = df[df['my']==32]['count_users'].values[0] + df[df['my']==33]['count_users'].values[0]\ndf.loc[df['my']==34, 'count_users'] = df[df['my']==33]['count_users'].values[0] + df[df['my']==34]['count_users'].values[0]\ndf.loc[df['my']==35, 'count_users'] = df[df['my']==34]['count_users'].values[0] + df[df['my']==35]['count_users'].values[0]\ndisplay(df.tail())","99720902":"fig, ax = plt.subplots(figsize=(15,7))\nax.plot(df['my'].values, df['count_users'].values,  color = 'darkblue')\nax.axvline(x = df[df['year']==1]['my'].max(), color = 'b', alpha=0.3, linestyle='dashdot')\nax.axvline(x = df[df['year']==2]['my'].max(), color = 'b', alpha=0.3, linestyle='dashdot')\nax.axvline(x = df[df['year']==3]['my'].max(), color = 'b', alpha=0.3, linestyle='dashdot')\nax.set_title('ZINDI USER BASE GROWTH BY MONTH SINCE YEAR 1')\nax.set_ylabel('Count Users')\nplt.show()","aac1861c":"# GETTING USER VALUE COUNTS\ntrain_vc = train_1['UserID'].value_counts().to_frame().reset_index()\ndisplay(train_vc)","62705500":"# COUNTS OF HOW MANY USERS SHARE THE SAME VALUE COUNTS\nseries = train_vc['UserID']\nseries.name = 'Value Count'\nsns.distplot(train_vc['UserID'], bins=50)","58610c0d":"# CLUSTER 1 CONTAINS ALL USERS WHO JOINED ZINDI 17 MONTHS AGO OR LATER\nc1 = train_vc[train_vc['UserID']>=17]['index'].tolist()\nprint(len(c1))\n\n# CLUSTER 3 CONTAINS ALL USERS WHO JOINED ZINDI 6 MONTHS AGO OR EARLIER\nc3 = train_vc[train_vc['UserID']<=6]['index'].tolist()\nprint(len(c3))\n\n# WE WILL ADD MORE USERS TO CLUSTER 3 FOR TRAINING PURPOSES (MORE DATA) BUT \n# WILL NOT USE THIS MODEL TO PREDICT ON THESE ADDED USERS\n### HERE WE SELECT USERS WHO JOINED ZINDI 11 MONTHS AGO OR LATER AND\n### WE'LL EXCLUDE THESE USERS FROM THE DATA USED TO TRAIN AND PREDICT\n### FOR CLUSTER 3 USERS\nc3_1 = train_vc[train_vc['UserID']>=11]['index'].tolist()\nprint(len(c3_1))\n\n# AND CLUSTER 2 USERS WILL BE USERS NOT IN CLUSTER 1 OR 3","34dd3543":"# WE WILL ONLY SELECT DATA POINTS FROM THE MOST RECENT YEAR\ntrain_1 = train_1[train_1['year']==3]\ntrain_2 = train_2[train_2['year']==3]\ntrain_3 = train_3[train_3['year']==3]","0e5b17b8":"# SUBSET OUR DATAFRAMES FOR EACH USER GROUP\ntrain1_c1 = train_1[train_1['UserID'].isin(c1)]\nprint(train1_c1.shape)\n# ---- train1_c2 is just train_1\ntrain1_c3 = train_1[~train_1['UserID'].isin(c3_1)]\nprint(train1_c3.shape)\n\ntrain2_c1 = train_2[train_2['UserID'].isin(c1)]\nprint(train2_c1.shape)\n# ---- train2_c2 is just train_2\n\ntrain3_c1 = train_3[train_3['UserID'].isin(c1)]\nprint(train3_c1.shape)\n# ---- train3_c2 is just train_3","61c787af":"# SAVE USER ORDER IN TEST DATASET\nfor_test_user_order = for_test[['UserID']].set_index(['UserID'])\ndisplay(for_test_user_order.head())\nprint(for_test_user_order.shape)\n\n# GROUP 1 USERS\nfor_test_c1 = for_test[for_test['UserID'].isin(c1)]\nprint(for_test_c1.shape)\n\n# GROUP 2 USERS ARENT IN GROUP ONE OR GROUP 3\nfor_test_c2 = for_test[~for_test['UserID'].isin(c1)]\nfor_test_c2 = for_test_c2[~for_test_c2['UserID'].isin(c3)]\nprint(for_test_c2.shape)\n\n# GROUP 3 USERS ARE THE MOST RECENT USERS, THEY ARE NOT IN GROUP 1 OR 2\nnot_c3 = list(for_test_c2['UserID'].unique()) + list(for_test_c1['UserID'].unique())\nfor_test_c3 = for_test[~for_test['UserID'].isin(not_c3)]\nprint(for_test_c3.shape)\n\n# DOES IT ALL ADD UP?\nprint(for_test_c1.shape[0]+for_test_c2.shape[0]+for_test_c3.shape[0])","6c596264":"def remove_outliers(df, columns, threshold, return_df=True):\n    df = df.copy()\n    # Calculate the lower and upper bounds on the Z distribution given a threshold value\n    lower_bound = sc.stats.norm.ppf(q=threshold \/ 2, loc=0, scale=1)\n    upper_bound = sc.stats.norm.ppf(q=1 - threshold \/ 2, loc=0, scale=1)\n\n    # Calculate X scores for affected columns\n    outlier_df = df.loc[:, columns].copy()\n    zscores = pd.DataFrame(sc.stats.zscore(outlier_df, axis=0),\n                           index=outlier_df.index,\n                           columns=outlier_df.columns)\n\n    # Get boolean arrays denoting the outlier examples\n    lower_outliers = (zscores < lower_bound).any(axis=1)\n    upper_outliers = (zscores >= upper_bound).any(axis=1)\n\n    # Get indicies of all outlier examples\n    outliers = list(df[pd.concat([lower_outliers, upper_outliers],\n                            axis=1).any(axis=1)].index)\n    \n    del zscores,lower_outliers,upper_outliers,outlier_df,columns\n    gc.collect()\n    \n    if return_df:\n        # Drop the outliers\n        df = df.drop(outliers, axis=0).reset_index(drop=True)\n\n        return df, outliers\n    else: \n        return outliers\n\ndef noise_removal(df, target_name, targets, threshold, col_thresholds,return_df=True):\n    print(f'\\n\\n{target_name.upper()}\\n')\n    less_noisy = pd.DataFrame()\n    all_the_noise = []\n    \n    # SELECTING WHICH COLUMNS TO USE IN FINDING OUTLIERS BY CONSIDERING THE \n    # AMOUNT OF UNIQUE VALUES IN THE COLUMN\n    tempdf = df.drop(['UserID'], axis=1).copy()\n    features = [c for c in tempdf.columns if c not in ['UserID', 'month', 'year']]\n    nunique_df = pd.DataFrame()\n\n    for col in features:\n        nunique_df[col] = [tempdf[col].nunique()]\n\n    nunique_df = nunique_df.T.reset_index()\n    nunique_df.rename(columns={0:'n_unique', 'index':'feature'}, inplace=True)\n    nunique_df = nunique_df.sort_values(['n_unique'])\n    \n    col_threshold_min, col_threshold_max = col_thresholds[0], col_thresholds[1]\n    # GET THE COLUMNS WITHIN THE N_UNIQUE BOUNDARIES\n    numerical_columns = nunique_df[nunique_df['n_unique'].isin(range(col_threshold_min,\n                                                                     col_threshold_max))]['feature'].tolist()\n    \n    del tempdf\n    \n    # REMOVE OUTLIERS WITHIN TARGET\n    for target in targets: \n        targetdf = df.loc[df[target_name]==target] \n        if return_df:\n            less_noise, the_noise = remove_outliers(df=targetdf,\n                                    columns=numerical_columns,\n                                    threshold=threshold,\n                                    return_df=True) \n            less_noisy = less_noisy.append([less_noise]) \n            del less_noise\n            \n        else:\n            the_noise = remove_outliers(df=targetdf,\n                                        columns=numerical_columns,\n                                        threshold=threshold,\n                                        return_df=False) \n        \n        all_the_noise += the_noise \n        \n        print(f'Targeted {len(the_noise)} outliers from target {target}')\n        del targetdf, the_noise\n        gc.collect()\n\n    del numerical_columns\n    gc.collect()\n\n    print('Train df:\\t\\t\\t',df.shape)\n    print('\\n Noise:',len(all_the_noise), '\\n')\n    \n    if return_df:\n        print('Train df after removing ouliers within each target group:', less_noisy.shape)\n        return less_noisy\n    else:\n        return all_the_noise","16a11b16":"%%time\nreturn_df = False\ntrain_1_outliers = noise_removal(train_1, 'target1',[0,1], 0.00000005, [15,1500], return_df=return_df)\ntrain_2_outliers = noise_removal(train_2, 'target2',[0,1], 0.000000000025, [15,1500], return_df=return_df)","b3048b55":"outliers_train_1 = train_1.loc[train_1_outliers]\n# ONLY CONSIDER REMOVING THE OUTLIERS NOT FROM GROUP 2\noutliers_train_1 = outliers_train_1[outliers_train_1['UserID'].isin(c1)]\ndisplay(outliers_train_1)\nprint(outliers_train_1['target1'].value_counts())","d8e8189c":"outliers_train_2 = train_2.loc[train_2_outliers]\n# ONLY CONSIDER REMOVING THE OUTLIERS NOT FROM GROUP 2\noutliers_train_2 = outliers_train_2[outliers_train_2['UserID'].isin(c1)]\ndisplay(outliers_train_2)\nprint(outliers_train_2['target2'].value_counts())","3206f593":"# GET THE INDEX OF THE OUTLIERS NOT FROM GROUP 2\nout1 = outliers_train_1.index\nout2 = outliers_train_2.index","5cf55748":"set_seed(42)\n\nparams_lgbm1 = {'boosting_type':     'gbdt',   \n               'verbosity':         -1,\n               'seed':              1,\n               'objective':         'binary',\n               'min_data_in_leaf':  33,\n               'num_leaves':        50,\n               'max_depth':         20,\n               'n_estimators':      5000,\n               'path_smooth':       500,\n               'n_jobs':            -1,\n               'first_metric_only': True,\n               'learning_rate':     0.007}\n\nparams_lgbm2 = {'boosting_type':     'gbdt',   \n               'verbosity':         -1,\n               'seed':              1,\n               'objective':         'binary',\n               'min_data_in_leaf':  40,\n               'num_leaves':        90,\n               'max_depth':         12,\n               'n_estimators':      4000,\n               'path_smooth':       30,\n               'n_jobs':            -1,\n               'first_metric_only': True,\n               'learning_rate':     0.01}\n\nparams_lgbm3 = {'boosting_type':     'gbdt',   \n               'verbosity':         -1,\n               'seed':              1,\n               'objective':         'binary',\n               'min_data_in_leaf':  35,\n               'num_leaves':        75,\n               'max_depth':         11,\n               'n_estimators':      4000,\n               'path_smooth':       10,\n               'n_jobs':            -1,\n               'first_metric_only': True,\n               'learning_rate':     0.009}\n\nmodel1 = LGBMClassifier(**params_lgbm1)\nmodel2 = LGBMClassifier(**params_lgbm2)\nmodel3 = LGBMClassifier(**params_lgbm3)","7bccce70":"def evaluate_lgb(x, y, test_, cv, params_lgbm, prefix='', task='TRAIN', path=''):\n\n    kfold = KFold(n_splits = cv, random_state = 1, shuffle = True)\n\n    # Create out of folds array\n    oof_predictions = np.zeros(x.shape[0])\n    test_predictions = np.zeros(test_.shape[0])\n    \n    feature_importances = np.zeros(x.shape[1])\n    \n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\n\n        X_train, X_test = x.iloc[trn_ind], x.iloc[val_ind]\n        y_train, y_test = y.iloc[trn_ind], y.iloc[val_ind]\n\n        print('Training Data: ',X_train.shape, y_train.shape)\n        print('Validation Data: ',X_test.shape, y_test.shape, '\\n')\n        \n        if task=='TRAIN':\n            print('\\n','*'*7,f'Training fold {fold + 1}','*'*7, '\\n')\n            model = LGBMClassifier(**params_lgbm)\n            model.fit(   \n                        X_train, y_train,\n                        eval_set=[(X_train, y_train), (X_test, y_test)],\n                        eval_metric=['auc'],\n                        early_stopping_rounds=100,\n                        verbose=100\n                     )  \n            with open(f'lgb_model{prefix}_useractivity_fold_{fold+1}.pkl', 'wb') as file:\n                pickle.dump(model, file)  \n        \n        else:\n            print('\\n','*'*7,f'Predicting fold {fold + 1}','*'*7, '\\n')\n            with open(path+f'lgb_model{prefix}_useractivity_fold_{fold+1}.pkl', 'rb') as file:\n                model = pickle.load(file)\n                    \n        gc.collect()\n        pred = model.predict_proba(X_test.values)[:,1]\n        pred_train = model.predict_proba(X_train.values)[:,1]\n        test_auc = roc_auc_score(y_test, pred)\n        train_auc = roc_auc_score(y_train, pred_train)\n        print(f'AUC TEST: {test_auc}')\n        print(f'AUC TRAIN: {train_auc}')\n        \n        oof_predictions[val_ind] = pred\n        \n        test_pred = model.predict_proba(test_.values)[:,1] \n        test_predictions += test_pred \/ cv\n        \n        feature_importances += model.feature_importances_ \/ cv\n        \n        del X_train, y_train, X_test, y_test\n        gc.collect()\n    \n    return test_predictions.reshape(-1,1), oof_predictions.reshape(-1,1), feature_importances\n\ndef get_importance_plot(feature_importances_, columns):\n    \n    feature_importances = pd.DataFrame({'feature': columns, \n                                        'importance': feature_importances_}).sort_values('importance',ascending = False) \n\n    # Cumulative importance plot\n    feature_importances['importance_normalized'] = feature_importances['importance'] \/ feature_importances['importance'].sum()\n    feature_importances['cumulative_importance'] = np.cumsum(feature_importances['importance_normalized'])\n\n    plt.figure(figsize = (10, 15))\n    plt.barh(feature_importances.sort_values('importance')['feature'][-30:], feature_importances.sort_values('importance')['importance'][-30:])\n    plt.xlabel('Importance', size=14) \n    plt.title('Feature Importance', size=17)\n    plt.grid(True)\n    plt.show()\n\n    plt.figure(figsize = (7, 4))\n    plt.plot(list(range(len(feature_importances))), feature_importances['cumulative_importance'], 'b')\n    plt.xlabel('Number of Features', size=11) \n    plt.ylabel('Cumulative Importance', size=11)\n    plt.title('Cumulative Feature Importance', size=15)\n    plt.grid(True)\n    plt.show()\n\n    num_important = feature_importances[feature_importances['cumulative_importance']< 0.993][['feature', 'importance']].shape[0]\n    print(f'{num_important} Features accounts for 99.3% of the cumulative importance.')\n\n    # ranked features\n    important_x =  feature_importances['feature'].tolist()\n    return important_x","60a1f72b":"model_path = '..\/input\/user-act-prediction\/'\ntask = 'TRAIN'","f2707e4c":"FS = False\nfeatures1 = []\ndrop = ['year']\ntarget = 'target1'\n\nif FS:\n    X = train1_c1[train1_c1['month']>1][features1]\n    for_test_1 = for_test_c1[features1]\nelse: \n    X = train1_c1[train1_c1['month']>1].drop(['UserID', target]+drop,1)\n    for_test_1 = for_test_c1.drop(['UserID']+drop,1)\n    \ny = train1_c1[train1_c1['month']>1][[target]]\n\nprint(X.shape, y.shape, for_test_1.shape)","d9a97a9a":"%%time\ntest_predictions1c1,oof_predictions1c1,feature_importances1c1 = evaluate_lgb(X, y, for_test_1, \n                                                                             cv=15, \n                                                                             params_lgbm=params_lgbm1,\n                                                                             prefix='target1c1',\n                                                                             task=task, path=model_path)","ed7c4789":"test_auc1c1 = roc_auc_score(y.values.reshape(-1,1), oof_predictions1c1.reshape(-1,1))\nimportantx1c1 = get_importance_plot(feature_importances1c1, list(X.columns))\n\nprint(f'TEST AUC TARGET1 CLUSTER 1: {test_auc1c1}\\n')\noof_pred1_01c1 = np.round(oof_predictions1c1,0)\nprint(metrics.confusion_matrix(y.values.reshape(-1,1), oof_pred1_01c1), '\\n')\nprint(metrics.classification_report(y.values.reshape(-1,1), oof_pred1_01c1))\n\nall_ys = []\nall_preds = []\nall_ys.append(y.values.reshape(-1,1))\nall_preds.append(oof_predictions1c1)","a8484af5":"FS = False\nfeatures1 = []\ndrop = ['year']\ntarget = 'target1'\n\nif FS:\n    X = train_1[features1]\n    for_test_1 = for_test_c2[features1]\nelse: \n    # DROPPING USERID, TARGET AND OUTLIERS\n    X = train_1.drop(['UserID', target]+drop,1).drop(out1, 0)\n    for_test_1 = for_test_c2.drop(['UserID']+drop,1)\n    \ny = train_1[[target]].drop(out1, 0)\n\nprint(X.shape, y.shape, for_test_1.shape)","df508fd2":"%%time\ntest_predictions1c2,oof_predictions1c2,feature_importances1c2 = evaluate_lgb(X, y, for_test_1, \n                                                                             cv=15, \n                                                                             params_lgbm=params_lgbm1,\n                                                                             prefix='target1c2',\n                                                                             task=task, path=model_path)","2d6cf4a9":"test_auc1c2 = roc_auc_score(y.values.reshape(-1,1), oof_predictions1c2.reshape(-1,1))\nimportantx1c2 = get_importance_plot(feature_importances1c2, list(X.columns))\n\nprint(f'TEST AUC TARGET1 CLUSTER 2: {test_auc1c2}\\n')\noof_pred1_01c2 = np.round(oof_predictions1c2,0)\nprint(metrics.confusion_matrix(y.values.reshape(-1,1), oof_pred1_01c2), '\\n')\nprint(metrics.classification_report(y.values.reshape(-1,1), oof_pred1_01c2))\n\nall_ys.append(y.values.reshape(-1,1))\nall_preds.append(oof_predictions1c2)","b6d6e22f":"drop = ['year']\ntarget = 'target1'\n\nif FS:\n    X = train1_c3[features1]\n    for_test_1 = for_test_c3[features1]\nelse: \n    X = train1_c3.drop(['UserID', target]+drop,1)\n    for_test_1 = for_test_c3.drop(['UserID']+drop,1)\n    \ny = train1_c3[[target]]\n\nprint(X.shape, y.shape, for_test_1.shape)","a89a0cee":"%%time\ntest_predictions1c3,oof_predictions1c3,feature_importances1c3 = evaluate_lgb(X, y, for_test_1, \n                                                                             cv=15, \n                                                                             params_lgbm=params_lgbm1,\n                                                                             prefix='target1c3',\n                                                                             task=task, path=model_path)","38fa7da0":"test_auc1c3 = roc_auc_score(y.values.reshape(-1,1), oof_predictions1c3.reshape(-1,1))\nimportantx1c3 = get_importance_plot(feature_importances1c3, list(X.columns))\n\nprint(f'TEST AUC TARGET1 CLUSTER 3: {test_auc1c3}\\n')\noof_pred1_01c3 = np.round(oof_predictions1c3,0)\nprint(metrics.confusion_matrix(y.values.reshape(-1,1), oof_pred1_01c3), '\\n')\nprint(metrics.classification_report(y.values.reshape(-1,1), oof_pred1_01c3))\n\nall_ys.append(y.values.reshape(-1,1))\nall_preds.append(oof_predictions1c3)","6b0ff42a":"# MERGING AND REORDERING OF TEST PREDICTIONS FOR 1 MONTH AHEAD\nfor_test_c1['target1'] = test_predictions1c1\nfor_test_c2['target1'] = test_predictions1c2\nfor_test_c3['target1'] = test_predictions1c3\nfor_test_c123 = for_test_c1[['UserID','target1']].append(for_test_c2[['UserID','target1']]).append(for_test_c3[['UserID','target1']])\nfor_test_c123 = for_test_c123.set_index('UserID').reindex(for_test_user_order.index)\nfor_test_c123 = for_test_c123.reset_index()\ntest_predictions1 = for_test_c123['target1'].values.reshape(-1,1)\n\ndel for_test_c123, for_test_c1['target1'], for_test_c2['target1'], for_test_c3\ngc.collect()","485211f1":"for_test_c2 = for_test[~for_test['UserID'].isin(c1)]\nprint(for_test_c2.shape)","c033efa6":"FS = False\nfeatures2 = []\ndrop = ['year']","ac646c41":"target = 'target2'\nif FS:\n    X = train_2[features2]\n    for_test_2 = for_test[features2]\nelse: \n    X = train2_c1.drop(['UserID', target]+drop,1)\n    for_test_2 = for_test_c1.drop(['UserID']+drop,1)\n    \ny = train2_c1[[target]]\n\nprint(X.shape, y.shape, for_test_2.shape)","bdb5c1aa":"%%time\ntest_predictions2c1,oof_predictions2c1,feature_importances2c1 = evaluate_lgb(X, y, for_test_2, \n                                                                             cv=15, \n                                                                             params_lgbm=params_lgbm2,\n                                                                             prefix='target2c1',\n                                                                             task=task, path=model_path)","6fe972f4":"test_auc2c1 = roc_auc_score(y.values.reshape(-1,1), oof_predictions2c1.reshape(-1,1))\nimportantx2c1 = get_importance_plot(feature_importances2c1, list(X.columns))\n\nprint(f'TEST AUC TARGET2 CLUSTER 1: {test_auc2c1}\\n')\noof_pred2_01c1 = np.round(oof_predictions2c1,0)\nprint(metrics.confusion_matrix(y.values.reshape(-1,1), oof_pred2_01c1), '\\n')\nprint(metrics.classification_report(y.values.reshape(-1,1), oof_pred2_01c1))\n\nall_ys.append(y.values.reshape(-1,1))\nall_preds.append(oof_predictions2c1)","7d912ff6":"FS = False\nfeatures2 = []\ndrop = ['year']","8018268a":"target = 'target2'\nif FS:\n    X = train_2[features2]\n    for_test_2 = for_test[features2]\nelse: \n    # DROPPING USERID, TARGET AND OUTLIERS\n    X = train_2.drop(['UserID', target]+drop,1).drop(out2, 0)\n    for_test_2 = for_test_c2.drop(['UserID']+drop,1)\n    \ny = train_2[[target]].drop(out2, 0)\n\nprint(X.shape, y.shape, for_test_2.shape)","9c38021c":"%%time\ntest_predictions2c2,oof_predictions2c2,feature_importances2c2 = evaluate_lgb(X, y, for_test_2, \n                                                                             cv=15, \n                                                                             params_lgbm=params_lgbm2,\n                                                                             prefix='target2c2',\n                                                                             task=task, path=model_path)","91913be9":"test_auc2c2 = roc_auc_score(y.values.reshape(-1,1), oof_predictions2c2.reshape(-1,1))\nimportantx2c2 = get_importance_plot(feature_importances2c2, list(X.columns))\n\nprint(f'TEST AUC TARGET2 CLUSTER 2: {test_auc2c2}\\n')\noof_pred2_01c2 = np.round(oof_predictions2c2,0)\nprint(metrics.confusion_matrix(y.values.reshape(-1,1), oof_pred2_01c2), '\\n')\nprint(metrics.classification_report(y.values.reshape(-1,1), oof_pred2_01c2))\n\nall_ys.append(y.values.reshape(-1,1))\nall_preds.append(oof_predictions2c2)","a8f0d27d":"# MERGING AND REORDERING OF TEST PREDICTIONS FOR 2 MONTHS AHEAD\nfor_test_c1['target2'] = test_predictions2c1\nfor_test_c2['target2'] = test_predictions2c2\nfor_test_c12 = for_test_c1[['UserID','target2']].append(for_test_c2[['UserID','target2']])\nfor_test_c12 = for_test_c12.set_index('UserID').reindex(for_test_user_order.index)\nfor_test_c12 = for_test_c12.reset_index()\ntest_predictions2 = for_test_c12['target2'].values.reshape(-1,1)\n\ndel for_test_c12, for_test_c1['target2'], for_test_c2['target2']\ngc.collect()","723f8dea":"FS = False\nfeatures3 = []\ndrop = ['year']\ntarget = 'target3'\n\nif FS:\n    X = train3_c1[features3]\n    for_test_3 = for_test_c1[features3]\nelse: \n    X = train3_c1.drop(['UserID', target]+drop,1)\n    for_test_3 = for_test_c1.drop(['UserID']+drop,1)\n    \ny = train3_c1[[target]]\n\nprint(X.shape, y.shape, for_test_3.shape)","e4e76ec9":"%%time\ntest_predictions3c1,oof_predictions3c1,feature_importances3c1 = evaluate_lgb(X, y, for_test_3, \n                                                                             cv=15, \n                                                                             params_lgbm=params_lgbm3,\n                                                                             prefix='target3c1',\n                                                                             task=task, path=model_path)","c82c917c":"test_auc3c1 = roc_auc_score(y.values.reshape(-1,1), oof_predictions3c1.reshape(-1,1))\nimportantx3c1 = get_importance_plot(feature_importances3c1, list(X.columns))\n\nprint(f'TEST AUC TARGET3 CLUSTER 1: {test_auc3c1}\\n')\noof_pred3_01c1 = np.round(oof_predictions3c1,0)\nprint(metrics.confusion_matrix(y.values.reshape(-1,1), oof_pred3_01c1), '\\n')\nprint(metrics.classification_report(y.values.reshape(-1,1), oof_pred3_01c1))\n\nall_ys.append(y.values.reshape(-1,1))\nall_preds.append(oof_predictions3c1)","398d881b":"FS = False\nfeatures3 = []\ndrop = ['year']\ntarget = 'target3'\n\nif FS:\n    X = train_3[features3]\n    for_test_3 = for_test_c2[features3]\nelse: \n    X = train_3.drop(['UserID', target]+drop,1)\n    for_test_3 = for_test_c2.drop(['UserID']+drop,1)\n    \ny = train_3[[target]]\n\nprint(X.shape, y.shape, for_test_3.shape)","fe533c9d":"%%time\ntest_predictions3c2,oof_predictions3c2,feature_importances3c2 = evaluate_lgb(X, y, for_test_3, \n                                                                             cv=15, \n                                                                             params_lgbm=params_lgbm3,\n                                                                             prefix='target3c2',\n                                                                             task=task, path=model_path)","87a51b45":"test_auc3c2 = roc_auc_score(y.values.reshape(-1,1), oof_predictions3c2.reshape(-1,1))\nimportantx3c2 = get_importance_plot(feature_importances3c2, list(X.columns))\n\nprint(f'TEST AUC TARGET3 CLUSTER 2: {test_auc3c2}\\n')\noof_pred3_01c2 = np.round(oof_predictions3c2,0)\nprint(metrics.confusion_matrix(y.values.reshape(-1,1), oof_pred3_01c2), '\\n')\nprint(metrics.classification_report(y.values.reshape(-1,1), oof_pred3_01c2))\n\nall_ys.append(y.values.reshape(-1,1))\nall_preds.append(oof_predictions3c2)","000ee252":"# MERGING AND REORDERING OF TEST PREDICTIONS FOR 3 MONTHS AHEAD\nfor_test_c1['target3'] = test_predictions3c1\nfor_test_c2['target3'] = test_predictions3c2\nfor_test_c12 = for_test_c1[['UserID','target3']].append(for_test_c2[['UserID','target3']])\nfor_test_c12 = for_test_c12.set_index('UserID').reindex(for_test_user_order.index)\nfor_test_c12 = for_test_c12.reset_index()\ntest_predictions3 = for_test_c12['target3'].values.reshape(-1,1)\n\ndel for_test_c12, for_test_c1, for_test_c2\ngc.collect()","0a897073":"# DELETE OLD VARIABLES\ndel for_test_1, for_test_2, for_test_3, X, y","0d095817":"all_ys = np.vstack(all_ys)\nall_preds = np.vstack(all_preds)\n\ntest_auc = roc_auc_score(all_ys,all_preds)\n\nprint(f'TEST AUC: {test_auc}\\n')\noof_preds = np.round(all_preds,0)\nprint(metrics.confusion_matrix(all_ys, oof_preds), '\\n')\nprint(metrics.classification_report(all_ys, oof_preds))","17d245d7":"print(f'TEST AUC TARGET1 C1: {test_auc1c1}')\nprint(f'TEST AUC TARGET1 C2: {test_auc1c2}')\nprint(f'TEST AUC TARGET1 C3: {test_auc1c3}\\n')\nprint(f'TEST AUC TARGET2 C1: {test_auc2c1}')\nprint(f'TEST AUC TARGET2 C2: {test_auc2c2}\\n')\nprint(f'TEST AUC TARGET3 C1: {test_auc3c1}')\nprint(f'TEST AUC TARGET3 C2: {test_auc3c2}\\n')","1d458407":"for_test_users = for_test['UserID']","e107d757":"test_predictions = np.concatenate([test_predictions1, test_predictions2, test_predictions3], axis=1)\nprint(test_predictions.shape)\ntest_predictions","f0ce4134":"# ADD PREDICTIONS TO TEST SET\nfor_test[['target1', 'target2', 'target3']] = test_predictions\nfor_test['UserID'] = for_test_users\nfor_test = for_test.sort_values('UserID')\nfor_test","bcdcd6b6":"# RESHAPE PREDICTIONS INTO 1D ARRAY OF LENGTH TESTSET X 3\ntargets_reshaped = for_test[['target1', 'target2', 'target3']].values.reshape(-1,1)\ntargets_reshaped.shape","e45bef25":"# CREATE 3 COPIES OF EACH USERID SAMPLE IN TESTSET\nfor_test = for_test.append([for_test]*2,ignore_index=True).sort_values('UserID')\n# ADD PREDICTIONS TO TESTSET\nfor_test['Target'] = targets_reshaped\n# SELECT ONLY THE COLUMNS WE NEED\ncols = ['UserID', 'month', 'year', 'Target']\nfor_test = for_test[cols].reset_index(drop=True)\ndisplay(for_test)","29c06c8c":"display(test.head())","3fbfc06a":"# MIMIC THE TEST DF FORMAT WE WERE GIVEN IN THE VERY BEGINNING\nfor_test['year'] = 4\nfor_test['month'] = [1,2,3]*20218\ndisplay(for_test)","709d1922":"display(sub.head())","82693c4c":"# MIMIC SAMPLE SUBMISSION FORMAT\nfor_test['UserMonthYear'] = for_test['UserID'].astype(str) +'_'+ for_test['month'].astype(str)+\\\n                            '_'+ for_test['year'].astype(str)\nfor_test = for_test[['UserMonthYear', 'Target']]\n\nfor_test = for_test.set_index('UserMonthYear')\nsub = sub.set_index('UserMonthYear')\n\n# LEFT JOIN WITH SAMPLE SUBMISSION\nsub = sub.merge(for_test, left_index=True, right_index=True, how='left')","8bb8115b":"# RENAME COLUMNS\nsub = sub.drop('Target_x',1).rename(columns={'Target_y':'Target'}).reset_index()\ndisplay(sub)","08ec8d3f":"# LOAD AND PREPROCESS USERS DATAFRAME AGAIN\nroot = '..\/input\/zindi-user-behaviour\/zindi-user-behaviour-birthday-challenge\/'\nusers = pd.read_csv(root+'Users.csv').drop_duplicates()\nusers.columns = users.columns.str.replace(' ', '_')\nusers = pd.get_dummies(users, columns=['FeatureY', 'Points', 'Country'], drop_first=False)","adc7bde9":"# LOAD IN SAVED TRAIN\ntrain = pd.read_feather('.\/train.feather')","56bb264f":"unseen_test_users = list(set(test['UserID'])  - set(train['UserID']))\nunseen_test_users_df = users[users['UserID'].isin(unseen_test_users)]\ndisplay(unseen_test_users_df.head(), unseen_test_users_df.shape)","1a8b8b58":"%%time\nusers = pd.read_csv(root+'Users.csv').drop_duplicates()\nusers.columns = users.columns.str.replace(' ', '_')\nusers = pd.get_dummies(users, columns=['FeatureY', 'Points'], drop_first=False)\n\ntrain_3months_in = train.groupby('UserID').head(3)\ntrain_3months_in = train_3months_in.reset_index(drop=True)\ntrain_vc = train_3months_in['UserID'].value_counts().to_frame().reset_index()\ntrain_new_users = train_vc[train_vc['UserID'].isin([3])]['index'].values\ntrain_3months_in = train_3months_in[train_3months_in['UserID'].isin(train_new_users)]\n\ncols = set(['UserID', 'month', 'year', 'Target'] + list(users.columns)) - set(['Country'])\ntrain_3months_in = train_3months_in[cols]\n\ncols = [c for c in train_3months_in if c not in ['UserID', 'month','year',\n                                                 'UserDate_Month', 'UserDate_Year', 'Target']]\ntrain_3months_in = train_3months_in[['UserID', 'month','year', 'UserDate_Month', \n                                     'UserDate_Year', 'Target']+cols]\n\n\ntrain_3months_in = train_3months_in.merge(users[['UserID','Country']], on='UserID', how='left')\ntrain_3months_in = pd.get_dummies(train_3months_in, columns=['Country'])\n    \ntrain_3months_in.reset_index(drop=True).to_feather('train_3months_in.feather')","6f5c324c":"train_3months_in['Target'] = train_3months_in.groupby('UserID')['Target'].\\\n                                              transform(lambda x:'_'.join(x.values.astype(str)))\n\ntrain_3months_in = train_3months_in.groupby('UserID').first().reset_index()\ntrain_3months_in['Target'] = train_3months_in['Target'].str.split('_')\ntrain_3months_in[['target1','target2', 'target3']] = pd.DataFrame(train_3months_in.Target.tolist(), index=train_3months_in.index)\ntrain_3months_in = train_3months_in.drop(['Target', 'month', 'year'], 1)\ndisplay(train_3months_in)\n\ncols = list(set(unseen_test_users_df.columns) - set(train_3months_in.columns))\ntrain_3months_in[cols] = 0\ntrain_3months_in[['target1', 'target2','target3']] = train_3months_in[['target1', 'target2','target3']].astype(int)","9fb809cc":"set_seed(42)\nparams_lgbm = {'boosting_type':     'gbdt',    \n               'verbosity':         -1,\n               'seed':              42,\n               'objective':         'binary',\n               'min_data_in_leaf':  20,\n               'num_leaves':        300,\n               'max_depth':         15,\n               'n_estimators':      2000,\n               'path_smooth':       4500,\n               'n_jobs':            -1,\n               'first_metric_only': True,\n               'learning_rate':     0.1  }\n\nbinary_rel_clf = BinaryRelevance(LGBMClassifier(**params_lgbm))\n\nX = train_3months_in.drop(['UserID', 'target1', 'target2','target3'],1)\ny = train_3months_in[['target1', 'target2','target3']]\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.001,random_state=42)","810c715b":"%%time\nset_seed(42)\n\n# TRAIN\nbinary_rel_clf.fit(X_train,y_train)\n\n# VALIDATE\nbr_prediction = binary_rel_clf.predict_proba(X_test)\nprint(roc_auc_score(y_test,br_prediction.toarray()))","cb64f66a":"X_test = unseen_test_users_df[X_train.columns]\n\ntest_predictions = binary_rel_clf.predict_proba(X_test)\ntest_predictions.toarray()","b4c64a73":"unseen_test_users_df[['target1', 'target2', 'target3']] = test_predictions.toarray()\nunseen_test_users_df = unseen_test_users_df.sort_values('UserID')\nunseen_test_users_df","7b245120":"targets_reshaped = unseen_test_users_df[['target1', 'target2', 'target3']].values.reshape(-1,1)\ntargets_reshaped.shape","d0e448f9":"unseen_test_users_df = unseen_test_users_df.append([unseen_test_users_df]*2,ignore_index=True).sort_values('UserID')\nunseen_test_users_df['Target'] = targets_reshaped\n\ncols = ['UserID', 'UserDate_Month', 'UserDate_Year', 'Target']\nunseen_test_users_df = unseen_test_users_df[cols].reset_index(drop=True)\n\nunseen_test_users_df.rename(columns={'UserDate_Month':'month', 'UserDate_Year':'year'}, inplace=True)\ndisplay(unseen_test_users_df)","1e00cc45":"%%time\ndef remonth(month):\n    start = month.values[0]\n    months = [start, start+1, start+2]\n    return months\n\nunseen_test_users_df['month'] = unseen_test_users_df.groupby('UserID')['month'].transform(remonth)\n\nunseen_test_users_df['UserMonthYear'] = unseen_test_users_df['UserID'].astype(str) +'_'+ unseen_test_users_df['month'].astype(str)+\\\n                                        '_'+ unseen_test_users_df['year'].astype(str)\ndisplay(unseen_test_users_df)","84892ea9":"# ONLY SELECT THE PREDICTIONS FOR THE THE MONTHS AND USERS WE NEED\ntest_ids = sub[sub['UserMonthYear'].isin(unseen_test_users_df.UserMonthYear.unique())]['UserMonthYear']\n\nunseen_test_users_df = unseen_test_users_df[unseen_test_users_df['UserMonthYear'].isin(test_ids)]\nunseen_test_users_df = unseen_test_users_df[['UserMonthYear', 'Target']]\ndisplay(unseen_test_users_df)","66032b0e":"unseen_test_users_df = unseen_test_users_df.set_index('UserMonthYear')\nsub = sub.set_index('UserMonthYear')\nsub.drop(unseen_test_users_df.index, 0, inplace=True)\nsub = sub.append(unseen_test_users_df).reset_index()","21307ba4":"sub.info()","3fc735b8":"display(sub)","10679093":"sub.to_csv('sub.csv', index=False)","61086031":"print(importantx1c1)","707eb618":"print(importantx1c2)","c60b283a":"print(importantx1c3)","4d45a87b":"print(importantx2c1)","ff30bd64":"print(importantx2c2)","7446170a":"print(importantx3c1)","943fc8a7":"print(importantx3c2)","8c8a3f38":"---\n#### NEW TRAIN DATAFRAME","bb469e23":"#### To create our submissions we will need to create a testset using the final month of the final year of the above dataframe. ","8feb96f0":"---","9ac00c10":"## LIGHT GBM _ NO.2\n## Predicting for unseen users","d46c886e":"---","530e9404":"### https:\/\/zindi.africa\/competitions\/zindi-user-behaviour-birthday-challenge","b648e46d":"#### PARTION THE TEST DATA BY USER GROUP","832eca52":"#### An active user is one that enters a competition, makes a submission, or engages through the discussion forums","9d7e6eb4":"### GETTING THE TARGETS\n#### If a user was active in the next month, i.e Target == 1 in that month, then the target for the current month should be 1. \n#### If a user was active 2 months ahead of the current month, the target for the current month should be 1. \n#### Same goes for 3 months ahead.","4de48cd1":"##### REMEMBER: WE NEED TO PREDICT 1 MONTH AHEAD, 2 MONTHS AHEAD AND THREE MONTHS AHEAD FOR EACH USER IN THE EXTRACTED TESTSET\n\nIf a user was present only once in the final dataframe, that means we dont have information on that user that can be used for training. This is because when predictiing 1 month ahead the target for each user for each month is whether that user was active in the following month. The logic is the same for predicting 2 months ahead, i.e. if a user was only present in the final dataframe twice, that means we dont have information that can be used for predicting wheher that user will be active in the next 2 months from the current month. The same goes for predicting 3 months ahead. \n\n\nWith that said, we can partion the final dataframe into 3 separate ones: The first one excluding users with value counts == 1 for predicting the next month, the second for excluding users with value counts == 1 and 2 for predicting 2 months ahead, and the third one by excluding users with value counts == 1,2 and 3 for predicting 3 months ahead. ","7bcea1f0":"# MODELLING","2545f25b":"#### DEALING WITH NaNs and inf VALUES BY FORWARD FILLING","588b4c10":"### Target 1 Cluster 3","a7ad0874":"#### We are provided with several datasets which can help us in finding out \"how\" active a user was in a particular month. First we need to clean these tables and then through the use of appropriate and intuitive feature engineering we can create features to help in predicting whether a user will be active in the next month.","260b51ce":"___\n---\n# Zindi User Behaviour Birthday Challenge\n## Time-series Binary Classification using Light Gradient Boosting\n\n\n---\n#### Author: Daniel Bruintjies\n\n#### January 2022\n\n---","24857293":"#### We can create cumsum features to capture the users activity over time. We will select some features from the train dataframe based on how many unique values are present in the column and create cumsum features using these columns.","9a8b0524":"#### RUNNING OUR THE PARTICIPATION FUNCTION","92e76f47":"### MERGE LATEST COMP DATA TO TRAIN DF USING pd.merge_asof","3048815d":"#### PREDICT THE NEXT FEW MONTHS IN ONE GO","74de9a44":"## PROBABILITY OF JOING A COMPETITION NEXT MONTH AS A FEATURE\n\n#### Using this improved training and validation AUC considerably but didn't translate well to public leaderboaord so I DID NOT USE THIS.\n#### The idea behind this: Because we are given data on competitions that are scheduled to start after the training data period, we can build a model that predicts the probability of a user joining the new competions.\n#### Note: The runtime of this section is 2 hours because a separate model is fitted on each user who participated in competition.\n","b703f804":"# PREPROCESSING","0b5d6b88":"### Calculating Months Left Before a Competition Ends","5f1a71a8":"### GETTING PREDICTIONS INTO SUBMISSION FORMAT","cd92b2c8":"---","126b3ac2":"---","c0271efe":"---\n# FINAL DATAFRAMES","5cd28da7":"#### STANDARDISE SOME CUMSUM FEATURES BY DIVIDING BY MONTHS SINCE A USER JOINED ZINDI","61af7c27":"#### There seems to be users in the provided test dataframe for which we have no data on in the train dataframe. We still need predictions for these users and will need to create a training set similar to these new users.","985a2b2b":"---","370ee580":"---","5b70817a":"---\n### CREATING CUMSUM FEATURES ","a54f6997":"---\n### When last was a user Active?","cbd1ca90":"### REOMVE OUTLIERS","26b1da92":"----","43c7935e":"#### CREATING A TRAINSET BY TAKING THE FIRST 3 MONTHS DATA OF EACH USER IN TRAIN DATAFRAME","fb048917":"---","7e44cf79":"#### LOAD SAVED PROBABILITY PREDICTIONS DICTIONARIES","77cc79a3":"----\n----","fca009af":"#### EXAMPLE OF RUNNING THE COMP PARTICIPATION FUNCTION","fc69a0ca":"## FINAL SUBMISSION ","25596f2f":"### Target 1 Cluster 1","fde522c6":"## LIGHT GBM _ NO.1\n## Predicting for seen users","f8190459":"---\n---","b782f136":"---","58922971":"#### Using the submissions data we can create a feature that represents how frequently a user makes submissions per month per comp by considering the difference in time between submission days of the week.","7d851bc8":"### MERGE MONTHS LEFT CALCULATION ONTO TRAIN DF","38d0732a":"#### THE UNSEEN NEW USERS","7aacf961":"### MERGE AGGREGATED DATA FROM COMMENTS TABLE ONTO TRAIN DF  ","0fb7405e":"#### CREATING DATAFRAMES FROM THE DICTS","32db210c":"#### GROUPING THE USERS","9a4a59bc":"### HOW HAS ZINDIS USER BASE GROWN OVER TIME?","f5568fa9":"# MODEL TRAINING","9f8ea301":"---","498e7831":"---\n#### The submission format:","fd8c6673":"## EVEN MORE FEATURE ENGINEERING","7c656442":"## IMPORTS","d3318768":"#### ONLY LOOKING FOR OUTLIERS WITHIN GROUP 2, AS THIS GROUP WILL BE TRAINIED ON ALL THE DATA, BUT PREDICTING FOR THOSE ONLY IN GROUP 2","ce0e72f4":"### TARGET DISTRIBUTION","6d4f1324":"#### The months left values need to decrease till the end of the competition. But each time the user joins a new comp, the months left vale is set to a calculation for that particular comp and we need to start monitoring that new comp. \n\n#### Therefore a solution would be to find the positions of all NaN values in the months left column, get a sort of count going from the start of the NaN position and when it hits a value not NaN, reset the count so when it gets to another NaN position the counter will start at 1 again. The subtract the count from months left value before it to fill the NaN spot with months left - 1 and so on.","6b20bea1":"### Target 3 Cluster 2","fae01ed4":"#### We still have a problem of calculating months left correctly. We currently only have values present for the months left columns where CompPart == 1. This means we need to fill those NaN months left values between comp parts for each user appropriately.","a0eab284":"## EXPLORATORY DATA ANALYSIS","ca609242":"---","9dabd7d4":"### Target 1 Cluster 2","59e69be5":"### CORRELATION WITH TARGET","66eef5ce":"#### FEATURES SORTED BY IMPORTANCE FROM EACH MODEL","1cdc378d":"---","deb140f0":"### MERGE AGGREGATED DATA FROM SUBMISSIONS AND COMPETITION PARTICIPATION TABLES ONTO TRAIN DF","de3b8f18":"#### The majority of users in testset need probabilities for being active in all 3 months, with only a few needing predictions for 1 or 2 months.","cfa60839":"---\n---\n# FEATURE ENGINEERING","1205e7b1":"---\n### Using Comments data","2d261015":"### Target 3 Cluster 1","d1e602aa":"### Test","9373ee11":"### Target 2 Cluster 1","6276eeab":"### Train","d43d71f8":"### OUT OF FOLD CV SCORE","956fe4d6":"---\n## MORE FEATURE ENGINEERING\n\n\n### Getting data on the latest competition a user participated in","e062e623":"#### PREDICT FOR TEST","2311a2f6":"### GETTING PREDICTIONS INTO SUBMISSION FORMAT","05760fed":"---\n### Using Submissions data","4b53e197":"#### If a UserID is present in the DISCUSSIONS, COMMENTS, COMPETITION PARTICIPATION, or SUBMISSIONS table in a particular month, then the Target in that particular month should be 1 for the UserID in the train df.","bd92dde6":"----\n----","fd344cda":"### MERGE USER INFO FROM USERS TABLE ONTO TRAIN DF","43c6a57f":"#### CREATING THE TARGET FOR NEXT FEW MONTHS","28d99274":"### ALL THE DATASETS","a0649c58":"### Target 2 Cluster 2","c6e82887":"### EXTRACTING THE TRAINSETS","7b97e2d9":"---","6fe2a888":"---","14214818":"### Using Competition Participation data and Competition data","8424b0d7":"# EXPLORATORY DATA ANALYSIS\n### THE DATA","05010b49":"---","d3593d39":"#### USING MERGE ASOF TO MERGE COMP INFO WITH TRAIN BY USER ID AND DATE","42d8536b":"### MERGE PREDICTIONS ONTO TRAIN DF","1d845d93":"#### CREATE SHIFTED FEATURES","99773822":"#### I separated the users into groups of 3 for predicting the next month, and groups of 2 for predicting the following 2 months. Users were grouped based on when they joined Zindi (i.e how many time they appear in the train dataframe)","4d2c63da":"#### For each user in the test dataset we need to predict whether they will be active on the Zindi platform during month 1,2, and 3 after the train dataset timeline. The month and year columns are in chronological order.","c1aa6657":"### THE EXTRACTED TESTSET "}}