{"cell_type":{"be063540":"code","528b251d":"code","89b1f14e":"code","114363b8":"code","21f71dc2":"code","9c78b11a":"code","04d95570":"code","64d50822":"code","8fcd4b3d":"code","9d51751a":"code","d888c6dd":"code","a28b0af8":"code","43ce6e88":"code","cc805bfb":"code","6bbac9ef":"code","f69f11f9":"code","b27186ae":"code","450c884d":"code","b213cbc9":"code","a15b2101":"code","40b1f2dc":"code","e41c63c6":"code","f500cbb7":"code","5c4c25e2":"markdown","215f5b3f":"markdown","c1fa58ba":"markdown","952dbf2b":"markdown","8ecb0ce2":"markdown","9564a618":"markdown","e36e2f8e":"markdown","e1519a7c":"markdown","4ae63d23":"markdown"},"source":{"be063540":"import pandas as pd\n\nimport torch as th\nimport torch.nn.functional as F\n\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm.notebook import tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = th.device('cpu')","528b251d":"df_train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ndf_test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\n\ndf_train.head()","89b1f14e":"print('Train shape:', df_train.shape)\nprint('Test shape:', df_test.shape)","114363b8":"df_train['label'].value_counts()","21f71dc2":"x = df_train[df_train.columns[1:]].values\ny = df_train['label'].values\n\nx_to_submit = df_test[df_train.columns[1:]].values","9c78b11a":"x = np.reshape(x, (x.shape[0], 28, 28))\nx_to_submit = np.reshape(x_to_submit, (x_to_submit.shape[0], 28, 28))","04d95570":"for i in range(8):\n    plt.subplot(2, 4, i+1)\n    plt.imshow(x[i])\n    plt.title(y[i])","64d50822":"x_train, x_test_val, y_train, y_test_val = train_test_split(x, y, test_size = .15, \n                                                            stratify=y, random_state=42)\nx_test, x_val, y_test, y_val = train_test_split(x_test_val, y_test_val, test_size = .25, \n                                                stratify=y_test_val, random_state=42)\n\nprint('X train shape:', x_train.shape)\nprint('Y train shape:', y_train.shape)\nprint()\nprint('X test shape:', x_test.shape)\nprint('Y test shape:', y_test.shape)\nprint()\nprint('X validation shape:', x_val.shape)\nprint('Y validation shape:', y_val.shape)","8fcd4b3d":"def img2tensor(arr, device):\n    arr_expand = np.expand_dims(arr, 3)\n    arr_transpose = np.transpose(arr_expand, (0, 3, 1, 2))\n    tensor = th.from_numpy(arr_transpose).to(device)\n    \n    return tensor.float()\n\nx_train = img2tensor(x_train, device)\nx_test = img2tensor(x_test, device)\nx_val = img2tensor(x_val, device)\n\nx_to_submit = img2tensor(x_to_submit, device)\n\ny_train = th.from_numpy(y_train).to(device)\ny_test = th.from_numpy(y_test).to(device)\ny_val = th.from_numpy(y_val).to(device)","9d51751a":"dataset = th.utils.data.TensorDataset(x_train, y_train)\n\ndata_loader = th.utils.data.DataLoader(dataset, batch_size=100, shuffle=True)","d888c6dd":"class CNN(th.nn.Module):\n    \n    def __init__(self, hidden_size, droupout_p, output_size):\n        super(CNN, self).__init__()\n        \n        self.hidden_size = hidden_size\n        self.droupout_p = droupout_p\n        self.output_size = output_size\n        \n        self.norm = th.nn.BatchNorm2d(1)\n        \n        self.conv1 = th.nn.Conv2d(1, 16, (3, 3))\n        self.conv2 = th.nn.Conv2d(16, 32, (4, 4))\n        \n        self.dropout = th.nn.Dropout(self.droupout_p)\n        \n        self.fc1 = th.nn.Linear(32*5*5, self.hidden_size)\n        self.fc2 = th.nn.Linear(self.hidden_size, self.output_size)\n        \n        \n    def forward(self, x):\n        out = self.norm(x)\n        \n        out = F.relu(self.conv1(out))\n        out = F.max_pool2d(out, (2, 2))\n        \n        out = F.relu(self.conv2(out))\n        out = F.max_pool2d(out, (2, 2))\n        \n        out = out.view(-1, 32*5*5)\n        \n        out = F.tanh(self.fc1(out))\n        out = self.dropout(out)\n        out = self.fc2(out)\n        \n        return out\n    \n    def predict(self, x):\n        out = self.forward(x)\n        _, y_pred = th.max(out, 1)\n    \n        return y_pred","a28b0af8":"def decrease_lr(opt, epoch, init_lr):\n    lr =  init_lr * (0.1 ** (epoch \/\/ 30))\n    for param_group in opt.param_groups:\n        param_group['lr'] = lr","43ce6e88":"lr = 0.001\nepochs = 90\n\nnet = CNN(hidden_size=32, droupout_p=.3, output_size=10).to(device)\n\nopt = th.optim.Adam(net.parameters(), lr)\ncriterion = th.nn.CrossEntropyLoss()","cc805bfb":"losses = []\ntest_accuracies = []\nval_accuracies = []\n\nfor epoch in range(epochs):\n    print('Epoch:', epoch+1)\n    \n    decrease_lr(opt, epoch, lr)\n    \n    net.train()\n    batch_losses = []\n    for feats, labels in tqdm(data_loader):      \n        output = net(feats)\n        \n        loss = criterion(output, labels)\n        \n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n        \n        batch_losses.append(loss.item())\n    \n    net.eval()\n    \n    loss_mean = np.array(batch_losses).mean()\n    test_acc = accuracy_score(y_test, net.predict(x_test).data.numpy())\n    val_acc = accuracy_score(y_val, net.predict(x_val).data.numpy())\n    \n    losses.append(loss_mean)\n    test_accuracies.append(test_acc)\n    val_accuracies.append(val_acc)\n    \n    print('Loss:', loss_mean)\n    print('Test acc:', test_acc)\n    print('Validation acc:', val_acc)","6bbac9ef":"plt.plot(losses, label='loss')\nplt.legend()","f69f11f9":"plt.plot(test_accuracies, label='test')\nplt.plot(val_accuracies, label='val')\nplt.legend()","b27186ae":"th.save(net.state_dict(), 'model-mnist.pth.tar')","450c884d":"def plot_confusion_matrix(cm, title):\n    plt.imshow(cm, interpolation='nearest')\n    plt.title(title)\n    plt.colorbar()\n    plt.tight_layout()\n    plt.ylabel('True')\n    plt.xlabel('Predicted')","b213cbc9":"net.eval()\ny_test_pred =  net.predict(x_test).data.numpy()\ncm_test = confusion_matrix(y_test, y_test_pred)\n\nprint(cm_test)\nplot_confusion_matrix(cm_test, 'Test')","a15b2101":"net.eval()\ny_val_pred =  net.predict(x_val).data.numpy()\ncm_val = confusion_matrix(y_val, y_val_pred)\n\nprint(cm_val)\nplot_confusion_matrix(cm_val, 'Validation')","40b1f2dc":"net.eval()\ny_to_submit = net.predict(x_to_submit)","e41c63c6":"df_submit = pd.DataFrame()\ndf_submit['ImageId'] = range(1, y_to_submit.size()[0]+1)\ndf_submit['Label'] = y_to_submit","f500cbb7":"df_submit.to_csv('submission.csv', index=False)","5c4c25e2":"### Reshaping features to transform it to a 28x28 image","215f5b3f":"### Transforming data to tensor\n\nNow we have (batch, height, width) features.\n\nFirst we will need to add the channel dimension with numpy's expand_dims, now having features with shape of (batch, height, width, channel).\n\nTensors needs to have shape of (batch, channel, height, width), so we will transpose our data.","c1fa58ba":"### Creating a DataLoader","952dbf2b":"### Importing data","8ecb0ce2":"### Splitting data into features and labels\nx = features\n\ny = labels","9564a618":"Decay the learning rate by 10 every 30 epochs","e36e2f8e":"### Creating our Convolutional neural network","e1519a7c":"### Splitting data for training, testing and validation","4ae63d23":"### Training our net"}}