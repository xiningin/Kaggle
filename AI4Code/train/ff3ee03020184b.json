{"cell_type":{"8ba1cf27":"code","d762066c":"code","b0faf111":"code","2c675e6b":"code","df747072":"code","640d3170":"code","1c248c92":"code","9eac9d04":"code","5f145cd4":"code","fd25f809":"code","c45d3742":"code","d5c02d73":"code","3ccf0d1f":"code","3d4308b3":"code","526d9719":"code","9d014ee6":"code","e1ce3717":"code","a7e15aa8":"code","e832e122":"code","496faaa5":"code","d1c061dd":"code","9fc188d0":"code","6fcbd2ba":"code","eb9646bc":"code","9f2acdac":"code","d2def4ae":"code","71306254":"code","bc4b7df5":"code","c598b550":"code","388a3be5":"code","2c51da35":"markdown","b4c5fc07":"markdown","54e4199d":"markdown","46b2bc7b":"markdown","6474f7e1":"markdown","cd0da44a":"markdown","db44cad5":"markdown","83b2cd6c":"markdown","2c6d3778":"markdown","1e64fa57":"markdown"},"source":{"8ba1cf27":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d762066c":"# to visualise al the columns in the dataframe\npd.pandas.set_option('display.max_columns', None)","b0faf111":"dataset=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndataset.head()","2c675e6b":"# Always remember there way always be a chance of data leakage so we need to split the data first and then apply feature Engineering\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(dataset,dataset['SalePrice'],test_size=0.1,random_state=0)","df747072":"X_train.shape, X_test.shape","640d3170":"features_nan=[feature for feature in dataset.columns if dataset[feature].isnull().sum()>1 and dataset[feature].dtypes=='O']\n\nfor feature in features_nan:\n    print(\"{}: {}% missing values\".format(feature,np.round(dataset[feature].isnull().mean(),4)))","1c248c92":"def replace_cat_feature(dataset,features_nan):\n    data=dataset.copy()\n    data[features_nan]=data[features_nan].fillna('Missing')\n    return data\n\ndataset=replace_cat_feature(dataset,features_nan)\n\ndataset[features_nan].isnull().sum()","9eac9d04":"dataset.head()","5f145cd4":"numerical_with_nan=[feature for feature in dataset.columns if dataset[feature].isnull().sum()>1 and dataset[feature].dtypes!='O']\n\n# We will print the numerical nan variables and percentage of missing values\n\nfor feature in numerical_with_nan:\n    print(\"{}: {}% missing value\".format(feature,np.around(dataset[feature].isnull().mean(),4)))","fd25f809":"for feature in numerical_with_nan:\n    ## We will replace by using median since there are outliers\n    median_value=dataset[feature].median()\n    \n    ## create a new feature to capture nan values\n    dataset[feature+'nan']=np.where(dataset[feature].isnull(),1,0)\n    dataset[feature].fillna(median_value,inplace=True)\n    \ndataset[numerical_with_nan].isnull().sum()","c45d3742":"dataset.head(50)","d5c02d73":"for feature in ['YearBuilt','YearRemodAdd','GarageYrBlt']:\n       \n    dataset[feature]=dataset['YrSold']-dataset[feature]","3ccf0d1f":"dataset.head()","3d4308b3":"dataset[['YearBuilt','YearRemodAdd','GarageYrBlt']].head()","526d9719":"dataset.head()","9d014ee6":"num_features=['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\n\nfor feature in num_features:\n    dataset[feature]=np.log(dataset[feature])","e1ce3717":"dataset.head()","a7e15aa8":"categorical_features=[feature for feature in dataset.columns if dataset[feature].dtype=='O']","e832e122":"categorical_features","496faaa5":"for feature in categorical_features:\n    temp=dataset.groupby(feature)['SalePrice'].count()\/len(dataset)\n    temp_df=temp[temp>0.01].index\n    dataset[feature]=np.where(dataset[feature].isin(temp_df),dataset[feature],'Rare_var')","d1c061dd":"dataset.head(50)","9fc188d0":"for feature in categorical_features:\n    labels_ordered=dataset.groupby([feature])['SalePrice'].mean().sort_values().index\n    labels_ordered={k:i for i,k in enumerate(labels_ordered,0)}\n    dataset[feature]=dataset[feature].map(labels_ordered)","6fcbd2ba":"dataset.head(10)","eb9646bc":"scaling_feature=[feature for feature in dataset.columns if feature not in ['Id','SalePerice'] ]\nlen(scaling_feature)","9f2acdac":"scaling_feature","d2def4ae":"dataset.head()","71306254":"feature_scale=[feature for feature in dataset.columns if feature not in ['Id','SalePrice']]\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\nscaler.fit(dataset[feature_scale])","bc4b7df5":"scaler.transform(dataset[feature_scale])","c598b550":"data = pd.concat([dataset[['Id', 'SalePrice']].reset_index(drop=True),\n                    pd.DataFrame(scaler.transform(dataset[feature_scale]), columns=feature_scale)],\n                    axis=1)","388a3be5":"data.head()","2c51da35":"## Feature Scaling","b4c5fc07":"## Handling Rare Categorical Feature\nWe will remove categorical variables that are present less than 1% of the observations","54e4199d":"## We will be performing all the below steps in Feature Engineering\n\n1. Missing values\n2. Temporal variables\n3. Categorical variables: remove rare labels\n4. Standarise the values of the variables to the same range","46b2bc7b":"## transform the train and test set, and add on the Id and SalePrice variables","6474f7e1":"## Check for numerical variables that contains missing values","cd0da44a":"## Replace missing value with a new label","db44cad5":"## Numerical Variables\nSince the numerical variables are skewed we will perform log normal distribution","83b2cd6c":"## Temporal Variables (Date Time Variables)","2c6d3778":"## Replacing the numerical Missing Values","1e64fa57":"## Missing Values\n->Capture all the nan values     \n->Firstly, lets handle Categorical features which are missing"}}