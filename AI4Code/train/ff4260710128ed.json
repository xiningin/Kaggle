{"cell_type":{"7b68e8e0":"code","66de37dc":"code","4e155629":"code","20c20943":"code","642aa1e9":"code","538e1d3f":"code","12535505":"code","1a2bd963":"code","64262c81":"code","c25dc0ff":"code","01b91ba6":"code","66a8d60c":"code","75c89093":"code","af1b0867":"code","c83c6963":"code","f7e48c27":"code","b97f0577":"code","3252d273":"code","8cbce0ce":"code","ae06d23f":"code","ed8bca23":"code","7142e5cc":"code","43db3e8c":"code","1c07a5ff":"code","3463d3ed":"code","7c196f70":"code","1da0b8df":"code","56948156":"code","a3f4fc96":"code","8c6a9592":"code","9918091f":"code","9e2685ec":"code","88618aea":"code","51deef7f":"code","9b422ca8":"code","5d9bd0d4":"code","1deb52cd":"code","1597c296":"code","0a80b90f":"code","66e43c3c":"code","8a39f429":"code","32085839":"code","9aa7f775":"code","61f50b99":"code","ab6e18b3":"code","458ae2e9":"code","5cd8d919":"code","7157b64a":"code","d3c16981":"code","79d9e424":"code","afa8b17a":"code","c4e26bae":"code","959cb3b4":"code","a64bdf25":"code","1cbaee2f":"code","3afa8e9d":"code","9500aa8e":"markdown","765ad0c2":"markdown","4d915b36":"markdown","e31c37c1":"markdown","55504848":"markdown","04505dc3":"markdown","441da9e2":"markdown","515f9cd2":"markdown","1e64f197":"markdown","873d7523":"markdown","04a46cd4":"markdown","cfa3e290":"markdown","3f580e70":"markdown","7f225087":"markdown","d54670d5":"markdown","4b04df27":"markdown","4577b8f9":"markdown","728cc023":"markdown","f25fb371":"markdown","49246df1":"markdown","da5e3f52":"markdown"},"source":{"7b68e8e0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing","66de37dc":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","4e155629":"train_df.shape, test_df.shape","20c20943":"train_df.head()","642aa1e9":"train_df.loc[:, 'keyword'].value_counts()","538e1d3f":"for i in train_df.loc[~train_df['keyword'].isnull(), 'keyword'].drop_duplicates():\n    print(i)","12535505":"print(train_df[train_df['target'] == 0].shape)\ntrain_df[train_df['target'] == 1].shape","1a2bd963":"train_df.loc[~train_df['keyword'].isnull(), :]","64262c81":"train_df.sample(10).loc[~train_df['keyword'].isnull(), ['keyword', 'text']]","c25dc0ff":"sum(train_df['keyword'].isnull()), len(train_df['keyword'].isnull())","01b91ba6":"train_df[train_df[\"target\"] == 0][\"text\"].values[19], train_df.loc[pd.Series(train_df[\"target\"] == 0), [\"keyword\", \"location\"]].values[19]","66a8d60c":"train_df[train_df[\"target\"] == 1][\"text\"].values[23], train_df.loc[pd.Series(train_df[\"target\"] == 1), [\"keyword\", \"location\"]].values[23]","75c89093":"pd.set_option('display.max_colwidth', -1)\ntrain_df[\"text\"].head(8)","af1b0867":"s = train_df[\"text\"][0:5].tolist()\n\ncount_vectorizer = feature_extraction.text.CountVectorizer()\n\n## let's get counts for the first 5 tweets in the data\nexample_train_vectors = count_vectorizer.fit_transform(s)\n\n# print(f\"VECTORES to dense --> {example_train_vectors.todense()}\")\n\nprint(count_vectorizer.get_feature_names(), len(count_vectorizer.get_feature_names()), example_train_vectors.todense().shape)\nprint(train_df[\"text\"].iloc[2])\nfor i in [5, 21, 37, 47]:\n    print(count_vectorizer.get_feature_names()[i])","c83c6963":"#Palabras que aparecen un par de veces al menos\ndef n_tweets(n):\n    return train_df[\"text\"].tolist()[0:n]\nn_tuit = 0\nprint(f\"WORDS -->  {count_vectorizer.get_feature_names()}\")\nfor tweet in n_tweets(5):\n    print(f\" Dense Vector --> {example_train_vectors[n_tuit].todense()}\")\n    print(f\"TWEET {tweet}\")\n    for i in np.where(example_train_vectors.todense()[n_tuit] >= 2)[1]:\n        print(f\"Words that appear more than 1 time --> {count_vectorizer.get_feature_names()[i]}\")\n    n_tuit += 1\n    print(\" \")","f7e48c27":"train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\n\n## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n# that the tokens in the train vectors are the only ones mapped to the test vectors - \n# i.e. that the train and test vectors use the same set of tokens.\ntest_vectors = count_vectorizer.transform(test_df[\"text\"])","b97f0577":"## Our vectors are really big, so we want to push our model's weights\n## toward 0 without completely discounting different words - ridge regression \n## is a good way to do this.\nclf = linear_model.RidgeClassifier(tol=1e-2, solver=\"sag\")\nclf","3252d273":"train_vectors, train_df[\"target\"]","8cbce0ce":"scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=5, scoring=\"f1\")\nscores","ae06d23f":"scores.mean()","ed8bca23":"clf.fit(train_vectors, train_df[\"target\"])","7142e5cc":"sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","43db3e8c":"sample_submission[\"target\"] = clf.predict(test_vectors)","1c07a5ff":"sample_submission.head()","3463d3ed":"sample_submission.to_csv(\"submission.csv\", index=False)","7c196f70":"from sklearn.svm import SVC\nclf1 = SVC(gamma='auto', kernel = 'poly')\n\nscores = model_selection.cross_val_score(clf1, train_vectors, train_df[\"target\"], cv=5, scoring=\"f1\")\nscores","1da0b8df":"clf1.fit(train_vectors, train_df[\"target\"])\n\n\n\nsample_submission[\"target2\"] = clf1.predict(test_vectors)","56948156":"sample_submission[sample_submission['target2'] != 0]","a3f4fc96":"train_vectors.todense()","8c6a9592":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=0)\nclf\n\nscores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=5, scoring=\"f1\")\nscores","9918091f":"def evaluate_model_list(lista_modelos):\n    for modelo in lista_modelos:\n        print(f\"<--- New model ---> {modelo.__class__.__name__}\") \n        scores = model_selection.cross_val_score(modelo, train_vectors, train_df[\"target\"], cv=5, scoring=\"f1\")\n        print(modelo, scores, scores.mean())\n        ","9e2685ec":"from sklearn.linear_model import LogisticRegression\nmodels = list()\nfor i in [0.1, 1, 100,1000]:\n    clf1 = LogisticRegression(C = i, penalty = 'l2', random_state=42, solver='liblinear')\n    clf2 = LogisticRegression(C = i, penalty = 'l1', random_state=42, solver='liblinear')\n    clf3 = linear_model.RidgeClassifier(alpha = i,tol=1e-2, solver=\"sag\")\n    models += [clf1, clf2, clf3]\n\n\nevaluate_model_list(models)","88618aea":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression( penalty = 'l1', random_state=0)\nclf\n\nscores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=5, scoring=\"f1\")\nscores","51deef7f":"clf.fit(train_vectors, train_df[\"target\"])\nsample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsample_submission[\"target\"] = clf.predict(test_vectors)\nsample_submission.to_csv(\"submission.csv\", index=False)","9b422ca8":"sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\ndef make_submissions(classif_model):\n    classif_model.fit(train_vectors, train_df[\"target\"])\n    sample_submission[\"target\"] = classif_model.predict(test_vectors)\n    sample_submission.to_csv(\"submission.csv\", index=False)\n    print(\"SUBMISSION SATISFACTORIA\")\n\nmake_submissions(linear_model.RidgeClassifier(alpha = 100,tol=1e-2, solver=\"sag\"))","5d9bd0d4":"sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\ndef make_submissions(classif_model):\n    classif_model.fit(train_vectors, train_df[\"target\"])\n    sample_submission[\"target\"] = classif_model.predict(test_vectors)\n    sample_submission.to_csv(\"submission.csv\", index=False)\n    print(\"SUBMISSION SATISFACTORIA\")\n\nmake_submissions(linear_model.RidgeClassifier(alpha = 100,tol=1e-2, solver=\"sag\"))","1deb52cd":"train_df.head()","1597c296":"from sklearn.naive_bayes import GaussianNB, MultinomialNB","0a80b90f":"gnb = GaussianNB()\nlista_NBS = [gnb]\nfor i in [0.1, 1.0, 10, 100]:\n    mnb = MultinomialNB(alpha=i)\n    lista_NBS += [mnb]\nlista_NBS","66e43c3c":"def evaluate_model_list(lista_modelos):\n    for modelo in lista_modelos:\n        scores = model_selection.cross_val_score(modelo, train_vectors.todense(), train_df[\"target\"], cv=5, scoring=\"f1\")\n        print(modelo, scores, scores.mean())\n        print(\"<--- New model ---> \")\nevaluate_model_list(lista_NBS)","8a39f429":"make_submissions(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))","32085839":"# from sklearn.tree import DecisionTreeClassifier\n\n# tree1 = DecisionTreeClassifier(criterion = 'gini' ,random_state=0, max_depth=5)\n# tree2 = DecisionTreeClassifier(criterion = 'entropy' ,random_state=0, max_depth=5)\n# tree_list = [tree1, tree2]\n# evaluate_model_list(tree_list)","9aa7f775":"# from sklearn.ensemble import AdaBoostClassifier\n\n# from timeit import default_timer as timer\n\n# start = timer()\n# adaboosts = [AdaBoostClassifier(n_estimators=i, random_state=42) for i in [1, 10, 20]]\n# evaluate_model_list(adaboosts)\n# end = timer()\n# print(end - start)\n\n","61f50b99":"# from sklearn.ensemble import RandomForestClassifier\n\n# start = timer()\n# evaluate_model_list([RandomForestClassifier(n_estimators = i, random_state = 42) for i in [1,10,50,100]])\n# end = timer()\n# print(end - start)\n","ab6e18b3":"## PODEMOS VER QUE LOS ARBOLES COMPUESTOS FUNCIONAN MEJOR QUE LOS SIMPLES\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(train_vectors, train_df['target'], test_size=0.3, random_state=0)\nfrom xgboost import XGBClassifier\n\nxgb_classifir = XGBClassifier(\n                              learning_rate=0.1,\n                              num_round=1000,\n                              max_depth=10,\n                              min_child_weight=2,\n                              colsample_bytree=0.8,\n                              subsample=0.9,\n                              gamma=0.4,\n                              reg_alpha=1e-5,\n                              reg_lambda=1,\n                              n_estimators=2000,\n                              objective='binary:logistic',\n                              eval_metric=[\"auc\", \"logloss\", \"error\"],\n                              early_stopping_rounds=50)\nxgb_classifir.fit(X_train, y_train, eval_set=[(X_train, y_train),(X_valid, y_valid)])\n","458ae2e9":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve\n\n\ny_pred_xgb = xgb_classifir.predict(X_valid)\nprint(confusion_matrix(y_valid, y_pred_xgb))\nprint(accuracy_score(y_valid, y_pred_xgb))\nprint(f1_score(y_valid, y_pred_xgb))\n\nsample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\n# sample_submission[\"target\"] = xgb_classifir.predict(X_valid)\n# sample_submission.to_csv(\"submission.csv\", index=False)","5cd8d919":"y_pred_xgb = xgb_classifir.predict(test_vectors)\ny_pred_xgb\nprint(sample_submission[\"target\"].head(10))\nsample_submission[\"target\"] = y_pred_xgb\nprint(\"NUEVOS\")\nprint(sample_submission[\"target\"].head(10))\n","7157b64a":"sample_submission.to_csv(\"submission.csv\", index=False)","d3c16981":"import nltk\nsentence = \"My name is George and I love NLP from New York\"\ntokens = nltk.word_tokenize(sentence)\nprint(tokens)","79d9e424":"import nltk\nfrom nltk.corpus import stopwords\n\nsentence = \"This is a sentence for removing stop words\"\nprint(f\"Sentence --> {sentence}\")\ntokens = nltk.word_tokenize(sentence)\n\nstop_words = stopwords.words('english')\nfiltered_tokens, unfiltered_tokens = [w for w in tokens if w not in stop_words], [w for w in tokens if w in stop_words]\nprint(f\"Filtered toekns {filtered_tokens}, and unfiltered {unfiltered_tokens}\")\n","afa8b17a":"print(stop_words,)\nprint(len(stop_words), len(stopwords.words('spanish')))\nprint(stopwords.words('spanish'),)","c4e26bae":"import nltk\n\nsnowball_stemmer = nltk.stem.SnowballStemmer('spanish')","959cb3b4":"def stemer (string):\n    return snowball_stemmer.stem(string)","a64bdf25":"cafes = ['caf\u00e9', 'cafetera', 'descafeinado', 'cafetero', 'cafeteria', 'cafe\u00edna']","1cbaee2f":"[stemer(palabra) for palabra in cafes]","3afa8e9d":"def stemer1 (string):\n    return nltk.stem.SnowballStemmer('english').stem(string)\n[stemer1(palabra) for palabra in ['cook', 'cooks', 'cooked', 'cooking']]","9500aa8e":"## TREES\n\nAdaboost --> AdaBoost is similar to Random Forest in that they both tally up the predictions made by each decision trees within the forest to decide on the final classification. There are however, some subtle differences. For instance, in AdaBoost, the decision trees have a depth of 1 (i.e. 2 leaves). In addition, the predictions made by each decision tree have varying impact on the final prediction made by the model. The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction.\n\nCriterion puede ser giny o entropy. Todos resultan en performance","765ad0c2":"The above tells us that:\n1. There are 54 unique words (or \"tokens\") in the first five tweets.\n2. The first tweet contains only some of those unique tokens - all of the non-zero counts above are the tokens that DO exist in the first tweet.\n\n\nNow let's create vectors for all of our tweets.","4d915b36":"The above scores aren't terrible! It looks like our assumption will score roughly 0.65 on the leaderboard. There are lots of ways to potentially improve on this (TFIDF, LSA, LSTM \/ RNNs, the list is long!) - give any of them a shot!\n\nIn the meantime, let's do predictions on our training set and build a submission for the competition.","e31c37c1":"## Modelling","55504848":"Buscamos un catastrofe y lo printeamos","04505dc3":"### Building vectors\n\nThe theory behind the model we'll build in this notebook is pretty simple: the words contained in each tweet are a good indicator of whether they're about a real disaster or not (this is not entirely correct, but it's a great place to start).\n\nWe'll use scikit-learn's `CountVectorizer` to count the words in each tweet and turn them into data our machine learning model can process.\n\nNote: a `vector` is, in this context, a set of numbers that a machine learning model can work with. We'll look at one in just a second.","441da9e2":"Buscamos un no catastrofe y lo printeamos","515f9cd2":"### A quick look at our data\n\nLet's look at our data... first, an example of what is NOT a disaster tweet.","1e64f197":"## RANDOM FOREST VS ADABOOST VS GRADIENT BOOSTED \nEnsemble learning, in general, is a model that makes predictions based on a number of different models. By combining individual models, the ensemble model tends to be more flexible\ud83e\udd38\u200d\u2640\ufe0f (less bias) and less data-sensitive\ud83e\uddd8\u200d\u2640\ufe0f (less variance).\n\nTwo most popular ensemble methods are **bagging and boosting**.\n\nBagging: Training a bunch of individual models in a parallel way. Each model is trained by a random subset of the data\n\nBoosting: Training a bunch of individual models in a sequential way. Each individual model learns from mistakes made by the previous model.\n\n\n\nBoth AdaBoost and Gradient Boosting build weak learners in a sequential fashion.\n\nIn Gradient Boosting, \u2018shortcomings\u2019 (of existing weak learners) are identified by gradients.\nIn Adaboost, \u2018shortcomings\u2019 are identified by high-weight data points.\n\nA random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default)","873d7523":"Can we know which words have appeared more than 1 time? Can we know which words are we using?","04a46cd4":"Now, in the viewer, you can submit the above file to the competition! Good luck!","cfa3e290":"### Our model\n\nAs we mentioned above, we think the words contained in each tweet are a good indicator of whether they're about a real disaster or not. The presence of particular word (or set of words) in a tweet might link directly to whether or not that tweet is real.\n\nWhat we're assuming here is a _linear_ connection. So let's build a linear model and see!","3f580e70":"Palabrostias que aparecen","7f225087":"## OPCIONES\n### No lasso. Ridge\nEn ridge cuanto mas alpha, mas coeff a 0\n### Logistic\nliblinear --> small datasets\nc --> Stronger reg cuanto menor","d54670d5":"## MORE MODELLING\nMultinomialNB --> naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice)\nNaive Bayes --> Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes\u2019 theorem with the \u201cnaive\u201d assumption of conditional independence between every pair of features given the value of the class variable","4b04df27":"Let's test our model and see how well it does on the training data. For this we'll use `cross-validation` - where we train on a portion of the known data, then validate it with the rest. If we do this several times (with different portions) we can get a good idea for how a particular model or method performs.\n\nThe metric for this competition is F1, so let's use that here.","4577b8f9":"## NLP Tutorial\n\nNLP - or *Natural Language Processing* - is shorthand for a wide array of techniques designed to help machines learn from text. Natural Language Processing powers everything from chatbots to search engines, and is used in diverse tasks like sentiment analysis and machine translation.\n\nIn this tutorial we'll look at this competition's dataset, use a simple technique to process it, build a machine learning model, and submit predictions for a score!","728cc023":"## Adaboost\nEl base estimator es DecisionTreeClassifier(max_depth = 1) y n_estimators es igual a 50.","f25fb371":"## APPEARANCES","49246df1":"### VAMOS A IR A LOWER CASE Y LOS PUNCTUATIONS\n","da5e3f52":"### Dado que es un problema de biclasificaci\u00f3n vamos a usar f1 "}}