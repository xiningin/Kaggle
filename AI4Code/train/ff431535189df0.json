{"cell_type":{"d6205cf9":"code","18da96e9":"code","1e42ccca":"code","b0492a2b":"code","5dc0301f":"code","1e25d2ca":"code","aa9dd6f5":"code","dd0bce09":"code","b243e437":"code","7d4cc883":"code","716aaee8":"code","25f7b36c":"code","f9d62612":"code","6adff9cc":"markdown","547607f8":"markdown","3950ffd1":"markdown"},"source":{"d6205cf9":"import os\nimport gc\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport pickle\n\nimport torch\nimport torch.nn as nn\n\nimport matplotlib.pyplot as plt","18da96e9":"device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","1e42ccca":"def calculate_wap1(df):\n    wap=(df['ask_price1'] * df['bid_size1']) + (df['bid_price1'] * df['ask_size1'])\n    wap\/=(df['ask_size1'] + df['bid_size1'])\n    return wap\n\ndef calculate_wap2(df):\n    wap=(df['ask_price2'] * df['bid_size2']) + (df['bid_price2'] * df['ask_size2'])\n    wap\/=(df['ask_size2'] + df['bid_size2'])\n    return wap\n\n\ndef calculate_wap3(wap1, wap2):\n    wap3=(wap1+wap2)\/2\n    return wap3\n\ndef get_features(df):\n    df['ask_size1']=df['ask_size1'].apply(lambda x: max(1, x))\n    df['ask_size2']=df['ask_size2'].apply(lambda x: max(1, x))\n    df['bid_size1']=df['bid_size1'].apply(lambda x: max(1, x))\n    df['bid_size2']=df['bid_size2'].apply(lambda x: max(1, x))\n    \n    df['wap1']=calculate_wap1(df)\n    df['wap2']=calculate_wap2(df)\n    df['wap3']=calculate_wap3(df['wap1'], df['wap2'])\n    \n    return df\n\ndef get_dummy_df(df, stock_id):\n    time_id=df.time_id.unique()\n    seconds_in_bucket=np.arange(600)\n    \n    \n    dummy_df=pd.DataFrame.from_dict({'time_id': np.repeat(time_id, seconds_in_bucket.shape[0]), \n                                     'seconds_in_bucket': np.tile(seconds_in_bucket, time_id.shape[0])})\n    dummy_df['stock_id']=stock_id\n    return dummy_df","b0492a2b":"mean_columns=[\n    'bid_price1', 'ask_price1', 'bid_price2', 'ask_price2', \n    'bid_size1', 'ask_size1', 'bid_size2', 'ask_size2',\n    'wap1', 'wap2', 'wap3'\n]\n\nlog_transform_features=['bid_size1', 'ask_size1', 'bid_size2', 'ask_size2']\ndef aggregate_buckets(df):\n    mean_df=df.groupby(['stock_id', 'time_id', 'bucket'])[mean_columns].mean().reset_index()\n    return mean_df","5dc0301f":"class config:\n    num_buckets= 200\n    num_features= 11\n    batch_size=256\n    epochs=20","1e25d2ca":"class OptiverDataset(torch.utils.data.Dataset):\n    def __init__(self, df):\n        self.df=df\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row=self.df.iloc[idx]\n        features=row.features\n        \n        X=torch.tensor(features, dtype=torch.float32)\n        \n        s=np.log(features[:, 8])\n        s[s==-np.inf] = 0.0\n        \n        s=torch.tensor(np.append(0, np.diff(s)), dtype=torch.float32)\n        rvs=torch.sqrt(torch.cumsum( s**2 , dim=0))\n        return (X, s.unsqueeze(dim=-1), rvs.unsqueeze(dim=-1))","aa9dd6f5":"#https:\/\/github.com\/KrisKorrel\/sparsemax-pytorch\n\nclass Sparsemax(nn.Module):\n    \"\"\"Sparsemax function.\"\"\"\n\n    def __init__(self, dim=None):\n        \"\"\"Initialize sparsemax activation\n        \n        Args:\n            dim (int, optional): The dimension over which to apply the sparsemax function.\n        \"\"\"\n        super(Sparsemax, self).__init__()\n\n        self.dim = -1 if dim is None else dim\n\n    def forward(self, input):\n        \"\"\"Forward function.\n        Args:\n            input (torch.Tensor): Input tensor. First dimension should be the batch size\n        Returns:\n            torch.Tensor: [batch_size x number_of_logits] Output tensor\n        \"\"\"\n        # Sparsemax currently only handles 2-dim tensors,\n        # so we reshape to a convenient shape and reshape back after sparsemax\n        input = input.transpose(0, self.dim)\n        original_size = input.size()\n        input = input.reshape(input.size(0), -1)\n        input = input.transpose(0, 1)\n        dim = 1\n\n        number_of_logits = input.size(dim)\n\n        # Translate input by max for numerical stability\n        input = input - torch.max(input, dim=dim, keepdim=True)[0].expand_as(input)\n\n        # Sort input in descending order.\n        # (NOTE: Can be replaced with linear time selection method described here:\n        # http:\/\/stanford.edu\/~jduchi\/projects\/DuchiShSiCh08.html)\n        zs = torch.sort(input=input, dim=dim, descending=True)[0]\n        range = torch.arange(start=1, end=number_of_logits + 1, step=1, device=device, dtype=input.dtype).view(1, -1)\n        range = range.expand_as(zs)\n\n        # Determine sparsity of projection\n        bound = 1 + range * zs\n        cumulative_sum_zs = torch.cumsum(zs, dim)\n        is_gt = torch.gt(bound, cumulative_sum_zs).type(input.type())\n        k = torch.max(is_gt * range, dim, keepdim=True)[0]\n\n        # Compute threshold function\n        zs_sparse = is_gt * zs\n\n        # Compute taus\n        taus = (torch.sum(zs_sparse, dim, keepdim=True) - 1) \/ k\n        taus = taus.expand_as(input)\n\n        # Sparsemax\n        self.output = torch.max(torch.zeros_like(input), input - taus)\n\n        # Reshape back to original shape\n        output = self.output\n        output = output.transpose(0, 1)\n        output = output.reshape(original_size)\n        output = output.transpose(0, self.dim)\n\n        return output\n\n    def backward(self, grad_output):\n        \"\"\"Backward function.\"\"\"\n        dim = 1\n\n        nonzeros = torch.ne(self.output, 0)\n        sum = torch.sum(grad_output * nonzeros, dim=dim) \/ torch.sum(nonzeros, dim=dim)\n        self.grad_input = nonzeros * (grad_output - sum.expand_as(grad_output))\n\n        return self.grad_input","dd0bce09":"sparsemax = Sparsemax(dim=-1)\n\ndef get_activation_fn(activation):\n    if activation=='gelu':\n        return nn.GELU()\n    elif activation=='relu':\n        return nn.ReLU()\n    \ndef attention(query, key, value, dropout=None):\n    d_k=query.size(-1)\n    scores=torch.matmul( query, key.transpose(-1, -2) )\/np.sqrt(d_k)\n    #scores=torch.tril(scores)\n    scores=scores.masked_fill(scores == 0, -1e9)\n    \n    p_attn=torch.softmax(scores, dim=-1)\n    #p_attn=sparsemax(scores)\n    x_attn=torch.matmul(p_attn, value)\n    if dropout:\n        x_attn=dropout(x_attn)        \n    return p_attn, x_attn\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_features, dmodel, nhead,activation,norm,dropout):\n        super().__init__()\n        self.dmodel=dmodel\n        self.nhead=nhead\n        self.d_k=dmodel\/\/nhead #Size\n        \n        self.activation=activation\n        self.norm=norm\n        self.dropout=dropout\n        \n        self.Q=nn.Linear(num_features, dmodel)\n        self.K=nn.Linear(num_features, dmodel)\n        self.V=nn.Linear(num_features, dmodel)\n        \n        #self.Q=nn.Conv1d(num_features, dmodel, 3, padding=1)\n        #self.K=nn.Conv1d(num_features, dmodel, 3, padding=1)\n        #self.V=nn.Conv1d(num_features, dmodel, 3, padding=1)\n        \n        \n        self.W=nn.Linear(dmodel, num_features)\n        \n        nn.init.uniform_(self.Q.weight, -1\/np.sqrt(2*num_features), 1\/np.sqrt(2*num_features))\n        nn.init.uniform_(self.K.weight, -1\/np.sqrt(2*num_features), 1\/np.sqrt(2*num_features))\n        nn.init.uniform_(self.V.weight, -1\/np.sqrt(2*num_features), 1\/np.sqrt(2*num_features))\n        nn.init.uniform_(self.W.weight, -1\/np.sqrt(2*num_features), 1\/np.sqrt(2*num_features))\n        \n        \n    def forward(self, x):\n        bsize=x.size(0)\n        x=self.norm(x)\n        #x=x.transpose(2, 1)\n        #query=self.Q(x).transpose(2, 1).view(bsize, -1, self.nhead, self.d_k)\n        #key=self.K(x).transpose(2, 1).view(bsize, -1, self.nhead, self.d_k)\n        #value=self.V(x).transpose(2, 1).view(bsize, -1, self.nhead, self.d_k)\n        \n        query=self.Q(x).view(bsize, -1, self.nhead, self.d_k)\n        key=self.K(x).view(bsize, -1, self.nhead, self.d_k)\n        value=self.V(x).view(bsize, -1, self.nhead, self.d_k)\n        \n        \n        p_attn, x_attn=attention(query, key, value, self.dropout)\n        x_attn=x_attn.view(bsize, -1, self.nhead*self.d_k)\n        \n        x_attn=self.W(x_attn)\n        #x=x.transpose(2, 1)\n        x=x+x_attn\n        return x\n\nclass TimeSeriesAttentionLayer(nn.Module):\n    def __init__(self,\n                 num_features=32,\n                 dmodel=128,\n                 nhead=4,\n                 dim_feed_forward=512,\n                 activation='relu', \n                 dropout=0.1):\n        \n        super().__init__()\n        self.num_features=num_features\n        self.dmodel=dmodel\n        self.nhead=nhead\n        self.dim_feed_forward=dim_feed_forward\n        self.activation=get_activation_fn(activation)\n        self.norm=nn.LayerNorm(num_features)\n        self.dropout=nn.Dropout(dropout)\n        \n        self.multihead_attn=MultiHeadAttention(num_features,\n                                               dmodel,\n                                               nhead,\n                                               self.activation,\n                                               self.norm,\n                                               self.dropout)\n        \n        self.linear1=nn.Linear(num_features, dim_feed_forward)\n        self.linear2=nn.Linear(dim_feed_forward, num_features)\n        \n    def forward(self, x):\n        x=self.multihead_attn(x)\n        x=self.norm(x)\n        x_ffn=self.linear2(self.dropout(self.activation(self.linear1(x))))\n        x=x+x_ffn\n        return x\n\n\nclass FeatureExtractorWith1DConv(nn.Module):\n    def __init__(self, input_size, output_size):\n        super().__init__()\n        self.pre_bn=nn.BatchNorm1d(input_size)\n        \n        self.linear1=nn.Linear(input_size, 2*output_size)\n        self.bn1=nn.BatchNorm1d(2*output_size)\n        \n        self.linear2=nn.Linear(2*output_size, output_size)\n        self.bn2=nn.BatchNorm1d(output_size)\n        #self.conv1=nn.Conv1d(input_size, output_size, 3, padding=1)\n        #self.bn1=nn.BatchNorm1d(output_size)\n        \n        self.activation=nn.GELU()\n        self.dropout=nn.Dropout(0.2)\n        \n    def forward(self, x):\n        x=self.pre_bn(x.transpose(2, 1)).transpose(2, 1)\n        x=self.dropout(self.activation(self.bn1( self.linear1(x).transpose(2, 1) ).transpose(2, 1)))\n        x=self.activation(self.bn2( self.linear2(x).transpose(2, 1) ).transpose(2, 1))\n        return x\n    \nclass Auxilary_FFN(nn.Module):\n    def __init__(self, sz):\n        super().__init__()\n        self.bn1=nn.BatchNorm1d(sz)\n        self.linear1=nn.Linear(sz, 2*sz)\n        \n        self.bn2=nn.BatchNorm1d(2*sz)\n        self.linear2=nn.Linear(2*sz, 2*sz)\n        \n        self.activation=nn.GELU()\n        self.dropout=nn.Dropout(0.2)\n        \n    def forward(self, x):\n        x=self.bn1(x.transpose(2, 1)).transpose(2, 1)\n        x=self.linear1( self.dropout(self.activation(x)) )\n        \n        x=self.bn2(x.transpose(2, 1)).transpose(2, 1)\n        x=self.linear2( self.dropout(self.activation(x)) )\n        return x\n\n\nclass FFN(nn.Module):\n    def __init__(self, sz):\n        super().__init__()\n        self.bn1=nn.BatchNorm1d(sz)\n        self.linear1=nn.Linear(sz, 2*sz)\n        \n        self.bn2=nn.BatchNorm1d(2*sz)\n        self.linear2=nn.Linear(2*sz, 2*sz)\n        \n        \n        self.activation=nn.GELU()\n        self.dropout=nn.Dropout(0.2)\n        \n    def forward(self, x):\n        x=self.linear1(self.dropout(self.activation(self.bn1(x))))\n        x=self.linear2(self.dropout(self.activation(self.bn2(x))))\n        return x\n\n\nclass AttentionHead(nn.Module):\n    def __init__(self, dmodel, dropout):\n        super().__init__()\n        self.dropout=dropout\n        self.W=nn.Linear(dmodel, 1)\n    def forward(self, x):\n        scores=self.W(x).squeeze(-1)\n        p_attn=torch.softmax(scores, dim=-1)\n        #p_attn=sparsemax(scores)\n        if self.dropout:\n            p_attn=self.dropout(p_attn)\n        x_attn=torch.matmul(p_attn, x)\n        x_attn=x_attn.sum(dim=1)\n        return p_attn, x_attn\n    \nclass PrimaryHead(nn.Module):\n    def __init__(self, hsize, num_layers):\n        super().__init__()\n        self.hsize=hsize\n        self.attn_dropout=nn.Dropout(0.1)\n        self.attn_head=AttentionHead(hsize, self.attn_dropout)\n        self.ffn=nn.ModuleList(\n            [FFN(hsize) for _ in range(num_layers)]\n        )\n        self.primary_out=nn.Linear(2*hsize, 1)\n        \n    def forward(self, x):\n        #p_attn, x=self.attn_head(x)\n        p_attn=None\n        x=x[:, -1, :]\n        for i, _ in enumerate(self.ffn):\n            x=self.ffn[i](x)\n        y=self.primary_out(x)\n        \n        return p_attn, y\n\nclass AuxilaryHead(nn.Module):\n    def __init__(self, hsize, num_layers):\n        super().__init__()\n        self.hsize=hsize\n        self.ffn=nn.ModuleList(\n            [Auxilary_FFN(hsize) for _ in range(num_layers)]\n        )\n        self.aux_out=nn.Linear(2*hsize, 1)\n    def forward(self, x):\n        for i, _ in enumerate(self.ffn):\n            x=self.ffn[i](x)\n        y=self.aux_out(x)\n        return y\n    \nfrom torch.autograd import Variable\nclass PositionalEncoding(nn.Module):\n    \"Implement the PE function.\"\n    def __init__(self, d_model, dropout, max_len=config.num_buckets):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) *\n                             -(np.log(10000.0) \/ d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n        \n    def forward(self, x):\n        x = x + Variable(self.pe[:, :x.size(1)], \n                         requires_grad=False)\n        return x\n    \n    \nclass OptiverModel(nn.Module):\n    def __init__(self, params):\n        super().__init__()\n        self.dmodel=params['dmodel']\n        self.in_features=params['in_features']\n        self.out_features=params['out_features']\n        self.num_ffn_layers=params['num_ffn_layers']\n        \n        \n        self.dropout=nn.Dropout(0.1)\n        self.feature_extractor=FeatureExtractorWith1DConv(self.in_features, self.out_features)\n        self.positions = PositionalEncoding(self.out_features, 0.1)\n        self.attn_layers=nn.ModuleList([TimeSeriesAttentionLayer(num_features=self.out_features,\n                                                                 dmodel=self.dmodel,\n                                                                 nhead=params['nhead'],\n                                                                 dim_feed_forward=params['dim_feed_forward'],\n                                                                ) for _ in range(params['num_attention_layers'])])\n        \n        self.primary_model=PrimaryHead(self.out_features, self.num_ffn_layers)\n        self.aux_model=AuxilaryHead(self.out_features, self.num_ffn_layers)\n        \n    def forward(self, x):\n        batch_size=x.size(0)\n        seq_len=x.size(1)\n        \n        x=self.positions(self.feature_extractor(x))\n        for attn_layer in self.attn_layers:\n            x=attn_layer(x)    \n        p_attn, yalpha=self.primary_model(x)\n        #yalpha_aux=self.aux_model(x)\n        \n        return {\n            'p_attn': p_attn,\n            'yalpha': yalpha.view(-1),\n            'yalpha_aux': None#yalpha_aux.squeeze(dim=-1)\n        }","b243e437":"def inference(model, test_dataloader):\n    model.eval()\n    ypred=[]\n    for i, (X, s, rvs) in enumerate(test_dataloader):\n        X=torch.cat([X, s], dim=-1)\n        X=X.to(device)\n        rvs=rvs.to(device)\n        \n        with torch.no_grad():\n            outputs=model(X)\n            yhat_alpha=outputs['yalpha']\n            y = yhat_alpha * rvs[:, -1, 0]\n            y=y.view(-1).detach().cpu()\n            y=torch.clamp(y, 0, 1.0)\n            \n            y=y.tolist()\n            ypred+=y\n    return ypred","7d4cc883":"params={\n    'dmodel': 128,\n    'nhead':8,\n    'in_features': 12,\n    'out_features': 32,\n    'dim_feed_forward': 256,\n    'num_attention_layers': 6,\n    'num_ffn_layers': 1\n}\n\nmodel=OptiverModel(params)\nmodel=torch.load('..\/input\/optiver-smooting-model\/best_rmspe.pt', map_location=device)\nmodel=model.to(device)","716aaee8":"def preprocess_book_data(book_folder):\n    all_df=[]\n    for i, filepath in enumerate(os.listdir(book_folder)):\n        path=os.path.join(book_folder, filepath)\n        stock_id=int(filepath.split('=')[-1])\n        \n        df=pd.read_parquet(path)\n        \n        dummy_df=get_dummy_df(df, stock_id)\n        df=dummy_df.merge(df, how='left')\n        df=df.sort_values(['time_id', 'seconds_in_bucket'])\n        df.fillna(method='ffill', inplace=True)\n        df.fillna(0.0, inplace=True)\n        df['bucket']=df['seconds_in_bucket']\/\/3\n        df=get_features(df)\n        df.fillna(0.0, inplace=True)\n        \n        stock_df=aggregate_buckets(df)\n        for colname in log_transform_features:\n            stock_df[colname]=np.log(1+stock_df[colname] )\n            \n        stock_df['features']=stock_df[mean_columns].values.tolist()\n        stock_df['features']=stock_df['features'].apply(np.array)\n        stock_df=stock_df.groupby(['stock_id', 'time_id'])[['features']].agg(list).reset_index()\n        stock_df['features']=stock_df['features'].apply(np.array)\n        \n        test_dataset=OptiverDataset(stock_df)\n        test_dataloader=torch.utils.data.DataLoader(test_dataset,\n                                                    batch_size=256,\n                                                    shuffle=False,\n                                                    num_workers=0,\n                                                    pin_memory=True,\n                                                    drop_last=False)\n        \n        stock_df['target']=inference(model, test_dataloader)\n        stock_df.drop(columns='features', inplace=True)\n    \n        all_df.append(stock_df)\n        gc.collect()\n        \n    all_df=pd.concat(all_df)\n    all_df.reset_index(drop=True, inplace=True)\n    return all_df","25f7b36c":"test_df=pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n\nfeatures_df=preprocess_book_data('..\/input\/optiver-realized-volatility-prediction\/book_test.parquet')\ntest_df=test_df.merge(features_df, how='left')\ntest_df.fillna(0.0, inplace=True)\ntest_df.head()","f9d62612":"test_df[['row_id', 'target']].to_csv('submission.csv', index=False)","6adff9cc":"# Dataset","547607f8":"# Inference","3950ffd1":"# Model"}}