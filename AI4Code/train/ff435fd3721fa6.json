{"cell_type":{"84f64df0":"code","d1a3472d":"code","191618f2":"code","c49e93e7":"code","2c00d73d":"code","c0816e87":"code","9ef358d3":"code","0d303421":"code","83cde186":"code","e9e045a9":"code","c30526e9":"code","927cf624":"code","985c7ee9":"code","1bfe6d9a":"code","e6478896":"code","1da48fba":"code","52b55ee3":"code","6cd26b7b":"code","79b63b29":"markdown"},"source":{"84f64df0":"# Importing the Keras libraries and packages\nimport numpy as np\nimport pandas as pd \nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D,BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import VGG19\nimport os","d1a3472d":"# Fitting the CNN to the images\n\nimage_width, image_height = 250, 250\n\ntrain_datagen = ImageDataGenerator(\n      rescale=1.\/255,\n      rotation_range=20,\n      shear_range=0.2,\n      zoom_range=0.2,\n      width_shift_range=0.3,\n      height_shift_range=0.3,\n      horizontal_flip=True,\n      fill_mode='nearest')\n\ntest_datagen = ImageDataGenerator(rescale = 1.\/255.)\n\ntrain_generator = train_datagen.flow_from_directory(\n                    \"..\/input\/sheep-dataset\/dataset_sheep_goat\/trainind_set\",\n                    batch_size=1,\n                    class_mode='binary',\n                    shuffle=True,\n                    target_size=(image_width, image_height)\n)     \n\ntest_generator =  test_datagen.flow_from_directory(\n                    \"..\/input\/sheep-dataset\/dataset_sheep_goat\/test_set\",\n                    batch_size=1,\n                    class_mode='binary',\n                    shuffle=True,\n                    target_size=(image_width, image_height)\n)","191618f2":"train_steps = train_generator.n \/\/ train_generator.batch_size\ntest_steps = test_generator.n \/\/ test_generator.batch_size\nprint(train_steps)\nprint(test_steps)","c49e93e7":"vgg_conv = VGG19(weights='imagenet', include_top=False, input_shape=(image_width, image_height, 3))","2c00d73d":"vgg_conv.summary()","c0816e87":"from tensorflow.keras.utils import plot_model\nfrom IPython.display import Image\nplot_model(vgg_conv, to_file='convnet.png', show_shapes=True,show_layer_names=True)\nImage(filename='convnet.png') ","9ef358d3":"for layers in (vgg_conv.layers)[:-10]:\n    layers.trainable = False","0d303421":"model = Sequential()","83cde186":"# Add the vgg convolutional base model\nmodel.add(vgg_conv)\n\n# Add new layers\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()","e9e045a9":"from tensorflow.keras.utils import plot_model\nfrom IPython.display import Image\nplot_model(model, to_file='convnet.png', show_shapes=True,show_layer_names=True)\nImage(filename='convnet.png') ","c30526e9":"lr = 1e-7\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(lr), metrics=['accuracy'])","927cf624":"mcp = ModelCheckpoint('VGG19.h5', verbose=1)","985c7ee9":"es = EarlyStopping(patience=1,verbose=1)","1bfe6d9a":"%time\nhistory = model.fit(train_generator,steps_per_epoch=train_steps,epochs=5,validation_data=test_generator,validation_steps=test_steps,verbose=1,callbacks=[mcp,es])","e6478896":"model.evaluate(test_generator, verbose=1, steps=test_steps)","1da48fba":"import matplotlib.pyplot as plt\n\naccuracy      = history.history['accuracy']\nval_accuracy  = history.history['val_accuracy']\nloss     = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs   = range(len(accuracy))\n\nplt.plot(epochs, accuracy)\nplt.plot(epochs, val_accuracy)\nplt.title('Training and validation accuracy')\nplt.figure()\n\n# Plot training and validation loss per epoch\nplt.plot(epochs, loss)\nplt.plot(epochs, val_loss)\nplt.title('Training and validation loss')","52b55ee3":"train_generator.class_indices","6cd26b7b":"from tensorflow.keras.preprocessing import image\nimport numpy as np\n\nimage_width, image_height = 250, 250\nimg = image.load_img('..\/input\/sheep-goat\/sheep_goat\/sheep\/sheep1.jpg', target_size=(image_width, image_height))\nimg = image.img_to_array(img)\nimg = np.expand_dims(img, axis = 0)\nresult = model.predict_classes(img)\n#prediction\ntrain_generator.class_indices\nif result[0][0] == 0:\n  prediction = 'Sheep'\nelse:\n    prediction = 'Goat'\n\nprint(prediction)","79b63b29":"<h1 style='background-color:red; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;' > Goat_Sheep Detection with VGG19 <\/h1>\n\n#### What Vgg 19\n\nVGG-19 is a convolutional neural network that is 19 layers deep. You can load a pretrained version of the network trained on more than a million images from the ImageNet database . The pretrained network can classify images into 1000 object categories, such as keyboard, mouse, pencil, and many animals.\n\n\n\n\n\n<img src=\"https:\/\/static-01.hindawi.com\/articles\/sv\/volume-2020\/8863388\/figures\/8863388.fig.001.svgz\" width=\"800px\">\n\n\n\n\n#### Dataset Link \n[Here](https:\/\/www.kaggle.com\/khotijahs1\/sheep-goat\/code)"}}