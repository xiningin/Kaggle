{"cell_type":{"b3ccf46e":"code","fff91f4e":"code","5abd8923":"code","8c7c69cb":"code","b8600cd6":"code","b09f2600":"code","925ab363":"code","25c10a06":"code","0cdad743":"code","7dfb3d12":"code","67cd3fdb":"code","fad9cd36":"code","74ff5631":"code","095a04f3":"code","945edf65":"markdown"},"source":{"b3ccf46e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fff91f4e":"import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n# from keras.preprocessing.image import img_to_array\n# from keras.preprocessing.image import array_to_img\n# from sklearn.model_selection import train_test_split\n# from PIL import Image\n# import scipy\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.losses import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.preprocessing.image import *\nfrom tensorflow.keras.utils import *\n# import pydot\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import *\nimport tensorflow.keras.backend as K\n\n# from tqdm import tqdm, tqdm_notebook\n# from colorama import Fore\n# import json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom glob import glob\nfrom skimage.io import *\n%config Completer.use_jedi = False\n# import time\n# from sklearn.decomposition import PCA\n# from sklearn.svm import LinearSVC\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.metrics import accuracy_score\n# import lightgbm as lgb\n# import xgboost as xgb\n# !pip install livelossplot\n# import livelossplot\n# from livelossplot import PlotLossesKeras\nimport warnings\nwarnings.filterwarnings('ignore')\nprint(\"All modules have been imported\")","5abd8923":"import itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize = (6,6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    cm = np.round(cm,2)\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","8c7c69cb":"info=pd.read_csv(\"..\/input\/prepossessed-arrays-of-binary-data\/1000_Binary Dataframe\")\ninfo=info.drop('Unnamed: 0',axis=1)\ninfo.head()","b8600cd6":"sns.set_style('darkgrid')\nfig, ax = plt.subplots(figsize=(10,5))\nsns.barplot(x=info.level.unique(),y=info.level.value_counts(),palette='Blues_r',ax=ax)","b09f2600":"sizes = info['level'].values\nsns.distplot(sizes, kde=False)","925ab363":"Binary_90 = np.load('..\/input\/prepossessed-arrays-of-binary-data\/1000_Binary_images_data_90.npz')\nX_90=Binary_90['a']\nBinary_128 = np.load('..\/input\/prepossessed-arrays-of-binary-data\/1000_Binary_images_data_128.npz')\nX_128=Binary_128['a']\nBinary_264 = np.load('..\/input\/prepossessed-arrays-of-binary-data\/1000_Binary_images_data_264.npz')\nX_264=Binary_264['a']\ny=info['level'].values\n\n\nprint(X_90.shape)\nprint(X_128.shape)\nprint(X_264.shape)\nprint(y.shape)","25c10a06":"print(\"Shape before reshaping X_90\" +str(X_90.shape))\nX_90=X_90.reshape(1000,90,90,3)\nprint(\"Shape after reshaping X_90\" +str(X_90.shape))\nprint(\"\\n\\n\")\n\nprint(\"Shape before reshaping X_128\" +str(X_128.shape))\nX_128=X_128.reshape(1000,128,128,3)\nprint(\"Shape after reshaping X_128\" +str(X_128.shape))\nprint(\"\\n\\n\")\n\nprint(\"Shape before reshaping X_264\" +str(X_264.shape))\nX_264=X_264.reshape(1000,264,264,3)\nprint(\"Shape after reshaping X_264\" +str(X_264.shape))","0cdad743":"plt.title(\"90*90*3 Image\")\nplt.imshow(X_90[1])\nplt.show()\n\nplt.title(\"128*128*3 Image\")\nplt.imshow(X_128[1])\nplt.show()\n\nplt.title(\"264*264*3 Image\")\nplt.imshow(X_264[1])\nplt.show()","7dfb3d12":"y.shape","67cd3fdb":"X=np.array(X_264)\nY=np.array(y)\nY=to_categorical(Y,5)\nx_train, x_test1, y_train, y_test1 = train_test_split(X, Y, test_size=0.4, random_state=42)\nx_val, x_test, y_val, y_test = train_test_split(x_test1, y_test1, test_size=0.5, random_state=42)\nprint(len(x_train),len(x_val),len(x_test))","fad9cd36":"# Initializing a Sequential model\nmodel = Sequential()\nmodel.add(ResNet101(input_shape=(264,264,3),include_top=True,weights=None))\nmodel.add(Flatten())\nmodel.add(BatchNormalization())\nmodel.add(Dense(64,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(256,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n# Creating an output layer\nmodel.add(Dense(units= 5, activation='softmax'))\n\nc3=tf.keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.1,\n    patience=2,\n    mode=\"auto\",\n    min_delta=0.0001,\n    cooldown=0,\n    min_lr=0.001\n)\nmodel.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy','AUC'])\nhistory=model.fit(x_train,y_train,epochs=20,batch_size=16,validation_split=0.2)","74ff5631":"y_test=np.argmax(y_test, axis=1)\npred=np.argmax(model.predict(x_test),axis=-1)\ncm=confusion_matrix(y_test,pred)\ncm_plot=plot_confusion_matrix(cm,classes=['0','1'])","095a04f3":"print(\"Performance Report:\")\ny_pred6=np.argmax(model.predict(x_test),axis=-1)\nY_test=to_categorical(y_test,5)\ny_pred_prb6=model.predict(x_test)\ntarget=['0','1']\nfrom sklearn import metrics\nprint('Accuracy score is :', metrics.accuracy_score(y_test, y_pred6))\nprint('Precision score is :', metrics.precision_score(y_test, y_pred6, average='weighted'))\nprint('Recall score is :',metrics.recall_score(y_test,y_pred6, average='weighted'))\nprint('F1 Score is :', metrics.f1_score(y_test, y_pred6,average='weighted'))\nprint('Cohen Kappa Score:', metrics.cohen_kappa_score(y_test, y_pred6))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test,pred,target_names=target))","945edf65":"# ResNet101 (5 Dense Layer)"}}