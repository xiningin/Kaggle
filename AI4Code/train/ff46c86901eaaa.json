{"cell_type":{"d988e701":"code","ec547c57":"code","5da0e537":"code","7a9b2497":"code","2e17ce75":"code","6f1a8aa5":"code","dceffdd1":"code","1fca72f0":"code","70b616c3":"code","3f2901b9":"code","8583516b":"code","3eff393b":"code","b92e663c":"code","7cee60d3":"code","e97c5709":"code","9c2ff700":"code","e36275e1":"code","ba3188c7":"code","c4cc963d":"code","3dd9f849":"code","4aa051cc":"code","0e55e259":"code","93380051":"code","c17b806e":"code","3d9b2d16":"code","da7ce192":"code","de27dd30":"code","942e1374":"code","d2b8e56c":"code","68b30a6f":"code","0bfb0005":"code","0339a864":"code","1ea1c58a":"code","8edd2e9f":"code","14f7a0b1":"code","413a1ae7":"code","14103f18":"code","af3cd373":"code","c59e83f5":"code","ee6f7689":"code","27656e8e":"code","1c014b80":"code","4b57ebc6":"code","18f7596c":"code","6181d5b9":"code","6034dd85":"code","4b68346b":"code","4efadf32":"code","28763a38":"code","72bda3e8":"code","5e63006d":"code","c89fd652":"code","109f4a42":"code","1c33dfb5":"code","2e52c681":"code","fc053a6b":"code","a0ec671c":"code","73ab8152":"code","fabdd93f":"code","3fbde302":"code","9d8d6e72":"code","0512b9d4":"code","2213cea2":"code","83a863ce":"code","8ad4b029":"code","ff17b401":"code","600a75df":"code","9598f6e5":"code","9f227f28":"code","dd9b7243":"code","af5acaf3":"code","cc84ed09":"code","82ff3e14":"code","4cba8a0a":"code","4d95e735":"code","dee40027":"code","7d28d3cb":"code","85bb7b01":"code","fcde9641":"code","6980fc8e":"markdown","c3f6f784":"markdown","d731cbda":"markdown","a76ff360":"markdown","e98a35d5":"markdown","4ff4f470":"markdown","a43fb063":"markdown","d17d1b20":"markdown","d066089b":"markdown","af34a721":"markdown","05701500":"markdown","0d644c07":"markdown","d6a325a7":"markdown","d449480c":"markdown","be099f43":"markdown","c4f35212":"markdown","2928df65":"markdown","bd1ab22c":"markdown","076c357b":"markdown","a907d32e":"markdown","ac248bd4":"markdown","224f9703":"markdown","dbc38686":"markdown","de969d7e":"markdown","9b99eadd":"markdown","5e586a64":"markdown","3d8d2df3":"markdown","119c3290":"markdown","f2d603fa":"markdown","5d0d7a12":"markdown","576f6bc9":"markdown","e19a0f6b":"markdown","a53fc84f":"markdown","11f9e80f":"markdown","01406847":"markdown","2ca08b25":"markdown","cf2625f7":"markdown","278fdfd8":"markdown","50d354e6":"markdown","0bbb13d6":"markdown","217774db":"markdown","724c0cfd":"markdown","a2f2e54b":"markdown","35b6f4c9":"markdown","37281bef":"markdown","aa9a60fc":"markdown","f9ee2bf2":"markdown","da1de94c":"markdown","843694d8":"markdown","d2aa1666":"markdown","32cbd914":"markdown","e22c0e50":"markdown","475ed84b":"markdown","83af3cd3":"markdown","7717a078":"markdown","07a487e3":"markdown"},"source":{"d988e701":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.color_palette(\"crest\", as_cmap=True)\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ec547c57":"data = pd.read_csv(\"..\/input\/buildingdatasetplus\/building-energy-cleaned (2).csv\")\ndata.head(5)","5da0e537":"data.info()\n","7a9b2497":"data.describe()","2e17ce75":"# Regardons les valeurs infinies et corrigeons les\ndata[(data.GFAPerBuilding == np.inf) | (data.GFAPerFloor == np.inf)].head()","6f1a8aa5":"data[(data.TotalGHGEmissions == np.inf) | (data.TotalGHGEmissions == np.inf)].head()","dceffdd1":"data['GFAPerBuilding'] = np.where(((data.GFAPerBuilding == np.inf) & (data.NumberofBuildings == 0)),0, data.GFAPerBuilding)\ndata['GFAPerFloor'] = np.where(((data.GFAPerFloor == np.inf) & (data.NumberofFloors == 0)),0, data.GFAPerFloor)","1fca72f0":"data.describe()","70b616c3":"font_title = {'family': 'serif',\n              'color':  '#1d479b',\n              'weight': 'bold',\n              'size': 18,\n             }\nfig = plt.figure(figsize=(12,8))\nsns.scatterplot(data = data, x='PropertyGFATotal', y='TotalGHGEmissions', hue='BuildingType')\nplt.title(f\"Emission de CO2  par surface totale au sol et par type de b\u00e2timent\\n\", \n          fontdict=font_title, fontsize=16)\n\nfig = plt.figure(figsize=(12,8))\nsns.scatterplot(data = data, x='PropertyGFATotal', y='SiteEnergyUse(kBtu)', hue='BuildingType')\nplt.title(f\"Consommations d'\u00e9nergie par surface totale au sol et par type de b\u00e2timent\\n\", \n          fontdict=font_title, fontsize=16)\n\nfig = plt.figure(figsize=(12,8))\nsns.scatterplot(data = data, x='PropertyGFATotal', y='ENERGYSTARScore', hue='BuildingType')\nplt.title(f\"Rapport de ENERGY STAR SCORE par surface totale au sol et par type de b\u00e2timent\\n\", \n          fontdict=font_title, fontsize=16)\nplt.show()","3f2901b9":"data.columns","8583516b":"numerical_features =  [  'NumberofBuildings', 'NumberofFloors',\n       'PropertyGFATotal', 'ENERGYSTARScore', 'SiteEnergyUse(kBtu)',\n       'SteamUse(kBtu)', 'Electricity(kBtu)', 'NaturalGas(kBtu)',\n       'TotalGHGEmissions', 'TotalUseTypeNumber', 'GFABuildingRate',\n       'GFAParkingRate', 'GFAPerBuilding', 'GFAPerFloor', 'BuildingAge',\n       'harvesine_distance']","3eff393b":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndegrees = 90\n\nfig = plt.figure(figsize=(15, 15))\nax = plt.axes()\nplt.xticks(rotation=degrees)\nax.set_ylim([-1e1,2e7])\nsns.boxplot(data=data[numerical_features]).set_title('quantitative boxplot');","b92e663c":"for feature in numerical_features:\n    plt.figure(figsize=(12,6));\n    sns.boxplot(x=data[feature]).set_title(feature);","7cee60d3":"data[data['SiteEnergyUse(kBtu)']>2*10**7]","e97c5709":"data.head(5)","9c2ff700":"data = data[data['SiteEnergyUse(kBtu)']<2*10**7]","e36275e1":"numerical_features = data.select_dtypes(include=['int64','float64'])\ncategorical_features = data.select_dtypes(exclude=['int64','float64']) ","ba3188c7":"numerical_features.info()","c4cc963d":"energystar_score = numerical_features['ENERGYSTARScore']\nnumerical_features = numerical_features.drop(['SteamUse(kBtu)','Electricity(kBtu)',\n                         'NaturalGas(kBtu)','ENERGYSTARScore'], axis=1)","3dd9f849":"['OSEBuildingID','ZipCode']\n\n\nnumerical_features = numerical_features.drop(['OSEBuildingID','ZipCode'], axis=1)","4aa051cc":"categorical_features.info()","0e55e259":"categorical_features = categorical_features.drop( ['PropertyName', 'Address'],axis=1 )","93380051":"categorical_features.nunique()","c17b806e":"list(numerical_features.columns)","3d9b2d16":"data_mix = pd.concat([categorical_features, numerical_features], axis=1)\ndata_mix","da7ce192":"from sklearn.preprocessing import StandardScaler, OrdinalEncoder, RobustScaler\nfrom category_encoders.target_encoder import TargetEncoder\nfrom sklearn.compose import ColumnTransformer\n\ntarget_features = ['BuildingType','PrimaryPropertyType','Neighborhood','LargestPropertyUseType']\ntarget_transformer = TargetEncoder()\n\nnumeric_features = ['harvesine_distance','NumberofBuildings','NumberofFloors',\n                    'PropertyGFATotal','BuildingAge','TotalUseTypeNumber',\n                    'GFABuildingRate','GFAParkingRate','GFAPerBuilding','GFAPerFloor']\nnumeric_transformer = RobustScaler(with_centering=True,with_scaling=True,quantile_range=(25.0, 75.0),\n                                          copy=True,)\n\npreprocessor = ColumnTransformer(transformers=[\n    ('target', target_transformer, target_features),\n    ('numeric', numeric_transformer, numeric_features)\n])","de27dd30":"import warnings\ndef get_feature_names(column_transformer):\n    \"\"\"Get feature names from all transformers.\n    Returns\n    -------\n    feature_names : list of strings\n        Names of the features produced by transform.\n        \n    ------\n    Code from :\n        https:\/\/johaupt.github.io\/\n    \"\"\"\n    # Remove the internal helper function\n    #check_is_fitted(column_transformer)\n    \n    # Turn loopkup into function for better handling with pipeline later\n    def get_names(trans):\n        # >> Original get_feature_names() method\n        if trans == 'drop' or (\n                hasattr(column, '__len__') and not len(column)):\n            return []\n        if trans == 'passthrough':\n            if hasattr(column_transformer, '_df_columns'):\n                if ((not isinstance(column, slice))\n                        and all(isinstance(col, str) for col in column)):\n                    return column\n                else:\n                    return column_transformer._df_columns[column]\n            else:\n                indices = np.arange(column_transformer._n_features)\n                return ['x%d' % i for i in indices[column]]\n        if not hasattr(trans, 'get_feature_names'):\n        # >>> Change: Return input column names if no method avaiable\n            # Turn error into a warning\n            warnings.warn(\"Transformer %s (type %s) does not \"\n                                 \"provide get_feature_names. \"\n                                 \"Will return input column names if available\"\n                                 % (str(name), type(trans).__name__))\n            # For transformers without a get_features_names method, use the input\n            # names to the column transformer\n            if column is None:\n                return []\n            else:\n                return [f for f in column]\n\n        return [f for f in trans.get_feature_names()]\n    \n    ### Start of processing\n    feature_names = []\n    \n    l_transformers = list(column_transformer._iter(fitted=True))\n    \n    \n    for name, trans, column, _ in l_transformers: \n        feature_names.extend(get_names(trans))\n    \n    return feature_names\n","942e1374":"data_mix = data_mix.replace([-np.inf,np.inf], np.nan)","d2b8e56c":"data_mix = data_mix .dropna(how=\"all\")","68b30a6f":" data_mix.dropna(inplace = True)","0bfb0005":"import numpy as np\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.model_selection import train_test_split\n\nX = data_mix.drop(['TotalGHGEmissions','SiteEnergyUse(kBtu)'], axis=1)\nY = data_mix[['TotalGHGEmissions','SiteEnergyUse(kBtu)']]\nimp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nimp.fit(X , Y)\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n\nprint(\"Entrainement: {} lignes,\\nTest: {} lignes.\\n\".format(X_train.shape[0],\n                                                            X_test.shape[0]))","0339a864":"from sklearn.preprocessing import FunctionTransformer\n\nlogtransformer = FunctionTransformer(np.log, inverse_func = np.exp, check_inverse = True)\nY_log = logtransformer.transform(Y)\n\nfig, axes = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False, figsize=(20,8))\nsns.histplot(data=Y, x='TotalGHGEmissions', stat=\"density\", ax=axes[0])\naxes[0].set_title(\"Donn\u00e9es initiales\", color='#2cb7b0')\nsns.histplot(data=Y_log, x='TotalGHGEmissions', stat=\"density\", ax=axes[1])\naxes[1].set_title(\"Application du logarithme\", color='#2cb7b0')\nplt.suptitle(\"Distribution des emissions de CO2 avec changement d'\u00e9chelle\", fontdict=font_title, fontsize=22)\nplt.show()","1ea1c58a":"from sklearn.compose import TransformedTargetRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import set_config\nset_config(display='diagram')\n\n\nparam_mlr = {\"regressor__fit_intercept\": [True, False],\n             \"regressor__normalize\": [True, False]}\n\nmlr_grid_cv = Pipeline([\n    ('preprocessor', preprocessor),\n    ('grid_search_mlr', GridSearchCV(\n                            TransformedTargetRegressor(\n                                regressor=LinearRegression(), \n                                func=np.log, \n                                inverse_func=np.exp),\n                            param_grid=param_mlr,\n                            cv=5,\n                            scoring=('r2','neg_mean_absolute_error'),\n                            return_train_score = True,\n                            refit='neg_mean_absolute_error',\n                            n_jobs = -1))])","8edd2e9f":"#Retour des meilleurs scores NMAE et R2\n#Stockage du dataframe de resultats du mod\u00e8le\ndef model_scores(pip,step):\n    df_results = pd.DataFrame.from_dict(pip.named_steps[step].cv_results_) \\\n                    .sort_values('rank_test_neg_mean_absolute_error')\n    best_nmae = pip.named_steps[step].best_score_\n    best_r2 = np.mean(df_results[df_results.rank_test_r2 == 1]['mean_test_r2'])\n    best_params = pip.named_steps[step].best_params_\n    training_time = round((np.mean(df_results.mean_fit_time)*X_train.shape[0]),2)\n    print(\"Meilleur score MAE : {}\\nMeilleur Score R2 : {}\\nMeilleurs param\u00e8tres : {}\\nTemps moyen d'entrainement : {}s\"\\\n         .format(round(best_nmae,3), round(best_r2,3), best_params, training_time))\n    return df_results\n","14f7a0b1":"#Entrainement sur les 2 variables \u00e0 expliquer :\nGHG_mlr_model = mlr_grid_cv.fit(X_train, Y_train['TotalGHGEmissions'])\nGHG_mlr_results = model_scores(GHG_mlr_model, 'grid_search_mlr')","413a1ae7":"SEU_mlr_model = mlr_grid_cv.fit(X_train, Y_train['SiteEnergyUse(kBtu)'])\nSEU_mlr_results = model_scores(SEU_mlr_model, 'grid_search_mlr')","14103f18":"GHG_mlr_results","af3cd373":"SEU_mlr_model","c59e83f5":"from sklearn.linear_model import ElasticNet\n\nparam_eNet = {\"regressor__max_iter\": [10, 100, 1000],\n              \"regressor__alpha\": np.logspace(-4, 0, num=5),\n              \"regressor__l1_ratio\": np.arange(0.0, 1.1, 0.1)}\n\neNet_grid_cv = Pipeline([\n    ('preprocessor', preprocessor),\n    ('grid_search_enet', GridSearchCV(\n                            TransformedTargetRegressor(\n                                regressor=ElasticNet(), \n                                func=np.log, \n                                inverse_func=np.exp),\n                            param_grid=param_eNet,\n                            cv=5,\n                            scoring=('r2','neg_mean_absolute_error'),\n                            return_train_score = True,\n                            refit='neg_mean_absolute_error',\n                            n_jobs = -1))])\n","ee6f7689":"GHG_eNet_model = eNet_grid_cv.fit(X_train, Y_train['TotalGHGEmissions'])\nGHG_eNet_results = model_scores(GHG_eNet_model, 'grid_search_enet')","27656e8e":"SEU_eNet_model = eNet_grid_cv.fit(X_train, Y_train['SiteEnergyUse(kBtu)'])\nSEU_eNet_results = model_scores(SEU_eNet_model, 'grid_search_enet')","1c014b80":"from sklearn.svm import LinearSVR\n\nparam_svr = {'regressor__C' : np.logspace(-4, 0, 5),\n             'regressor__epsilon' : [0, 0.01, 0.1, 0.5, 1, 2],\n             'regressor__loss' : [\"epsilon_insensitive\",\"squared_epsilon_insensitive\"],\n             'regressor__max_iter': [10, 100, 1000]}\n\nsvr_grid_cv = Pipeline([\n    ('preprocessor', preprocessor),\n    ('grid_search_svr', GridSearchCV(\n                            TransformedTargetRegressor(\n                                regressor=LinearSVR(), \n                                func=np.log, \n                                inverse_func=np.exp),\n                            param_grid=param_svr,\n                            cv=5,\n                            scoring=('r2','neg_mean_absolute_error'),\n                            refit='neg_mean_absolute_error',\n                            return_train_score = True,\n                            n_jobs = -1))])","4b57ebc6":"GHG_svr_model = svr_grid_cv.fit(X_train, Y_train['TotalGHGEmissions'])\nGHG_svr_results = model_scores(GHG_svr_model, 'grid_search_svr')","18f7596c":"SEU_svr_model = svr_grid_cv.fit(X_train, Y_train['SiteEnergyUse(kBtu)'])\nSEU_svr_results = model_scores(SEU_svr_model, 'grid_search_svr')","6181d5b9":"from sklearn.ensemble import RandomForestRegressor\n\nparam_rfr = {'regressor__max_features' : ['sqrt', 'log2'],\n             'regressor__max_depth': [5, 15, 25, 50],\n             'regressor__min_samples_split': [2, 5, 10],\n             'regressor__bootstrap' : [True, False],\n             'regressor__min_samples_leaf': [1,2,5,10]}\n\nrfr_grid_cv = Pipeline([\n    ('preprocessor', preprocessor),\n    ('grid_search_rfr', GridSearchCV(\n                            TransformedTargetRegressor(\n                                regressor=RandomForestRegressor(), \n                                func=np.log, \n                                inverse_func=np.exp),\n                            param_grid=param_rfr,\n                            cv=5,\n                            scoring=('r2','neg_mean_absolute_error'),\n                            refit='neg_mean_absolute_error',\n                            return_train_score = True,\n                            n_jobs = -1))])\n","6034dd85":"GHG_rfr_model = rfr_grid_cv.fit(X_train, Y_train['TotalGHGEmissions'])\nGHG_rfr_results = model_scores(GHG_rfr_model, 'grid_search_rfr')","4b68346b":"SEU_rfr_model = rfr_grid_cv.fit(X_train, Y_train['SiteEnergyUse(kBtu)'])\nSEU_rfr_results = model_scores(SEU_rfr_model, 'grid_search_rfr')\n","4efadf32":"import xgboost as xgb\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_xgb = {'regressor__learning_rate' : [ 0.01,0.001,0.1,0.2,0.3],\n             'regressor__gamma': [0,0.01,0.001,0.1,0.25,0.5],\n             'regressor__max_depth': [3,6,9,12,15],\n             'regressor__min_child_weight' : [ 1,2,3,5,7,9],\n             'regressor__n_estimators': [ 50,100,125,150]}\n\nxgb_grid_cv = Pipeline([\n    ('preprocessor', preprocessor),\n    ('grid_search_xgb', RandomizedSearchCV(\n                            TransformedTargetRegressor(\n                                regressor=xgb.XGBRegressor(tree_method='gpu_hist'), \n                                func=np.log, \n                                inverse_func=np.exp),\n                            param_distributions=param_xgb,\n                            n_iter=20,\n                            cv=5,\n                            scoring=('r2','neg_mean_absolute_error'),\n                            refit='neg_mean_absolute_error',\n                            return_train_score = True,\n                            n_jobs = -1))])","28763a38":"GHG_xgb_model = xgb_grid_cv.fit(X_train, Y_train['TotalGHGEmissions'])\nGHG_xgb_results = model_scores(GHG_xgb_model, 'grid_search_xgb')","72bda3e8":"SEU_xgb_model = xgb_grid_cv.fit(X_train, Y_train['SiteEnergyUse(kBtu)'])\nSEU_xgb_results = model_scores(SEU_xgb_model, 'grid_search_xgb')","5e63006d":"metrics = ['mean_fit_time', 'mean_score_time',\n           'mean_test_neg_mean_absolute_error',\n           'mean_train_neg_mean_absolute_error']\nGHG_compare_metrics = pd.concat([pd.DataFrame(GHG_rfr_results[metrics].mean(), columns=['RandomForest']),\n           pd.DataFrame(GHG_xgb_results[metrics].mean(), columns=['XGBoost']),\n           pd.DataFrame(GHG_svr_results[metrics].mean(), columns=['LinearSVR']),\n           pd.DataFrame(GHG_eNet_results[metrics].mean(), columns=['ElasticNet']),\n           pd.DataFrame(GHG_mlr_results[metrics].mean(), columns=['LinearRegression'])\n          ], axis=1)\nGHG_final_metrics_compare = pd.DataFrame(columns=metrics, \n                                     index=['RandomForest','XGBoost',\n                                            'LinearSVR','ElasticNet',\n                                            'LinearRegression'])\nfor m in metrics:\n    GHG_final_metrics_compare[m] = GHG_compare_metrics.loc[m]","c89fd652":"GHG_final_metrics_compare","109f4a42":"#On \u00e9limine le mod\u00e8le SVR de cette repr\u00e9sentation car hors normes\nGHG_final_metrics_compare = GHG_final_metrics_compare[GHG_final_metrics_compare.index != 'LinearSVR']\nx = np.arange(len(GHG_final_metrics_compare.index))\nwidth = 0.35\n\nfig, ax = plt.subplots(1,2,figsize=(20,8), sharey=False, sharex=False)\n\nscores1 = ax[0].bar(x - width\/2, -1*GHG_final_metrics_compare['mean_test_neg_mean_absolute_error'], width, label='Test')\nscores2 = ax[0].bar(x + width\/2, -1*GHG_final_metrics_compare['mean_train_neg_mean_absolute_error'], width, label='Train')\nax[0].set_ylabel('Mean Absolute Error')\nax[0].set_title('Comparaison des scores par mod\u00e8le')\nax[0].set_xticks(x)\nax[0].set_xticklabels(GHG_final_metrics_compare.index)\nax[0].legend()\nax[0].bar_label(scores1, padding=3)\nax[0].bar_label(scores2, padding=3)\n\ntimes1 = ax[1].bar(x - width\/2, GHG_final_metrics_compare['mean_score_time'], width, label='Predict')\ntimes2 = ax[1].bar(x + width\/2, GHG_final_metrics_compare['mean_fit_time'], width, label='Fit')\nax[1].set_ylabel('Temps(s)')\nax[1].set_title(\"Comparaison des temps d'entrainement et pr\u00e9diction\")\nax[1].set_xticks(x)\nax[1].set_xticklabels(GHG_final_metrics_compare.index)\nax[1].legend()\nax[1].bar_label(times1, padding=3, fmt='%.3f')\nax[1].bar_label(times2, padding=3, fmt='%.3f')\n\nplt.suptitle(\"Mod\u00e9lisations sur la variable TotalGHGEmissions\", fontdict=font_title, fontsize=22)\nfig.tight_layout()\n\nplt.show()","1c33dfb5":"#Fonction d'affichage des scores de GridSearch pour chacun des param\u00e8tres\ndef plot_search_results(grid, title): \n       \n    ## R\u00e9sultats de la GridSearch\n    results = grid.cv_results_\n    means_test = results['mean_test_neg_mean_absolute_error']\n    stds_test = results['std_test_neg_mean_absolute_error']\n    means_train = results['mean_train_neg_mean_absolute_error']\n    stds_train = results['std_train_neg_mean_absolute_error']\n\n    ## Index de valeurs par hyper-param\u00e8tre\n    masks=[]\n    masks_names= list(grid.best_params_.keys())\n    for p_k, p_v in grid.best_params_.items():\n        masks.append(list(results['param_'+p_k].data==p_v))\n\n    params=grid.param_grid\n\n    \n    ## Plot des r\u00e9sultats\n    fig, ax = plt.subplots(1,len(params),sharex='none', sharey='all',figsize=(20,5))\n    fig.suptitle('Scores par param\u00e8tres pour la variable {}'.format(title), \n                 fontdict=font_title, fontsize=22)\n    fig.text(0.04, 0.5, 'NEG MEAN ABSOLUTE ERROR SCORE', va='center', rotation='vertical')\n    pram_preformace_in_best = {}\n    for i, p in enumerate(masks_names):\n        m = np.stack(masks[:i] + masks[i+1:])\n        pram_preformace_in_best\n        best_parms_mask = m.all(axis=0)\n        best_index = np.where(best_parms_mask)[0]\n        x = np.array(params[p])\n        y_1 = np.array(means_test[best_index])\n        e_1 = np.array(stds_test[best_index])\n        y_2 = np.array(means_train[best_index])\n        e_2 = np.array(stds_train[best_index])\n        ax[i].errorbar(x, y_1, e_1, linestyle='--', marker='o', label='test', color=\"#2cb7b0\")\n        ax[i].errorbar(x, y_2, e_2, linestyle='--', marker='o', label='train', color=\"#337da4\")\n        ax[i].set_xlabel(p.upper())\n\n    plt.legend()\n    plt.show()\n    \n    print(\"\\nRappel des meilleurs param\u00e8tres :\\n{}\".format(grid.best_params_))","2e52c681":"plot_search_results(GHG_rfr_model.named_steps['grid_search_rfr'], title=\"TotalGHGEmissions\")","fc053a6b":"feature_importance = GHG_rfr_model.named_steps['grid_search_rfr'].best_estimator_.regressor_.feature_importances_ \nfeatures_names = get_feature_names(GHG_rfr_model.named_steps['preprocessor'])\nstd = np.std([\n    tree.feature_importances_ for tree in GHG_rfr_model.named_steps['grid_search_rfr'].best_estimator_.regressor_], axis=0)\ndf_feature_importance = pd.Series(feature_importance, index=features_names)\n\nfig, ax = plt.subplots(figsize=(12,8))\ndf_feature_importance.plot.bar(yerr=std, ax=ax)\nax.set_title(\"Feature importances du mod\u00e8le RandomForestRegressor sur les \u00e9missions de CO2\", fontdict=font_title)\nax.set_ylabel(\"Diminution moyenne des impuret\u00e9s\")\nfig.tight_layout()","a0ec671c":"SEU_compare_metrics = pd.concat([pd.DataFrame(SEU_rfr_results[metrics].mean(), columns=['RandomForest']),\n           pd.DataFrame(SEU_xgb_results[metrics].mean(), columns=['XGBoost']),\n           pd.DataFrame(SEU_svr_results[metrics].mean(), columns=['LinearSVR']),\n           pd.DataFrame(SEU_eNet_results[metrics].mean(), columns=['ElasticNet']),\n           pd.DataFrame(SEU_mlr_results[metrics].mean(), columns=['LinearRegression'])\n          ], axis=1)\nSEU_final_metrics_compare = pd.DataFrame(columns=metrics, \n                                     index=['RandomForest','XGBoost',\n                                            'LinearSVR','ElasticNet',\n                                            'LinearRegression'])\nfor m in metrics:\n    SEU_final_metrics_compare[m] = SEU_compare_metrics.loc[m]\nSEU_compare_metrics","73ab8152":"SEU_final_metrics_compare = SEU_final_metrics_compare[SEU_final_metrics_compare.index != 'LinearRegression']\nx = np.arange(len(SEU_final_metrics_compare.index))\nwidth = 0.35\n\nfig, ax = plt.subplots(1,2,figsize=(20,8), sharey=False, sharex=False)\n\nscores1 = ax[0].bar(x - width\/2, -1*SEU_final_metrics_compare['mean_test_neg_mean_absolute_error'], width, label='Test')\nscores2 = ax[0].bar(x + width\/2, -1*SEU_final_metrics_compare['mean_train_neg_mean_absolute_error'], width, label='Train')\nax[0].set_ylabel('Mean Absolute Error')\nax[0].set_title('Comparaison des scores par mod\u00e8le')\nax[0].set_xticks(x)\nax[0].set_xticklabels(SEU_final_metrics_compare.index)\nax[0].legend()\nax[0].bar_label(scores1, padding=3)\nax[0].bar_label(scores2, padding=3)\n\ntimes1 = ax[1].bar(x - width\/2, SEU_final_metrics_compare['mean_score_time'], width, label='Predict')\ntimes2 = ax[1].bar(x + width\/2, SEU_final_metrics_compare['mean_fit_time'], width, label='Fit')\nax[1].set_ylabel('Temps(s)')\nax[1].set_title(\"Comparaison des temps d'entrainement et pr\u00e9diction\")\nax[1].set_xticks(x)\nax[1].set_xticklabels(SEU_final_metrics_compare.index)\nax[1].legend()\nax[1].bar_label(times1, padding=3, fmt='%.3f')\nax[1].bar_label(times2, padding=3, fmt='%.3f')\n\nplt.suptitle(\"Mod\u00e9lisations sur la variable SiteEnergyUse\", fontdict=font_title, fontsize=22)\nfig.tight_layout()\n\nplt.show()\n","fabdd93f":"plot_search_results(SEU_rfr_model.named_steps['grid_search_rfr'], title=\"SiteEnergyUse(kBtu)\")","3fbde302":"feature_importance = SEU_rfr_model.named_steps['grid_search_rfr'].best_estimator_.regressor_.feature_importances_ \nfeatures_names = get_feature_names(SEU_rfr_model.named_steps['preprocessor'])\nstd = np.std([\n    tree.feature_importances_ for tree in SEU_rfr_model.named_steps['grid_search_rfr'].best_estimator_.regressor_], axis=0)\ndf_feature_importance = pd.Series(feature_importance, index=features_names)\n\nfig, ax = plt.subplots(figsize=(12,8))\ndf_feature_importance.plot.bar(yerr=std, ax=ax)\nax.set_title(\"Feature importances du mod\u00e8le RandomForestRegressor sur la consommation d'\u00e9nergie\")\nax.set_ylabel(\"Diminution moyenne des impuret\u00e9s\")\nfig.tight_layout()","9d8d6e72":"def metrics_model(y_true, y_pred):\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    diff = y_true - y_pred\n    mae = np.mean(abs(diff))\n    r2 = 1-(sum(diff**2)\/sum((y_true-np.mean(y_true))**2))\n    dict_metrics = {\"M\u00e9trique\":[\"MAE\", \"R\u00b2\"], \"R\u00e9sultats\":[mae, r2]}\n    df_metrics = pd.DataFrame(dict_metrics)\n    return df_metrics","0512b9d4":"def plot_pred_true(y_true, y_pred, color=None, title=None):\n    X_plot = [y_true.min(), y_true.max()]\n    fig = plt.figure(figsize=(12,8))\n    plt.scatter(y_true, y_pred, color=color, alpha=.6)\n    plt.plot(X_plot, X_plot, color='r')\n    plt.xlabel(\"Valeurs r\u00e9\u00e9lles\")\n    plt.ylabel(\"Valeurs pr\u00e9dites\")\n    plt.title(\"Valeurs pr\u00e9dites VS valeurs r\u00e9\u00e9lles | Variable {}\".format(title), \n              fontdict=font_title, fontsize=18)\n    plt.show()","2213cea2":"#Mod\u00e8le avec les meilleurs param\u00e8tres pour les \u00e9missions de CO2\n\nimport time\nstart_time = time.time()\n\nGHG_pred = GHG_rfr_model.predict(X_test)\n\nprint(\"Temps d'execution de l'agorithme : {:.2} s.\".format((time.time() - start_time)))","83a863ce":"#Calcul des m\u00e9triques pour les \u00e9missions de CO2\nGHGmetrics = metrics_model(Y_test['TotalGHGEmissions'],GHG_pred)\nGHGmetrics","8ad4b029":"#Affichage des valeurs pr\u00e9dites vs valeurs r\u00e9\u00e9lles pour \u00e9missions de CO2\nplot_pred_true(Y_test['TotalGHGEmissions'],GHG_pred, color=\"#9C3E2D\", title=\"TotalGHGEmissions\")","ff17b401":"GHG_test_results = GHG_rfr_results[['split0_test_neg_mean_absolute_error',\n                'split1_test_neg_mean_absolute_error',\n                'split2_test_neg_mean_absolute_error',\n                'split3_test_neg_mean_absolute_error',\n                'split4_test_neg_mean_absolute_error',\n                ]][GHG_rfr_results['rank_test_neg_mean_absolute_error']==1].values\nGHG_train_results = GHG_rfr_results[['split0_train_neg_mean_absolute_error',\n                'split1_train_neg_mean_absolute_error',\n                'split2_train_neg_mean_absolute_error',\n                'split3_train_neg_mean_absolute_error',\n                'split4_train_neg_mean_absolute_error',\n                ]][GHG_rfr_results['rank_test_neg_mean_absolute_error']==1].values","600a75df":"x = np.arange(0,5,1)\nfig, ax = plt.subplots(1,2, figsize=(20,8), sharey=False, sharex=False)\nax[0].plot(range(0,5), GHG_test_results.reshape(-1))\nax[0].set_xticks(x)\nax[0].set_xticklabels([\"Split\"+str(n) for n in range(0,5)])\nax[0].set_ylabel('Neg Mean Absolute Error')\nax[0].set_title('Scores de test')\n\nax[1].plot(range(0,5), GHG_train_results.reshape(-1), color='b')\nax[1].set_xticks(x)\nax[1].set_xticklabels([\"Split\"+str(n) for n in range(0,5)])\nax[1].set_title('Scores de train')\n\nplt.suptitle(\"Evolution des scores \u00e0 travers les Splits de Cross-validation\", fontdict=font_title, fontsize=22)\nfig.tight_layout()\n\nplt.show()","9598f6e5":"start_time = time.time()\n\nSEU_pred = SEU_rfr_model.predict(X_test)\n\nprint(\"Temps d'execution de l'agorithme : {:.2} s.\".format((time.time() - start_time)))","9f227f28":"#Calcul des m\u00e9triques pour les \u00e9missions de CO2\nSEUmetrics = metrics_model(Y_test['SiteEnergyUse(kBtu)'],SEU_pred)\nSEUmetrics","dd9b7243":"#Affichage des valeurs pr\u00e9dites vs valeurs r\u00e9\u00e9lles pour \u00e9missions de CO2\nplot_pred_true(Y_test['SiteEnergyUse(kBtu)'],SEU_pred, color=\"#6D9C0E\", title=\"SiteEnergyUse(kBtu)\")","af5acaf3":"final_SEU_test = pd.concat([X_test,Y_test],axis=1)\nfinal_SEU_test['SEU_pred'] = SEU_pred\ncompare_final_SEU_test = final_SEU_test = final_SEU_test.groupby(by='BuildingType').mean()","cc84ed09":"x = np.arange(len(compare_final_SEU_test.index))\nwidth = 0.35\n\nfig, ax = plt.subplots(figsize=(20,8), sharey=False, sharex=False)\n\nscores1 = ax.bar(x - width\/2, compare_final_SEU_test['SiteEnergyUse(kBtu)'], width, label='SiteEnergyUse(kBtu)')\nscores2 = ax.bar(x + width\/2, compare_final_SEU_test['SEU_pred'], width, label='Pr\u00e9dictions')\nax.set_ylabel('(kBtu)')\nax.set_xticks(x)\nax.set_xticklabels(compare_final_SEU_test.index)\nax.legend()\nax.bar_label(scores1, padding=3)\nax.bar_label(scores2, padding=3)\n\nplt.suptitle(\"Ecarts de pr\u00e9dictions sur la variable SiteEnergyUse(kBtu) par type de b\u00e2timent\", fontdict=font_title, fontsize=22)\nfig.tight_layout()\n\nplt.show()","82ff3e14":"#Ajout de la variable \u00e0 nos variables X\nX['energystar_score'] = energystar_score\n#Ajout de la variable dans les variables num\u00e9rique du preprocessor\nnumeric_features.append('energystar_score')","4cba8a0a":"#Filtrage des donn\u00e9es ayant un Energy Star Score renseign\u00e9\nX = X[X['energystar_score'].isnull()==False]\nY = Y[Y.index.isin(list(X.index))]","4d95e735":"fig, axes = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False, figsize=(20,8))\nsns.histplot(data=X, x='energystar_score', stat=\"density\", ax=axes[0])\naxes[0].set_title(\"Distribution\", color='#2cb7b0')\nsns.scatterplot(data=pd.concat([X,Y], axis=1), y='TotalGHGEmissions', x='energystar_score', ax=axes[1])\naxes[1].set_title(\"ENERGY STAR score en fonction de TotalGHGEmissions\", color='#2cb7b0')\nplt.suptitle(\"Analyse de la variable ENERGY STAR Score\", fontdict=font_title, fontsize=22)\nplt.show()","dee40027":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\nprint(\"Entrainement: {} lignes,\\nTest: {} lignes.\\n\".format(X_train.shape[0],\n                                                            X_test.shape[0]))","7d28d3cb":"rfr_grid_cv_eStar = Pipeline([\n    ('preprocessor', preprocessor),\n    ('grid_search_rfr_eStar', GridSearchCV(\n                            TransformedTargetRegressor(\n                                regressor=RandomForestRegressor(), \n                                func=np.log, \n                                inverse_func=np.exp),\n                            param_grid=param_rfr,\n                            cv=5,\n                            scoring=('r2','neg_mean_absolute_error'),\n                            refit='neg_mean_absolute_error',\n                            n_jobs = -1))])\n\nGHG_rfr_model_eStar = rfr_grid_cv_eStar.fit(X_train, Y_train['TotalGHGEmissions'])\nGHG_rfr_results_eStar = model_scores(GHG_rfr_model_eStar, 'grid_search_rfr_eStar')","85bb7b01":"GHG_pred_star = GHG_rfr_model_eStar.predict(X_test)\n#Calcul des m\u00e9triques pour les \u00e9missions de CO2\nGHGmetricsES = metrics_model(Y_test['TotalGHGEmissions'],GHG_pred_star)\nGHGmetrics = GHGmetrics.rename(columns={\"R\u00e9sultats\" : \"Sans ENERGY STAR\"})\nGHGmetrics['Avec ENERGY STAR'] = GHGmetricsES['R\u00e9sultats']\nGHGmetrics","fcde9641":"#Affichage des valeurs pr\u00e9dites vs valeurs r\u00e9\u00e9lles pour \u00e9missions de CO2\nplot_pred_true(Y_test['TotalGHGEmissions'],GHG_pred_star, color=\"#9C3E2D\", title=\"TotalGHGEmissions\")","6980fc8e":"#### Cette fois encore, nous allons comparer les m\u00e9triques obtenues sur les diff\u00e9rents mod\u00e8les :","c3f6f784":"#### Tous les r\u00e9sultats des GridSearchCV sont stock\u00e9s dans un DataFrame pour chaque variable \u00e0 pr\u00e9dire :","d731cbda":"### Pour la variable SiteEnergyUse, le mod\u00e8le RandomForest offre \u00e0 nouveau les meilleurs scores MAE et les meilleurs temps d'entrainement et de pr\u00e9diction. Nous allons donc s\u00e9l\u00e9ctionner le mod\u00e8le RandomForestRegressor pour pr\u00e9dire la variable SiteEnergyUse.\n\n### Regardons maintenant l'impact des hyperparam\u00e8tres de la Grille de recherche :","a76ff360":"### 5.1. Mod\u00e8le de pr\u00e9diction des \u00e9missions de CO2","e98a35d5":"## Nous pouvons nous passer des variables d'identifications,ainsi que l'ann\u00e9e des donn\u00e9es qui n'apportent rien aux pr\u00e9dictions","4ff4f470":"## Importons les donn\u00e9es nettoy\u00e9es","a43fb063":"#### Nous avons \u00e0 pr\u00e9sent nos m\u00e9triques de d\u00e9part obtenues avec notre mod\u00e8le de r\u00e9gression lin\u00e9aire multivari\u00e9 servant de baseline. Nous allons r\u00e9aliser nos premi\u00e8res mod\u00e9lisations en utilisant des mod\u00e8les lin\u00e9aires","d17d1b20":"# 3. Mod\u00e8le lin\u00e9aires : ElasticNet et SVR","d066089b":"#### Concernant nos variables \u00e0 pr\u00e9dire, nous allons regarder l'impact du passage \u00e0 l'\u00e9chelle logarithmique sur les distribution :","af34a721":"### 4.2. Mod\u00e8le XGBoost (eXtreme Gradient Boosting)","05701500":"### Le mod\u00e8le XGBoost non-lin\u00e9aire a des niveaux de performances de pr\u00e9diction sur les 2 variables quasi similaire au mod\u00e8le RandomForestRegressor mais nous avons d\u00fb utiliser le GPU pour acc\u00e9lerer le temps de calcul.","0d644c07":"### Sur ce mod\u00e8le SVR lin\u00e9aire, les m\u00e9triques sont encore l\u00e9g\u00e9rement am\u00e9lior\u00e9es pour les 2 variables \u00e0 pr\u00e9dire compar\u00e9es au mod\u00e8le ElasticNet.\n### Nous allons \u00e0 pr\u00e9sent nous pencher sur des mod\u00e8les non-lin\u00e9aires.\n","d6a325a7":"### Supprimons les variables d'identifications qui n'apportent rien aux pr\u00e9dictions","d449480c":"# 1. Preprocessing","be099f43":"### 4.1. Mod\u00e8le RandomForestRegressor","c4f35212":"### Avec la projection graphique ci-dessus, on constate que le mod\u00e8le RandomForestRegressor offre le meilleur compromis score \/ temps. Il est en effet meilleur en terme de score MAE et \u00e9galement bien plus rapide que le mod\u00e8le XGBoost.\n\n### Le mod\u00e9le retenu pour la mod\u00e9lisation de la variable TotalGHGEmissions est donc le mod\u00e8le RandomForestRegressor. Nous allons \u00e0 pr\u00e9sent visualiser l'impact des hyperparam\u00e8tres de la GridSearch :","2928df65":"### On voit ici que les scores des diff\u00e9rents splits de cross-validation, pour les meilleurs param\u00e8tres obtenus, \u00e9voluent correctement lors de l'entrainement et des test, tout en restant dans la m\u00eame \u00e9chelle.\n\n### Les \u00e9carts et mauvais r\u00e9sultats obtenus d\u00e9pendent donc du faible nombre de donn\u00e9es qui impactent le Train_Test_Split initial. Le mod\u00e8le est correctement entrain\u00e9 mais n'obtient pas de bon r\u00e9sultats sur le jeu de test (pas d'overfiting constat\u00e9 dans les entrainements).","bd1ab22c":"#### On utilise ici le meilleur mod\u00e8le calcul\u00e9 sur la variable TotalGHGEmissions en incluant l'ENERGY STAR Score :","076c357b":"#### Regardons maintenant l'effet de la surface totale GFAsur nos varaibles a predire:        \n\n    TotalGHGEmissions   ,  SiteEnergyUse(kBtu)   ,     ENERGYSTARScore ","a907d32e":"### Ici encore, le m\u00eame probl\u00e8me que pour la variable TotalGHG se pose. Le mod\u00e8le est performant en entrainement mais ne parveint pas \u00e0 g\u00e9n\u00e9raliser sur le jeu de test.\n\n### Nous allons regarder les \u00e9carts de pr\u00e9diction en fonction du type de b\u00e2timent pour v\u00e9rifier si des \u00e9carts sont plus importants dans certaines cat\u00e9gories :","ac248bd4":"# 3.2. Mod\u00e8le Support Vector Regression (SVR)","224f9703":"#### Pour les donn\u00e9es cat\u00e9gorielles, nous allons devoir encoder les valeurs. ","dbc38686":"### Pour ces variables, nous utiliserons la m\u00e9thode TargetEncoder de la librairie Category_Encoders que nous int\u00e9grerons dans un pipeline Sklearn.\n### Cet encodeur Ggoupe les donn\u00e9es par chaque cat\u00e9gorie et compte de nombre d'occurrences de chaque cible et Calcul de la probabilit\u00e9 que chaque cible se produise pour chaque groupe sp\u00e9cifique.","de969d7e":"### 1.2. Pr\u00e9paration des jeux d'entrainement et de test","9b99eadd":"#### Les valeurs pr\u00e9dites sont en effet beaucoup plus reserr\u00e9es sur la premi\u00e8re bissectrice et les m\u00e9triques se sont am\u00e9lior\u00e9es gr\u00e2ce \u00e0 la prise en compte de l'ENERGY STAR Score.\n\n#### En revanche, cette variable est encore peu renseign\u00e9e et le jeu de donn\u00e9es comporte peu d'entr\u00e9es. Il est donc difficile de savoir si cette am\u00e9lioration est r\u00e9\u00e9llement significative. Il faut \u00e9galement prendre en compte le b\u00e9n\u00e9fice vis \u00e0 vis du co\u00fbt de cet ENERGY STAR Score","5e586a64":"### Apr\u00e8s avoir test\u00e9 les premiers param\u00e8tres, nous avons \u00e0 pr\u00e9sent d\u00e9fini notre mod\u00e8le pour la pr\u00e9diction des \u00e9missions de CO2. Regardons \u00e0 pr\u00e9sent l'importance des variables dans notre mod\u00e8le de for\u00eats al\u00e9atoires :","3d8d2df3":"# INTRODUCTION","119c3290":"### Nous r\u00e9aliserons un centrage-r\u00e9duction via la m\u00e9thode StandardScaler de Scikit-Learn.","f2d603fa":"#### On remarque ici que le score ENERGY STAR ne semble pas avoir de corr\u00e9lation importante avec les \u00e9missions de CO2. La distribution ne suit pas de loi normale et la majorit\u00e9 des batiments a un score sup\u00e9rieur \u00e0 50 (de bonne qualit\u00e9 voir de tr\u00e8s bonne qualit\u00e9).","5d0d7a12":"\n#### Pour atteindre l'objectif de ville neutre en \u00e9missions de carbone en 2050, la ville de Seattle s\u2019int\u00e9resse de pr\u00e8s aux \u00e9missions des b\u00e2timents non destin\u00e9s \u00e0 l\u2019habitation.\n\n#### Des relev\u00e9s minutieux ont \u00e9t\u00e9 effectu\u00e9s en 2015 et en 2016. Cependant, ces relev\u00e9s sont co\u00fbteux \u00e0 obtenir, et \u00e0 partir de ceux d\u00e9j\u00e0 r\u00e9alis\u00e9s, nous devons tenter de pr\u00e9dire les \u00e9missions de CO2 et la consommation totale d\u2019\u00e9nergie de b\u00e2timents pour lesquels elles n\u2019ont pas encore \u00e9t\u00e9 mesur\u00e9es.\n\n# *************************************************************************************************************************\n\n\n#### La premi\u00e8re partie nous a permis de r\u00e9aliser un nettoyage des donn\u00e9es et une courte analyse exploratoire.\n#### Dans cette seconde partie, nous allons r\u00e9aliser les diverses mod\u00e9lisations gr\u00e2ce \u00e0 des approches lin\u00e9aires et non-lin\u00e9aire afin de pr\u00e9dire les \u00e9missions de CO2 et les consommations d'\u00e9nergie des b\u00e2timents.","576f6bc9":"# 7. Influence du score ENERGY STAR\n### Le score ENERGY STAR fournit un aper\u00e7u complet de la performance \u00e9nerg\u00e9tique d'un b\u00e2timent, en tenant compte des actifs physiques, des op\u00e9rations et du comportement des occupants du b\u00e2timent. Il est exprim\u00e9 sur une \u00e9chelle de 1 \u00e0 100 facile \u00e0 comprendre : plus le score est \u00e9lev\u00e9, meilleure est la performance \u00e9nerg\u00e9tique du b\u00e2timent.\n\n###  Ce score permet de r\u00e9aliser plusieurs actions :\n\n### \u00c9valuer les donn\u00e9es \u00e9nerg\u00e9tiques r\u00e9elles factur\u00e9es,\n### Normaliser pour l'activit\u00e9 commerciale (heures, travailleurs, climat),\n### Comparer les b\u00e2timents \u00e0 la population nationale,\n### Indiquer le niveau de performance \u00e9nerg\u00e9tique.\n### Nous allons donc \u00e9valuer si ce score \u00e0 un impact significatif sur les performances de notre mod\u00e9lisation.","e19a0f6b":"### Pour les consommations d'\u00e9nergie, la surface de la propri\u00e9t\u00e9 \u00e0 ici \u00e9galement une importance bien sup\u00e9rieure aux autres variables.","a53fc84f":"#### Les surfaces (GFA) de la propri\u00e9t\u00e9 et le type d'utilisation principale ont un poids plus important dans les d\u00e9cisions de notre mod\u00e8le. En revanche, le type de b\u00e2timent a un impact tr\u00e8s limit\u00e9. On remarque \u00e9galement que l'age du batiment ou encore son emplacement g\u00e9ographique n'ont pas un impact tr\u00e8s important non plus.","11f9e80f":"### Comme nous l'avons remarqu\u00e9 dans le Notebook de nettoyage, les Campus sont tr\u00e8s consommateurs en energie. Apr\u00e9s avoir v\u00e9rifier la distribution des consommations en fonction des surfaces totales au sol par cat\u00e9gorie de b\u00e2timent. D'\u00e9ventuels valeurs hors-normes pourraient poser des probl\u00e8mes pour les mod\u00e9lisations.\n### Un b\u00e2timent de type campus est tr\u00e8s sup\u00e9rieur aux autres donn\u00e9es. Il ne s'agit sans doute pas d'une valeur ab\u00e9rrante mais d'une valeur atypique qui est tr\u00e8s isol\u00e9e. Nous allons ici la supprimer de nos donn\u00e9es sources.","01406847":"###  Maintenant separons nos variables num\u00e9riques des variables cat\u00e9gorielles","2ca08b25":"# Sommaire\n\n ### 1.Preprocessing\n   #### 1.1. Encodage et standardisation\n   #### 1.2. Pr\u00e9paration des jeux d'entrainement et de test\n\n ### 2.Mod\u00e8le Baseline : R\u00e9gression lin\u00e9aire multivari\u00e9e\n\n ### 3.Mod\u00e8le lin\u00e9aires : ElasticNet et SVR\n   #### 3.1. Mod\u00e8le ElasticNet\n   #### 3.2. Mod\u00e8le Support Vector Regression (SVR)\n\n ### 4.Mod\u00e8le non-lin\u00e9aires : XGBoost et RandomForestRegressor\n   #### 4.1. Mod\u00e8le RandomForestRegressor\n   #### 4.2. Mod\u00e8le XGBoost (eXtreme Gradient Boosting)\n\n ### 5.S\u00e9lection des meilleurs mod\u00e8les\n  #### 5.1. Mod\u00e8le de pr\u00e9diction des \u00e9missions de CO2\n  #### 5.2. Mod\u00e8le de pr\u00e9diction des consommations d'\u00e9nergie\n\n ### 6.Test des mod\u00e8les s\u00e9lectionn\u00e9s\n  #### 6.1. Pr\u00e9diction des \u00e9missions de CO2\n  #### 6.2. Pr\u00e9diction des consommation d'\u00e9nergie\n\n ### 7.Influence du score ENERGY STAR","cf2625f7":"### Sur les 4 mod\u00e8les test\u00e9s,les mod\u00e8les lin\u00e9aires retournent de moins bonnes m\u00e9triques en g\u00e9n\u00e9ral. Si nous prenons en consid\u00e9ration le score MAE, qui aura du sens sur les mod\u00e8les lin\u00e9aires et non-lin\u00e9aires, les algorithmes XGBoost et RandomForestRegressor offrent des performances \u00e0 peu pr\u00e8s similaires pour la qualit\u00e9 des pr\u00e9dictions mais les temps de calculs sont meilleurs sur le mod\u00e8le RandomForestRegressor.\n\n### Nous allons donc regarder de plus pr\u00e8s les r\u00e9sultats obtenus sur nos 2 variables \u00e0 pr\u00e9dire avec les diff\u00e9rents mod\u00e8les :","278fdfd8":"# 4. Mod\u00e8le non-lin\u00e9aires : XGBoost et RandomForestRegressor","50d354e6":"### Comme nous l'avons vu dans l'analyse exploratoire, un mod\u00e8le simple de r\u00e9gression lin\u00e9aire multivari\u00e9 ne pourrait pas r\u00e9pondre totalement \u00e0 notre probl\u00e8me de pr\u00e9diction. Nous allons donc utiliser ce premier mod\u00e8le comme baseline et tester les m\u00e9triques principales : R\u00b2 et MAE.","0bbb13d6":"### Les valeurs pr\u00e9dites sont tr\u00e8s \u00e9loign\u00e9es de la premi\u00e8re bissectrice. En effet, le transformer log() \/ exp() de notre variable Y emplifie les erreurs lors de la transformation inverse.\n\n### Nous allons regarder les scores obtenus sur les diff\u00e9rents splits de la Cross-validation pour v\u00e9rifier les \u00e9carts :","217774db":"### et les scores pour la variable SiteEnergyUse(kBtu) :","724c0cfd":"### On affiche les scores de la GridSearch avec validation crois\u00e9e pour la variable TotalGHGEmissions :","a2f2e54b":"### L'\u00e9cart est tr\u00e8s important sur la cat\u00e9gorie \"Campus\" qui est faiblement repr\u00e9sent\u00e9e dans le jeu de donn\u00e9es mais qui pr\u00e9sente les plus grandes consommations.","35b6f4c9":"### Nous allons calculer 2 principales m\u00e9triques pour \u00e9valuer nos mod\u00e8les :\n\n### MAE : Mean Absolute Error.\n### R\u00b2 : Coeficient de d\u00e9termination, carr\u00e9 du coefiscient de corr\u00e9lation lin\u00e9aire","37281bef":"### A pr\u00e9sent, nous allons cr\u00e9er un mod\u00e8le baseline pour \u00e9valuer les performances de nos futurs mod\u00e8les et v\u00e9rifier qu'ils am\u00e9liorent les pr\u00e9dictions. Pour cette baseline, nous utiliserons une r\u00e9gression lin\u00e9aire multivari\u00e9e.","aa9a60fc":"# 2. Mod\u00e8le Baseline : R\u00e9gression lin\u00e9aire multivari\u00e9e","f9ee2bf2":"### Le but de notre demarche est de supprimer le recours au relev\u00e9s qui reviennent ch\u00e9rs et de s'en passer pour les ann\u00e9es \u00e0 venir. Nous allons donc exclure toutes les donn\u00e9es de rel\u00e8ve de notre dataset.Quant \u00e0 l'ENERGYSTARScore nous allons l'exclure car il est insuffisament fourni et voir seseffets sur les predictions \u00e0 la fin.","da1de94c":"### Les m\u00e9triques sur le jeu de donn\u00e9es de test sont tr\u00e8s d\u00e9grad\u00e9es comparativement aux m\u00e9triques obtenues avec la GridSearch avec le mod\u00e8le de RandomForestRegressor. Nous allons v\u00e9rifier la distribution des valaurs pr\u00e9dites en fonction des valeurs r\u00e9\u00e9lles :","843694d8":"#### En passant les donn\u00e9es \u00e0 l'\u00e9chelle logarithmique, nous obtenons une distribution normale des donn\u00e9es \u00e0 pr\u00e9dire. Nous allons donc appliquer cette transformation dans notre pipeline gr\u00e2ce \u00e0 la fonction TransformedTargetRegressor de la librairie Sklearn.\n\n### La fonction inverse (exp) sera donc pass\u00e9e dans les pr\u00e9dictions.","d2aa1666":"## Regardons maintenant les variables categorielles","32cbd914":"# 6. Test des mod\u00e8les s\u00e9lectionn\u00e9s\n### Nous allons \u00e0 pr\u00e9sent tester les mod\u00e8les s\u00e9lectionn\u00e9s sur nos donn\u00e9es test et v\u00e9rifier leurs performances.\n\n### 6.1. Pr\u00e9diction des \u00e9missions de CO2","e22c0e50":"# 5. S\u00e9lection des meilleurs mod\u00e8les","475ed84b":"### Suite \u00e0 cette premi\u00e8re mod\u00e9lisation par ElasticNet, les m\u00e9triques ne se sont que l\u00e9g\u00e9rement am\u00e9lior\u00e9es pour les pr\u00e9dictions de la consommation d'\u00e9nergie et des \u00e9missions de CO2. Nous allons \u00e0 pr\u00e9sent tester un second mod\u00e8le lin\u00e9aire : SVR.","83af3cd3":"### 5.2. Mod\u00e8le de pr\u00e9diction des consommations d'\u00e9nergie","7717a078":"### 1.1. Encodage et standardisation","07a487e3":"### 6.2. Pr\u00e9diction des consommations d'\u00e9nergie"}}