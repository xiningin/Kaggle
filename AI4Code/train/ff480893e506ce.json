{"cell_type":{"aedffa79":"code","52eb2672":"code","3f2b81f6":"code","18bdef5e":"code","8ab20e70":"code","17001d0e":"code","9e4f82ed":"code","6f052cab":"code","165d5167":"code","3ab2dc9c":"code","5fed4de0":"code","e35087e7":"code","9a878377":"code","3fec611a":"code","4059dc47":"code","bdf7cf0d":"code","8c6ea1de":"code","5491129f":"code","b942666e":"code","17b6abad":"code","e490f99c":"code","d070a09a":"code","0166c681":"code","4541ca03":"code","31edf6ad":"code","638e8b19":"code","ce8c43a4":"code","7f31db1d":"code","5ad92f76":"code","394b46ff":"code","215bdba5":"code","1bcf45a3":"code","9c32bad2":"code","14e188a5":"code","197d2b76":"code","39da504d":"code","8737d9d7":"code","0fe16aa3":"code","d46583ab":"code","e273816b":"code","f3a98964":"code","ac7b775f":"code","62469613":"code","12a71561":"code","aefcb338":"code","f2d39408":"code","b98c487c":"code","3814db6b":"code","1abf2c04":"code","6ac5cc8c":"code","9f0ad9e2":"code","2acd820f":"code","c716f39c":"code","3b70f34d":"code","9ebd5963":"code","b2597779":"code","678e09cb":"code","10dd23a7":"code","0d2f3a4a":"code","0825df4a":"code","a0c8ff81":"code","f8f1a3a6":"code","fb5b02e8":"code","f13f2a7b":"code","902fb74a":"code","3ba04b50":"code","b6578868":"code","58eaf670":"code","fae64275":"code","24ce7c5b":"markdown","1f66debe":"markdown","1c59f0a1":"markdown","9a48b28c":"markdown","6e045900":"markdown","b3faf319":"markdown","e2d09b6d":"markdown","dee9e27f":"markdown","43d87098":"markdown","4192f42b":"markdown","e0c4401d":"markdown","50389ec0":"markdown","6baeeef8":"markdown","884bee4e":"markdown","0704253a":"markdown","c4353273":"markdown","528e0a1a":"markdown","8ed87f06":"markdown","e375b9f4":"markdown","1ece98db":"markdown","06892530":"markdown","3981ee1c":"markdown","ac829d29":"markdown","fbc09fa2":"markdown","8fda0fe8":"markdown","21ab6882":"markdown","ae1ef460":"markdown","d10b0952":"markdown","76459cd5":"markdown","36d99292":"markdown","12bcbeed":"markdown","846c9179":"markdown","bfdcb43a":"markdown","6b92c8ae":"markdown","89199737":"markdown","fc7c414f":"markdown","9f2607fe":"markdown","c12b2d39":"markdown"},"source":{"aedffa79":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","52eb2672":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.style.use(\"fivethirtyeight\")\nsns.set_style(\"darkgrid\")","3f2b81f6":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nimport xgboost\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report, roc_curve, roc_auc_score, auc, precision_recall_curve, precision_score, recall_score\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n\nfrom sklearn.preprocessing import StandardScaler","18bdef5e":"train = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")","8ab20e70":"display(train.head())\ndisplay(train.tail())","17001d0e":"train.shape","9e4f82ed":"print(train.columns)","6f052cab":"train.info()","165d5167":"train[\"Class\"].value_counts()","3ab2dc9c":"sns.countplot(\"Class\", data=train)","5fed4de0":"sns.set(style=\"whitegrid\")\nlabels = ['Not Fraud', 'Fraud']\nsizes = train['Class'].value_counts(sort = True)\n\ncolors = [\"lightblue\",\"red\"]\nexplode = (0.05,0) \n \nplt.figure(figsize=(7,7))\nplt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=90,)\n\nplt.title('Frauds in the dataset')\nplt.legend()\nplt.show()","e35087e7":"plt.figure(figsize=(10, 6))\nsns.scatterplot(x=\"V1\", y=\"V2\", hue=\"Class\", data=train);","9a878377":"plt.figure(figsize=(10, 6))\nsns.lmplot(x=\"V15\", y=\"V16\", hue='Class', data=train, fit_reg=True);","3fec611a":"train.isnull().sum()","4059dc47":"train.isnull().sum().sum()","bdf7cf0d":"train.describe()","8c6ea1de":"train.corr()['Class'].sort_values(ascending=False)","5491129f":"a = train.drop('Class', axis=1)\na.corrwith(train['Class']).plot(kind='bar', figsize=(15,10), color=['salmon'])\nplt.title(\"Correlation Matrix\")\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.show()","b942666e":"plt.figure(figsize=(15,10))\nsns.heatmap(train.corr(), annot=True, cbar=False, fmt='.1f', cmap='summer')\nplt.show()","17b6abad":"plt.figure(figsize=(15,10))\nsns.boxplot(data=train)\nplt.xticks(rotation=90)\nplt.show()","e490f99c":"plt.tight_layout()\nsns.boxplot(x=train['Amount'], data=train)\nplt.show()","d070a09a":"display(train.skew())","0166c681":"sns.distplot(train['Amount'])\nskew_Class = train['Amount'].skew()\nplt.title('Skew:'+str(skew_Class))","4541ca03":"sns.distplot(train['V28'])\nskew_V28 = train['V28'].skew()\nplt.title('skew:'+str(skew_V28))","31edf6ad":"sc = StandardScaler()\n\n# normalise the amount column\ntrain['normAmount'] = sc.fit_transform(np.array(train['Amount']).reshape(-1, 1))\n  \n# drop Time and Amount columns as they are not relevant for prediction purpose \ntrain = train.drop(['Time', 'Amount'], axis = 1)","638e8b19":"train.head()","ce8c43a4":"# Independant variable\nX = train.drop('Class', axis=1)\n\n# Dependant variable\ny = train['Class']","7f31db1d":"# split into 70:30 ration\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)","5ad92f76":"# describes info about train and test set\nprint(\"Number transactions X_train dataset: \", X_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions X_test dataset: \", X_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)","394b46ff":"# logistic regression object\nlr = LogisticRegression()\n  \n# train the model on train set\nlr.fit(X_train, y_train.ravel())","215bdba5":"predictions = lr.predict(X_test)\n\naccuracy = accuracy_score(y_test, predictions)\nprint('accuracy of base model :', accuracy)\nprint(\"\\nBase Model: Train Score {:.2f} & Test Score {:.2f} \\n\".format(lr.score(X_train, y_train), lr.score(X_test, y_test)))\n\n# print confusion matrix\nprint('Confusion Matrix :\\n\\n', confusion_matrix(y_test, predictions))\n  \n# print classification report\nprint('\\n\\nClassification Report :\\n', classification_report(y_test, predictions))","1bcf45a3":"print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))","9c32bad2":"# import SMOTE module from imblearn library\n# pip install imblearn (if you don't have imblearn in your system)\n\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state = 2)\nX_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())","14e188a5":"print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n  \nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0)))","197d2b76":"lr1 = LogisticRegression()\nlr1.fit(X_train_res, y_train_res.ravel())\npredictions = lr1.predict(X_test)","39da504d":"accuracy = accuracy_score(y_test, predictions)\nprint('accuracy of base model :', accuracy)\nprint(\"\\nBase Model: Train Score {:.2f} & Test Score {:.2f} \\n\".format(lr.score(X_train_res, y_train_res), lr.score(X_test, y_test)))\n\n# print confusion matrix\nprint('Confusion Matrix :\\n\\n', confusion_matrix(y_test, predictions))\n\n# print classification report\nprint(classification_report(y_test, predictions))","8737d9d7":"class_names = ['not_fraud', 'fraud']\nmatrix = confusion_matrix(y_test, predictions)\nprint(matrix)\n\n# Create pandas dataframe\ndf = pd.DataFrame(matrix, index=class_names, columns=class_names)","0fe16aa3":"y_pred_logit_proba = lr1.predict_proba(X_test)[::,1]\nfpr_logit, tpr_logit, _ = roc_curve(y_test,  y_pred_logit_proba)\nauc_logit = roc_auc_score(y_test, y_pred_logit_proba)\nprint(\"AUC Logistic Regression :\", auc_logit)","d46583ab":"# ROC Curve\nplt.plot(fpr_logit,tpr_logit,label=\"Logistic Regression, auc={:.3f})\".format(auc_logit))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('Logistic Regression ROC curve')\nplt.legend(loc=4)\nplt.show()","e273816b":"logit_precision, logit_recall, _ = precision_recall_curve(y_test, y_pred_logit_proba)\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', color='black', label='No Skill')\nplt.plot(logit_recall, logit_precision, color='orange', label='Logistic')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall curve')\nplt.legend()\nplt.show()","f3a98964":"print(\"Before Undersampling, counts of label '1': {}\".format(sum(y_train == 1)))\nprint(\"Before Undersampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))\n  \n# apply near miss\nfrom imblearn.under_sampling import NearMiss\nnr = NearMiss()\n  \nX_train_miss, y_train_miss = nr.fit_sample(X_train, y_train.ravel())\n  \nprint('After Undersampling, the shape of train_X: {}'.format(X_train_miss.shape))\nprint('After Undersampling, the shape of train_y: {} \\n'.format(y_train_miss.shape))\n  \nprint(\"After Undersampling, counts of label '1': {}\".format(sum(y_train_miss == 1)))\nprint(\"After Undersampling, counts of label '0': {}\".format(sum(y_train_miss == 0)))","ac7b775f":"# train the model on train set\nlr2 = LogisticRegression()\nlr2.fit(X_train_miss, y_train_miss.ravel())\npredictions = lr2.predict(X_test)","62469613":"accuracy = accuracy_score(y_test, predictions)\nprint('accuracy of base model :', accuracy)\nprint(\"\\nBase Model: Train Score {:.2f} & Test Score {:.2f} \\n\".format(lr.score(X_train_res, y_train_res), lr.score(X_test, y_test)))\n\n# print confusion matrix\nprint('Confusion Matrix :\\n\\n', confusion_matrix(y_test, predictions))\n\n# print classification report\nprint(classification_report(y_test, predictions))","12a71561":"import imblearn\nfrom imblearn.under_sampling import RandomUnderSampler \n\nundersample = RandomUnderSampler(sampling_strategy=0.5)","aefcb338":"cols = train.columns.tolist()\ncols = [c for c in cols if c not in [\"Class\"]]\ntarget = \"Class\"","f2d39408":"X = train[cols]\nY = train[target]\n\n# undersample\nX_under, Y_under = undersample.fit_resample(X, Y)","b98c487c":"test = pd.DataFrame(Y_under, columns = ['Class'])","3814db6b":"# visualizing undersampling results\nfig, axs = plt.subplots(ncols=2, figsize=(13,4.5))\nsns.countplot(x=\"Class\", data=train, ax=axs[0])\nsns.countplot(x=\"Class\", data=test, ax=axs[1])\n\nfig.suptitle(\"Class prepartition before and after undersampling\")\na1=fig.axes[0]\na1.set_title(\"Before\")\na2=fig.axes[1]\na2.set_title(\"After\")","1abf2c04":"xgb = XGBClassifier(random_state=2)\nxgb.fit(X_train, y_train)","6ac5cc8c":"y_pred_xgb = xgb.predict(X_test)","9f0ad9e2":"print(\"Accuracy XGB:\", accuracy_score(y_test, y_pred_xgb))\nprint(\"Precision XGB:\", precision_score(y_test, y_pred_xgb))\nprint(\"Recall XGB:\", recall_score(y_test, y_pred_xgb))\nprint(\"F1 Score XGB:\", f1_score(y_test, y_pred_xgb))","2acd820f":"matrix_xgb = confusion_matrix(y_test, y_pred_xgb)\ncm_xgb = pd.DataFrame(matrix_xgb, index=['not_fraud', 'fraud'], columns=['not_fraud', 'fraud'])\n\nsns.heatmap(cm_xgb, annot=True, cbar=None, cmap=\"Blues\", fmt = 'g')\nplt.title(\"Confusion Matrix XGBoost\"), plt.tight_layout()\nplt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\nplt.show()","c716f39c":"y_pred_xgb_proba = xgb.predict_proba(X_test)[::,1]\nfpr_xgb, tpr_xgb, _ = roc_curve(y_test,  y_pred_xgb_proba)\nauc_xgb = roc_auc_score(y_test, y_pred_xgb_proba)\nprint(\"AUC XGBoost :\", auc_xgb)","3b70f34d":"# ROC Curve\nplt.plot(fpr_xgb,tpr_xgb,label=\"XGBoost, auc={:.3f})\".format(auc_xgb))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('XGBoost ROC curve')\nplt.legend(loc=4)\nplt.show()","9ebd5963":"xgb_precision, xgb_recall, _ = precision_recall_curve(y_test, y_pred_xgb_proba)\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', color='black', label='No Skill')\nplt.plot(xgb_recall, xgb_precision, color='orange', label='XGB')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall curve')\nplt.legend()\nplt.show()","b2597779":"!pip install pycaret","678e09cb":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","10dd23a7":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.preprocessing import LabelEncoder\n\n# Importing module and initializing setup\nfrom pycaret.classification import *","0d2f3a4a":"# Data Preprocessing\nclf1 = setup(data = df, target = 'Class', silent = True)","0825df4a":"# Comparing models\ncompare_models()","a0c8ff81":"rf = create_model('rf')","f8f1a3a6":"#plot_model(estimator = tuned_rf, plot = 'auc')\nplot_model(estimator = rf, plot = 'auc')","fb5b02e8":"plot_model(estimator = rf, plot = 'pr')","f13f2a7b":"#plot_model(estimator = tuned_rf, plot = 'confusion_matrix')\nplot_model(estimator = rf, plot = 'confusion_matrix')","902fb74a":"#plot_model(estimator=tuned_rf, plot='feature')\nplot_model(estimator=rf, plot='feature')","3ba04b50":"#evaluate_model(tuned_rf)\nevaluate_model(rf)","b6578868":"predict_model(rf)","58eaf670":"final_rf = finalize_model(rf)\nfinal_rf","fae64275":"predict_model(final_rf)","24ce7c5b":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:160%; text-align:left;\"> Plot Confusion Matrix <\/h1>","1f66debe":"### Objective\n- The dataset contains a very minute percentage transacions, which are fraudulent. We need to find out those transactions which belong to the Fraud Class\n\n- Based on the data we have to generate a set of insights and recommendations that will help the credit card company from preventing the customers to be charged falsly!","1c59f0a1":"### ravel\n- The numpy module of Python provides a function called **numpy.ravel**, which is used to **change a 2-dimensional array or a multi-dimensional array into a contiguous flattened array**. The returned array has the same data type as the source array or input array. If the input array is a masked array, the returned array will also be a masked array.","9a48b28c":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:180%; text-align:left; border-radius: 0px 0px;\"> 5.2) XGBoost Classifier <\/h1>","6e045900":"- Data is highly imbalanced","b3faf319":"#### The NearMiss Algorithm has **undersampled** the **majority instances** and made it **equal to majority class**.\n#### Here, the **majority class** has been **reduced** to the total number of **minority class**, so that both classes will have equal number of records.","e2d09b6d":"#### SMOTE Algorithm has **oversampled the minority instances** and made it **equal to majority class**.\n#### **Both categories have equal amount of records**. More specifically, the minority class has been increased to the total number of majority class.","dee9e27f":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 2) Load Data <\/h1>","43d87098":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 3) EDA(Exploratory Data Analysis) <\/h1>","4192f42b":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 10px;\"> Table of Contents <\/h1>\n\n* [1) Loading all Required Libraries](#1)\n* [2) Load Data](#2)\n* [3) EDA(Exploratory Data Analysis)](#3)\n     * [3.1) Handling Missing Values](#3.1)\n     * [3.2) Correlation between features ](#3.2)\n     * [3.3) Outliers](#3.3)\n     * [3.4) Skewness and Kurtosis](#3.4)\n* [4) Data Preprocessing](#4)\n     * [4.1) Handling Imbalanced Data](#4.1)\n* [5) Model Building and Evaluation](#5)\n     * [5.1) Logistic Regression](#5.1)\n          * [5.1.1) train the model without handling the imbalanced class distribution](#5.1.1)\n          * [5.1.2) imbalanced data handling techniques](#5.1.2)\n               * [a) SMOTE](#a)\n               * [b) Near Miss](#b)\n               * [c) Random Under Sample](#c)                    \n     * [5.2) XGBoost Classifier](#5.2)\n* [6) PyCaret](#6)","e0c4401d":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:165%; text-align:left; border-radius: 0px 0px;\"> c) Random Under Sample <\/h1>","50389ec0":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:160%; text-align:left; border-radius: 0px 0px;\"> 5.1.1) train the model without handling the imbalanced class distribution <\/h1>","6baeeef8":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 4) Data Preprocessing <\/h1>","884bee4e":"#### This model is better than the first model because it classifies better and also the **recall** value of **minority class is 95 %**. But due to **undersampling of majority class**, its recall has **decreased to 56 %**.\n#### So in this case, **SMOTE** is giving me a **great accuracy and recall**, I\u2019ll go ahead and use that model!","0704253a":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:160%; text-align:left;\"> Predict Data <\/h1>","c4353273":"#### Hypertuning of model\ntuned_rf = tune_model(rf)","528e0a1a":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:160%; text-align:left; border-radius: 0px 0px;\"> 5.1.2) imbalanced data handling techniques <\/h1>","8ed87f06":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:160%; text-align:left;\"> Evaluating the model <\/h1>","e375b9f4":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:160%; text-align:left;\"> Plot feature Importance <\/h1>","1ece98db":"- from above graph observe that correlation is less for features : **V13, V15, V22, V23, V24, V25, V26, V27, V28, Amount & Time**","06892530":"- As you can see, the non-fraud transactions far outweigh the fraud transactions. If we train a binary classification model without fixing this problem, the model will be completely biased. It also impacts the correlations between features ","3981ee1c":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:180%; text-align:left; border-radius: 0px 0px;\"> 3.3) Outliers <\/h1>","ac829d29":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:180%; text-align:left; border-radius: 0px 0px;\"> 5.1) Logistic Regression <\/h1>","fbc09fa2":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:180%; text-align:left; border-radius: 0px 0px;\"> 3.4) Skewness and Kurtosis <\/h1>","8fda0fe8":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:160%; text-align:left; border-radius: 0px 0px;\"> b) NearMiss <\/h1>","21ab6882":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:160%; text-align:left;\"> Plot AUC curve <\/h1>","ae1ef460":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 1) Loading all Required Libraries <\/h1>","d10b0952":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 6) PyCaret <\/h1>","76459cd5":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:160%; text-align:left;\"> Precision-Recall Curve <\/h1>","36d99292":"#### We have reduced the accuracy to **98%** as compared to previous model but the **recall** value of **minority class** has also improved to **92 %**. This is a good model compared to the previous one. ","12bcbeed":"### About the Dataset\n\n - The datasets contains transactions made by credit cards in September 2013 by european cardholders.\n\n - This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions.\n\n - The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\n - It contains only numerical input variables which are the result of a PCA transformation.\n\n - Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data.\n\n - Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'.\n\n - Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset.\n\n - The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning.\n\n - Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.","846c9179":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:160%; text-align:left; border-radius: 0px 0px;\"> a) SMOTE <\/h1>","bfdcb43a":"### The accuracy comes out to be 100% but did you notice something strange ?\n\n- The **recall** of the **minority class in very less**. It proves that the model is **more biased towards majority class**. So, it proves that this is not the best model.\n\n### Now, we will apply different **imbalanced data handling techniques** and see their accuracy and recall results","6b92c8ae":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:180%; text-align:left; border-radius: 0px 0px;\"> Handling Imbalanced Data <\/h1>\n\n- Generally happens when observations in one of the **class are much higher or lower than the other classes**. As Machine Learning algorithms tend to increase accuracy by reducing the error, they do not consider the class distribution. This problem is prevalent in examples such as **Fraud Detection, Anomaly Detection, Facial recognition** etc.\n\n- Standard ML techniques such as **Decision Tree and Logistic Regression** have a **bias towards the majority class**, and they tend to **ignore the minority class**. They tend only to predict the majority class, hence, having major misclassification of the minority class in comparison with the majority class. In more technical words, if we have imbalanced data distribution in our dataset then our model becomes more prone to the case when minority class has negligible or very lesser recall.\n\n **Imbalanced Data Handling Techniques:** There are mainly 2 algorithms that are widely used for handling imbalanced class distribution.\n\n - 1) SMOTE\n\n - 2) Near Miss Algorithm\n \n### 1) SMOTE (Synthetic Minority Oversampling Technique) \u2013 Oversampling:\n\n - SMOTE (synthetic minority oversampling technique) is one of the most commonly used oversampling methods to solve the imbalance problem. It aims to balance class distribution by randomly increasing minority class examples by replicating them. SMOTE synthesises new minority instances between existing minority instances. It generates the virtual training records by linear interpolation for the minority class. These synthetic training records are generated by randomly selecting one or more of the k-nearest neighbors for each example in the minority class. After the oversampling process, the data is reconstructed and several classification models can be applied for the processed data.\n \n### 2) NearMiss Algorithm \u2013 Undersampling\n\n - NearMiss is an under-sampling technique. It aims to balance class distribution by randomly eliminating majority class examples. When instances of two different classes are very close to each other, we remove the instances of the majority class to increase the spaces between the two classes. This helps in the classification process. To prevent problem of information loss in most under-sampling techniques, near-neighbor methods are widely used.\n \n#### The basic intuition about the working of near-neighbor methods is as follows:\n\n- **Step 1:** The method first finds the distances between all instances of the majority class and the instances of the minority class. Here, majority class is to be under-sampled.\n\n- **Step 2:** Then, n instances of the majority class that have the smallest distances to those in the minority class are selected.\n\n- **Step 3:** If there are k instances in the minority class, the nearest method will result in k*n instances of the majority class.\n\n#### For finding n closest instances in the majority class, there are several variations of applying NearMiss Algorithm :\n\n- **NearMiss \u2013 Version 1 :** It selects samples of the majority class for which average distances to the k closest instances of the minority class is smallest.\n\n- **NearMiss \u2013 Version 2 :** It selects samples of the majority class for which average distances to the k farthest instances of the minority class is smallest.\n\n- **NearMiss \u2013 Version 3 :** It works in 2 steps. Firstly, for each minority class instance, their M nearest-neighbors will be stored. Then finally, the majority class instances are selected for which the average distance to the N nearest-neighbors is the largest.","89199737":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:180%; text-align:left; border-radius: 0px 00px;\"> 3.2) Correlation between features <\/h1>","fc7c414f":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 5) Model Building and Evaluation <\/h1>","9f2607fe":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:180%; text-align:left; border-radius: 0px 0px;\"> Class <\/h1>","c12b2d39":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:180%; text-align:left; border-radius: 0px 0px;\"> 3.1) Handling Missing Values <\/h1>"}}