{"cell_type":{"bb7d807b":"code","6b11e665":"code","8ceac814":"code","32b56973":"code","52944ecd":"code","86c49570":"code","c7c9882c":"code","d8c5b6b2":"code","446cbce7":"code","b0f78306":"code","fb26795d":"code","1fd1cde8":"code","030d8ef4":"code","f72da63a":"code","f7079eeb":"code","63d6b097":"code","d5ac42d3":"code","18eb83c7":"code","44f88f75":"code","1badca63":"code","659c7b1c":"code","2d7fa322":"code","6ab8a6ac":"code","15467c22":"code","0bbef6be":"code","4937da96":"code","13f97eb0":"code","0bec1bdd":"code","865f5a62":"code","8974a6ca":"code","7be2b9cb":"code","254554bd":"code","4b6abc45":"code","cdf42f01":"code","1495a196":"code","9a03f9ac":"code","3aa2f130":"code","136b597f":"code","517a5080":"code","666ad285":"code","e1d2445c":"code","da3ec005":"code","0a039836":"code","ab04dc7c":"code","6e500cbe":"code","a88e634b":"code","07d5923b":"code","93438036":"code","8086d803":"code","083f383f":"code","13903aac":"code","a9fb1c9b":"code","becb5d48":"code","bb0c0e5f":"code","7edb2fa7":"code","c3805346":"code","0ea715fc":"code","fda01c22":"code","580e889a":"code","47c048da":"code","65681050":"code","c213f723":"code","8a59ab83":"code","49c75bb2":"code","a8f81c55":"code","bd9da067":"code","27c77784":"code","9d327207":"code","2559dbca":"code","3c0f7c8b":"code","3f7393f1":"code","effe920a":"code","5439c2b6":"code","cf6c2668":"code","80a02235":"code","9a0d890d":"code","0484e28c":"code","a7cf40b2":"code","84a90aff":"code","be14f48a":"code","4e960892":"code","f871b2a7":"code","33245f1d":"code","8ad12c77":"markdown","17ef1ac4":"markdown","0457f033":"markdown","6b130b85":"markdown","81e96a40":"markdown","79fecb07":"markdown","c108fc06":"markdown","339a5350":"markdown","e633f0ff":"markdown","37400452":"markdown","d7346771":"markdown","a176b46f":"markdown","155a89e0":"markdown","298aa973":"markdown","c6348cbe":"markdown","03ba509a":"markdown","6560ff7c":"markdown","4769fe8d":"markdown","8d4985fc":"markdown","b3fccee0":"markdown","8f0cb3d9":"markdown","cb30944f":"markdown","f12e09a2":"markdown","20d6f443":"markdown","7251027c":"markdown","d7b20898":"markdown","cdd10c5b":"markdown","f98cde65":"markdown","5e4010f8":"markdown","4b81ee5f":"markdown","1e3355be":"markdown","ecf9abd3":"markdown","3be212e1":"markdown","93d7b316":"markdown","6689b40a":"markdown","21c3085d":"markdown","9bcab75a":"markdown","740c91c7":"markdown","672b37a4":"markdown","caaac3a2":"markdown","a61c1a76":"markdown","83756a2a":"markdown","2b1c334e":"markdown","9c4b7e68":"markdown","69024d6c":"markdown","06bbabbe":"markdown","0932f045":"markdown","12641111":"markdown","018aefb2":"markdown","295a6174":"markdown","2f315ff4":"markdown","73accfc3":"markdown","d1fe8ee7":"markdown","123a1da9":"markdown","5a9c89b8":"markdown","b4141bd0":"markdown","0c99599f":"markdown","427da4ef":"markdown","de971577":"markdown","fab0bd65":"markdown","4257486b":"markdown","c3da6a50":"markdown","5920926b":"markdown","6ae7b3f5":"markdown"},"source":{"bb7d807b":"#import\n\n#pandas\nimport pandas as pd\nfrom pandas import Series,DataFrame\n\n#numpy, matplotlib, seaborn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\n\nimport time\nimport random\nimport sklearn\n\n\n#machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB","6b11e665":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\n\ntrain = train_df.copy(deep = True)\n","8ceac814":"train.head(5)","32b56973":"#conserve test passengerID in variable\n\ntest_passengerid = test_df.PassengerId\n\n#drop unnecessary columns\n\ntrain = train.drop(['PassengerId','Ticket'], axis = 1)\n\ntest_df = test_df.drop(['PassengerId','Ticket'], axis = 1)","52944ecd":"print(\"The null value of each column in train data:\\n \", train.isnull().sum())\nprint(\"--------------------------------\")\nprint(\"The null value of each column in test data:\\n \",test_df.isnull().sum())","86c49570":"\n\n  #Embarked:fill in with mode\ntrain['Embarked'].fillna(train['Embarked'].mode()[0], inplace = True)\ntest_df['Embarked'].fillna(test_df['Embarked'].mode()[0], inplace = True)\n  \n  #Fare\ntrain['Fare'].fillna(train['Fare'].median(), inplace = True)\ntest_df['Fare'].fillna(test_df['Fare'].median(),inplace = True)\n\n#age\ntrain['Age'].fillna(train['Age'].median(), inplace = True)\ntest_df['Age'].fillna(test_df['Age'].median(), inplace = True)\n\n\n\ntrain.drop('Cabin', axis = 1, inplace = True)\ntest_df.drop('Cabin',axis = 1, inplace = True)\n\n\n\n","c7c9882c":"#check the data again\ntrain.info()\ntest_df.info()","d8c5b6b2":"pal = {'male':\"green\", 'female':\"Pink\"}\nplt.subplots(figsize = (15,8))\nax = sns.barplot(x = \"Sex\", \n            y = \"Survived\", \n            data=train, \n            palette = pal,\n            linewidth=2 )\nplt.title(\"Survived Passenger Gender Distribution\", fontsize = 25)\nplt.ylabel(\"% of passenger survived\", fontsize = 15)\nplt.xlabel(\"Sex\",fontsize = 15);","446cbce7":"pal = {1:\"seagreen\", 0:\"gray\"}\nsns.set(style=\"darkgrid\")\nplt.subplots(figsize = (15,8))\nax = sns.countplot(x = \"Sex\", \n                   hue=\"Survived\",\n                   data = train, \n                   linewidth=2, \n                   palette = pal\n)\n\n## Fixing title, xlabel and ylabel\nplt.title(\"Passenger Gender Distribution - Survived vs Not-survived\", fontsize = 25)\nplt.xlabel(\"Sex\", fontsize = 15);\nplt.ylabel(\"# of Passenger Survived\", fontsize = 15)\n\n## Fixing xticks\n#labels = ['Female', 'Male']\n#plt.xticks(sorted(train.Sex.unique()), labels)\n\n## Fixing legends\nleg = ax.get_legend()\nleg.set_title(\"Survived\")\nlegs = leg.texts\nlegs[0].set_text(\"No\")\nlegs[1].set_text(\"Yes\")\nplt.show()","b0f78306":"plt.subplots(figsize = (15,10))\nsns.barplot(x = \"Pclass\", \n            y = \"Survived\", \n            data=train, \n            linewidth=2)\nplt.title(\"Passenger Class Distribution - Survived vs Non-Survived\", fontsize = 25)\nplt.xlabel(\"Socio-Economic class\", fontsize = 15);\nplt.ylabel(\"% of Passenger Survived\", fontsize = 15);\nlabels = ['Upper', 'Middle', 'Lower']\n#val = sorted(train.Pclass.unique())\nval = [0,1,2] ## this is just a temporary trick to get the label right. \nplt.xticks(val, labels);","fb26795d":"# Kernel Density Plot\nfig = plt.figure(figsize=(15,8),)\n## I have included to different ways to code a plot below, choose the one that suites you. \nax=sns.kdeplot(train.Pclass[train.Survived == 0] , \n               color='gray',\n               shade=True,\n               label='not survived')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Pclass'] , \n               color='g',\n               shade=True, \n               label='survived')\nplt.title('Passenger Class Distribution - Survived vs Non-Survived', fontsize = 25)\nplt.ylabel(\"Frequency of Passenger Survived\", fontsize = 15)\nplt.xlabel(\"Passenger Class\", fontsize = 15)\n## Converting xticks into words for better understanding\nlabels = ['Upper', 'Middle', 'Lower']\nplt.xticks(sorted(train.Pclass.unique()), labels);","1fd1cde8":"#Kernel Density Plot\nfig = plt.figure(figsize=(15,8),)\nax=sns.kdeplot(train.loc[(train['Survived'] == 0),'Fare'] , color='gray',shade=True,label='not survived')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Fare'] , color='g',shade=True, label='survived')\nplt.title('Fare Distribution Survived vs Non Survived', fontsize = 25)\nplt.ylabel(\"Frequency of Passenger Survived\", fontsize = 15)\nplt.xlabel(\"Fare\", fontsize = 15)","030d8ef4":"# Kernel Density Plot\nfig = plt.figure(figsize=(15,8),)\nax=sns.kdeplot(train.loc[(train['Survived'] == 0),'Age'] , color='gray',shade=True,label='not survived')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Age'] , color='g',shade=True, label='survived')\nplt.title('Age Distribution - Survivors V.S. Non Survivors', fontsize = 25)\nplt.xlabel(\"Age\", fontsize = 15)\nplt.ylabel('Frequency', fontsize = 15);","f72da63a":"g = sns.FacetGrid(train,size=5, col=\"Sex\", row=\"Embarked\", margin_titles=True, hue = \"Survived\",\n                  palette = pal\n                  )\ng = g.map(plt.hist, \"Age\", edgecolor = 'white').add_legend();\ng.fig.suptitle(\"Survived by Embarked,Sex and Age\", size = 25)\nplt.subplots_adjust(top=0.90)","f7079eeb":"g = sns.FacetGrid(train, size=5,hue=\"Survived\", col =\"Sex\", margin_titles=True,\n                palette=pal,)\ng.map(plt.scatter, \"Fare\", \"Age\",edgecolor=\"w\").add_legend()\ng.fig.suptitle(\"Survived by Sex, Fare and Age\", size = 25)\nplt.subplots_adjust(top=0.85)","63d6b097":"## dropping the three outliers where Fare is over $500 \ntrain = train[train.Fare < 500]","d5ac42d3":"## factor plot\nsns.factorplot(x = \"Parch\", y = \"Survived\", data = train,kind = \"point\",size = 8)\nplt.title(\"Factorplot of Parents\/Children survived\", fontsize = 25)\nplt.subplots_adjust(top=0.85)","18eb83c7":"sns.factorplot(x =  \"SibSp\", y = \"Survived\", data = train,kind = \"point\",size = 8)\nplt.title('Factorplot of Sibilings\/Spouses survived', fontsize = 25)\nplt.subplots_adjust(top=0.85)","44f88f75":"train.describe()","1badca63":"# Overview(Survived vs non survied)\nsurvived_summary = train.groupby(\"Survived\")\nsurvived_summary.mean().reset_index()","659c7b1c":"# Placing 0 for female and \n# 1 for male in the \"Sex\" column. \ntrain['Sex'] = train.Sex.apply(lambda x: 0 if x == 'female' else 1)\ntest_df['Sex'] = test_df.Sex.apply(lambda x: 0 if x == 'female' else 1)","2d7fa322":"train.Sex","6ab8a6ac":"pd.DataFrame(abs(train.corr()['Survived']).sort_values(ascending = False))","15467c22":"## get the most important variables. \ncorr = train.corr()**2\ncorr.Survived.sort_values(ascending=False)","0bbef6be":"## heatmeap to see the correlation between features. \n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(train.corr(), dtype=np.bool)\n#mask[np.triu_indices_from(mask)] = True\n\nplt.subplots(figsize = (15,12))\nsns.heatmap(train.corr(), \n            annot=True,\n            #mask = mask,\n            cmap = 'RdBu_r',\n            linewidths=0.1, \n            linecolor='white',\n            vmax = .9,\n            square=True)\nplt.title(\"Correlations Among Features\", y = 1.03,fontsize = 20);","4937da96":"male_mean = train[train['Sex'] == 'male'].Survived.mean()\n\nfemale_mean = train[train['Sex'] == 'female'].Survived.mean()\nprint (\"Male survival mean: \" + str(male_mean))\nprint (\"female survival mean: \" + str(female_mean))\n\nprint (\"The mean difference between male and female survival rate: \" + str(female_mean - male_mean))","13f97eb0":"train.Sex.head()","0bec1bdd":"# separating male and female dataframe. \nmale = train[train['Sex'] == 'male']\nfemale = train[train['Sex'] == 'female']\n\n# getting 50 random sample for male and female. \nimport random\nmale_sample = random.sample(list(male['Survived']),50)\nfemale_sample = random.sample(list(female['Survived']),50)\n\n# Taking a sample means of survival feature from male and female\nmale_sample_mean = np.mean(male_sample)\nfemale_sample_mean = np.mean(female_sample)\n\n# Print them out\nprint (\"Male sample mean: \" + str(male_sample_mean))\nprint (\"Female sample mean: \" + str(female_sample_mean))\nprint (\"Difference between male and female sample mean: \" + str(female_sample_mean - male_sample_mean))","865f5a62":"import scipy.stats as stats\n\nprint (stats.ttest_ind(male_sample, female_sample))\nprint (\"This is the p-value when we break it into standard form: \" + format(stats.ttest_ind(male_sample, female_sample).pvalue, '.32f'))","8974a6ca":"## Family_size seems like a good feature to create\ntrain['family_size'] = train.SibSp + train.Parch+1\ntest_df['family_size'] = test_df.SibSp + test_df.Parch+1\n\n","7be2b9cb":"def family_group(size):\n    a = ''\n    if (size <= 1):\n        a = 'loner'\n    elif (size <= 4):\n        a = 'small'\n    else:\n        a = 'large'\n    return a\n  \ntrain['family_group'] = train['family_size'].map(family_group)\ntest_df['family_group'] = test_df['family_size'].map(family_group)","254554bd":"train['is_alone'] = [1 if i<2 else 0 for i in train.family_size]\ntest_df['is_alone'] = [1 if i<2 else 0 for i in test_df.family_size]\n","4b6abc45":"train['Title'] = train['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\ntest_df['Title'] = test_df['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]","cdf42f01":"stat_min = 10 #while small is arbitrary, we'll use the common minimum in statistics: http:\/\/nicholasjjackson.com\/2012\/03\/08\/sample-size-is-10-a-magic-number\/\ntitle_names = (train['Title'].value_counts() < stat_min)\n\ntrain['Title'] = train['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n\n\n","1495a196":"title_names_test = (test_df['Title'].value_counts() < stat_min)\n\ntest_df['Title'] = test_df['Title'].apply(lambda x: 'Misc' if title_names_test.loc[x] == True else x)","9a03f9ac":"print(train['Title'].value_counts())","3aa2f130":"train['FareBin'] = pd.qcut(train['Fare'], 4)\ntest_df['FareBin'] = pd.qcut(test_df['Fare'], 4)","136b597f":"train['AgeBin'] = pd.cut(train['Age'].astype(int), 5)\ntest_df['AgeBin'] = pd.cut(test_df['Age'].astype(int), 5)","517a5080":"train.head()","666ad285":"train = pd.get_dummies(train, columns=['Sex','Title',\"Pclass\",'Embarked', 'family_group', 'FareBin','AgeBin'], drop_first=True)\ntest_df = pd.get_dummies(test_df, columns=['Sex','Title',\"Pclass\",'Embarked', 'family_group', 'FareBin','AgeBin'], drop_first=True)\n","e1d2445c":"train.drop([ 'family_size','Name', 'Fare'], axis=1, inplace=True)\ntest_df.drop(['Name','family_size',\"Fare\"], axis=1, inplace=True)","da3ec005":"X = train.drop(['Survived'], axis = 1)\nY = train[\"Survived\"]","0a039836":"from sklearn.model_selection import train_test_split\ntrain_x, test_x, train_y, test_y = train_test_split(X,Y,test_size = .33, random_state = 0)","ab04dc7c":"# Feature Scaling\n## We will be using standardscaler to transform\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\n## transforming \"train_x\"\ntrain_x = sc.fit_transform(train_x)\n## transforming \"train_x\"\ntest_x = sc.transform(test_x)\n\n## transforming \"The testset\"\ntest_df = sc.transform(test_df)","6e500cbe":"train.head()","a88e634b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_absolute_error, accuracy_score\n\n## call on the model object\nlogreg = LogisticRegression()\n\n## fit the model with \"train_x\" and \"train_y\"\nlogreg.fit(train_x,train_y)\n\n## Once the model is trained we want to find out how well the model is performing, so we test the model. \n## we use \"test_x\" portion of the data(this data was not used to fit the model) to predict model outcome. \ny_pred = logreg.predict(test_x)\n\n## Once predicted we save that outcome in \"y_pred\" variable.\n## Then we compare the predicted value( \"y_pred\") and actual value(\"test_y\") to see how well our model is performing. \n\nprint (\"Accuracy Score: {}\".format(accuracy_score(y_pred, test_y)))","07d5923b":"# Applying k-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = logreg, X = train_x, y = train_y, cv = 10, n_jobs = -1)\nlogreg_accy = accuracies.mean()\nprint (round((logreg_accy),3))","93438036":"#note: this is an alternative to train_test_split\nfrom sklearn import model_selection\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, random_state = 0 ) # run model 10x with 60\/30 split intentionally leaving out 10%\ncv_results = model_selection.cross_validate(logreg, X,Y, cv  = cv_split)","8086d803":"print (cv_results)\ncv_results['train_score'].mean()","083f383f":"from sklearn.model_selection import GridSearchCV","13903aac":"C_vals = [0.099,0.1,0.2,0.5,12,13,14,15,16,16.5,17,17.5,18]\npenalties = ['l1','l2']\n\nparam = {'penalty': penalties, \n         'C': C_vals \n        }\ngrid_search = GridSearchCV(estimator=logreg, \n                           param_grid = param,\n                           scoring = 'accuracy', \n                           cv = 10\n                          )","a9fb1c9b":"grid_search = grid_search.fit(train_x, train_y)","becb5d48":"print (grid_search.best_params_)\nprint (grid_search.best_score_)","bb0c0e5f":"logreg_grid = grid_search.best_estimator_","7edb2fa7":"logreg_accy = logreg_grid.score(test_x, test_y)\nlogreg_accy","c3805346":"from sklearn.metrics import classification_report, confusion_matrix\nprint (classification_report(test_y, y_pred, labels=logreg_grid.classes_))\nprint (confusion_matrix(y_pred, test_y))","0ea715fc":"from sklearn.metrics import roc_curve, auc\nplt.style.use('seaborn-pastel')\ny_score = logreg_grid.decision_function(test_x)\n\nFPR, TPR, _ = roc_curve(test_y, y_score)\nROC_AUC = auc(FPR, TPR)\nprint (ROC_AUC)\n\nplt.figure(figsize =[11,9])\nplt.plot(FPR, TPR, label= 'ROC curve(area = %0.2f)'%ROC_AUC, linewidth= 4)\nplt.plot([0,1],[0,1], 'k--', linewidth = 4)\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel('False Positive Rate', fontsize = 18)\nplt.ylabel('True Positive Rate', fontsize = 18)\nplt.title('ROC for Titanic survivors', fontsize= 18)\nplt.show()","fda01c22":"from sklearn.metrics import precision_recall_curve\n\nplt.style.use('seaborn-pastel')\n\ny_score = logreg_grid.decision_function(test_x)\n\nprecision, recall, _ = precision_recall_curve(test_y, y_score)\nPR_AUC = auc(recall, precision)\n\nplt.figure(figsize=[11,9])\nplt.plot(recall, precision, label='PR curve (area = %0.2f)' % PR_AUC, linewidth=4)\nplt.xlabel('Recall', fontsize=18)\nplt.ylabel('Precision', fontsize=18)\nplt.title('Precision Recall Curve for Titanic survivors', fontsize=18)\nplt.legend(loc=\"lower right\")\nplt.show()","580e889a":"from sklearn.neighbors import KNeighborsClassifier\n## choosing the best n_neighbors\nnn_scores = []\nbest_prediction = [-1,-1]\nfor i in range(1,100):\n    knn = KNeighborsClassifier(n_neighbors=i, weights='distance', metric='minkowski', p =2)\n    knn.fit(train_x,train_y)\n    score = accuracy_score(test_y, knn.predict(test_x))\n    #print i, score\n    if score > best_prediction[1]:\n        best_prediction = [i, score]\n    nn_scores.append(score)\n    \nprint (best_prediction)\nplt.plot(range(1,100),nn_scores)","47c048da":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\n#n_neighbors: specifies how many neighbors will vote on the class\n#weights: uniform weights indicate that all neighbors have the same weight while \"distance\" indicates\n        # that points closest to the \n#metric and p: when distance is minkowski (the default) and p == 2 (the default), this is equivalent to the euclidean distance metric\nknn.fit(train_x, train_y)\ny_pred = knn.predict(test_x)\nknn_accy = round(accuracy_score(test_y, y_pred), 3)\nprint (knn_accy)","65681050":"from sklearn.model_selection import StratifiedKFold\n\nn_neighbors=[1,2,3,4,5,6,7,8,9,10]\nweights=['uniform','distance']\nparam = {'n_neighbors':n_neighbors, \n         'weights':weights}\ngrid2 = GridSearchCV(knn, \n                     param,\n                     verbose=False, \n                     cv=StratifiedKFold(n_splits=5, random_state=15, shuffle=True)\n                    )\ngrid2.fit(train_x, train_y)","c213f723":"print (grid2.best_params_)\nprint (grid2.best_score_)","8a59ab83":"## using grid search to fit the best model.\nknn_grid = grid2.best_estimator_","49c75bb2":"##accuracy_score =(knn_grid.predict(x_test), y_test)\nknn_accy = knn_grid.score(test_x, test_y)\nknn_accy","a8f81c55":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(train_x, train_y)\ny_pred = gaussian.predict(test_x)\ngaussian_accy = round(accuracy_score(y_pred, test_y), 3)\nprint(gaussian_accy)","bd9da067":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC(kernel = 'rbf', probability=True, random_state = 1, C = 3)\nsvc.fit(train_x, train_y)\ny_pred = svc.predict(test_x)\nsvc_accy = round(accuracy_score(y_pred, test_y), 3)\nprint(svc_accy)","27c77784":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndectree = DecisionTreeClassifier()\ndectree.fit(train_x, train_y)\ny_pred = dectree.predict(test_x)\ndectree_accy = round(accuracy_score(y_pred, test_y), 3)\nprint(dectree_accy)","9d327207":"from sklearn.ensemble import BaggingClassifier\nBaggingClassifier = BaggingClassifier()\nBaggingClassifier.fit(train_x, train_y)\ny_pred = BaggingClassifier.predict(test_x)\nbagging_accy = round(accuracy_score(y_pred, test_y), 3)\nprint(bagging_accy)","2559dbca":"from sklearn.ensemble import RandomForestClassifier\nrandomforest = RandomForestClassifier(n_estimators=100,max_depth=9,min_samples_split=6, min_samples_leaf=4)\n#randomforest = RandomForestClassifier(class_weight='balanced', n_jobs=-1)\nrandomforest.fit(train_x, train_y)\ny_pred = randomforest.predict(test_x)\nrandom_accy = round(accuracy_score(y_pred, test_y), 3)\nprint (random_accy)","3c0f7c8b":"n_estimators = [100,120]\nmax_depth = range(1,30)\n\n\n\nparameters = {'n_estimators':n_estimators, \n         'max_depth':max_depth, \n        }\nrandomforest_grid = GridSearchCV(randomforest,\n                                 param_grid=parameters,\n                                 cv=StratifiedKFold(n_splits=20, random_state=15, shuffle=True),\n                                 n_jobs = -1\n                                )","3f7393f1":"randomforest_grid.fit(train_x, train_y) ","effe920a":"randomforest_grid.score(test_x, test_y)","5439c2b6":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngradient = GradientBoostingClassifier()\ngradient.fit(train_x, train_y)\ny_pred = gradient.predict(test_x)\ngradient_accy = round(accuracy_score(y_pred, test_y), 3)\nprint(gradient_accy)","cf6c2668":"from xgboost import XGBClassifier\nXGBClassifier = XGBClassifier()\nXGBClassifier.fit(train_x, train_y)\ny_pred = XGBClassifier.predict(test_x)\nXGBClassifier_accy = round(accuracy_score(y_pred, test_y), 3)\nprint(XGBClassifier_accy)","80a02235":"from sklearn.ensemble import AdaBoostClassifier\nadaboost = AdaBoostClassifier()\nadaboost.fit(train_x, train_y)\ny_pred = adaboost.predict(test_x)\nadaboost_accy = round(accuracy_score(y_pred, test_y), 3)\nprint(adaboost_accy)","9a0d890d":"from sklearn.ensemble import ExtraTreesClassifier\nExtraTreesClassifier = ExtraTreesClassifier()\nExtraTreesClassifier.fit(train_x, train_y)\ny_pred = ExtraTreesClassifier.predict(test_x)\nextraTree_accy = round(accuracy_score(y_pred, test_y), 3)\nprint(extraTree_accy)","0484e28c":"from sklearn.gaussian_process import GaussianProcessClassifier\nGaussianProcessClassifier = GaussianProcessClassifier()\nGaussianProcessClassifier.fit(train_x, train_y)\ny_pred = GaussianProcessClassifier.predict(test_x)\ngau_pro_accy = round(accuracy_score(y_pred, test_y), 3)\nprint(gau_pro_accy)","a7cf40b2":"all_models = [GaussianProcessClassifier, gaussian, ExtraTreesClassifier, BaggingClassifier, XGBClassifier,knn_grid, knn,  dectree, gradient, randomforest, svc, logreg, logreg_grid  ]\n\nc = {}\nfor i in all_models:\n    a = i.predict(test_x)\n    b = accuracy_score(a, test_y)\n    c[i] = b","84a90aff":"MLA_columns = ['MLA Name','Accuracy_score']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\nrow_index = 0\nfor alg in all_models:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'Accuracy_score'] = c[alg]\n    row_index+=1\n\n\n  ","be14f48a":"MLA_compare.sort_values(by = ['Accuracy_score'], ascending = False, inplace = True)\nMLA_compare","4e960892":"import matplotlib.pyplot as plt\n\n#barplot \nsns.barplot(x='Accuracy_score', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https:\/\/matplotlib.org\/api\/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","f871b2a7":"test_prediction = (max(c, key=c.get)).predict(test_df)\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_passengerid,\n        \"Survived\": test_prediction\n    })\n\nsubmission.PassengerId = submission.PassengerId.astype(int)\nsubmission.Survived = submission.Survived.astype(int)\n\nsubmission.to_csv(\"titanic1_submission.csv\", index=False)","33245f1d":"#calculate the predicted survival rate\n\nsurvival = submission[submission.Survived == 0]\n\nsurvival_count =len(survival.index)\n\npassenger_count = len(submission.index)\n\nsurvival_rate = survival_count\/passenger_count\n\nsurvival_rate","8ad12c77":"## 4.2 Creating dummy variables","17ef1ac4":"## 6.3 Gaussian Naive Bayes","0457f033":"## 5.1 Separating dependent and independent variables","6b130b85":"## 1.1 Import packages","81e96a40":"## 3.2 Statistical Test for Correlation","79fecb07":"### 2.1.1 Gender and Survival","c108fc06":"#### Train info","339a5350":"### 4.1 Create New Feature","e633f0ff":"## 6.8 Gradient Boosting Classifier","37400452":"## 6.5 Decision Tree Classifier","d7346771":"We can assume that people travel with their relatives had lower chance of survivle. Thus, we can combine Parch\/SibSp to create new feature: whether the passenger is alone.","a176b46f":"### Farebin\n","155a89e0":"## 6.7 Random Forest Classifier","298aa973":"### 2.1.2 Passenger class and Survival","c6348cbe":"First, we have a look on data and get a feeling of how data looks like.","03ba509a":"Sex is the most important correlated feature with Survived(dependent variable) feature followed by Pclass.","6560ff7c":"There are two ways to deal with the null value:\n\n> -delete the rows with null value\n\n\n\n\n\n> -fill in the null value using mode (qualitative value), median\/mean\/mean+randomized standard deviation\n\n\nThe following columns have missing values:\n\n\n*   Cabin: Qualitative, lots of missing value. It may represent social level of passengers and could be assumed from fare. Thus, we can drop this column.\n*   Age: Quantitative, will be an important factor of survival because children have the prioity of boarding on lifeboat. We could use  Random Forest Regressor to impute missing value.\n*   Embarked: Qualitative, show where people embark. Can be filled in using mode.\n*   Fare: Qualitative, only one missing value, can be imputed using median.\n\n\n\n\n\n\n","4769fe8d":"## 2.2 Combined Feature Relations\nCombning more than two feature relations in a single graph to see whether we can group features with similar impact on target to improve model efficiency.","8d4985fc":"#### Grid Search on Logistic Regression\n","b3fccee0":"### 2.2.3 Parents\/Children","8f0cb3d9":"## 2.1 Visualization of a single feature","cb30944f":"### 2.2.4 Siblings","f12e09a2":"### 2.1.3 Fare and Survival","20d6f443":"### Correlation","7251027c":"# 2. Exploratory Data Analysis and Visualization of features","d7b20898":"## 6.6  Bagging Classifier","cdd10c5b":"# 7. Compare the accuracy of models","f98cde65":"#### XGBClassifier","5e4010f8":"## 6.2 K-Nearest Neighbor classifier (KNN)","4b81ee5f":"## 6.9 AdaBoost Classifier","1e3355be":"Squaring the correlation feature not only gives on positive correlations but also amplifies the relationships.","ecf9abd3":"After removing unnecessary columns, we should look at null value of each column because some models cannot handle null value. ","3be212e1":"## 5.2 Splitting the training data","93d7b316":"### 2.2.2 Sex, Fare and Age","6689b40a":"We don't need passengerID, Name and Ticket while predicting survival rate. \nHowever, we do need passenger ID to display the prediction result in the test data. \nThus, we conserve the passengerID in test data in a variable then drop those unnecessrary column in train and test data.","21c3085d":"### Agebin","9bcab75a":"## 1.3 Data Cleaning","740c91c7":"#### Observation:\nA great number of children were saved, showing that children and infants were the priority.","672b37a4":"# 3. Statistical Overview","caaac3a2":"### is_alone","a61c1a76":"### Positive Correlation Features:\nFare and Survived: 0.26\n\n### Negative Correlation Features:\nFare and Pclass: -0.6\nSex and Survived: -0.55\nPclass and Survived: -0.33","83756a2a":"## 3.1 Correlation Matrix and Heatmap","2b1c334e":"#### Observation\uff1a\nFemale passengers had higher survival rate than male passengers.","9c4b7e68":"# 5. Pre-Modeling Tasks","69024d6c":"<a href=\"https:\/\/colab.research.google.com\/github\/lanzizuan\/hello-world\/blob\/master\/Titanic.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","06bbabbe":"# 6. Modeling the Data\n\n\nWe will use following models to make the prediction\n\n*   Logistic Regression\n\n*   K-Nearest Neighbors(KNN)\n\n*   Gaussian Naive Bayes\n\n*   Support Vector Machines\n\n*   Decision Tree Classifier\n\n*   Bagging on Decision Tree Classifier\n\n*   Random Forest Classifier\n\n*   Gradient Boosting Classifier\n\n*   AdaBoost Classifier\n\n*   Extra Trees Classifier\n\n*   Gaussian Process Classifier\n\n\n\n\n\n\n\n\n\n\n\n\n","0932f045":"### 2.1.4 Age and Survival","12641111":"### Whether the mean difference between male and female survival rate are significant","018aefb2":"#### Observation:\nPassenger class indicates passenger's social-eco situation. The first class passengers had higher survival rate than passengers in second and third class.","295a6174":"Observation:\n\n\n*   Lots of passengers who bought the ticket under 100 dollars didn't survive.\n\n\n","2f315ff4":"### Title","73accfc3":"Observations:\n\n*  There are three outliers whose fare was over 500 dollars. We can delete them.\n*  Most passengers were with the fare lower than 100 dollars.","d1fe8ee7":"## 5.3 Feature scaling","123a1da9":"## 6.11 Gaussian Process Classifier","5a9c89b8":"The p-value is very small compared to our significance level( \u03b1 )of 0.05. Our observation sample is statistically significant. Therefore, our null hypothesis is ruled out, and our alternative hypothesis is valid, which is \"There is a significant difference in the survival rate between the male and female passengers.\"","b4141bd0":"## 6.1 Logistic Regression","0c99599f":"### Grid search on KNN classifier","427da4ef":"# 1. Preparation before model building","de971577":"## 1.2 Import Data ","fab0bd65":"### 2.2.1 Embarked, Sex and Age","4257486b":"### family_size","c3da6a50":"# 4. Feature Engineering","5920926b":"## 6.4 Support Vector Machines","6ae7b3f5":"## 6.10 Extra Trees Classifier"}}