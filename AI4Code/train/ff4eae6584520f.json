{"cell_type":{"0cb48209":"code","787886cc":"code","3fac47f5":"code","1be38d82":"code","6be3f0d5":"code","1e3e25a5":"code","07524580":"code","9a45c91e":"code","a0ac8d2b":"code","7ff0c061":"code","886f5702":"code","30838999":"code","4d23a7fc":"code","091377d9":"code","f67a49b4":"code","6d1e8c94":"code","56229d80":"code","b8ec0ee3":"code","d996efdc":"code","2cec35a3":"code","db012ac6":"code","02be6599":"code","9f9e7e7e":"markdown","023f98d0":"markdown","fad5a004":"markdown","8c4ff403":"markdown","1d891986":"markdown","daebddb5":"markdown"},"source":{"0cb48209":"ENV = 'kaggle'\nassert ENV in ['colab', 'kaggle']\n \nPHASE = 'train'\nassert PHASE in ['train', 'eval_oof', 'inference']","787886cc":"# !pip install transformers==4.5.1\n!pip install torch==1.9.0","3fac47f5":"!nvidia-smi","1be38d82":"if ENV=='colab':\n    from google.colab import drive\n    drive.mount('\/content\/drive')","6be3f0d5":"import os\nimport math\nimport random\nimport time\n \nimport numpy as np\nimport pandas as pd\n \nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n \nimport transformers\nfrom transformers import AdamW\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\nfrom transformers import get_cosine_schedule_with_warmup\n \nfrom sklearn.model_selection import KFold\n \nimport gc, json, pickle, shutil\ngc.enable()\n\nfrom tqdm.auto import tqdm\nfrom matplotlib import pyplot as plt","1e3e25a5":"print(transformers.__version__)","07524580":"print(torch.__version__)","9a45c91e":"def set_random_seed(random_seed):\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n\n    torch.manual_seed(random_seed)\n    torch.cuda.manual_seed(random_seed)\n    torch.cuda.manual_seed_all(random_seed)\n\n    torch.backends.cudnn.deterministic = True","a0ac8d2b":"class LitDataset(Dataset):\n    def __init__(self, df, inference_only=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        self.text = df.excerpt.tolist()\n        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n        \n        if not self.inference_only:\n            self.target = torch.tensor(df.target.values, dtype=torch.float32)\n            self.bins = torch.tensor(df.bins.values, dtype=torch.long)\n    \n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding = 'max_length',            \n            max_length = MAX_LEN,\n            truncation = True,\n            return_attention_mask = True,\n            return_token_type_ids = not NO_TOKEN_TYPE\n        )\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        out_dict = {'input_ids':input_ids, 'attention_mask':attention_mask}\n        \n        if not NO_TOKEN_TYPE:\n            out_dict['token_type_ids'] = torch.tensor(self.encoded['token_type_ids'][index])\n        \n        if sa_complex is not None:\n            if sa_complex == 'hdd':\n                with open(f'SelfAttComplex\/{str(index).zfill(4)}.pkl','rb') as f:\n                    out_dict['sa_complex'] = pickle.load(f)\n            else:\n                out_dict['sa_complex'] = sa_complex[index]\n\n        if not self.inference_only:\n            out_dict['target'] = self.target[index]\n            out_dict['bins'] = self.bins[index]\n\n        return out_dict","7ff0c061":"def SelfAttention_Complexity(df: pd.DataFrame, output_device):\n    pre_dataset = LitDataset(df, inference_only=True)\n    pre_loader = DataLoader(pre_dataset, batch_size=BATCH_SIZE,\n                            drop_last=False, shuffle=False)\n    \n    if output_device == 'hdd':\n        os.makedirs('SelfAttComplex', exist_ok=True)\n\n    cfg_update = {\"output_attentions\":True, \"hidden_dropout_prob\": 0.0,\n                  \"layer_norm_eps\": 1e-7}\n    if PHASE=='train':\n        config = AutoConfig.from_pretrained(MODEL_NAME)\n        config.update(cfg_update)\n        backbone = AutoModel.from_pretrained(MODEL_NAME, config=config).to(DEVICE)\n    elif PHASE=='eval_oof' or PHASE=='inference':\n        config = AutoConfig.from_pretrained(LOAD_BACKBONE_DIR)\n        config.update(cfg_update)\n        backbone = AutoModel.from_pretrained(LOAD_BACKBONE_DIR, config=config).to(DEVICE)\n\n    backbone.resize_token_embeddings(len(tokenizer))\n\n    output_sa_complex = []\n    backbone.eval()\n    idx = 0\n    with torch.no_grad():\n        for batch_num, dsargs in enumerate(tqdm(pre_loader)):\n\n            kwargs = {}\n            kwargs['input_ids'] = dsargs['input_ids'].to(DEVICE)\n            if not NO_TOKEN_TYPE:\n                kwargs['token_type_ids'] = dsargs['token_type_ids'].to(DEVICE)\n            kwargs['attention_mask'] = dsargs['attention_mask'].to(DEVICE)\n\n            if 't5' in MODEL_NAME.lower() and HAS_DECODER:\n                # shift to right\n                kwargs['decoder_input_ids'] = torch.cat([tokenizer.pad_token_id * torch.ones(kwargs['input_ids'].size(0), 1).long().to(DEVICE),\n                                                        kwargs['input_ids'][:,:-1]], dim=1)\n            \n            # self attention\n            output_backbone = backbone(**kwargs)\n            self_att = torch.stack(output_backbone.attentions, dim=1) #[batch, layer, head, seq, seq]\n            seq_len = self_att.size(-1)\n            self_att = self_att.view(self_att.size(0), -1, seq_len, seq_len) #[batch, layer*head, seq, seq]\n            self_att *= kwargs['attention_mask'].unsqueeze(1).unsqueeze(-1)\n\n            # self attention complexity\n            distance_from_diag = (torch.arange(seq_len).view(1, -1) - torch.arange(seq_len).view(-1, 1)) \/ (seq_len - 1)\n            distance_from_diag = distance_from_diag.to(DEVICE)\n            sa_complex = []\n            temp = self_att * distance_from_diag.unsqueeze(0).unsqueeze(1).clip(min=0)\n            temp = temp.sum(dim=-1) #[batch, layer*head, seq]\n            sa_complex.append(temp)\n            temp = self_att * distance_from_diag.unsqueeze(0).unsqueeze(1).clip(max=0).abs()\n            temp = temp.sum(dim=-1) #[batch, layer*head, seq]\n            sa_complex.append(temp)\n            sa_complex = torch.cat(sa_complex, dim=1).transpose(-2,-1) #[batch, seq, layer*head*2]\n\n            if output_device == 'hdd':\n                for batch_item in sa_complex:\n                    with open(f'SelfAttComplex\/{str(idx).zfill(4)}.pkl','wb') as f:\n                        pickle.dump(batch_item, f)\n                    idx += 1\n            else:\n                output_sa_complex.append(sa_complex)\n    \n    if output_device == 'hdd':\n        return 'hdd'\n    else:\n        output_sa_complex = torch.cat(output_sa_complex, dim=0)\n        return output_sa_complex.to(output_device)","886f5702":"class LitModel(nn.Module):\n    def __init__(self, benchmark_token=None, use_max_pooling=False, sa_complex_dim=0):\n        super().__init__()\n \n        self.benchmark_token = benchmark_token\n        self.use_max_pooling = use_max_pooling\n        self.sa_complex_dim = sa_complex_dim\n        \n        cfg_update = {\"output_hidden_states\":True, \"hidden_dropout_prob\": 0.0,\n                      \"layer_norm_eps\": 1e-7}\n        if PHASE=='train':\n            config = AutoConfig.from_pretrained(MODEL_NAME)\n            config.save_pretrained(f'{SAVE_DIR}\/backbone')\n            config.update(cfg_update)                       \n            self.backbone = AutoModel.from_pretrained(MODEL_NAME, config=config)\n            self.backbone.save_pretrained(f'{SAVE_DIR}\/backbone')\n        elif PHASE=='eval_oof' or PHASE=='inference':\n            config = AutoConfig.from_pretrained(LOAD_BACKBONE_DIR)\n            config.update(cfg_update)                       \n            self.backbone = AutoModel.from_pretrained(LOAD_BACKBONE_DIR, config=config)\n            \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(NUM_HIDDEN_LAYERS).view(-1, 1, 1, 1))\n \n        # Dropout layers\n        self.dropouts_regr = nn.ModuleList([\n            nn.Dropout(0.5) for _ in range(5)\n        ])\n        self.dropouts_clsi = nn.ModuleList([\n            nn.Dropout(0.5) for _ in range(5)\n        ])\n \n        if self.use_max_pooling:\n            num_pool = 2\n        else:\n            num_pool = 1\n        self.attention_layer_norm = nn.LayerNorm(HIDDEN_SIZE * num_pool + sa_complex_dim)\n        self.attention = nn.Sequential(            \n            nn.Linear(HIDDEN_SIZE * num_pool + sa_complex_dim, 512 * num_pool),            \n            nn.Tanh(),                       \n            nn.Linear(512 * num_pool, 1),\n            nn.Softmax(dim=1)\n            )        \n        self.head_regressor = nn.Linear(HIDDEN_SIZE * num_pool + sa_complex_dim, 1)\n        self.head_classifier = nn.Linear(HIDDEN_SIZE * num_pool + sa_complex_dim, NUM_BINS)                   \n \n    def forward(self, input_ids, token_type_ids, attention_mask, self_att_complex):\n\n        kwargs = {}\n        if self.benchmark_token is None:\n            kwargs['input_ids'] = input_ids\n            if not NO_TOKEN_TYPE:\n                kwargs['token_type_ids'] = token_type_ids\n            kwargs['attention_mask'] = attention_mask\n        else:\n            benchmark_input_ids, benchmark_token_type_ids, benchmark_attention_mask = self.benchmark_token\n            kwargs['input_ids'] = torch.cat((input_ids, benchmark_input_ids), dim = 0)\n            if not NO_TOKEN_TYPE:\n                kwargs['token_type_ids'] = torch.cat((token_type_ids, benchmark_token_type_ids), dim = 0)\n            kwargs['attention_mask'] = torch.cat((attention_mask, benchmark_attention_mask), dim = 0)\n\n        if 't5' in MODEL_NAME.lower() and HAS_DECODER:\n            # shift to right\n            kwargs['decoder_input_ids'] = torch.cat([tokenizer.pad_token_id * torch.ones(kwargs['input_ids'].size(0), 1).long().to(DEVICE),\n                                                     kwargs['input_ids'][:,:-1]], dim=1)\n        output_backbone = self.backbone(**kwargs)\n        \n        # Extract output\n        if HAS_DECODER:\n            hidden_states = output_backbone.encoder_hidden_states + output_backbone.decoder_hidden_states[1:]\n        else:\n            hidden_states = output_backbone.hidden_states\n \n        # Mean\/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = nn.functional.softmax(self.hidden_layer_weights, dim = 0)\n        output_backbone = torch.sum(hidden_states * layer_weight, dim = 0)\n        if self.use_max_pooling:\n            out_max, _ = torch.max(hidden_states, dim = 0)\n            output_backbone = torch.cat((output_backbone, out_max), dim = -1)\n        if self.sa_complex_dim != 0:\n            self_att_complex = torch.cat((self_att_complex, benchmark_sa_complex), dim = 0)\n            output_backbone = torch.cat((output_backbone, self_att_complex), dim = -1)\n        \n        output_backbone = self.attention_layer_norm(output_backbone)\n \n        # Attention Pooling\n        weights = self.attention(output_backbone)\n        context_vector = torch.sum(weights * output_backbone, dim=1)        \n \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_regr):\n            if i == 0:\n                output_regr = self.head_regressor(dropout(context_vector))\n                output_clsi = self.head_classifier(self.dropouts_clsi[i](context_vector))\n            else:\n                output_regr += self.head_regressor(dropout(context_vector))\n                output_clsi += self.head_classifier(self.dropouts_clsi[i](context_vector))\n \n        output_regr \/= len(self.dropouts_regr)\n        output_clsi \/= len(self.dropouts_clsi)\n\n        if self.benchmark_token is not None:\n            output_regr = output_regr[:-1] - output_regr[-1]\n            output_clsi = output_clsi[:-1]\n\n        # Now we reduce the context vector to the prediction score.\n        return output_regr, nn.functional.softmax(output_clsi, dim=-1)","30838999":"class QuadraticWeightedKappaLoss(nn.Module):\n    def __init__(self, num_cat, device = 'cpu'):\n        super(QuadraticWeightedKappaLoss, self).__init__()\n        self.num_cat = num_cat\n        cats = torch.arange(num_cat).to(device)\n        self.weights = (cats.view(-1,1) - cats.view(1,-1)).pow(2) \/ (num_cat - 1)**2\n        \n    def _confusion_matrix(self, pred_smax, true_cat):\n        confusion_matrix = torch.zeros((self.num_cat, self.num_cat)).to(pred_smax.device)\n        for t, p in zip(true_cat.view(-1), pred_smax):\n            confusion_matrix[t.long()] += p\n        return confusion_matrix\n        \n    def forward(self, pred_smax, true_cat):\n        # Confusion matrix\n        O = self._confusion_matrix(pred_smax, true_cat)\n        \n        # Count elements in each category\n        true_hist = torch.bincount(true_cat, minlength = self.num_cat)\n        pred_hist = pred_smax.sum(dim = 0)\n        \n        # Expected values\n        E = torch.outer(true_hist, pred_hist)\n        \n        # Normlization\n        O = O \/ torch.sum(O)\n        E = E \/ torch.sum(E)\n        \n        # Weighted Kappa\n        numerator = torch.sum(self.weights * O)\n        denominator = torch.sum(self.weights * E)\n        \n        return COEF_QWK * numerator \/ denominator","4d23a7fc":"class BradleyTerryLoss(nn.Module):\n    def __init__(self):\n        super(BradleyTerryLoss, self).__init__()\n\n    def forward(self, pred_mean, true_mean):\n        batch_size = len(pred_mean)\n        true_comparison = true_mean.view(-1,1) - true_mean.view(1,-1)\n        pred_comparison = pred_mean.view(-1,1) - pred_mean.view(1,-1)\n        \n        return COEF_BT * (torch.log(1 + torch.tril(torch.exp(-true_comparison * pred_comparison))).sum()\n                          \/ (batch_size * (batch_size - 1) \/ 2))","091377d9":"def eval_mse(model, data_loader):\n    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n    model.eval()            \n    mse_sum = 0\n\n    all_pred_r = []\n    with torch.no_grad():\n        for batch_num, dsargs in enumerate(data_loader):\n            input_ids = dsargs['input_ids'].to(DEVICE)\n            attention_mask = dsargs['attention_mask'].to(DEVICE)\n            target = dsargs['target'].to(DEVICE)\n            bins = dsargs['bins'].to(DEVICE)\n\n            token_type_ids = None\n            if not NO_TOKEN_TYPE:\n                token_type_ids = dsargs['token_type_ids'].to(DEVICE)\n\n            self_att_complex = None\n            if USE_SELF_ATT:\n                self_att_complex = dsargs['sa_complex'].to(DEVICE)\n\n            pred_r, _ = model(input_ids, token_type_ids, attention_mask, self_att_complex)                       \n\n            mse_sum += nn.MSELoss(reduction=\"sum\")(pred_r.flatten(), target).item()\n            all_pred_r.append(pred_r)\n\n    return mse_sum \/ len(data_loader.dataset), torch.cat(all_pred_r, dim=0).squeeze()","f67a49b4":"def train(model, model_path, train_loader, val_loader,\n          optimizer, num_epochs, fold, scheduler=None):    \n    best_val_rmse = None\n    best_epoch = 0\n    step = 0\n    last_eval_step = 0\n    eval_period = EVAL_SCHEDULE[0][1]    \n\n    start = time.time()\n\n    history = {'step':[], 'epoch':[], 'batch_num':[], 'val_rmse':[],\n               'trn_rmse':[], 'trn_qwk':[], 'trn_bt':[]}\n    \n    for epoch in range(num_epochs):\n        val_rmse = None         \n\n        epoch_target, epoch_bins, epoch_pred_r, epoch_pred_c = (torch.tensor([]),)*4\n        epoch_bins = epoch_bins.long()\n    \n        for batch_num, dsargs in enumerate(train_loader):\n            input_ids = dsargs['input_ids'].to(DEVICE)\n            attention_mask = dsargs['attention_mask'].to(DEVICE)\n            target = dsargs['target'].to(DEVICE)\n            bins = dsargs['bins'].to(DEVICE)\n\n            token_type_ids = None\n            if not NO_TOKEN_TYPE:\n                token_type_ids = dsargs['token_type_ids'].to(DEVICE)\n\n            self_att_complex = None\n            if USE_SELF_ATT:\n                self_att_complex = dsargs['sa_complex'].to(DEVICE)\n\n            optimizer.zero_grad()\n            \n            model.train()\n\n            pred_r, pred_c = model(input_ids, token_type_ids, attention_mask, self_att_complex)\n                                                        \n            loss = (nn.MSELoss(reduction=\"mean\")(pred_r.flatten(), target)\n                    + QWKloss(pred_c, bins) + BTloss(pred_r.flatten(), target))\n                        \n            loss.backward()\n            \n            epoch_target = torch.cat([epoch_target.to(DEVICE), target.clone().detach()], dim=0)\n            epoch_bins = torch.cat([epoch_bins.to(DEVICE), bins.clone().detach()], dim=0)\n            epoch_pred_r = torch.cat([epoch_pred_r.to(DEVICE), pred_r.clone().detach()], dim=0)\n            epoch_pred_c = torch.cat([epoch_pred_c.to(DEVICE), pred_c.clone().detach()], dim=0)\n\n            optimizer.step()\n            if scheduler:\n                scheduler.step()\n            \n            if step >= last_eval_step + eval_period:\n                # Evaluate the model on val_loader.\n                elapsed_seconds = time.time() - start\n                num_steps = step - last_eval_step\n                print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n                last_eval_step = step\n                \n                mse, _ = eval_mse(model, val_loader)\n                val_rmse = math.sqrt(mse)\n                trn_rmse = nn.MSELoss(reduction=\"mean\")(epoch_pred_r.flatten(), epoch_target).item()\n                trn_qwk  = QWKloss(epoch_pred_c, epoch_bins).item()\n                trn_bt  = BTloss(epoch_pred_r.flatten(), epoch_target).item()\n\n                print(f\"Epoch: {epoch} batch_num: {batch_num}\", \n                      f\"val_rmse: {val_rmse:0.4}\", f\"train_rmse: {trn_rmse:0.4}\",\n                      f\"train_qwk: {trn_qwk:0.4}\", f\"train_bt: {trn_bt:0.4}\")\n\n                for rmse, period in EVAL_SCHEDULE:\n                    if val_rmse >= rmse:\n                        eval_period = period\n                        break\n                percent = step \/ (num_epochs * len(train_loader))\n                if 0.5 <= percent and percent <= 0.8:\n                    eval_period = min([eval_period, 8])\n                \n                if not best_val_rmse or val_rmse < best_val_rmse:                    \n                    best_val_rmse = val_rmse\n                    best_epoch = epoch\n                    torch.save(model.state_dict(), model_path)\n                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n                else:       \n                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n                          f\"(from epoch {best_epoch})\")\n\n                ''' history json dump '''\n                history['step'].append(step)\n                history['epoch'].append(epoch)\n                history['batch_num'].append(batch_num)\n                history['val_rmse'].append(val_rmse)\n                history['trn_rmse'].append(trn_rmse)\n                history['trn_qwk'].append(trn_qwk)\n                history['trn_bt'].append(trn_bt)\n                with open(f'{SAVE_DIR}\/{MODEL_VER}_fold{fold+1}_history.json', 'w') as f:\n                    json.dump(history, f, indent=4)\n                    \n                start = time.time()\n                                            \n            step += 1\n\n        del epoch_target, epoch_bins, epoch_pred_r, epoch_pred_c\n        \n        print('\\nHidden Layer Weights:')\n        print(model.hidden_layer_weights.squeeze())\n        print(nn.functional.softmax(model.hidden_layer_weights.squeeze(),dim=0))\n    \n    return best_val_rmse","6d1e8c94":"def predict(model, data_loader):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    \n    with torch.no_grad():\n        for batch_num, dsargs in enumerate(data_loader):\n            input_ids = dsargs['input_ids'].to(DEVICE)\n            attention_mask = dsargs['attention_mask'].to(DEVICE)\n\n            token_type_ids = None\n            if not NO_TOKEN_TYPE:\n                token_type_ids = dsargs['token_type_ids'].to(DEVICE)\n\n            self_att_complex = None\n            if USE_SELF_ATT:\n                self_att_complex = dsargs['sa_complex'].to(DEVICE)\n                        \n            pred_r, _ = model(input_ids, token_type_ids, attention_mask, self_att_complex)                        \n\n            result[index : index + pred_r.shape[0]] = pred_r.flatten().to(\"cpu\")\n            index += pred_r.shape[0]\n\n    return result","56229d80":"def create_optimizer(model):\n    named_parameters = list(model.named_parameters())\n    \n    backbone_parameters = [(n, p) for n, p in named_parameters if n.startswith('backbone')]\n    attention_parameters = [(n, p) for n, p in named_parameters if n.startswith('attention')]\n    hidden_wts_parameters = [(n, p) for n, p in named_parameters if n.startswith ('hidden_layer_weights')]\n    head_parameters = [(n, p) for n, p in named_parameters if n.startswith('head')]\n        \n    attention_group = [params for (name, params) in attention_parameters]\n    hidden_wts_group = [params for (name, params) in hidden_wts_parameters]\n    head_group = [params for (name, params) in head_parameters]\n \n    parameters = []\n    parameters.append({\"params\": attention_group})\n    parameters.append({\"params\": hidden_wts_group, 'weight_decay': 0.0, 'lr': HIDDEN_WTS_LR})\n    parameters.append({\"params\": head_group})\n \n    no_decay = ['bias', 'LayerNorm.weight', 'layer_norm']\n \n    if 'roberta' in MODEL_NAME.lower() or 'electra' in MODEL_NAME.lower():\n        layers = [getattr(model, 'backbone').embeddings] + list(getattr(model, 'backbone').encoder.layer)\n    elif 'gpt2' in MODEL_NAME.lower():\n        layers = [getattr(model, 'backbone').wte] + list(getattr(model, 'backbone').h)\n    elif 'xlnet' in MODEL_NAME.lower():\n        layers = [getattr(model, 'backbone').word_embedding] + list(getattr(model, 'backbone').layer)\n    elif 'bart' in MODEL_NAME.lower():\n        enc_layers = ([getattr(model, 'backbone').encoder.embed_positions] +\n                      list(getattr(model, 'backbone').encoder.layers) +\n                      [getattr(model, 'backbone').encoder.layernorm_embedding])\n        dec_layers = ([getattr(model, 'backbone').decoder.embed_positions] +\n                      list(getattr(model, 'backbone').decoder.layers) + \n                      [getattr(model, 'backbone').decoder.layernorm_embedding])\n        assert len(enc_layers)==len(dec_layers)\n        layers = [getattr(model, 'backbone').shared]\n        for e, d in zip(enc_layers, dec_layers):\n            layers += [e, d]\n    elif 't5' in MODEL_NAME.lower():\n        enc_layers = (list(getattr(model, 'backbone').encoder.block) +\n                      [getattr(model, 'backbone').encoder.final_layer_norm])\n        dec_layers = (list(getattr(model, 'backbone').decoder.block) + \n                      [getattr(model, 'backbone').decoder.final_layer_norm])\n        assert len(enc_layers)==len(dec_layers)\n        layers = [getattr(model, 'backbone').shared]\n        for e, d in zip(enc_layers, dec_layers):\n            layers += [e, d]\n    else:\n        raise RuntimeError('specify the parameters for backbone.')\n \n    layers.reverse()\n    layerwise_learning_rate_decay = LAYERWISE_LR_DECAY**(1.0\/len(layers))\n    lr = BACKBONE_LR\n    for i, layer in enumerate(layers):\n        lr *= layerwise_learning_rate_decay\n        parameters += [\n            {\n                'params': [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n                'weight_decay': 0.01,\n                'lr': lr,\n            },\n            {\n                'params': [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n                'weight_decay': 0.0,\n                'lr': lr,\n            },\n        ]\n \n    return AdamW(parameters)","b8ec0ee3":"def convert_examples_to_features(text, tokenizer, max_len, is_test = False, return_tensor = False):\n    # Take from https:\/\/www.kaggle.com\/rhtsingh\/commonlit-readability-prize-roberta-torch-fit\n    text = text.replace('\\n', '')\n    if return_tensor:\n        tok = tokenizer.encode_plus(\n            text, \n            max_length = max_len, \n            padding = 'max_length', \n            return_tensors = 'pt',\n            truncation = True,\n            return_attention_mask = True,\n            return_token_type_ids = not NO_TOKEN_TYPE\n        )\n    else:\n        tok = tokenizer.encode_plus(\n            text, \n            max_length = max_len, \n            padding = 'max_length', \n            truncation = True,\n            return_attention_mask = True,\n            return_token_type_ids = not NO_TOKEN_TYPE\n        )\n    return tok","d996efdc":"def Train_or_Validation():\n    list_val_rmse = []\n \n    oof = []\n    for fold in range(NUM_FOLDS):\n        print(f\"\\nFold {fold + 1}\/{NUM_FOLDS}\")\n            \n        set_random_seed(SEED + fold)\n        \n        train_dataset = LitDataset(train_df[train_df['kfold'] != fold])\n        val_dataset = LitDataset(train_df[train_df['kfold'] == fold])\n        val_df = train_df[train_df['kfold'] == fold].copy()\n            \n        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                                  drop_last=True, shuffle=True, num_workers=0)    \n        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n                                drop_last=False, shuffle=False, num_workers=0)    \n        \n        sa_complex_dim = 0\n        if USE_SELF_ATT:\n            sa_complex_dim = benchmark_sa_complex.size(-1)\n        \n        model = LitModel(benchmark_token = benchmark_token, use_max_pooling = USE_MAX_POOLING,\n                         sa_complex_dim = sa_complex_dim).to(DEVICE)\n        \n        # Update vocabulary size\n        model.backbone.resize_token_embeddings(len(tokenizer))\n \n        if PHASE=='train':\n            model_path = f\"{SAVE_DIR}\/model_{fold + 1}.bin\"\n            set_random_seed(SEED + fold)    \n \n            optimizer = create_optimizer(model)                        \n            scheduler = get_cosine_schedule_with_warmup(\n                optimizer,\n                num_training_steps = NUM_EPOCHS * len(train_loader) * 11\/\/10,\n                num_warmup_steps = 50)\n            \n            list_val_rmse.append(train(model, model_path, train_loader, val_loader, optimizer, \n                                       num_epochs=NUM_EPOCHS, fold=fold, scheduler=scheduler, ))\n        \n        elif PHASE=='eval_oof':\n            model_path = f\"{MODEL_DIR}\/model_{fold + 1}.bin\"\n            model.load_state_dict(torch.load(model_path))\n            model.to(DEVICE)\n            \n            mse, pred_r = eval_mse(model, val_loader)\n            val_df['pred'] = pred_r.to('cpu').detach().numpy().copy()\n            oof.append(val_df)\n            list_val_rmse.append(math.sqrt(mse))\n \n        del model\n        gc.collect()\n        \n        print(\"\\nPerformance estimates:\")\n        print(list_val_rmse)\n        print(\"Mean:\", np.array(list_val_rmse).mean())\n\n    if PHASE=='eval_oof':\n        oof = pd.concat(oof)\n\n    return oof","2cec35a3":"def Inference():\n    all_predictions = np.zeros((NUM_FOLDS, len(test_df)))\n\n    test_dataset = LitDataset(test_df, inference_only=True)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                             drop_last=False, shuffle=False, num_workers=0)\n\n    for fold in range(NUM_FOLDS):            \n\n        sa_complex_dim = 0\n        if USE_SELF_ATT:\n            sa_complex_dim = benchmark_sa_complex.size(-1)\n\n        model = LitModel(benchmark_token = benchmark_token, use_max_pooling = USE_MAX_POOLING,\n                         sa_complex_dim = sa_complex_dim).to(DEVICE)\n\n        # Update vocabulary size\n        model.backbone.resize_token_embeddings(len(tokenizer))\n\n        model_path = f\"{MODEL_DIR}\/model_{fold + 1}.bin\"\n        print(f\"\\nUsing {model_path}\")\n                            \n        model.load_state_dict(torch.load(model_path))    \n        \n        all_predictions[fold] = predict(model, test_loader)\n        \n        del model\n        gc.collect()\n\n    predictions = all_predictions.mean(axis=0)\n    output_df = submission_df.copy()\n    output_df.target = predictions\n    print(output_df)\n\n    return output_df","db012ac6":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nif ENV=='colab':\n    BASE_DIR = '\/content\/drive\/MyDrive\/Colab Notebooks\/CLR\/input'\n    TRAIN_DATA_DIR = BASE_DIR\nelif ENV=='kaggle':\n    BASE_DIR = '..\/input\/commonlitreadabilityprize'\n    TRAIN_DATA_DIR = '..\/input\/step-1-create-folds'\n\ntrain_df = pd.read_csv(f'{TRAIN_DATA_DIR}\/train_folds.csv')\nbenchmark = train_df[(train_df.target == 0) & (train_df.standard_error == 0)].copy()\ntrain_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n              inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\ntest_df = pd.read_csv(f\"{BASE_DIR}\/test.csv\")\nsubmission_df = pd.read_csv(f\"{BASE_DIR}\/sample_submission.csv\")","02be6599":"SEED = 1000\nNUM_FOLDS = 5\nNUM_EPOCHS = 4\nBATCH_SIZE = 8\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.52, 32), (0.49, 16), (0.48, 8), (0.47, 4), (-1., 2)]\nMODEL_NAME = 'roberta-large'\nMODEL_VER = 'CLRP_LightBase_031s_RoBERTaL'\n \nNUM_HIDDEN_LAYERS = 24\nHIDDEN_SIZE = 1024\nNUM_BINS = 29\nCOEF_QWK = 0.0 # coefficient of QWK loss\nCOEF_BT = 1.0 # coefficient of Bradley-Terry loss\n\nUSE_MAX_POOLING = True\nUSE_SELF_ATT = True\nNO_TOKEN_TYPE = False\nHAS_DECODER = False\n\nBACKBONE_LR = 2e-5\nHIDDEN_WTS_LR = 1e-2\nLAYERWISE_LR_DECAY = 0.1\n\nif ENV=='colab':\n    MODEL_DIR = f'\/content\/drive\/MyDrive\/Colab Notebooks\/CLR\/{MODEL_VER}'\n    SAVE_DIR = MODEL_DIR\n    LOAD_BACKBONE_DIR = f'{MODEL_DIR}\/backbone'\nelif ENV=='kaggle':\n    MODEL_DIR = '..\/input\/clrp-lightbase-031s-robertal-dat'\n    SAVE_DIR = '.'\n    LOAD_BACKBONE_DIR = '..\/input\/robertalarge'\n\nQWKloss = QuadraticWeightedKappaLoss(num_cat=NUM_BINS, device=DEVICE)\nBTloss = BradleyTerryLoss()\ntrain_df['bins'] = pd.cut(train_df['target'], bins=NUM_BINS, labels=False)\n\n# Setup Tokenizer\nif PHASE=='train':\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    tokenizer.save_pretrained(f'{SAVE_DIR}\/backbone')\nelif PHASE=='eval_oof' or PHASE=='inference':\n    tokenizer = AutoTokenizer.from_pretrained(LOAD_BACKBONE_DIR)\nif 'gpt2' in MODEL_NAME.lower():\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\n# Tokenize the benchmark text\nbenchmark_token = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer, MAX_LEN, return_tensor = True)\nif NO_TOKEN_TYPE:\n    benchmark_token = (benchmark_token['input_ids'].to(DEVICE), None, benchmark_token['attention_mask'].to(DEVICE))\nelse:\n    benchmark_token = (benchmark_token['input_ids'].to(DEVICE), benchmark_token['token_type_ids'].to(DEVICE), benchmark_token['attention_mask'].to(DEVICE))\n\n# Main\nif PHASE=='train' or PHASE=='eval_oof':\n    sa_complex = None # Self-Attention Complexity in Pretrained Model\n    if USE_SELF_ATT:\n        sa_complex = SelfAttention_Complexity(train_df, 'cpu')\n        benchmark_sa_complex = SelfAttention_Complexity(benchmark, DEVICE)\n    oof_df = Train_or_Validation()\n\nif PHASE=='eval_oof':\n    oof_df.to_csv(f'oof_{MODEL_VER}.csv')\n\nif PHASE=='inference':\n    sa_complex = None # Self-Attention Complexity in Pretrained Model\n    if USE_SELF_ATT:\n        sa_complex = SelfAttention_Complexity(test_df, 'cpu')\n        benchmark_sa_complex = SelfAttention_Complexity(benchmark, DEVICE)\n    submission_df = Inference()\n    submission_df.to_csv(\"submission.csv\", index=False)\n\nif os.path.isdir('SelfAttComplex'):\n    shutil.rmtree('SelfAttComplex')","9f9e7e7e":"# loss function","023f98d0":"# Training, Validation","fad5a004":"# Main","8c4ff403":"# Dataset","1d891986":"# Self Attention Complexity in Pretrained Model","daebddb5":"# Model\nThe model is inspired by the one from [Maunish](https:\/\/www.kaggle.com\/maunish\/clrp-roberta-svm)."}}