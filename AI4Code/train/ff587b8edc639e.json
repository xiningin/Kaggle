{"cell_type":{"0b4907b2":"code","70c43cd5":"code","f169f29e":"code","bf89fe72":"code","568f5df4":"code","cec588d3":"code","ba926c00":"code","25e8d2f4":"code","42e9b82f":"code","e9055752":"code","42316acb":"code","3d3b30d6":"code","d1108e52":"code","8666083f":"code","567c9dd1":"code","6901953f":"code","bf91538d":"code","b374aee7":"code","4a44ec1b":"code","cae72be6":"markdown","3be1012a":"markdown","207c9b62":"markdown","401a0e73":"markdown","415f15f1":"markdown","8e8ab194":"markdown","11a998af":"markdown","b67fac78":"markdown","900c3a00":"markdown","cb3630f3":"markdown","46ae3a0e":"markdown","95530d9d":"markdown","edea2e07":"markdown","6cdc965d":"markdown","7be6f2fb":"markdown","516a1d3c":"markdown","e49c85d2":"markdown","e1396257":"markdown","46ad5aa4":"markdown"},"source":{"0b4907b2":"%matplotlib inline\n%load_ext autoreload\n%autoreload 2\n%config InlineBackend.figure_format = 'retina'\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns ","70c43cd5":"data_path = '..\/input\/Admission_Predict_Ver1.1.csv'\n\nadmissions = pd.read_csv(data_path)","f169f29e":"admissions.head()","bf89fe72":"admissions.describe()","568f5df4":"fig,ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(admissions.corr(), ax=ax, annot=True, linewidths=0.05, fmt= '.2f',cmap=\"magma\")\nplt.show()\n","cec588d3":"cols=admissions.drop(labels='Serial No.',axis=1)\nsns.pairplot(data=cols,hue='Research')","ba926c00":"y = np.array([admissions[\"TOEFL Score\"].min(),admissions[\"TOEFL Score\"].mean(),admissions[\"TOEFL Score\"].max()])\nx = [\"Lowest\",\"Mean\",\"Highest\"]\nplt.bar(x,y)\nplt.title(\"TOEFL Scores\")\nplt.xlabel(\"Level\")\nplt.ylabel(\"TOEFL Score\")\nplt.show()","25e8d2f4":"admissions[\"GRE Score\"].plot(kind = 'hist',bins = 200,figsize = (6,6))\nplt.title(\"GRE Scores\")\nplt.xlabel(\"GRE Score\")\nplt.ylabel(\"Frequency\")\nplt.show()","42e9b82f":"admissions.plot(kind='scatter', x='University Rating', y='CGPA')","e9055752":"s = admissions[admissions[\"Chance of Admit \"] >= 0.75][\"University Rating\"].value_counts()\nplt.title(\"University Ratings of Candidates with a 75% acceptance chance\")\ns.plot(kind='bar',figsize=(20, 10))\nplt.xlabel(\"University Rating\")\nplt.ylabel(\"Candidates\")\nplt.show()","42316acb":"fig = plt.figure(figsize = (20, 25))\nj = 0\nfor i in admissions.columns:\n    plt.subplot(6, 4, j+1)\n    j += 1\n    sns.distplot(admissions[i][admissions['Chance of Admit ']<0.72], color='r', label = 'Not Got Admission')\n    sns.distplot(admissions[i][admissions['Chance of Admit ']>0.72], color='g', label = 'Got Admission')\n    plt.legend(loc='best')\nfig.suptitle('Admission Chance In University ')\nfig.tight_layout()\nfig.subplots_adjust(top=0.95)\nplt.show()","3d3b30d6":"dummy_fields = ['University Rating', 'Research']\none_hot_admissions = admissions[:]\nfor each in dummy_fields:\n    dummies = pd.get_dummies(one_hot_admissions[each], prefix=each, drop_first=False)\n    one_hot_admissions = pd.concat([one_hot_admissions, dummies], axis=1)\n\nto_be_dropped = ['University Rating', 'Research', 'Serial No.']\none_hot_admissions = one_hot_admissions.drop(to_be_dropped, axis=1)\none_hot_admissions.head()","d1108e52":"\nprocessed_data = one_hot_admissions[:]\n\nprocessed_data = processed_data\/processed_data.max()\n#processed_data = (processed_data - np.min(processed_data)) \/ (np.max(processed_data) - np.min(processed_data))\n","8666083f":"train_features = processed_data.drop('Chance of Admit ', axis=1)\ntrain_targets = processed_data['Chance of Admit '].values\n\n###This is another option####\nfrom sklearn.model_selection import train_test_split\ntrain_features,test_features,train_targets,test_targets = train_test_split(train_features,train_targets,test_size = 0.20,random_state = 42)\n\n","567c9dd1":"# Imports\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.optimizers import SGD\nfrom keras.utils import np_utils\n\n# Building the model\nmodel = Sequential()\nmodel.add(Dense(64, activation='relu', input_shape=(train_features.shape[1],)))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compiling the model\nmodel.compile(loss = 'mean_squared_error', optimizer='adam', metrics=['mse', 'mae'])\nmodel.summary()","6901953f":"# Training the model\nhistory = model.fit(train_features, train_targets, validation_split=0.2, epochs=100, batch_size=8, verbose=0)","bf91538d":"#print(vars(history))\nplt.plot(history.history['loss'])\n\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","b374aee7":"# Evaluating the model on the training and testing set\nscore = model.evaluate(train_features, train_targets)\nprint(\"score: \", score)\nprint(\"\\n Training Accuracy:\", score)\nscore = model.evaluate(test_features, test_targets)\nprint(\"score: \", score)\nprint(\"\\n Testing Accuracy:\", score)","4a44ec1b":"y_pred = model.predict(test_features)\nplt.plot(test_targets)\nplt.plot(y_pred)\nplt.title('Prediction')","cae72be6":"### Splitting the data into training, testing, and validation sets\n\nWe'll save the data for the last approximately 10% to use as a test set after we've trained the network. We'll use this set to make predictions and compare them with the actual percentage of admissions.","3be1012a":"### One hot encoding\nHere we have some categorical variables like University Rankig and Research. To include these in our model, we'll need to make binary dummy variables (or do one-hot encoding). This is simple to do with Pandas thanks to `get_dummies()`.","207c9b62":"## Load and prepare the data\n\nA critical step in working with neural networks is preparing the data correctly. Variables on different scales make it difficult for the network to efficiently learn the correct weights. Below, we've written the code to load and prepare the data.","401a0e73":"### Analyzing GRE Scores\n\nAnalyzing the spread of GRE scores of students","415f15f1":"## Evaluating the model\n","8e8ab194":"### Pair plot  \n\nThe below pair plot indicates that GRE score TOEFL score and CGPA all are linearly related to each other Also it can be inferred that Research Students tend to Score higher by all means","11a998af":"## Training the model","b67fac78":"### Analyzing CGPA\n\nFrom the below chart, we could understand that the students coming from higher ranked universities have better GPAs","900c3a00":"## Initial Analysis\n\nWe would do the following in the notebook\n\n1. Load and prepare the data \n2. Checking out the data with correlation table and plot tables to see which features add more value\n3. Analyze various features with graphs to understand which are important and which can be neglected\n4. Normalizing the data and getting training and test data\n5. Coming up with a tuned model model that would predict the next input","cb3630f3":"### Analyzing TOEFL Scores","46ae3a0e":"### Candidates who graduate from good universities have higher percentage of admissions","95530d9d":"### Correlation Table\n\nLet us draw a correlation table to find out which are the important parameters to consider.\nHere we can see that the chance of admit is highly correlated with CGPA, GRE and TOEFEL scores are also correlated.","edea2e07":"# Introduction\n\nThis notebook is built to help students determine if they are eligible for an admission in their college preferred.\n\nP.S. Some of the graphs are other people's ideas. I have put together things that I think is intuitive","6cdc965d":"### Libaries that are used","7be6f2fb":"Analyzing the lowest, highest and mean TOEFL scores scored by the students","516a1d3c":"### Normalizing the variables\nWe could normalize variables like GRE, TOEFL, SOP, LOR and CGPA","e49c85d2":"## Checking out the data\nThis dataset has the admission percentage for a university based on various factors like GRE Score, TOEFL Score, University Rankig, SOP, LOR, CGPA and Research. \nBelow is a plot showing the number of students getting admitted just basedon the university ranking","e1396257":"## Prediction vs original labels","46ad5aa4":"## Finally we have prepared our data. Now it's time to train it with neural nets !!! "}}