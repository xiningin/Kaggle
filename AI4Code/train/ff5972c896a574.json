{"cell_type":{"155eb55a":"code","9d155818":"code","a9a5ae68":"code","52d009bf":"code","2049f4c4":"code","91d097c7":"code","684932bb":"code","0d9ed5fb":"code","12cec623":"code","ee5e4938":"code","a222513e":"code","a0d00860":"code","5c78e15e":"code","7ef7a0dc":"code","8863fdea":"code","eb107746":"code","fe1f27bb":"code","1f5b5fc1":"code","bfd4bc1f":"code","eaf60e8b":"code","abdd026e":"code","ae706e8a":"code","07be0c1c":"code","cf80c691":"code","f23cd711":"code","8f1e3c63":"code","dbded122":"markdown","61e3f757":"markdown","b6cc07d4":"markdown"},"source":{"155eb55a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9d155818":"df = pd.read_csv('..\/input\/amazon-alexa-reviews\/amazon_alexa.tsv', delimiter='\\t')\ndf.head()","a9a5ae68":"df.info()","52d009bf":"np.bincount(df['feedback'])\n# looks like a pretty imbalanced classes","2049f4c4":"df.describe()\n# looks like the positive feedback: 1 is the predominant class, mostly around a rating of 5","91d097c7":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.countplot(\"rating\", hue=\"feedback\", data=df)\n# it is quite expected that a rating under 3 to be a negative one.","684932bb":"plt.figure(figsize=(40,8))\nsns.countplot(\"variation\", hue=\"feedback\", data=df)\n# looks like Charcoal Fabric and Black Dot are the most bought \/ rated","0d9ed5fb":"plt.figure(figsize=(60,8))\nsns.boxplot('date', 'rating', hue=\"feedback\", data=df)\n# During May and June mostly positive feedback, while starting with July predominantly negative.","12cec623":"df.isna().sum()\n# now we can proceed with NLP.","ee5e4938":"# will show some love to sklearn, a library not that much used for NLP\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer() # will try stop words and apply other improvements later\nvect.fit(df.verified_reviews)","a222513e":"print(f\"Vocabulary size: {len(vect.vocabulary_)}\") \nprint(f\"Vocabulary content:\\n {vect.vocabulary_}\")","a0d00860":"X = vect.transform(df.verified_reviews) \ny = df.feedback","5c78e15e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                test_size=0.33, random_state=42, stratify=y) # stratify: class in-balance in the data would be preserved.","7ef7a0dc":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nscores = cross_val_score(LogisticRegression(solver='lbfgs'), X_train, y_train, cv=5) \nprint(f\"Mean cross-validation accuracy: {np.mean(scores):.2f}\")","8863fdea":"from sklearn.model_selection import GridSearchCV\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\ngrid = GridSearchCV(LogisticRegression(solver='lbfgs'), param_grid, cv=5) \ngrid.fit(X_train, y_train)\nprint(f\"Best cross-validation score: {grid.best_score_:.2f}\") \nprint(f\"Best parameters: {grid.best_params_}\")","eb107746":"print(f\"Test score is: {grid.score(X_test, y_test):.2f}\")","fe1f27bb":"y_pred = grid.predict(X_test)\nfrom sklearn.metrics import classification_report, confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\ncr = classification_report(y_test, y_pred)\nprint(cr)","1f5b5fc1":"LR = LogisticRegression(C=1, random_state=9, solver='lbfgs')\nLR.fit(X_train, y_train)\n\nfeatures = vect.get_feature_names()\nfeature_importance = abs(LR.coef_[0])\n#feature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nindices = np.argsort(feature_importance)\n\nplt.figure(figsize=(12,600))\n#sns.set(rc={'figure.figsize':(6,600)})\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), feature_importance[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()\n\n# positive words as: love, great and \n# negative words as: poor, awful have top importance","bfd4bc1f":"# vect.get_params().keys()","eaf60e8b":"# just curious if removing the stopwords and \n# cutting on uninformative features and\n# adding pair of tokens(which increases the features no) improves the score:\n\nvect = CountVectorizer(min_df=5, ngram_range=(2, 2), stop_words=\"english\").fit(df.verified_reviews) \n\nX = vect.transform(df.verified_reviews)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                test_size=0.33, random_state=42, stratify=y)\ngrid = GridSearchCV(LogisticRegression(solver='lbfgs'), param_grid, cv=5) \ngrid.fit(X_train, y_train)\nprint(f\"Best cross-validation score: {grid.best_score_:.2f}\") \nprint(f\"Best parameters: {grid.best_params_}\")","abdd026e":"# rescaling with tf-idf (instead of dropping features):\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\ntfidf = TfidfVectorizer(min_df=5, norm=None)\nLR = LogisticRegression(solver='lbfgs')\npipe = Pipeline([('tfidf', tfidf),('LR', LR)])\nparam_grid = {'LR__C': [0.001, 0.01, 0.1, 1, 10],\n             'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]}\ngrid = GridSearchCV(pipe, param_grid, cv=5)","ae706e8a":"X = df.verified_reviews\ny = df.feedback","07be0c1c":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                test_size=0.33, random_state=42, stratify=y)\ngrid.fit(X_train, y_train)\nprint(f\"Best cross-validation score: {grid.best_score_:.2f}\")\nprint(f\"Best parameters: {grid.best_params_}\")","cf80c691":"print(grid.score(X_test, y_test))\ny_pred = grid.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\ncr = classification_report(y_test, y_pred)\nprint(cr)","f23cd711":"import spacy\nT = X[2]\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp(T)\n\nfor token in doc:\n    token_text = token.text\n    token_pos = token.pos_\n    token_dep = token.dep_\n    print(f'{token_text:<12} {token_pos:<10} {token_dep:<10}')\nfor ent in doc.ents:\n    print(\"\\n\", ent.text, ent.label_)","8f1e3c63":"from spacy.matcher import Matcher\nmatcher = Matcher(nlp.vocab)\npattern = [ {'LEMMA': 'love', 'POS': 'VERB'},\n            { 'POS': 'DET', 'OP': '?'},\n            {'POS': 'NOUN'} ]\nmatcher.add('Positive Reviews incl Love', None, pattern)\nfor doc in nlp.pipe(X):\n    #doc = nlp(X[i]) # unsing nlp.pipe for more efficient text processing\n    matches = matcher(doc)\n    for match_id, start, end in matches:\n        matched_span = doc[start:end]\n        print(matched_span.text)","dbded122":"Hi everyone and welcome to my take on Amazon's Alexa reviews.\n\nEDA: will have a look at the data, maybe we can spot some trends, insights.\n\nNLP: tokenization, vocabulary building, encoding.\n\nML: picking a model and compare some results.","61e3f757":"This is it for now. Hope you've enjoyed it. Thanks!","b6cc07d4":"Now let's have a look at spacy:"}}