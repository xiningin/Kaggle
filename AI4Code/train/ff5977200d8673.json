{"cell_type":{"f7767720":"code","0c71b938":"code","76076e3b":"code","e0ef8ebd":"code","1ed400d4":"code","3fb0ee46":"code","6bd8adb6":"code","c2d4f811":"code","38cd3532":"code","447636cd":"code","8077e57e":"code","53f8b9eb":"code","a03aa6c9":"code","18871bab":"code","6c9d50c6":"code","f6ce9ab9":"code","a5db99f3":"code","6ac89582":"code","242443d9":"markdown","e4e2731a":"markdown","a7d199cf":"markdown","c2232f93":"markdown","bd24b157":"markdown","b87b5e25":"markdown","5b732c2f":"markdown","5b02f0d4":"markdown","21bf8b36":"markdown","cca9af39":"markdown","6fea2496":"markdown","f3e8f55d":"markdown"},"source":{"f7767720":"import numpy as np             # for algebric functions\nimport pandas as pd            # to handle dataframes\nimport os                      # to import files \n#!pip install transformers\nimport transformers            # Transformers (pytorch-transformers \/pytorch-pretrained-bert) provides general-purpose architectures (BERT, RoBERTa,..)\nimport tokenizers              # A tokenizer is in charge of preparing the inputs for a model. \nimport string                  \nimport torch                   # pytorch\nimport torch.nn as nn   \nfrom torch.nn import functional as F\nfrom tqdm import tqdm          # TQDM is a progress bar library\nimport re                      # regular expression\nimport json\nimport requests","0c71b938":"MAX_LEN = 192\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 8\nEPOCHS = 5\nROBERTA_PATH = 'roberta-base'","76076e3b":"pre_voc_file = transformers.RobertaTokenizer.pretrained_vocab_files_map\nmerges_file  = pre_voc_file.get('merges_file').get('roberta-base')\nvocab_file = pre_voc_file.get('vocab_file').get('roberta-base')\nmodel_bin = transformers.modeling_roberta.ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP.get('roberta-base')","e0ef8ebd":"#pre_voc_file","1ed400d4":"# Download Vocab file & Merge file\njson_f = requests.get(vocab_file)\ntxt_f = requests.get(merges_file)\nmod_bin = requests.get(model_bin)\n\ndata = json_f.json()\n#saving json vocab file\nwith open('vocab.json', 'w') as f:\n    json.dump(data, f)\n#saving merge.txt file\nopen('merge.txt', 'wb').write(txt_f.content)\nopen('model.bin', 'wb').write(mod_bin.content)","3fb0ee46":"TOKENIZER = tokenizers.ByteLevelBPETokenizer(vocab_file=f\"vocab.json\", \n                                             merges_file=f\"merge.txt\", \n                                             lowercase=True,\n                                             add_prefix_space=True)","6bd8adb6":"class TweetModel(transformers.BertPreTrainedModel):\n    def __init__(self, conf):\n        super(TweetModel, self).__init__(conf)\n        self.roberta = transformers.RobertaModel.from_pretrained(ROBERTA_PATH, config=conf)\n        self.drop_out = nn.Dropout(0.1)\n        self.l0 = nn.Linear(768 * 2, 2)\n        torch.nn.init.normal_(self.l0.weight, std=0.02)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, _, out = self.roberta(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n        out = torch.cat((out[-1], out[-2]), dim=-1)\n        out = self.drop_out(out)\n        logits = self.l0(out)\n\n        start_logits, end_logits = logits.split(1, dim=-1)\n\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits","c2d4f811":"def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n    tweet = \" \" + \" \".join(str(tweet).split())\n    selected_text = \" \" + \" \".join(str(selected_text).split())\n\n    len_st = len(selected_text) - 1\n    idx0 = None\n    idx1 = None\n\n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n        if \" \" + tweet[ind: ind+len_st] == selected_text:\n            idx0 = ind\n            idx1 = ind + len_st - 1\n            break\n\n    char_targets = [0] * len(tweet)\n    if idx0 != None and idx1 != None:\n        for ct in range(idx0, idx1 + 1):\n            char_targets[ct] = 1\n    \n    tok_tweet = tokenizer.encode(tweet)\n    input_ids_orig = tok_tweet.ids\n    tweet_offsets = tok_tweet.offsets\n    \n    target_idx = []\n    for j, (offset1, offset2) in enumerate(tweet_offsets):\n        if sum(char_targets[offset1: offset2]) > 0:\n            target_idx.append(j)\n    \n    targets_start = target_idx[0]\n    targets_end = target_idx[-1]\n\n    sentiment_id = {\n        'positive': 1313,\n        'negative': 2430,\n        'neutral': 7974\n    }\n    \n    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n    mask = [1] * len(token_type_ids)\n    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n    targets_start += 4\n    targets_end += 4\n\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([1] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n    \n    return {\n        'ids': input_ids,\n        'mask': mask,\n        'token_type_ids': token_type_ids,\n        'targets_start': targets_start,\n        'targets_end': targets_end,\n        'orig_tweet': tweet,\n        'orig_selected': selected_text,\n        'sentiment': sentiment,\n        'offsets': tweet_offsets\n    }\n\n\nclass TweetDataset:\n    def __init__(self, tweet, sentiment, selected_text):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n    \n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n        data = process_data(\n            self.tweet[item], \n            self.selected_text[item], \n            self.sentiment[item],\n            self.tokenizer,\n            self.max_len\n        )\n\n        return {\n            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n            'orig_tweet': data[\"orig_tweet\"],\n            'orig_selected': data[\"orig_selected\"],\n            'sentiment': data[\"sentiment\"],\n            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n        }","38cd3532":"def calculate_jaccard_score(\n    original_tweet, \n    target_string, \n    sentiment_val, \n    idx_start, \n    idx_end, \n    offsets,\n    verbose=False):\n    \n    if idx_end < idx_start:\n        idx_end = idx_start\n    \n    filtered_output  = \"\"\n    for ix in range(idx_start, idx_end + 1):\n        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n            filtered_output += \" \"\n\n    if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n        filtered_output = original_tweet\n\n    if sentiment_val != \"neutral\" and verbose == True:\n        if filtered_output.strip().lower() != target_string.strip().lower():\n            print(\"********************************\")\n            print(f\"Output= {filtered_output.strip()}\")\n            print(f\"Target= {target_string.strip()}\")\n            print(f\"Tweet= {original_tweet.strip()}\")\n            print(\"********************************\")\n\n    jac = 0\n    return jac, filtered_output\n","447636cd":"df_test = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/test.csv\")\ndf_test.loc[:, \"selected_text\"] = df_test.text.values","8077e57e":"device = torch.device(\"cuda\")\nmodel_config = transformers.RobertaConfig.from_pretrained('roberta-base')             # to download from internet\nmodel_config.output_hidden_states = True","53f8b9eb":"TweetDataset(tweet=df_test.text.values,\n             sentiment=df_test.sentiment.values,\n             selected_text=df_test.selected_text.values)","a03aa6c9":"model = TweetModel(conf=model_config)\nmodel.to(device)\nmodel.eval()\nprint(\"k\")","18871bab":"final_output = []","6c9d50c6":"test_dataset = TweetDataset(\n        tweet=df_test.text.values,\n        sentiment=df_test.sentiment.values,\n        selected_text=df_test.selected_text.values\n    )\n\ndata_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    shuffle=False,\n    batch_size=VALID_BATCH_SIZE,\n    num_workers=0\n)\n","f6ce9ab9":"with torch.no_grad():\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        sentiment = d[\"sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_tweet = d[\"orig_tweet\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        offsets = d[\"offsets\"].numpy()\n\n        ids            = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask           = mask.to(device, dtype=torch.long)\n        targets_start  = targets_start.to(device, dtype=torch.long)\n        targets_end    = targets_end.to(device, dtype=torch.long)\n\n        outputs_start1, outputs_end1 = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n        outputs_start = outputs_start1\n        outputs_end = outputs_end1\n        \n        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n        jaccard_scores = []\n        for px, tweet in enumerate(orig_tweet):\n          selected_tweet = orig_selected[px]\n          tweet_sentiment = sentiment[px]\n          _, output_sentence = calculate_jaccard_score(original_tweet=tweet,\n                                                       target_string=selected_tweet,\n                                                       sentiment_val=tweet_sentiment,\n                                                       idx_start=np.argmax(outputs_start[px, :]),\n                                                       idx_end=np.argmax(outputs_end[px, :]),\n                                                       offsets=offsets[px])\n          final_output.append(output_sentence)\n","a5db99f3":"sample = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/sample_submission.csv\")\nsample.loc[:, 'selected_text'] = final_output\nsample.to_csv(\"submission.csv\", index=False)","6ac89582":"sample.head()","242443d9":"## Roberta Model","e4e2731a":"For Roberta Vocablary file: \n* https:\/\/huggingface.co\/transformers\/_modules\/transformers\/modeling_roberta.html\n\nFor Roberta PreTrainedModel File:\n* https:\/\/huggingface.co\/transformers\/_modules\/transformers\/tokenization_roberta.html","a7d199cf":"# Downloading Vocab & Merge File from Roberta","c2232f93":"# Packages used in this kernal:","bd24b157":"# Prediction","b87b5e25":"# Imports\n","5b732c2f":"The RoBERTa model was proposed in RoBERTa: A Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. It is based on Google\u2019s BERT model released in 2018.\n\nWe are using [Roberta Model](https:\/\/huggingface.co\/transformers\/model_doc\/roberta.html#robertamodel) In this kernal. For more models [Click Here](https:\/\/huggingface.co\/models)\n\n","5b02f0d4":"# Submission","21bf8b36":"# Evaluation Model","cca9af39":"\n\n* Transformers\n\n  Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet\u2026) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.\n\n  This is the documentation of our [repository transformers](https:\/\/huggingface.co\/transformers\/index.html).\n\n\n* Tokenizers\n  \n  A tokenizer is in charge of preparing the inputs for a model. The library comprise tokenizers for all the models. Most of the tokenizers are available in two flavors: a full python implementation and a \u201cFast\u201d implementation based on the Rust library tokenizers.\n\n  For more Info: [Tokenizers from transformers](https:\/\/huggingface.co\/transformers\/main_classes\/tokenizer.html)\n\n* Pytorch (torch)\n\n  PyTorch is an open source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing. It is primarily developed by Facebook's AI Research lab.\n\n  For PyTorch Beginner Tutorial:\n  * https:\/\/www.kaggle.com\/anandsubbu007\/pytorch-basics-tutorial-1 \n  * https:\/\/www.kaggle.com\/anandsubbu007\/pytorch-autograd-tutorial-2 \n  * https:\/\/www.kaggle.com\/anandsubbu007\/deep-nn-pytorch-tutorial-3 \n  * https:\/\/www.kaggle.com\/anandsubbu007\/cnn-cifar10-pytorch-tutorial-4\n\n  ","6fea2496":"# Importing Test Data","f3e8f55d":"In This Kernal we are going to train model using predefined RoBert BERT Model, not with given data.\nThis kernal is created from this [kernal](https:\/\/www.kaggle.com\/abhishek\/roberta-inference-5-folds)"}}