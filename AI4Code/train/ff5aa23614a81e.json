{"cell_type":{"9a4239f7":"code","7bfe61c2":"code","e7a7e479":"code","d9273b2d":"code","d5ae3e11":"code","23740990":"code","239bbeb6":"code","b7d23af0":"code","581c4c37":"code","ace2c8a6":"code","c102c742":"code","1d37de9a":"code","ad841970":"code","cd2ccbae":"markdown","f3b7bbcc":"markdown","7afa3c6b":"markdown","a458c1cc":"markdown"},"source":{"9a4239f7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport numpy as np\nimport pandas as pd\nfrom sympy import simplify, cos, sin, Symbol, Function, tanh, pprint, init_printing, exp\nfrom sympy.functions import Min,Max\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","7bfe61c2":"# the winner variables with former values after the hashtag\nA = 0.058823499828577    \nB = 0.841127 # 0.885868\nC = 0.138462007045746 \nD = 0.31830988618379069\nE = 2.810815 # 2.675679922103882 \nF = 0.63661977236758138\nG = 5.428569793701172   \nH = 3.1415926535897931\nI = 0.592158 #0.623655974864960\nJ = 4.869778 #  2.770736 # 2.212120056152344\nK = 0.063467 # 1.5707963267948966\nL = -0.091481 # 0.094339601695538 \nM = 0.0821533 \nN = 0.720430016517639\nO = 0.230145 \nP = 9.89287 \nQ = 785 \nR = 1.07241 \nS = 281\nT = 734\nU = 5.3\nV = 67.0\nW = 2.484848\nX = 8.48635 \nY = 63\nZ = 12.6275 \nAA = 0.735354 # 0.7\nAB = 727\nAC = 2.5\nAD = 2.6 \nAE = 0.3\nAF = 3.0\nAG = 0.226263 #0.1\nAH = 2.0\nAI = 12.4148\nAJ = 96\nAK = 0.130303 # 0.2\nAL = 176\nAM = 3.2\nBIG = [A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,AA,AB,AC,AD,AE,AF,AG,AH,AI,AJ,AK,AL,AM]\n","e7a7e479":"# Now may I present: The winning gen function, Inspired by Akshat's notebook:\n# https:\/\/www.kaggle.com\/akshat113\/titanic-dataset-analysis-level-2\ndef GeneticFunction(data,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,AA,AB,AC,AD,AE,AF,AG,AH,AI,AJ,AK,AL,AM):\n    return ((np.minimum( ((((A + data[\"Sex\"]) - np.cos((data[\"Pclass\"] \/ AH))) * AH)),  ((B))) * AH) +\n            np.maximum( ((data[\"SibSp\"] - AC)),  ( -(np.minimum( (data[\"Sex\"]),  (np.sin(data[\"Parch\"]))) * data[\"Pclass\"]))) +\n            (AG * ((np.minimum( (data[\"Sex\"]),  (((data[\"Parch\"] \/ AH) \/ AH))) * data[\"Age\"]) - data[\"Cabin\"])) +\n            np.minimum( ((np.sin((data[\"Parch\"] * ((data[\"Fare\"] - AA) * AH))) * AH)),  ((data[\"SibSp\"] \/ AH))) +\n            np.maximum( (np.minimum( ( -np.cos(data[\"Embarked\"])),  (C))),  (np.sin(((data[\"Cabin\"] - data[\"Fare\"]) * AH)))) +\n            -np.minimum( ((((data[\"Age\"] * data[\"Parch\"]) * data[\"Embarked\"]) + data[\"Parch\"])),  (np.sin(data[\"Pclass\"]))) +\n            np.minimum( (data[\"Sex\"]),  ((np.sin( -(data[\"Fare\"] * np.cos((data[\"Fare\"] * W)))) \/ AH))) +\n            np.minimum( ((O)),  (np.sin(np.minimum( (((V \/ AH) * np.sin(data[\"Fare\"]))),  (D))))) +\n            np.sin((np.sin(data[\"Cabin\"]) * (np.sin((Z)) * np.maximum( (data[\"Age\"]),  (data[\"Fare\"]))))) +\n            np.sin(((np.minimum( (data[\"Fare\"]),  ((data[\"Cabin\"] * data[\"Embarked\"]))) \/ AH) *  -data[\"Fare\"])) +\n            np.minimum( (((AD * data[\"SibSp\"]) * np.sin(((AJ) * np.sin(data[\"Cabin\"]))))),  (data[\"Parch\"])) +\n            np.sin(np.sin((np.maximum( (np.minimum( (data[\"Age\"]),  (data[\"Cabin\"]))),  ((data[\"Fare\"] * AK))) * data[\"Cabin\"]))) +\n            np.maximum( (np.sin(((AI) * (data[\"Age\"] \/ AH)))),  (np.sin((-AF * data[\"Cabin\"])))) +\n            (np.minimum( (np.sin((((np.sin(((data[\"Fare\"] * AH) * AH)) * AH) * AH) * AH))),  (data[\"SibSp\"])) \/ AH) +\n            ((data[\"Sex\"] - data[\"SibSp\"]) * (np.cos(((data[\"Embarked\"] - AA) + data[\"Age\"])) \/ AH)) +\n            ((np.sin(data[\"Cabin\"]) \/ AH) - (np.cos(np.minimum( (data[\"Age\"]),  (data[\"Embarked\"]))) * np.sin(data[\"Embarked\"]))) +\n            np.minimum( (AE),  ((data[\"Sex\"] * (J * (N - np.sin((data[\"Age\"] * AH))))))) +\n            (np.minimum( (np.cos(data[\"Fare\"])),  (np.maximum( (np.sin(data[\"Age\"])),  (data[\"Parch\"])))) * np.cos((data[\"Fare\"] \/ AH))) +\n            np.sin((data[\"Parch\"] * np.minimum( ((data[\"Age\"] - K)),  ((np.cos((data[\"Pclass\"] * AH)) \/ AH))))) +\n            (data[\"Parch\"] * (np.sin(((data[\"Fare\"] * (I * data[\"Age\"])) * AH)) \/ AH)) +\n            (D * np.cos(np.maximum( ((0.5 * data[\"Fare\"])),  ((np.sin(N) * data[\"Age\"]))))) +\n            (np.minimum( ((data[\"SibSp\"] \/ AH)),  (np.sin(((data[\"Pclass\"] - data[\"Fare\"]) * data[\"SibSp\"])))) * data[\"SibSp\"]) +\n            np.tanh((data[\"Sex\"] * np.sin((U * np.sin((data[\"Cabin\"] * np.cos(data[\"Fare\"]))))))) +\n            (np.minimum( (data[\"Parch\"]),  (data[\"Sex\"])) * np.cos(np.maximum( ((np.cos(data[\"Parch\"]) + data[\"Age\"])),  (AM)))) +\n            (np.minimum( (np.tanh(((data[\"Cabin\"] \/ AH) + data[\"Parch\"]))),  ((data[\"Sex\"] + np.cos(data[\"Age\"])))) \/ AH) +\n            (np.sin((np.sin(data[\"Sex\"]) * (np.sin((data[\"Age\"] * data[\"Pclass\"])) * data[\"Pclass\"]))) \/ AH) +\n            (data[\"Sex\"] * (np.cos(((data[\"Sex\"] + data[\"Fare\"]) * ((X) * (Y)))) \/ AH)) +\n            np.minimum( (data[\"Sex\"]),  ((np.cos((data[\"Age\"] * np.tanh(np.sin(np.cos(data[\"Fare\"]))))) \/ AH))) +\n            (np.tanh(np.tanh( -np.cos((np.maximum( (np.cos(data[\"Fare\"])),  (L)) * data[\"Age\"])))) \/ AH) +\n            (np.tanh(np.cos((np.cos(data[\"Age\"]) + (data[\"Age\"] + np.minimum( (data[\"Fare\"]),  (data[\"Age\"])))))) \/ AH) +\n            (np.tanh(np.cos((data[\"Age\"] * ((-AH + np.sin(data[\"SibSp\"])) + data[\"Fare\"])))) \/ AH) +\n            (np.minimum( (((S) - data[\"Fare\"])),  (np.sin((np.maximum( ((AL)),  (data[\"Fare\"])) * data[\"SibSp\"])))) * AH) +\n            np.sin(((np.maximum( (data[\"Embarked\"]),  (data[\"Age\"])) * AH) * (((Q) * H) * data[\"Age\"]))) +\n            np.minimum( (data[\"Sex\"]),  (np.sin( -(np.minimum( ((data[\"Cabin\"] \/ AH)),  (data[\"SibSp\"])) * (data[\"Fare\"] \/ AH))))) +\n            np.sin(np.sin((data[\"Cabin\"] * (data[\"Embarked\"] + (np.tanh( -data[\"Age\"]) + data[\"Fare\"]))))) +\n            (np.cos(np.cos(data[\"Fare\"])) * (np.sin((data[\"Embarked\"] - ((T) * data[\"Fare\"]))) \/ AH)) +\n            ((np.minimum( (data[\"SibSp\"]),  (np.cos(data[\"Fare\"]))) * np.cos(data[\"SibSp\"])) * np.sin((data[\"Age\"] \/ AH))) +\n            (np.sin((np.sin((data[\"SibSp\"] * np.cos((data[\"Fare\"] * AH)))) + (data[\"Cabin\"] * AH))) \/ AH) +\n            (((data[\"Sex\"] * data[\"SibSp\"]) * np.sin(np.sin( -(data[\"Fare\"] * data[\"Cabin\"])))) * AH) +\n            (np.sin((data[\"SibSp\"] * ((((G + V) * AH) \/ AH) * data[\"Age\"]))) \/ AH) +\n            (data[\"Pclass\"] * (np.sin(((data[\"Embarked\"] * data[\"Cabin\"]) * (data[\"Age\"] - (R)))) \/ AH)) +\n            (np.cos((((( -data[\"SibSp\"] + data[\"Age\"]) + data[\"Parch\"]) * data[\"Embarked\"]) \/ AH)) \/ AH) +\n            (D * np.sin(((data[\"Age\"] * ((data[\"Embarked\"] * np.sin(data[\"Fare\"])) * AH)) * AH))) +\n            ((np.minimum( ((data[\"Age\"] * A)),  (data[\"Sex\"])) - F) * np.tanh(np.sin(data[\"Pclass\"]))) +\n            -np.minimum( ((np.cos(((AB) * ((data[\"Fare\"] + data[\"Parch\"]) * AH))) \/ AH)),  (data[\"Fare\"])) +\n            (np.minimum( (np.cos(data[\"Fare\"])),  (data[\"SibSp\"])) * np.minimum( (np.sin(data[\"Parch\"])),  (np.cos((data[\"Embarked\"] * AH))))) +\n            (np.minimum( (((data[\"Fare\"] \/ AH) - E)),  (C)) * np.sin((K * data[\"Age\"]))) +\n            np.minimum( ((M)),  (((np.sin(data[\"Fare\"]) + data[\"Embarked\"]) - np.cos((data[\"Age\"] * (P)))))))","d9273b2d":"#  Helper Functions to clean Titanic data and replace categorical values with numbers. \n# I can recommend the \"Advanced Feature Engineering\" notebook mentioned above for a deep dive\n# into why and how this is done\n\ndef CleanData(data):\n    # Sex\n    data.drop(['Ticket', 'Name'], inplace=True, axis=1)\n    data.Sex.fillna('0', inplace=True)\n    data.loc[data.Sex != 'male', 'Sex'] = 0\n    data.loc[data.Sex == 'male', 'Sex'] = 1\n    # Cabin\n    data.Cabin.fillna('0', inplace=True)\n    data.loc[data.Cabin.str[0] == 'A', 'Cabin'] = 1\n    data.loc[data.Cabin.str[0] == 'B', 'Cabin'] = 2\n    data.loc[data.Cabin.str[0] == 'C', 'Cabin'] = 3\n    data.loc[data.Cabin.str[0] == 'D', 'Cabin'] = 4\n    data.loc[data.Cabin.str[0] == 'E', 'Cabin'] = 5\n    data.loc[data.Cabin.str[0] == 'F', 'Cabin'] = 6\n    data.loc[data.Cabin.str[0] == 'G', 'Cabin'] = 7\n    data.loc[data.Cabin.str[0] == 'T', 'Cabin'] = 8\n    # Embarked\n    data.loc[data.Embarked == 'C', 'Embarked'] = 1\n    data.loc[data.Embarked == 'Q', 'Embarked'] = 2\n    data.loc[data.Embarked == 'S', 'Embarked'] = 3\n    data.Embarked.fillna(0, inplace=True)\n    data.fillna(-1, inplace=True)\n    return data.astype(float)\n\n# This function rounds values to either 1 or 0, because the GeneticFunction below returns floats and no\n# definite values\ndef Outputs(data):\n    return np.round(1.-(1.\/(1.+np.exp(-data))))\n","d5ae3e11":"# load our data\nraw_train = pd.read_csv('..\/input\/titanic\/train.csv')\nraw_test = pd.read_csv('..\/input\/titanic\/test.csv')\n\ncleanedTrain = CleanData(raw_train)\ncleanedTest = CleanData(raw_test)","23740990":"# run a check on the Training dataset. See section \"Programm your own gen. algorithm\" below on how to \n# construct your own genetic algorithm\nthisArray = BIG.copy()\ntestPredictions = Outputs(GeneticFunction(cleanedTrain,thisArray[0],thisArray[1],thisArray[2],thisArray[3],thisArray[4],thisArray[5],thisArray[6],thisArray[7],thisArray[8],thisArray[9],thisArray[10],thisArray[11],thisArray[12],thisArray[13],thisArray[14],thisArray[15],thisArray[16],thisArray[17],thisArray[18],thisArray[19],thisArray[20],thisArray[21],thisArray[22],thisArray[23],thisArray[24],thisArray[25],thisArray[26],thisArray[27],thisArray[28],thisArray[29],thisArray[30],thisArray[31],thisArray[32],thisArray[33],thisArray[34],thisArray[35],thisArray[36],thisArray[37],thisArray[38]))\npdcheck = pd.DataFrame({'Survived': testPredictions.astype(int)})\nret = pdcheck.Survived.where(pdcheck[\"Survived\"].values==cleanedTrain[\"Survived\"].values).notna()\nt,f = ret.value_counts()\nscore = 100\/(t+f)*t\nprint(\"Training set score: \",score)\n# remember this is the score on our training set which is not the same as having the score on the test set\n# which is the result we see in Kaggle (almost)","239bbeb6":"# Predict results using Genetic Function on our Test data\ntestPredictions = Outputs(GeneticFunction(cleanedTest,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,AA,AB,AC,AD,AE,AF,AG,AH,AI,AJ,AK,AL,AM))\npdtest = pd.DataFrame({'PassengerId': cleanedTest.PassengerId.astype(int),\n                        'Survived': testPredictions.astype(int)})\npdtest.to_csv('submission.csv', index=False)\npdtest.head()","b7d23af0":"# imports\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler,MinMaxScaler\nimport pickle\nimport string","581c4c37":"# now this number indicates the number of generations, which can quite long.\n# I recommend something around the number of 1000, but 100 is timewise okay (~5 min)\n# Currently it is at 10 to reduce runtime. Of course the more iterations the better the algorithm\n# Some code taken from: https:\/\/github.com\/innjoshka\/Genetic-Programming-Titanic-Kaggle\nHOWMANYITERS = 10","ace2c8a6":"def GP_deap(evolved_train):\n    global HOWMANYITERS\n    import operator\n    import math\n    import random\n\n\n    from deap import algorithms\n    from deap import base, creator\n    from deap import tools\n    from deap import gp\n\n    # dropping Survived and Passenger ID because we can not use them for training\n    outputs = evolved_train['Survived'].values.tolist()\n    evolved_train = evolved_train.drop([\"Survived\",\"PassengerId\"],axis=1)\n    inputs = evolved_train.values.tolist() # to np array\n    \n\n\n    def protectedDiv(left, right):\n        try:\n            return left \/ right\n        except ZeroDivisionError:\n            return 1\n\n    def randomString(stringLength=10):\n        \"\"\"Generate a random string of fixed length \"\"\"\n        letters = string.ascii_lowercase\n        return ''.join(random.choice(letters) for i in range(stringLength))\n    #choosing Primitives\n    pset = gp.PrimitiveSet(\"MAIN\", len(evolved_train.columns))  # add here\n    pset.addPrimitive(operator.add, 2)\n    pset.addPrimitive(operator.sub, 2)\n    pset.addPrimitive(operator.mul, 2)\n    pset.addPrimitive(protectedDiv, 2)\n    pset.addPrimitive(math.cos, 1)\n    pset.addPrimitive(math.sin, 1)\n    pset.addPrimitive(math.tanh,1)\n    pset.addPrimitive(max, 2)\n    pset.addPrimitive(min, 2)\n    pset.addEphemeralConstant(randomString(), lambda: random.uniform(-10,10))\n    # 50 as a precaution. 34 would be enough\n    pset.renameArguments(ARG0='x1')\n    pset.renameArguments(ARG1='x2')\n    pset.renameArguments(ARG2='x3')\n    pset.renameArguments(ARG3='x4')\n    pset.renameArguments(ARG4='x5')\n    pset.renameArguments(ARG5='x6')\n    pset.renameArguments(ARG6='x7')\n    pset.renameArguments(ARG7='x8')\n    pset.renameArguments(ARG8='x9')\n    pset.renameArguments(ARG9='x10')\n    pset.renameArguments(ARG10='x11')\n    pset.renameArguments(ARG11='x12')\n    pset.renameArguments(ARG12='x13')\n    pset.renameArguments(ARG13='x14')\n    pset.renameArguments(ARG14='x15')\n    pset.renameArguments(ARG15='x16')\n    pset.renameArguments(ARG16='x17')\n    pset.renameArguments(ARG17='x18')\n    pset.renameArguments(ARG18='x19')\n    pset.renameArguments(ARG19='x20')\n    pset.renameArguments(ARG20='x21')\n    pset.renameArguments(ARG21='x22')\n    pset.renameArguments(ARG22='x23')\n    pset.renameArguments(ARG23='x24')\n    pset.renameArguments(ARG24='x25')\n    pset.renameArguments(ARG25='x26')\n    pset.renameArguments(ARG26='x27')\n    pset.renameArguments(ARG27='x28')\n    pset.renameArguments(ARG28='x29')\n    pset.renameArguments(ARG29='x30')\n    pset.renameArguments(ARG30='x31')\n    pset.renameArguments(ARG31='x32')\n    pset.renameArguments(ARG32='x33')\n    pset.renameArguments(ARG33='x34')\n    pset.renameArguments(ARG34='x35')\n    pset.renameArguments(ARG35='x36')\n    pset.renameArguments(ARG36='x37')\n    pset.renameArguments(ARG37='x38')\n    pset.renameArguments(ARG38='x39')\n    pset.renameArguments(ARG39='x40')\n    pset.renameArguments(ARG40='x41')\n    pset.renameArguments(ARG41='x42')\n    pset.renameArguments(ARG42='x43')\n    pset.renameArguments(ARG43='x44')\n    pset.renameArguments(ARG44='x45')\n    pset.renameArguments(ARG45='x46')\n    pset.renameArguments(ARG46='x47')\n    pset.renameArguments(ARG47='x48')\n    pset.renameArguments(ARG48='x49')\n    pset.renameArguments(ARG49='x50')\n\n    # two object types is needed: an individual containing the genotype\n    # and a fitness -  The reproductive success of a genotype (a measure of quality of a solution)\n    creator.create(\"FitnessMin\", base.Fitness, weights=(1.0,))\n    creator.create(\"Individual\", gp.PrimitiveTree, fitness=creator.FitnessMin)\n\n\n    #register some parameters specific to the evolution process.\n    toolbox = base.Toolbox()\n    toolbox.register(\"expr\", gp.genHalfAndHalf, pset=pset, min_=1, max_=3) #\n    toolbox.register(\"individual\", tools.initIterate, creator.Individual, toolbox.expr)\n    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n    toolbox.register(\"compile\", gp.compile, pset=pset)\n\n\n    #evaluation function, which will receive an individual as input, and return the corresponding fitness.\n    def evalSymbReg(individual):\n        # Transform the tree expression in a callable function\n        func = toolbox.compile(expr=individual)\n        # Evaluate the accuracy of individuals \/\/ 1|0 == survived\n        return math.fsum(np.round(1.-(1.\/(1.+np.exp(-func(*in_))))) == out for in_, out in zip(inputs, outputs)) \/ len(evolved_train),\n\n\n    toolbox.register(\"evaluate\", evalSymbReg)\n    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n    toolbox.register(\"mate\", gp.cxOnePoint)\n    toolbox.register(\"expr_mut\", gp.genFull, min_=0, max_=3)\n    toolbox.register(\"mutate\", gp.mutUniform, expr=toolbox.expr_mut, pset=pset)\n\n    toolbox.decorate(\"mate\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=17))\n    toolbox.decorate(\"mutate\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=17))\n\n    pop = toolbox.population(n=300)\n    hof = tools.HallOfFame(1)\n\n    #Statistics over the individuals fitness and size\n    stats_fit = tools.Statistics(lambda ind: ind.fitness.values)\n    stats_size = tools.Statistics(len)\n    stats = tools.MultiStatistics(fitness=stats_fit, size=stats_size)\n    stats.register(\"avg\", np.mean)\n    stats.register(\"std\", np.std)\n    stats.register(\"min\", np.min)\n    stats.register(\"max\", np.max)\n\n\n    pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.7, mutpb=0.3, ngen=HOWMANYITERS, stats=stats,\n                                   halloffame=hof, verbose=True)\n\n    #Parameters:\n    #population \u2013 A list of individuals.\n    #toolbox \u2013 A Toolbox that contains the evolution operators.\n    #cxpb \u2013 The probability of mating two individuals.\n    #mutpb \u2013 The probability of mutating an individual.\n    #ngen \u2013 The number of generation.\n    #stats \u2013 A Statistics object that is updated inplace, optional.\n    #halloffame \u2013 A HallOfFame object that will contain the best individuals, optional.\n    #verbose \u2013 Whether or not to log the statistics.\n\n    # Transform the tree expression of hof[0] in a callable function and return it\n    func2 = toolbox.compile(expr=hof[0]) \n\n    return func2\n\ndef manualtree(df):\n    # using manualtree from https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n    #initialize table to store predictions\n    Model = pd.DataFrame(data = {'manual_tree':[]})\n    male_title = ['Master'] #survived titles\n\n    for index, row in df.iterrows():\n\n        #Question 1: Were you on the Titanic; majority died\n        Model.loc[index, 'manual_tree'] = 0\n\n        #Question 2: Are you female; majority survived\n        if (df.loc[index, 'Sex'] == 'female'):\n                  Model.loc[index, 'manual_tree'] = 1\n\n        #Question 3A Female - Class and Question 4 Embarked gain minimum information\n\n        #Question 5B Female - FareBin; set anything less than .5 in female node decision tree back to 0       \n        if ((df.loc[index, 'Sex'] == 'female') & \n            (df.loc[index, 'Pclass'] == 3) & \n            (df.loc[index, 'Embarked'] == 'S')  &\n            (df.loc[index, 'Fare'] > 8)\n\n           ):\n                  Model.loc[index, 'manual_tree'] = 0\n\n        #Question 3B Male: Title; set anything greater than .5 to 1 for majority survived\n        if ((df.loc[index, 'Sex'] == 'male') &\n            (df.loc[index, 'Title'] == 3)\n            ):\n            Model.loc[index, 'manual_tree'] = 1\n        \n        \n    return Model\n\n\ndef MungeData(data):\n\n    title_list = [\n                'Dr', 'Mr', 'Master',\n                'Miss', 'Major', 'Rev',\n                'Mrs', 'Ms', 'Mlle','Col',\n                'Capt', 'Mme', 'Countess',\n                'Don', 'Jonkheer'\n                                ]\n\n    #replacing all people's name by their titles\n    def replace_names_titles(x):\n        for title in title_list:\n            if title in x:\n                return title\n    data['Title'] = data.Name.apply(replace_names_titles)\n    data['Title'] = data['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss\/Mrs\/Ms')\n    data['Title'] = data['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr\/Military\/Noble\/Clergy')\n    data['Title'] = data.Title.map({ 'Dr':1, 'Mr':2, 'Master':3, 'Miss':4, 'Major':5, 'Rev':6, 'Mrs':7, 'Ms':8, 'Mlle':9,\n                     'Col':10, 'Capt':11, 'Mme':12, 'Countess':13, 'Don': 14, 'Jonkheer':15\n                    })\n    data = data.drop(['Name'],axis = 1)\n    data.Title.fillna(0, inplace=True)\n    data['Is_Married'] = 0\n    data['Is_Married'].loc[data['Title'] == 7] = 1\n    # manual_tree\n    data[\"manual_tree\"] = manualtree(data)\n    # Age\n    data['Age'] = data.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\n    # Relatives\n    data['Relatives'] = data.SibSp + data.Parch\n    # Fare per person\n    data['Fare_per_person'] = data.Fare \/ np.mean(data.SibSp + data.Parch + 1)\n    #data.drop(['Fare'], inplace=True, axis=1)\n    med_fare = data.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\n    data = data.drop(['SibSp', 'Parch'], axis=1)\n    # Filling the missing value in Fare with the median Fare of 3rd class alone passenger\n    data['Fare'] = data['Fare'].fillna(med_fare)\n    # Ticket\n    # Sex\n    data.Sex.fillna('0', inplace=True)\n    data.loc[data.Sex != 'male', 'Sex'] = 0\n    data.loc[data.Sex == 'male', 'Sex'] = 1\n    data['Ticket_Frequency'] = data.groupby('Ticket')['Ticket'].transform('count')\n    data = data.drop(['Ticket'], axis=1)\n    # Cabin\n    data.Cabin.fillna('0', inplace=True)\n    data.loc[data.Cabin.str[0] == 'A', 'Cabin'] = 1\n    data.loc[data.Cabin.str[0] == 'B', 'Cabin'] = 1\n    data.loc[data.Cabin.str[0] == 'C', 'Cabin'] = 1\n    data.loc[data.Cabin.str[0] == 'D', 'Cabin'] = 2\n    data.loc[data.Cabin.str[0] == 'E', 'Cabin'] = 2\n    data.loc[data.Cabin.str[0] == 'F', 'Cabin'] = 3\n    data.loc[data.Cabin.str[0] == 'G', 'Cabin'] = 3\n    data.loc[data.Cabin.str[0] == 'T', 'Cabin'] = 3\n    # Embarked\n    data.loc[data.Embarked == 'C', 'Embarked'] = 1\n    data.loc[data.Embarked == 'Q', 'Embarked'] = 2\n    data.loc[data.Embarked == 'S', 'Embarked'] = 3\n    data.Embarked.fillna(3, inplace=True)\n    #data.fillna(0, inplace=True)\n    #print(data.columns)#data[\"Survived\"] = svd_tmp\n    data[\"Cabin\"] = data[\"Cabin\"].astype(int)\n    # now for encoding - first we scale numeric features. E.g. Fare will have bigger values as Age, which \n    # could confuse an algorithm. therefore we normalize values in the range (-1,1)\n    numeric_features = ['Relatives','Fare_per_person', 'Fare', 'Age','Ticket_Frequency']\n    for feature in numeric_features:  \n        x = data[feature].values #returns a numpy array\n        min_max_scaler = MinMaxScaler()\n        x_scaled = min_max_scaler.fit_transform(x.reshape(-1, 1) )\n        data[feature] = pd.DataFrame(x_scaled)\n\n    # Categorial features\n    # Now the best thing for algorithms to work with categories is to have the category values in different\n    # columns as either 1 or 0. \n    cat_features = ['Pclass','Embarked', 'Sex', 'Cabin', 'Title','manual_tree','Is_Married']\n    encoded_features = []\n    for feature in cat_features:\n        encoded_feat = OneHotEncoder().fit_transform(data[feature].values.reshape(-1, 1)).toarray()\n        n = data[feature].nunique()\n        cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n        encoded_df.index = data.index\n        encoded_features.append(encoded_df)\n    data = pd.concat([data, *encoded_features], axis=1)\n    return data.astype(float)\n","c102c742":"# load data for your genetic algorithm\nraw_train = pd.read_csv('..\/input\/titanic\/train.csv')\nraw_test = pd.read_csv('..\/input\/titanic\/test.csv')\n\npass_id_train = raw_train[\"PassengerId\"] # copy it before deleting it in the MungeData step\nsurvived_train = raw_train[\"Survived\"] # copy it before deleting it in the MungeData step\npass_id_test = raw_test[\"PassengerId\"] # copy it before deleting it in the MungeData step\n\nevolved_train = MungeData(raw_train)\nevolved_test = MungeData(raw_test)","1d37de9a":"# Starts the genetic function. Remember this can have a huge comp. effort depending on the value you set \n# above in HOWMANYITERS\nGeneticFunctionObject = GP_deap(evolved_train)\n# optional, save our genetic function for later, good idea if computation took ages\nwith open(\"geneticfunction.pickle\",\"wb\") as file:\n    pickle.dump(GeneticFunction,file)","ad841970":"\nevolved_train = evolved_train.drop([\"PassengerId\",\"Survived\"],axis=1) # drop PassengerId because we will not use it\n\ntrain_nparray = evolved_train.values.tolist() \n\ntrainPredictions = Outputs(np.array([GeneticFunctionObject(*x) for x in train_nparray]))\nprint(\"Your score based on Train set (Remember, Kaggle\/Test set score will be different):\")\nprint(accuracy_score(survived_train.astype(int),trainPredictions.astype(int)))\npd_train = pd.DataFrame({'PassengerId': pass_id_train.astype(int),\n                        'Predicted': trainPredictions.astype(int),\n                        'Survived': survived_train.astype(int)})\npd_train.to_csv('gptrain_yourgenalgo.csv', index=False)\n\n# Test set submission\nevoled_test = evolved_test.drop([\"PassengerId\"],axis=1) # drop PassengerId because we will not use it\ntest_nparray = evolved_test.values.tolist()\ntestPredictions = Outputs(np.array([GeneticFunctionObject(*x) for x in test_nparray]))\n\npd_test = pd.DataFrame({'PassengerId': pass_id_test.astype(int),\n                        'Survived': testPredictions.astype(int)})\npd_test.to_csv('submission_yourgenalgo.csv', index=False) # change this to \"submission.csv\" only if you want\n                                                       # to submit results to kaggle\n","cd2ccbae":"## 1. What is a Genetic Algorithm?\nAs described above a Genetic Algorithm is not \"intelligent\" in the classic sense. It just tries Millions of random variables to achieve the best score. You can compare it to the evolution in that sense, that the algorithm tries out random \"new mutations\" in the formula, and chooses the winner each round to again mutate it in the next iteration. \n\nSome good introduction material about Genetic Algorithms:\n- https:\/\/www.youtube.com\/watch?v=9c1qo1eU1kY\n- https:\/\/medium.com\/analytics-vidhya\/understanding-genetic-algorithms-in-the-artificial-intelligence-spectrum-7021b7cc25e7\n\nThere is a python library for programming Genetic Algorithms which we will use later on:\n- https:\/\/github.com\/DEAP\/deap","f3b7bbcc":"# Content\n\n## Introduction\n\nA quick note about myself, I'm Justin G\u00fcse, Data & Cloud enthusiast, reachable here or via Linkedin https:\/\/www.linkedin.com\/in\/justin-guese\/. If you spot any errors or improvements feel free to comment!\n\nI can only recommend to everyone to use the Titanic Challenge as a way to dive into Machine Learning. Before diving into a Genetic Algorithm I would recommend the notebooks of \"LD Freeman\" and \"Gunes Evitan\", dealing with the \"basic\" approach of Data Science using Feature Engineering and the models as we know it:\n- https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n- https:\/\/www.kaggle.com\/gunesevitan\/advanced-feature-engineering-tutorial-with-titanic\n\nNow if you do not know yet the 1.0 scores in the Leaderboard are cheated, which \"Tarun\" demonstrated in his notebook:\n- https:\/\/www.kaggle.com\/tarunpaparaju\/how-top-lb-got-their-score-use-titanic-to-learn\n\nYour ML model should never reach a score of 1.0, because that basically means you are massively overfitting or including the \"solution\" in your dataset.\n\nGenetic Algorithms are a totally different approach. Imagine classic ML models as an \"art\", whilst Genetic Algorithms basically randomly try Millions of variables to improve the overall score, with several rounds where only the fittest variables will survive (hehe, Evolution jokes).\nMy former Genetic model has been based on \"Akshat Goel\", check out his notebook here:\n- https:\/\/www.kaggle.com\/akshat113\/titanic-dataset-analysis-level-2\n\n\n## Overview\n\n1. What is a Genetic Algorithm?\n2. My Algorithm to reach a score of 91% on the Titanic Dataset\n3. Build your own algorithm\n4. Building the basic score model using deap (library)\n5. Fine-Tuning of the parameters","7afa3c6b":"# 3. Build your own algorithm\nNow let us have a look on how you are able to build your own algorithm","a458c1cc":"##  My Algorithm to reach a score of 91% on the Titanic Dataset"}}