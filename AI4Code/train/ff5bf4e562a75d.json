{"cell_type":{"6a438e83":"code","03a2eabd":"code","17dfd356":"code","9e533f12":"code","f4ff261f":"code","167a2568":"code","0abc0c8c":"code","723062d3":"code","e55bae7a":"code","ee8c5a52":"code","c1f53370":"code","8cd9dc46":"code","2e91e9e1":"code","9f3a83f9":"code","34e59c9c":"code","8f54452a":"code","78b24004":"code","bd5a0262":"code","5139ceb6":"code","cc9a3df3":"code","8075be1a":"code","41fc7473":"code","7b4146ea":"code","95b67395":"code","77e481f7":"code","d10d46b1":"code","b6cd27a8":"code","153ff3c8":"code","3a640bb5":"code","7b48f475":"code","21ff5cbb":"code","3d8c5fe3":"code","f2f7be49":"code","b7efb558":"code","e7c56f17":"code","1ee852c8":"code","cfab4533":"code","2fbf2a31":"code","997ba2ca":"markdown","d8eba1be":"markdown","97b9d7be":"markdown","78bc07a3":"markdown","1e161f70":"markdown","a09c63f9":"markdown","f02c182d":"markdown","d628077f":"markdown","cb5bdfa8":"markdown","df57ec3b":"markdown","a4bbf749":"markdown","ed025f16":"markdown","872a865b":"markdown","29e1a36d":"markdown","191c6029":"markdown","474a0d8c":"markdown","e8e02451":"markdown","6139e424":"markdown","c6c54584":"markdown","046f690d":"markdown","bd8e1c2e":"markdown","b962ae24":"markdown","e4d80c9d":"markdown","1aa0f770":"markdown","44094ca9":"markdown","dcc8d1c3":"markdown"},"source":{"6a438e83":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","03a2eabd":"import tensorflow as tf\nimport json\nfrom pandas.io.json import json_normalize\nmetadata = pd.read_json(\"..\/input\/gutenberg-dammit\/gutenberg-dammit-files\/gutenberg-metadata.json\")\nprint(metadata.shape)\nmetadata.head()","17dfd356":"metadata.loc[1000:1050]","9e533f12":"with open('..\/input\/gutenberg-dammit\/gutenberg-dammit-files\/010\/01074.txt', 'r') as text:\n    data = text.read().replace('\\n', ' ')\n    print(data)","f4ff261f":"open_text = open(\"..\/input\/gutenberg-dammit\/gutenberg-dammit-files\/010\/01074.txt\", \"r\")\nprint(open_text.read())","167a2568":"print ('Length of text: {} characters'.format(len(data)))","0abc0c8c":"import nltk\nfrom nltk.tokenize import sent_tokenize\ntokenized_text=sent_tokenize(data)\nprint(tokenized_text)","723062d3":"from nltk.tokenize import word_tokenize\ntokenized_word=word_tokenize(data)\nprint(tokenized_word)","e55bae7a":"from nltk.probability import FreqDist\nfdist = FreqDist(tokenized_word)\nprint(fdist)","ee8c5a52":"#What are the two most common words?\nfdist.most_common(2)","c1f53370":"#What are the ten most common?\nfdist.most_common(10)","8cd9dc46":"#What are the 10 least common?\nfdist.most_common()[-10:]","2e91e9e1":"import matplotlib.pyplot as plt\nfdist.plot(30,cumulative=False) #show me the 30 most common words\nplt.show()","9f3a83f9":"fdist.plot(50,cumulative=False) #show me the 50 most common words\nplt.show()","34e59c9c":"from nltk.corpus import stopwords\nstop_words=set(stopwords.words(\"english\"))\nprint(stop_words)","8f54452a":"filtered_sent=[]  #create an empty list for the text empty of stop words\nfor w in tokenized_text:     #go through every word in the text\n    if w not in stop_words:  #if it's not a stop word\n        filtered_sent.append(w)  #add it to the list\nprint(\"Tokenized Sentence:\",tokenized_text)  #print the text","78b24004":"print(\"Filtered Sentence:\",filtered_sent) #print the text without stop words","bd5a0262":"print ('Length of text: {} characters'.format(len(data))) \nprint ('Length of filtered text: {} characters'.format(len(filtered_sent)))","5139ceb6":"vocab = sorted(set(data))\nprint ('{} unique characters'.format(len(vocab)))","cc9a3df3":"#Each unique character gets assigned a number\nchar2idx = {u:i for i, u in enumerate(vocab)}\nidx2char = np.array(vocab)\n\ntext_as_int = np.array([char2idx[c] for c in data])\n\n#print the table\nprint('{')\nfor char,_ in zip(char2idx, range(20)):\n    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\nprint('  ...\\n}')","8075be1a":"#print the first 13 characters as an array\nprint ('{} ---- characters mapped to int ---- > {}'.format(repr(data[:13]), text_as_int[:13]))","41fc7473":"# The maximum length sentence we want for a single input in characters\nseq_length = 100\nexamples_per_epoch = len(data)\/\/(seq_length+1)\n\n# Create training examples \/ targets\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n\n# Print the first 5 characters\nfor i in char_dataset.take(5): \n  print(idx2char[i.numpy()])","7b4146ea":"# Turn the character set into a sequence using .batch\nsequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n\n# Print 10 sequences\nfor item in sequences.take(10):\n  print(repr(''.join(idx2char[item.numpy()])))","95b67395":"# Input & target text mapping function\ndef split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\n# Apply the function to each of my sequences\ndataset = sequences.map(split_input_target)\n\n# Print one mapped sequence\nfor input_example, target_example in  dataset.take(1):\n  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))","77e481f7":"# for each of the first five characters of the sequence, print out the input and the target\nfor i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n    print(\"Step {:4d}\".format(i))\n    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))","d10d46b1":"# Batch size\nBATCH_SIZE = 64\n\n# Buffer size to shuffle the dataset\n# (TF data is designed to work with possibly infinite sequences,\n# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n# it maintains a buffer in which it shuffles elements).\nBUFFER_SIZE = 10000\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\ndataset","b6cd27a8":"# Length of the vocabulary in characters\nvocab_size = len(vocab)\n\n# The embedding dimension\nembedding_dim = 256\n\n# Number of RNN units\nrnn_units = 1024\n\n# Build the model using 3 layers.\ndef build_model(vocab_size, embedding_dim, rnn_units, batch_size): #let's specify the dimensions\n  model = tf.keras.Sequential([  #used to embed all three layers\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, #input later\n                              batch_input_shape=[batch_size, None]),\n    tf.keras.layers.GRU(rnn_units, #RNN\n                        return_sequences=True,\n                        stateful=True,  #it needs tobe stateful! \n                        recurrent_initializer='glorot_uniform'),\n    tf.keras.layers.Dense(vocab_size) #output layer\n  ])\n  return model\n\nmodel = build_model(\n  vocab_size = len(vocab),\n  embedding_dim=embedding_dim,\n  rnn_units=rnn_units,\n  batch_size=BATCH_SIZE)","153ff3c8":"for input_example_batch, target_example_batch in dataset.take(1):\n  example_batch_predictions = model(input_example_batch)\n  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")","3a640bb5":"model.summary()","7b48f475":"sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\nsampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\nsampled_indices","21ff5cbb":"print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\nprint()\nprint(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))","3d8c5fe3":"# Function that sets the from_logits flag\ndef loss(labels, logits):\n  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n\nexample_batch_loss = loss(target_example_batch, example_batch_predictions)\nprint(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\nprint(\"scalar_loss:      \", example_batch_loss.numpy().mean())","f2f7be49":"model.compile(optimizer='adam', loss=loss)","b7efb558":"# Directory where the checkpoints will be saved\ncheckpoint_dir = '.\/training_checkpoints'\n\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)","e7c56f17":"EPOCHS=10\nhistory = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])","1ee852c8":"#start training from the latest checkpoint\ntf.train.latest_checkpoint(checkpoint_dir)","cfab4533":"model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n\nmodel.build(tf.TensorShape([1, None]))\n\nmodel.summary()","2fbf2a31":"def generate_text(model, start_string):\n  # Evaluation step (generating text using the learned model)\n\n  # Number of characters to generate\n  num_generate = 1000\n\n  # Converting the first string to numbers (vectorizing)\n  input_eval = [char2idx[s] for s in start_string]\n  input_eval = tf.expand_dims(input_eval, 0)\n\n  # Empty string to store our results\n  text_generated = []\n\n  # Temperature is the 'creativity' variable:\n  # Low temperatures result in more predictable text\n  # Higher temperatures result in more surprising text\n  temperature = 1.0\n\n  # Here batch size == 1\n  model.reset_states()\n  for i in range(num_generate):\n      predictions = model(input_eval)\n      # remove the batch dimension\n      predictions = tf.squeeze(predictions, 0)\n\n      # using a categorical distribution to predict the character returned by the model\n      predictions = predictions \/ temperature\n      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n      # We pass the predicted character as the next input to the model\n      # along with the previous hidden state\n      input_eval = tf.expand_dims([predicted_id], 0)\n\n      text_generated.append(idx2char[predicted_id])\n\n  return (start_string + ''.join(text_generated))\n\nprint(generate_text(model, start_string=u\"The\")) #specifiy the first word of the text as a prompt","997ba2ca":"Below is the text 'as is'. I somehow couldn't perform an analysis on this version of the text because it registered as an 'open file object' and not as strings.","d8eba1be":"Now, let's shuffle the data and pack it into batches. <br>\n(I tried to understand why the data needs to be shuffled and it seems like it has to do with the statefulness of the model. If not shuffled, the samples are reset at each sequence and not propagated to the next bach. Not sure why this matters but there is an interesting discussion here: http:\/\/philipperemy.github.io\/keras-stateful-lstm\/)","97b9d7be":"Each character of the sequence is processed one step at a time. In the first step, the model will process \"T\" and then try to predict \"h\", and then do the same thing for all the following characters. But being a RNN, it also keeps a memory of the previous steps in addition to the current input character. \n\nLet's see if it does this correctly.","78bc07a3":"# Vectorization\n\nThe first step is to vectorize the text, i.e. map strings to a numerical representation.","1e161f70":"Let's first tokenize the text.","a09c63f9":"Now let's look at stop words. Stop words are commonly used words that are like 'noise' in the text.","f02c182d":"I tried to browse the json file with Pandas's **groupby** command to see if it would show me say all the unique 'title' values for each author. \n\n> group = metadata.groupby('Author') <br>\n> byauthor = group.apply(lambda x: x['Title'].unique())\n\nBut I kept getting the error **unhashable type: 'list'**. Apparently a list object cannot be used as key because it's not hashable.\n\nSoooo I chose a text 'at random', based on interest and what some crude segmenting of the file showed me. I picked Jack London's *The Sea Wolf* as my training dataset. Here's what the 'text as string' version looks like.\n","d628077f":"# Data prep\nSome data prep is needed before we start training the model.\n\nThe first step is to divide the text into sequences (**seq_length**) with the **tf.data.Dataset.from_tensor_slices** function, which turns the text vector into a stream of character indices.\n\nThe idea, as I understand it, is to use batches as training sets. For each set, we're asking the model: given all the characters seen so far, what is the most probable next character?\n","cb5bdfa8":"# Building the model\nNow let's build the model.\n\nWe'll use **tf.keras.Sequential** to build the model, which groups a linear stack of layers. We'll use 3 layers for this model:\n\n**tf.keras.layers.Embedding** --> the input layer (a table that will map the number of each character to a vector) <br>\n**tf.keras.layers.GRU** --> a type of RNN (could also have used a LSTM layer)<br>\n**tf.keras.layers.Dense** --> the output later ","df57ec3b":"Now the actual training!!! We'll keep it at 10 epochs first so that it doesn't take forever. I foresee so-so results but let's try it first.","a4bbf749":"# Mini project 2: Text generation with TensorFlow\n\nThis mini project is really guided by various tutorials on RNN text generation, mainly Max Woolf's and Tensorflow. I had already trained and used textgenrnn, which is a Python module for text generation built on top of Keras\/Tensorflow but wanted to learn how to build my own model and train it.\n\nI tried to give a detailed account of each step, which was a good way to make sure I understood what was going on, at least at a high level.\n\nThere's also a little bit of NLTK experimentation, which I read is a clunky but powerful Python NLP library.\n\nhttps:\/\/colab.research.google.com\/drive\/1mMKGnVxirJnqDViH7BDJxFqWrsXlPSoK <br>\nhttps:\/\/www.tensorflow.org\/tutorials","ed025f16":"Now that we can divide up the text into sequences, we need to **map** the text to input and target sequences. The targets contain the same length of text except shifted to one character to the right. \n\nExample: <br>\nText: 'Hello'<br>\nInput seq: 'Hell'<br>\nTarget seq: 'ello'\n\nWe can do this (map the text) by applying a function to each sequence.","872a865b":"These are the predictions for the next character in each timestep of the sample. Let's decode the array to see what the current state of the model's prediction is. ","29e1a36d":"Ok, the model's built! Now let's try it.\n\nFirst we need to check the shape of the ouput.","191c6029":"It works! Now let's try with bigger batches. Here the **char_dataset** is turned into a sequence using **batch**. We'll print 10 sequences.","474a0d8c":"# Training the model\nFirst step: attach a loss function using **tf.keras.losses.sparse_categorical_crossentropy**, which computes the sparse categorical crossentropy loss.","e8e02451":"Now let's look at the frequency distribution.","6139e424":"# Training data selection and exploration\nLet's import TensorFlow and json so that I can look at my data via the json file that contains all the database metadata.","c6c54584":"Now let's look at a model summary.","046f690d":"Now let's import **matplotlib** to do some data viz.","bd8e1c2e":"I can't see much of a difference between the tokenized and filtered texts. Let's print out their characters to see if it actually did anything.","b962ae24":"Configure the training procedure with using the **tf.keras.Model.compile** method (which compiles an optimizer, the loss function, the loss weights, and other stuff). Here the optimizer is 'adam'.","e4d80c9d":"Then let's import the NLTK and do some text analysis.","1aa0f770":"Let's look at a sample.\n","44094ca9":"Now I need to configure checkpoints to ensure they are saved during the training. Checkpoints are used to save a model or weights at every determined interval so that the model can be loaded later to continue the training from the state saved.\n\nhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/ModelCheckpoint","dcc8d1c3":"Lol. Let's train that model."}}