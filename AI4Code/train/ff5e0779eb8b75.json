{"cell_type":{"be4cf0b1":"code","9909d266":"code","e8267d38":"code","18b740fb":"code","456f7445":"code","139f4473":"code","17cc7819":"code","baa38927":"code","12594502":"code","e9fd3246":"code","547d4bb5":"code","6c8f8f26":"code","caf962d8":"code","34c058a8":"code","b2c19e67":"code","4e188cc7":"code","7e5ac4ec":"code","5cfbddd0":"code","b37c68cd":"code","8475f7e6":"code","cde83b41":"code","c591743d":"code","fba275b7":"markdown","b208cb8b":"markdown","307b879a":"markdown","f6b562df":"markdown","b372a0ce":"markdown","38f7eb3a":"markdown","430c5871":"markdown","cb9d4bba":"markdown","fc16ea11":"markdown","e453d607":"markdown"},"source":{"be4cf0b1":"import numpy as np\nimport pandas as pd\nimport plotly.express as px#visualization\nimport plotly.graph_objects as go\nimport os","9909d266":"data=pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndata.head()","e8267d38":"data.info()","18b740fb":"data.shape","456f7445":"data.duplicated().sum()","139f4473":"data.drop_duplicates(inplace = True)","17cc7819":"data.duplicated().sum()","baa38927":"fig = go.Figure(data=[go.Pie(labels=data['Class'], pull=[0.4,0],hole=0.5)])\nfig.show()","12594502":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve\n# from sklearn.preprocessing import StandardScaler\n\nX = data.drop(\"Class\",axis=1)\ny = data['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify = y,random_state=142)","e9fd3246":"from sklearn.ensemble import GradientBoostingClassifier\n\nxgb = GradientBoostingClassifier(\n    max_features='auto',\n    min_samples_leaf=1,\n    n_estimators=400,\n    learning_rate=0.5,\n    max_depth=5,\n    random_state=1,\n    )\nxgb.fit(X_train,y_train)","547d4bb5":"predictxgb = xgb.predict(X_test)\nprint(classification_report(predictxgb,y_test))\n","6c8f8f26":"print(\"train score ===> \",xgb.score(X_train,y_train))\nprint(\"test score ==>\",xgb.score(X_test,y_test))","caf962d8":"from imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler(random_state=142)\nX_us, y_us = rus.fit_resample(X_train, y_train)","34c058a8":"fig = go.Figure(data=[go.Pie(labels=y_us,hole=0.5)])\nfig.show()","b2c19e67":"X_train_us, X_test_us, y_train_us, y_test_us = train_test_split(X_us, y_us, test_size=0.2, stratify = y_us,random_state=142)","4e188cc7":"xgbus = GradientBoostingClassifier(\n    max_features='auto',\n    min_samples_leaf=1,\n    n_estimators=500,\n    learning_rate=0.5,\n    max_depth=5,\n    random_state=3,\n    )\nxgbus.fit(X_train_us,y_train_us)","7e5ac4ec":"predictxgbus = xgbus.predict(X_test_us)\nprint(classification_report(predictxgbus,y_test_us))\n","5cfbddd0":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=142)\n\nX_os, y_os = smote.fit_resample(X_train, y_train)\n\n","b37c68cd":"fig = go.Figure(data=[go.Pie(labels=y_os,hole=0.5)])\nfig.show()","8475f7e6":"X_train_os, X_test_os, y_train_os, y_test_os = train_test_split(X_os, y_os, test_size=0.2, stratify = y_os,random_state=142)","cde83b41":"xgbus = GradientBoostingClassifier(\n    max_features='auto',\n    min_samples_leaf=1,\n    n_estimators=500,\n    learning_rate=0.5,\n    max_depth=5,\n    random_state=3,\n    )\nxgbus.fit(X_train_us,y_train_us)","c591743d":"predictxgbos = xgbus.predict(X_test_os)\nprint(classification_report(predictxgbos,y_test_os))\n","fba275b7":"# Models with unbalance","b208cb8b":"# Problem\n\nwe have unbalanced data according to results of pie plot and classification report \n\n`Why is data imbalance a problem ?`\n\n1. The learning issue might be complicated by training a machine learning model on an unbalanced dataset. Imbalanced data is a classification problem in which the number of observations per class is not evenly distributed; you'll frequently have a lot of data\/observations for one class (referred to as the majority class) and a lot less for one or more other classes (referred to as the minority classes)\n\n2. Therefore, most predictions are biased towards the majority category\n\n","307b879a":"# Take a look at the data","f6b562df":"# solution \n** just use `SMOTE ,Oversampling` or ` Undersampling` **\n\n1. What SMOT is ?\n* SMOTE stands for `Synthetic Minority Oversampling Technique`. This is a statistical technique for increasing the number of cases in your dataset in a balanced way. The module works by generating new instances from existing minority cases that you supply as input. This implementation of SMOTE does not change the number of majority cases.  \n\n2. What Random Oversampling is ?\n* Randomly duplicate examples in the minority class.\n3. What Random Undersampling is\n* Randomly delete examples in the majority class.\n\n\n`let's try`\n\n","b372a0ce":"# END \nIf you like it, please vote for it and leave your feedback\n\n## resources \n1. [here](https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/smote)\n2. [here](https:\/\/machinelearningmastery.com\/combine-oversampling-and-undersampling-for-imbalanced-classification\/)\n3. [here](https:\/\/www.jeremyjordan.me\/imbalanced-data\/)","38f7eb3a":"## Gradient Boosting Classifier with unbalance","430c5871":"# Random Under Sampler","cb9d4bba":"# introduction \n* The imbalance is one of the most annoying problems in the dataset so after making searches i got the most easiest ways to solve this problem\n\n![image.png](attachment:bb3e05b8-9b31-4a18-8e1a-7490ed93a8f2.png)\n","fc16ea11":"# over_sampling","e453d607":"The data is completely unbalanced using some techniques to deal with it later"}}