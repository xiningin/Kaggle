{"cell_type":{"1cb1ab92":"code","2d3e6a3f":"code","3ead9c8c":"code","a350e437":"code","ebe9f70e":"code","3629bd88":"code","5e1179ed":"code","432f7c44":"code","d6eb2b19":"code","5d957274":"code","b84e6337":"code","e02d21a1":"code","3f91be61":"code","90522296":"code","41d8d1fb":"code","e146448b":"code","b312c4a7":"code","6d16c51b":"code","3107cf14":"code","90790efe":"code","97da68e3":"code","1d258719":"code","db3e1194":"code","29f95348":"code","d0fd9e64":"code","e880a960":"code","69fa2939":"code","f84ac89f":"code","f6396c6d":"code","1c395c69":"code","87debf7f":"code","b346cec0":"code","129895e9":"code","c480857f":"code","ca26bbf9":"code","9748ecd1":"code","b3d3a63e":"code","15cb7401":"code","f70a5f23":"code","a635bdc0":"code","ec5a0c67":"code","754d0f20":"code","32afe42a":"code","9011177f":"code","51141339":"markdown","3c891962":"markdown","dc848244":"markdown","767721fb":"markdown","20de34ab":"markdown","c2ec55cc":"markdown","e60b98ee":"markdown","4df21dcf":"markdown","58b3b60f":"markdown","49cacdcb":"markdown","9a257c44":"markdown","c3f58633":"markdown","468256dc":"markdown","e2707bb5":"markdown","f3ac4cf0":"markdown","835c1214":"markdown","ea75bf43":"markdown","1100a91f":"markdown","5c466fda":"markdown","08aec67b":"markdown","4381f967":"markdown"},"source":{"1cb1ab92":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2d3e6a3f":"import sys\nsys.path.append('\/kaggle\/input\/iterative-stratification\/iterative-stratification-master')","3ead9c8c":"import sys\n#sys.path.append('..\/input\/iterativestratification')\n\nimport numpy as np\nimport random\nimport pandas as pd\nimport os\nimport copy\nimport gc\nfrom scipy.special import erfinv\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nfrom datetime import datetime\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn.modules.loss import _WeightedLoss\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#os.listdir('..\/input\/lish-moa')\n\npd.set_option('max_columns', 2000)","a350e437":"train_features = pd.read_csv(\"\/kaggle\/input\/lish-moa\/train_features.csv\")\ntrain_targets_scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\n#train_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')","ebe9f70e":"n_comp_GENES = int(0.6*610)\nn_comp_CELLS = int(0.6*55)\nVarianceThreshold_for_FS = 0.82","3629bd88":"train_features","5e1179ed":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","432f7c44":"# RankGauss - transform to Gauss\ntrain_len = len(train_features.values)\ntest_len = len(test_features.values)\n\nfor col in (GENES + CELLS):\n\n    epsilon = 1e-6\n    \n    \n    \n    train_1col = train_features[col].values.reshape(train_len, 1)\n    test_1col = test_features[col].values.reshape(test_len, 1)\n\n    np_arr_total = np.vstack((train_1col, test_1col))\n  \n    res = np_arr_total[:,0].argsort().argsort()\n    res = (res\/res.max()-0.5)*2 \n    res = np.clip(res,-1+epsilon,1-epsilon)\n    res = erfinv(res) \n    np_arr_total_r = np_arr_total \n    np_arr_total_r[:,0] = res * np.sqrt(2)\n\n    train_features[col] = (np_arr_total_r[:train_len]).reshape(1, train_len)[0]\n    test_features[col] = (np_arr_total_r[train_len:]).reshape(1, test_len)[0]","d6eb2b19":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","5d957274":"# GENES\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (PCA(n_components=n_comp_GENES, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp_GENES)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp_GENES)])\n\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","b84e6337":"len(GENES)","e02d21a1":"# CELLS\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp_CELLS, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp_CELLS)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp_CELLS)])\n\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","3f91be61":"len(CELLS)","90522296":"train_features.shape","41d8d1fb":"train_features.head(5) ","e146448b":"var_thresh = VarianceThreshold(VarianceThreshold_for_FS)\ndata = train_features.append(test_features)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\ntrain_features.shape","b312c4a7":"train_features.head(5)","6d16c51b":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget = train[train_targets_scored.columns]","3107cf14":"train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","90790efe":"train.head(5)","97da68e3":"target_cols = target.drop('sig_id', axis=1).columns.values.tolist()","1d258719":"folds = train.copy()\n\nmskf = MultilabelStratifiedKFold(n_splits=7)\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n    folds.loc[v_idx, 'kfold'] = int(f)\n\nfolds['kfold'] = folds['kfold'].astype(int)\nfolds","db3e1194":"print(train.shape)\nprint(folds.shape)\nprint(test.shape)\nprint(target.shape)\nprint(sample_submission.shape)","29f95348":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n    ","d0fd9e64":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds","e880a960":"class SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","69fa2939":"def process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data","f84ac89f":"feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\nlen(feature_cols)","f6396c6d":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')    \nEPOCHS = 22\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 7 #7\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\n#hidden_size=1500  ","1c395c69":"\n\nclass Model(nn.Module):\n    def recalibrate_layer(self, layer):\n      if(torch.isnan(layer.weight_v).sum() > 0):\n          layer.weight_v = torch.nn.Parameter(torch.where(torch.isnan(layer.weight_v), torch.zeros_like(layer.weight_v), layer.weight_v))\n          layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n\n      if(torch.isnan(layer.weight).sum() > 0):\n          layer.weight = torch.where(torch.isnan(layer.weight), torch.zeros_like(layer.weight), layer.weight)\n          layer.weight += 1e-7\n\n    def __init__(self, num_features, num_targets, hidden_size_1d, dropout_rate_1d):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size_1d[0]))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size_1d[0])\n        self.dropout2 = nn.Dropout(dropout_rate_1d[0])\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size_1d[0], hidden_size_1d[1]))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size_1d[1])\n        self.dropout3 = nn.Dropout(dropout_rate_1d[1])\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size_1d[1], num_targets))\n    \n    def forward(self, x):     \n        x = self.batch_norm1(x)\n        #self.recalibrate_layer(self.dense1)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        #self.recalibrate_layer(self.dense2)\n        x = F.leaky_relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        #self.recalibrate_layer(self.dense3)\n        x = self.dense3(x)\n        \n        return x\n    \nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing \/ (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))    \n","87debf7f":"def run_training(fold, seed, hidden_size, dropout_rate):\n    \n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(test)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size_1d=hidden_size,\n        dropout_rate_1d=dropout_rate,\n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n   \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n        \n        elif (EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size_1d=hidden_size,\n        dropout_rate_1d=dropout_rate,\n    )\n    \n    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions","b346cec0":"def run_network(fold, seed, NFOLDS):\n    hidden_size_arr = [[1000, 1500], [800, 1400], [1500, 1000], [1500, 1500]]     \n    dropout_rate_arr = [[1\/6, 0.25], [2\/15, 7\/30], [0.25, 1\/6], [0.25, 0.25]]     \n    weight_rate_arr = [[0.3], [0.3], [0.2], [0.2]]\n    oof_this_fold = np.zeros((len(train), len(target_cols)))\n    predictions_this_fold = np.zeros((len(test), len(target_cols)))\n    \n    for this_network_i in range(len(hidden_size_arr)):\n        print(\"network:\",this_network_i)\n        oof_, pred_ = run_training(fold, seed, hidden_size = hidden_size_arr[this_network_i], dropout_rate = dropout_rate_arr[this_network_i])\n        if np.amax(oof_) == 0 and np.amin(oof_) == 0:\n            print(\"max network: \", np.amax(oof_))\n            print(\"min network: \", np.amin(oof_))\n        predictions_this_fold += pred_ * weight_rate_arr[this_network_i]\n        oof_this_fold += oof_ * weight_rate_arr[this_network_i]\n    \n    return oof_this_fold, predictions_this_fold","129895e9":"def run_k_fold(NFOLDS, seed):\n    oof_this_seed = np.zeros((len(train), len(target_cols)))\n    predictions_this_seed = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        print(\"fold:\", fold)\n        oof_, pred_ = run_network(fold, seed, NFOLDS)\n\n        if np.amax(oof_) == 0 and np.amin(oof_) == 0:\n            print(\"max fold:\", np.amax(oof_))\n            print(\"min fold:\", np.amin(oof_))\n        \n        predictions_this_seed += pred_ \/ NFOLDS\n        oof_this_seed += oof_\n        \n    return oof_this_seed, predictions_this_seed","c480857f":"from pytz import timezone","ca26bbf9":"datetime.now(timezone('America\/Toronto'))","9748ecd1":"# Averaging on multiple SEEDS\n#1.42GB\nSEED = [0, 1, 2, 3, 4, 5, 6]\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\n\n\nfor seed in SEED:\n    print(\"seed:\", seed)\n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ \/ len(SEED)\n    predictions += predictions_ \/ len(SEED)\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions","b3d3a63e":"datetime.now(timezone('America\/Toronto'))","15cb7401":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ \/ target.shape[1]\n    \nprint(\"CV log_loss: \", score)  ","f70a5f23":"\npd.DataFrame(\n    {\n        \"1st hidden layer\": [1500, 1500, 1000, 800],\n        \"2nd hidden layer\": [1000, 1500, 1500, 1400],\n        \"1st dropout rate\": [0.25, 0.25, 1\/6, 2\/15],\n        \"2nd dropout rate\": [1\/6, 0.25, 0.25, 7\/30],\n        \"Kaggle Public Score\": [0.01841, 0.01841, 0.01840, 0.01840],\n        \"Ratio\": [0.3, 0.3, 0.2, 0.2]\n    }\n)\n","a635bdc0":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left')\n","ec5a0c67":"np.isnan(sub.values[:, 1:].astype('float64')).any()  ","754d0f20":"sub = sub.fillna(0)\nsub.to_csv('submission.csv', index=False)                             ","32afe42a":"sub.shape","9011177f":"sub","51141339":"### 3.6 Dataset Classes<a class=\"anchor\" id=\"3.6\"><\/a>\n\n[Back to Table of Contents](#0)","3c891962":"\n### [1. Import libraries](#1)\n\n### [2. Download data](#2)\n\n### [3. FE & Data Preprocessing](#3)\n\n*   [3.1 Gauss Rank Data normalization](#3.1)\n*   [3.2 Seed](#3.2)\n*   [3.3 PCA features](#3.3)\n*   [3.4 Feature selection](#3.4)\n*   [3.5 CV folds](#3.5)\n*   [3.6 Dataset Classes](#3.6)\n*   [3.7 Smoothing](#3.7)\n*   [3.8 Preprocessing](#3.8)   \n\n### [4. Modeling](#4)\n\n### [5. Submission](#5)\n\n\n","dc848244":"After the competition ended, I saw people saying Gauss Rank is better than QuantileTransformer, so I decided to try on my own, and Gauss Rank did turn out to be better than quantile transformer. Gauss Rank improved my score by 0.00002 and its result is more normally distributed. Maybe it's because QuantileTransformer is non-linear, and it may distorted some linear corrlation within the data (I'm just guessing the reason here, if you know why, please post a comment below, I'll be very thankful for that).","767721fb":"### 3.7 Smoothing<a class=\"anchor\" id=\"3.7\"><\/a>\n\n[Back to Table of Contents](#0)","20de34ab":"## 5. Submission<a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0)","c2ec55cc":"### 3.1 Gauss Rank Data normalization <a class=\"anchor\" id=\"3.1\"><\/a>\n\n[Back to Table of Contents](#0)","e60b98ee":"## 4. Modeling<a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0)","4df21dcf":"## Table of contents <a class=\"anchor\" id=\"0\"><\/a>","58b3b60f":"## 3. FE & Data Preprocessing <a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0)","49cacdcb":"### 3.4 FS by Variance Encoding<a class=\"anchor\" id=\"3.4\"><\/a>\n\n[Back to Table of Contents](#0)","9a257c44":"## 2. Download data<a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0)","c3f58633":"### 3.3 PCA features<a class=\"anchor\" id=\"3.3\"><\/a>\n\n[Back to Table of Contents](#0)","468256dc":"### 3.2 Seed<a class=\"anchor\" id=\"3.2\"><\/a>\n\n[Back to Table of Contents](#0)","e2707bb5":"## Intro","f3ac4cf0":"Hi, I'm a current high school student in Ottawa, Ontario, Canada. I started studying machine learning last year, and I first ran into neural network at the beginning of this year. I first used RNN to predict the trends of the stock market, but there is no luck yet. So if you seen anything I did wrong or misinterpreted anything, please don't hesitate to point them out.\n\nI joined this research competition in September. At first I thought I was going to classify different drugs, but after some extra researches, I discover the purpose of this research is to predict a drug's effect in human cells(the predictions are in percentages). At first I also thought the score is accuracy of prediction, only later I discovered my stupidity by realizing the score is calculate using log loss.\n\nDuring September, I first implemented random forest classifier, and only took the top 20 features with the highest corrolation for each MoA into the process of training. The score wasn't good, about 0.10115 in log loss. \n\nThen I saw others using neural network so I decided to build my own. I first used Sequential TensorFlow to build my model and I improved my socre to 0.01891 in log loss. \n\nAfter that, I saw [Mechanisms of Action (MoA) Tutorial by Sina MhD](https:\/\/https:\/\/www.kaggle.com\/sinamhd9\/mechanisms-of-action-moa-tutorial\/comments)\nand realized that only using one network and over-emphasizing the sophistication and the complexity of a single network may lead to over-fitting. So I started averaging out the results of different networks. This brought my score to 0.01869. And my current algorithm also inherited this idea with the exception of using ratios rather than simply taking the average. \n\nDuring November, I learned Pytorch, going from Tensorflow to Pytorch was a leap for me. This reminds me the time I went from using the language of Quantpian to Backtrader library in python. \n\nAnd I also discovered that QuantileTransformer is much more maneuverable than MinMaxScale is this case, since MinMaxScale transforms negative data into values between 0 and 0.005, but QuantileTransformer doesn't seem to have this problem with negative values. Also I discovered VarianceThreshold, which can be used to mow out the less relevent data. \n\nAfter the competition ended, I also used Gauss Rank to normalize the data. This also improved my score by 0.00002. \n\nIn the rest of notebook, many parts came from [MoA: Pytorch-RankGauss-PCA-NN upgrade & 3D visual](https:\/\/https:\/\/www.kaggle.com\/vbmokin\/moa-pytorch-rankgauss-pca-nn-upgrade-3d-visual) by [Prof. Vitalii Mokin](https:\/\/https:\/\/www.kaggle.com\/vbmokin) from Vinnytsia National Technical University. ","835c1214":"### 3.5 CV folds<a class=\"anchor\" id=\"3.5\"><\/a>\n\n[Back to Table of Contents](#0)","ea75bf43":"### 3.8 Preprocessing<a class=\"anchor\" id=\"3.8\"><\/a>\n\n[Back to Table of Contents](#0)","1100a91f":"Gauss Rank:\n![__results___19_2.png](attachment:__results___19_2.png)\n\n\nQuantile Transformer:\n![__results___20_2.png](attachment:__results___20_2.png)\nHere is also is visualized comparison between the results of QuantileTransformer and Gauss Rank.","5c466fda":"## 1. Import libraries<a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0)","08aec67b":"After several testings, I found out the networks tend to perform better than the others are the ones with more nodes in the second hidden layer than the first. Which I still consider surprising but queer, because theoratically hidden layers are perfered to have descending numbers of hidden units, in order to get rid of the \"noise\" in the data.\n\nAnd as I increase the numbers of nodes closer to 3000, the score tend to decrease.\n\nDropout rate = **number of node\/6000**\n\nIn the next cell's output is a dataframe of the summery of the scores of different networks.\n\nAnd the final result is the sum of each of the network's result multiplied by each of the network's ratio, the ratio is set based on network's Kaggle Public Score.\n\n","4381f967":"VarianceThreshold remove all features with low variances, in this case, lower than 0.82."}}