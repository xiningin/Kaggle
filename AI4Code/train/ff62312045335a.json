{"cell_type":{"7c2d9d36":"code","a94f1975":"code","d462ed90":"code","6347125e":"code","471651aa":"code","a4f39b76":"code","03d73d77":"code","8e2b6e16":"code","ddd3282c":"code","397df54a":"code","db003a61":"markdown","f5e1f695":"markdown","1553a175":"markdown","5ba42e9c":"markdown","916601b0":"markdown"},"source":{"7c2d9d36":"!pip install -U tensorflow\n","a94f1975":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.python.data.experimental.ops import interleave_ops\nfrom tensorflow.python.data.ops import dataset_ops\nfrom tensorflow.python.data.ops import iterator_ops\nfrom tensorflow.python.data.ops import readers\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import function\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import functional_ops\nfrom tensorflow.python.tpu import datasets\nimport numpy as np\nimport pandas as pd\nimport pickle","d462ed90":"#trainImgDir = KaggleDatasets().get_gcs_path('bengali-sign-language-dataset\/RESIZED_DATASET')\ntrainImgDir = '\/kaggle\/input\/bengali-sign-language-dataset\/RESIZED_DATASET\/'\nIMG_DIM = 224","6347125e":"new_input = tf.keras.Input(shape=(IMG_DIM, IMG_DIM,3))\nmodel = tf.keras.applications.VGG19(weights='imagenet', include_top=False, input_tensor=new_input, input_shape=(224, 224, 3))\nfor layer in model.layers:\n    layer.trainable = False\nflat1 = tf.keras.layers.GlobalAveragePooling2D()(model.layers[-1].output)\nclass2 = tf.keras.layers.Dense(256, activation='relu')(flat1)\nclass1 = tf.keras.layers.Dense(256, activation='relu')(class2)\nclass3 = tf.keras.layers.Dense(128, activation='relu')(class1)\n#drop = tf.keras.layers.Dropout(.3) (class1) \noutput = tf.keras.layers.Dense(38, activation='softmax')(class3)\n\nmodel = tf.keras.Model(inputs=model.inputs, outputs=output)\nmodel.summary()","471651aa":"import cv2\nfrom PIL import Image\ndef myFunc(image):\n    #image = np.array(image)\n    #hsv_image = cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n    image = tf.keras.applications.vgg19.preprocess_input(image)\n    return Image.fromarray(image)","a4f39b76":"data_augmentor = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1 \/ 255.0, validation_split=0.1)\n\ntrain_generator = data_augmentor.flow_from_directory(directory = '\/kaggle\/input\/bengali-sign-language-dataset\/RESIZED_DATASET\/', target_size=(224, 224), batch_size=16,\n                                                     shuffle=True, class_mode=\"categorical\", seed=42, subset=\"training\")\nvalid_generator = data_augmentor.flow_from_directory(directory = '\/kaggle\/input\/bengali-sign-language-dataset\/RESIZED_DATASET\/', target_size=(224, 224), batch_size=16, \n                                                   shuffle=True, class_mode=\"categorical\", seed=42, subset=\"validation\")\ntrain_generator.dtype","03d73d77":"reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='loss', factor=.2, patience=5, verbose=0, mode='auto',\n   min_lr=0.000001\n)\nterminate_NAN = tf.keras.callbacks.TerminateOnNaN()\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=20)\ncallback = ['reduce_lr', 'early_stop']","8e2b6e16":"opt = tf.keras.optimizers.SGD(lr=0.0001)\nmodel.compile(optimizer=opt, loss=['categorical_crossentropy'], metrics=['accuracy'])","ddd3282c":"\nmodel.fit(tf.data.Dataset.from_generator(lambda: train_generator,output_types=(tf.float32, tf.float32),\n    output_shapes = ([None,224,224,3],[None,38])), steps_per_epoch=train_generator.n \/\/ train_generator.batch_size,\n           validation_data=tf.data.Dataset.from_generator(lambda: valid_generator,output_types=(tf.float32, tf.float32),\n    output_shapes = ([None,224,224,3],[None,38])), validation_steps=valid_generator.n \/\/ valid_generator.batch_size,\n           epochs=200, callbacks = [reduce_lr, terminate_NAN, early_stop]).history","397df54a":"model.save_weights(\"model_weights.h5\")","db003a61":"datasets.StreamingFilesDataset(","f5e1f695":"# **** Not used in the code ****\n### Tried to implement TPU but Keras Data Augmentor didn't work on TPU. \n#### detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n#### instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n#### instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n    new_input = tf.keras.Input(shape=(IMG_DIM, IMG_DIM,3))\n    model = tf.keras.applications.VGG19(weights='imagenet', include_top=False, input_tensor=new_input, input_shape=(224, 224, 3))\n    for layer in model.layers:\n        layer.trainable = False\n    flat1 = tf.keras.layers.Flatten()(model.layers[-2].output)\n    class1 = tf.keras.layers.Dense(256, activation='relu')(flat1)\n    drop = tf.keras.layers.Dropout(.3) (class1) \n    output = tf.keras.layers.Dense(38, activation='softmax')(drop)\n\n    model = tf.keras.Model(inputs=model.inputs, outputs=output)\n    model.summary()","1553a175":"**Deep CNN based Bengali sign language detection model. This is a classification task having 38 different classes. Tensorflow 2 is used and tf.data.dataset pipeline is used for faster training and complex preprocessing. By testing different state-of-the-art models, found VGG19 giving the best output of having accuracy above 40%. As the data is noisy, gaining accuracy higher than this will need more advanced level preprocessing techniques. But still, this is considered a good score considering the dataset.**","5ba42e9c":"def generator(generate):\n    #model.fit_generator(train_generator, epochs=5, validation_data=valid_generator)\n    data_list = []\n    batch_index = 0\n\n    while batch_index <= generate.batch_index:\n        data = generate.next()\n        data_list.append(data[0])\n        batch_index = batch_index + 1\n    \n    \n    # now, data_array is the numeric data of whole images\n    array = np.asarray(data_list)\n    return array","916601b0":"steps_per_epoch=train_generator.n \/\/ train_generator.batch_size, validation_steps=valid_generator.n \/\/ valid_generator.batch_size,"}}