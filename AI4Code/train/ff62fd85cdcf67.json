{"cell_type":{"c1987e43":"code","8c8565ba":"code","22ae51c6":"code","4dd675b5":"code","b419151e":"code","cc61c012":"code","b0181134":"code","e9939fed":"code","b722dce2":"code","ad01d6dc":"code","614a7af7":"code","2362105f":"code","9b0a29b8":"code","8bf3d476":"code","c9f177e6":"code","2a203eca":"code","fc2d327c":"code","776c2dd1":"code","6a98441a":"code","a373a9a9":"code","a3f1721c":"code","2df1bb88":"code","14ad63f7":"markdown","d646c41e":"markdown","32523c4f":"markdown","28b41993":"markdown","586b3c2d":"markdown","c0355eda":"markdown","eb8f1e96":"markdown","3797a161":"markdown","880ac116":"markdown","b38ee9c2":"markdown","a380c366":"markdown","f5b9bbad":"markdown","041fb5d5":"markdown","10f3221c":"markdown","a5815a47":"markdown","f0688734":"markdown","9bff7f1c":"markdown","a063a1b0":"markdown"},"source":{"c1987e43":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8c8565ba":"import spacy\nnlp=spacy.load('en_core_web_sm')\ndoc = nlp(\"The 22-year-old recently won ATP Challenger tournament.\")\nfor tok in doc:\n    print(tok.text, \"...\", tok.dep_)","22ae51c6":"doc = nlp(\"Nagal won the first set.\")\n\nfor tok in doc:\n  print(tok.text, \"...\", tok.dep_)","4dd675b5":"import re\nimport pandas as pd\nimport bs4\nimport requests\nimport spacy\nfrom spacy import displacy\nnlp = spacy.load('en_core_web_sm')\n\nfrom spacy.matcher import Matcher \nfrom spacy.tokens import Span \n\nimport networkx as nx\n\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\npd.set_option('display.max_colwidth', 200)\n%matplotlib inline","b419151e":"# import wikipedia sentences\ncandidate_sentences = pd.read_csv(\"..\/input\/wiki_sentences_v2.csv\")\ncandidate_sentences.shape","cc61c012":"candidate_sentences['sentence'].sample(5)","b0181134":"doc = nlp(\"the drawdown process is governed by astm standard d823\")\n\nfor tok in doc:\n  print(tok.text, \"...\", tok.dep_)","e9939fed":"def get_entities(sent):\n  ## chunk 1\n  ent1 = \"\"\n  ent2 = \"\"\n\n  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n  prv_tok_text = \"\"   # previous token in the sentence\n\n  prefix = \"\"\n  modifier = \"\"\n\n  #############################################################\n  \n  for tok in nlp(sent):\n    ## chunk 2\n    # if token is a punctuation mark then move on to the next token\n    if tok.dep_ != \"punct\":\n      # check: token is a compound word or not\n      if tok.dep_ == \"compound\":\n        prefix = tok.text\n        # if the previous word was also a 'compound' then add the current word to it\n        if prv_tok_dep == \"compound\":\n          prefix = prv_tok_text + \" \"+ tok.text\n      \n      # check: token is a modifier or not\n      if tok.dep_.endswith(\"mod\") == True:\n        modifier = tok.text\n        # if the previous word was also a 'compound' then add the current word to it\n        if prv_tok_dep == \"compound\":\n          modifier = prv_tok_text + \" \"+ tok.text\n      \n      ## chunk 3\n      if tok.dep_.find(\"subj\") == True:\n        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n        prefix = \"\"\n        modifier = \"\"\n        prv_tok_dep = \"\"\n        prv_tok_text = \"\"      \n\n      ## chunk 4\n      if tok.dep_.find(\"obj\") == True:\n        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n        \n      ## chunk 5  \n      # update variables\n      prv_tok_dep = tok.dep_\n      prv_tok_text = tok.text\n  #############################################################\n\n  return [ent1.strip(), ent2.strip()]\n","b722dce2":"get_entities(\"the film had 200 patents\")","ad01d6dc":"entity_pairs = []\n\nfor i in tqdm(candidate_sentences[\"sentence\"]):\n  entity_pairs.append(get_entities(i))\n","614a7af7":"entity_pairs[10:20]","2362105f":"def get_relation(sent):\n\n  doc = nlp(sent)\n\n  # Matcher class object \n  matcher = Matcher(nlp.vocab)\n\n  #define the pattern \n  pattern = [{'DEP':'ROOT'}, \n            {'DEP':'prep','OP':\"?\"},\n            {'DEP':'agent','OP':\"?\"},  \n            {'POS':'ADJ','OP':\"?\"}] \n\n  matcher.add(\"matching_1\", None, pattern) \n\n  matches = matcher(doc)\n  k = len(matches) - 1\n\n  span = doc[matches[k][1]:matches[k][2]] \n\n  return(span.text)\n","9b0a29b8":"get_entities(\"John completed the task\")","8bf3d476":"relations = [get_relation(i) for i in tqdm(candidate_sentences['sentence'])]","c9f177e6":"pd.Series(relations).value_counts()[:50]","2a203eca":"# extract subject\nsource = [i[0] for i in entity_pairs]\n\n# extract object\ntarget = [i[1] for i in entity_pairs]\n\nkg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})","fc2d327c":"kg_df.head()","776c2dd1":"# create a directed-graph from a dataframe\nG=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())","6a98441a":"plt.figure(figsize=(12,12))\n\npos = nx.spring_layout(G)\nnx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","a373a9a9":"G=nx.from_pandas_edgelist(kg_df[kg_df['edge']==\"composed by\"], \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())\n\nplt.figure(figsize=(12,12))\npos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\nnx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","a3f1721c":"G=nx.from_pandas_edgelist(kg_df[kg_df['edge']==\"written by\"], \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())\n\nplt.figure(figsize=(12,12))\npos = nx.spring_layout(G, k = 0.5)\nnx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","2df1bb88":"G=nx.from_pandas_edgelist(kg_df[kg_df['edge']==\"released in\"], \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())\n\nplt.figure(figsize=(12,12))\npos = nx.spring_layout(G, k = 0.5)\nnx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","14ad63f7":"Next, we will use the networkx library to create a network from this dataframe. The nodes will represent the entities and the edges or connections between the nodes will represent the relations between the nodes.\n\nIt is going to be a directed graph. In other words, the relation between any connected node pair is not two-way, it is only from one node to another. For example, \u201cJohn eats pasta\u201d:","d646c41e":"Lets see the subject and object of these sentences","32523c4f":"# Knowledge Graph - A Powerful Data Science Technique to mine Information from Text","28b41993":"I have created a function below to extract the subject and the object (entities) from a sentence while also overcoming the challenges mentioned above. I have partitioned the code into multiple chunks for your convenience:","586b3c2d":"Let\u2019s take a look at the most frequent relations or predicates that we have just extracted:","c0355eda":"# Building a Knowledge Graph from Text Data","eb8f1e96":"# Build a Knowledge Graph\nWe will finally create a knowledge graph from the extracted entities (subject-object pairs) and the predicates (relation between entities).\n\nLet\u2019s create a dataframe of entities and predicates:","3797a161":"I can see quite a few interesting information in this graph. For example, look at this relationship \u2013 \u201cseveral action horror movies released in the 1980s\u201d and \u201cpk released on 4844 screens\u201d. These are facts and it shows us that we can mine such facts from just text. That\u2019s quite amazing!\nThank You!!!","880ac116":"Lets check our function","b38ee9c2":"That\u2019s a much cleaner graph.\nLet\u2019s check out a few more relations.\n\nSince writing is an important role in any movie, I would like to visualize the graph for the \u201cwritten by\u201d relation:","a380c366":"Let\u2019s get the relations for all the Wikipedia sentences:","f5b9bbad":"Well, this is not exactly what we were hoping for (still looks quite a sight though!).\n\nIt turns out that we have created a graph with all the relations that we had. It becomes really hard to visualize a graph with these many relations or predicates.\n\nSo, it\u2019s advisable to use only a few important relations to visualize a graph. I will take one relation at a time. Let\u2019s start with the relation \u201ccomposed by\u201d:","041fb5d5":"Now we can use this function to extract these entity pairs for all the sentences in our data:","10f3221c":"## What is Knowledge graph?\n\n#### A knowledge graph is made of a graph data store coupled with a knowledge toolkit. It stores a domain\u2019s data as entities and relationships using a graph model, which abides by an ontology.\n\n#### The knowledge toolkit supports continuous integration of data from multiple sources, allows end-users to query the graph effectively, and draws inferences from existing information.****","a5815a47":"Lets get the data","f0688734":"Awesome! This knowledge graph is giving us some extraordinary information. Guys like Vivek, Krishna Chaitanya, and Jaideep are all famous lyricists and this graph beautifully captures this relationship.\n\nLet\u2019s see the knowledge graph of another important predicate, i.e., the \u201creleased in\u201d:","9bff7f1c":"Let\u2019s plot the network:","a063a1b0":"Relation \/ Predicate Extraction"}}