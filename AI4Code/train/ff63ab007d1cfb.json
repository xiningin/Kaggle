{"cell_type":{"358a4664":"code","cf8d6628":"code","7281e665":"code","8b1ff93a":"code","08bb1e95":"code","aca5556d":"code","79012806":"code","1779fcd7":"code","e04df3d1":"code","8ecddbd6":"code","b5aa2fe4":"code","640e2724":"code","123bb032":"code","2e19f1e8":"code","1cda1e48":"code","dd026e9c":"code","21e96286":"code","138c801a":"code","3e672af7":"code","f1c7e9dc":"code","45e58956":"markdown","4750efba":"markdown","3fbb59e2":"markdown","6106eda8":"markdown","4fd44d71":"markdown","f621ba90":"markdown","7f076822":"markdown","292384b7":"markdown","151118fb":"markdown","0cb4affa":"markdown"},"source":{"358a4664":"!pip install ..\/input\/readability-package -qq\n\n!mkdir -p \/tmp\/pip\/cache\/\n!cp ..\/input\/syntok\/wheels\/syntok-1.3.1.xyz \/tmp\/pip\/cache\/syntok-1.3.1.tar.gz\n!cp ..\/input\/syntok\/wheels\/regex-2021.4.4-cp37-cp37m-manylinux2014_x86_64.whl \/tmp\/pip\/cache\/\n!pip install --no-index --find-links \/tmp\/pip\/cache\/ syntok","cf8d6628":"import readability\nimport numpy as np\nimport pandas as pd \nimport os\nimport syntok.segmenter as segmenter\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm.auto import tqdm\nimport scipy\nfrom scipy.cluster import hierarchy as hc\nfrom matplotlib import pyplot as plt","7281e665":"# https:\/\/www.kaggle.com\/takamichitoda\/commonlit-classical-methods-for-text-readability\ndef _calc_readability(text):\n    tokenized = '\\n\\n'.join(\n         '\\n'.join(' '.join(token.value for token in sentence)\n            for sentence in paragraph)\n         for paragraph in segmenter.analyze(text))\n    return readability.getmeasures(tokenized, lang='en')\n\ndef _extract_feat(row):\n    dic = {}\n    for k, v in row.items():\n        for kk, vv in v.items():\n            key = f'{\"_\".join(k.split())}_{kk}'\n            dic.update({key: vv})\n    return dic\n\n# https:\/\/github.com\/fastai\/fastbook\/blob\/master\/utils.py\ndef cluster_columns(df, figsize=(10,6), font_size=12):\n    corr = np.round(scipy.stats.spearmanr(df).correlation, 4)\n    corr_condensed = hc.distance.squareform(1-corr)\n    z = hc.linkage(corr_condensed, method='average')\n    fig = plt.figure(figsize=figsize)\n    hc.dendrogram(z, labels=df.columns, orientation='left', leaf_font_size=font_size)\n    plt.show()","8b1ff93a":"df = pd.read_csv('..\/input\/step-1-create-folds\/train_folds.csv')\ndf['readability'] = df['excerpt'].map(lambda x: _calc_readability(x))\n\ndf_features = pd.DataFrame(df['readability'].map(_extract_feat).tolist())\ndf_features['kfold'] = df['kfold']\ndf_features['target'] = df['target']\n\ndf_train = df_features[df_features.kfold != 0].reset_index(drop=True)\ndf_valid = df_features[df_features.kfold == 0].reset_index(drop=True)\n\ntrain_features = df_train.drop(['kfold', 'target'], axis=1)\nvalid_features = df_valid.drop(['kfold', 'target'], axis=1)\ntrain_labels = df_train.target.values\nvalid_labels = df_valid.target.values","08bb1e95":"def rf(train_features, valid_features, train_labels, valid_labels):\n    model = RandomForestRegressor(random_state=42)\n    model.fit(train_features, train_labels)\n    valid_preds = model.predict(valid_features)\n    rmse = mean_squared_error(valid_labels, valid_preds, squared=False)\n    return model, rmse\n\nmodel, rmse = rf(train_features, valid_features, train_labels, valid_labels)\nrmse","aca5556d":"def rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)","79012806":"fi = rf_feat_importance(model, train_features)\nfi[:5]","1779fcd7":"def plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(fi);","e04df3d1":"to_keep = fi[fi.imp>0.01].cols\n\ntrain_features_imp = train_features[to_keep]\nvalid_features_imp = valid_features[to_keep]\n\nmodel, rmse = rf(train_features_imp, valid_features_imp, train_labels, valid_labels)\nrmse","8ecddbd6":"cluster_columns(train_features_imp)","b5aa2fe4":"def get_score(train_feats, f):\n    model = RandomForestRegressor(random_state=42)\n    model.fit(train_feats.drop(f, axis=1), train_labels)\n    valid_preds = model.predict(valid_features_imp.drop(f, axis=1))\n    rmse = mean_squared_error(valid_labels, valid_preds, squared=False)\n    return rmse","640e2724":"{f:get_score(train_features_imp, f) for f in (\n    'sentence_info_characters_per_word', 'readability_grades_Coleman-Liau', 'readability_grades_Kincaid',\n    'readability_grades_ARI', 'readability_grades_LIX', 'readability_grades_RIX')}","123bb032":"train_features_final = train_features_imp.drop('readability_grades_RIX', axis=1)\nvalid_features_final = valid_features_imp.drop('readability_grades_RIX', axis=1)\n\nmodel, rmse = rf(train_features_final, valid_features_final, train_labels, valid_labels)\nrmse","2e19f1e8":"plot_fi(rf_feat_importance(model, train_features_final));","1cda1e48":"test_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\ntest_df['readability'] = test_df['excerpt'].map(lambda x: _calc_readability(x))\ntest_df_features = pd.DataFrame(test_df['readability'].map(_extract_feat).tolist())","dd026e9c":"def get_preds(features):\n    test_features = test_df_features[features]\n\n    fold_scores = []\n    fold_preds = []\n\n    for fold in tqdm(range(5)):\n        df_train = df_features[df_features.kfold != fold].reset_index(drop=True)\n        df_valid = df_features[df_features.kfold == fold].reset_index(drop=True)\n\n        train_features = df_train[features]\n        valid_features = df_valid[features]\n\n        train_labels = df_train.target.values\n        valid_labels = df_valid.target.values\n\n        model, rmse = rf(train_features, valid_features, train_labels, valid_labels)\n        fold_scores.append(rmse)\n\n        test_preds = model.predict(test_features)\n        fold_preds.append(test_preds)\n\n    return np.mean(fold_scores), fold_scores, fold_preds","21e96286":"features = [x for x in df_features.columns.tolist() if x not in ['kfold', 'target']]\nscore, fold_scores, fold_preds = get_preds(features)\nscore","138c801a":"features = train_features_final.columns.tolist()\nscore, fold_scores, fold_preds = get_preds(features)\nscore","3e672af7":"preds = np.stack(fold_preds).mean(axis=0)","f1c7e9dc":"sub = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\nsub.target = preds\nsub.to_csv('submission.csv', index=False)\nsub.head()","45e58956":"# CommonLit Readability Feature Importances\n\nThis competition is interesting because it can be approached with a variety of features and approaches - tabular features, NLP, CV etc. In this notebook, I'd like to explore the tabular features and their importances. I'll use the `readability` package (thanks to @takamichitoda for the demo notebook). The feature importance investigation follows the process from **Deep Learning for Coders with Fastai and PyTorch** book by @jhoward and @sgugger. \n\nSources:\n- https:\/\/github.com\/fastai\/fastbook\/blob\/master\/09_tabular.ipynb\n- https:\/\/www.kaggle.com\/takamichitoda\/commonlit-classical-methods-for-text-readability\n- https:\/\/github.com\/andreasvc\/readability\/","4750efba":"Let's try to cut off the less important features and see the effect on the score. ","3fbb59e2":"# 5-fold Training and Test Inference\n\nLet's confirm that our feature removal also works when evaluated in a 5-fold cv setting. We'll get the mean RMSE score for both *all features* and *final features* setting. ","6106eda8":"## Feature Importances\n\nWe can now check the relative feature importances and plot them. ","4fd44d71":"## Random Forest Regressor\n\nLet's start by building a simple random forest regressor model with all the readability features and see its score on a single fold (using folds by @abhishek)","f621ba90":"## Submission","7f076822":"## Similar Features\n\nWe can now see which features are highly correlated, and see if removing them helps us get a better score. ","292384b7":"Our final features perform sligthly better than all features, so we will use them to make our final submission. ","151118fb":"So the biggest improvement comes from removing `readability_grades_RIX` feature, which incidentally is also the most important feature! Glad we discovered this, as written for example [here](https:\/\/readable.com\/blog\/the-lix-and-rix-readability-formulas), LIX and RIX features are highly correlated, so we should be safe to remove RIX from our feature set. Let's also review our final feature importances. ","0cb4affa":"The score improved! Fewer features means smaller risk of overfitting, so this is promissing. "}}