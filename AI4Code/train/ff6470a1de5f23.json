{"cell_type":{"b1c23ff0":"code","3338e7d0":"code","c6c2d8c3":"code","f135cd93":"code","9d9455cb":"code","22ab69dd":"code","9845a580":"code","9cc51483":"code","e87307ce":"code","a7698505":"code","66bfbb23":"code","acc530cf":"code","60c4a286":"code","599894a5":"code","a616215a":"code","e753177f":"markdown","cf27a2f7":"markdown","99dab68a":"markdown","779805ee":"markdown","5dd37e3d":"markdown","70a9fc8a":"markdown","91a79845":"markdown","07b41407":"markdown","4cd64e64":"markdown","d12579d2":"markdown","5e26f1b1":"markdown","e5d47f84":"markdown","8f0467fb":"markdown","f1b106a7":"markdown","8f5ea10c":"markdown","f1f8fb3a":"markdown","eea939c8":"markdown","bef69062":"markdown","46616118":"markdown","108ce9dc":"markdown","343b0d68":"markdown","7af1c84d":"markdown","0c94ad9f":"markdown","7a375894":"markdown","045fce12":"markdown","c898ac76":"markdown","2518783b":"markdown","09e4da40":"markdown","942d9e41":"markdown","48a69902":"markdown","dfc49b81":"markdown","b8c8d3d0":"markdown","f53028b6":"markdown","5bd79aba":"markdown","f78ae4f1":"markdown","1ed1d8cd":"markdown","c765d6cd":"markdown","d7ffce6f":"markdown","5235e616":"markdown","20bc1d47":"markdown","ff30d255":"markdown","7e09771d":"markdown","067729c8":"markdown","c89c5c63":"markdown","f497c380":"markdown","b183da7e":"markdown","3c09b202":"markdown","526734cc":"markdown","a6acb28a":"markdown","56308a24":"markdown","4ff1a81c":"markdown","568eaabc":"markdown","eb5a44a2":"markdown","8434b898":"markdown","6a3532fe":"markdown","5441d611":"markdown","897a24a3":"markdown","8d236338":"markdown","71c0c6fd":"markdown","ee8b0b2a":"markdown","842ef55b":"markdown","369f8a1d":"markdown","03cfe691":"markdown","8df1515c":"markdown","7ccdf5a9":"markdown","76a93c35":"markdown","bd73f278":"markdown","74422b9d":"markdown","1bc44ce6":"markdown","82d2c72a":"markdown","8d3b056c":"markdown","291cf1b2":"markdown","9f180b5a":"markdown","d038148a":"markdown","e37c1009":"markdown","3106abdc":"markdown","4857c0ee":"markdown","ad23c0ef":"markdown","4c4f0a26":"markdown","1850180f":"markdown","931f015c":"markdown"},"source":{"b1c23ff0":"import sys\nprint(sys.version)","3338e7d0":"from IPython.display import YouTubeVideo\n\nYouTubeVideo('Q81RR3yKn30', width=800, height=300)\n","c6c2d8c3":"\"\"\"\nGenerate a 10x10 Hilbert matrix\n\"\"\"\n\nfrom scipy.linalg import hilbert\n\nx = hilbert(10)\nx","f135cd93":"#Type your code here-","9d9455cb":"\"\"\"\nMultiply Hilbert transposed matrix with the original one\n\"\"\"\n\nimport numpy as np\n\nnp.matrix(x).T * np.matrix(x)","22ab69dd":"#Type your code here-\n","9845a580":"\"\"\"\nCalculate the correlation coefficient of the Hilbert matrix \n\"\"\"\n\nimport pandas as pd\n\npd.DataFrame(x, columns=['x%d'%i for i in range(1,11)]).corr()","9cc51483":"\"\"\"\nDefine linear distribution function and the actual parameters\n\"\"\"\n\nfrom scipy.optimize import leastsq\nimport numpy as np\nfrom scipy.linalg import hilbert\n\nx = hilbert(10) # Generate 10x10 Hilbert Matrix\nnp.random.seed(10) # Random number seed will guarantee the same random number generated every time\nw = np.random.randint(2,10,10) # Generate w randomly\ny_temp = np.matrix(x) * np.matrix(w).T # Calculate y\ny = np.array(y_temp.T)[0] #Convert y to 1D row vector\n\nprint(\"Actual Parameters w: \", w)\nprint(\"Actual Values y: \", y)","e87307ce":"\"\"\"\nLeast squares linear fitting\n\"\"\"\n\nfunc=lambda p,x: np.dot(x, p) # Function\nerr_func = lambda p, x, y: func(p, x)-y # Loss function \np_init=np.random.randint(1,2,10) # Init all parameters as 1\n\nparameters = leastsq(err_func, p_init, args=(x, y)) # LS method\nprint(\"Fitted Parameters w: \",parameters[0])","a7698505":"YouTubeVideo('NGf0voTMlcs', width=800, height=300)","66bfbb23":"\"\"\"\nRidge regression fitting\n\"\"\"\n\nfrom sklearn.linear_model import Ridge\n\nridge_model = Ridge(fit_intercept=False) # Without intercept\nridge_model.fit(x, y)","acc530cf":"ridge_model.coef_ # Print parameters","60c4a286":"\"\"\"\nDifferent alpha to get different fitting results\n\"\"\"\n\nalphas = np.linspace(-3,2,20)\n\ncoefs = []\nfor a in alphas:\n    ridge = Ridge(alpha=a, fit_intercept=False)\n    ridge.fit(x, y)\n    coefs.append(ridge.coef_)","599894a5":"\"\"\"\nPlot the results with different alphas\n\"\"\"\n\nfrom matplotlib import pyplot as plt\n#get_ipython().run_line_magic('matplotlib', 'inline')\n%matplotlib inline\n\nplt.plot(alphas, coefs) # Plot w-alpha\nplt.scatter(np.linspace(0,0,10), parameters[0]) # Add w of OLS in the figure\nplt.xlabel('alpha')\nplt.ylabel('w')\nplt.title('Ridge Regression')","a616215a":"\"\"\"\nLASSO regression fitting and plot\n\"\"\"\n\nfrom sklearn.linear_model import Lasso\n\nalphas = np.linspace(-2,2,10)\nlasso_coefs = []\n\nfor a in alphas:\n    lasso = Lasso(alpha=a, fit_intercept=False)\n    lasso.fit(x, y)\n    lasso_coefs.append(lasso.coef_)\n    \nplt.plot(alphas, lasso_coefs) # plot w-alpha\nplt.scatter(np.linspace(0,0,10), parameters[0]) # Add w of OLS in the figure\nplt.xlabel('alpha')\nplt.ylabel('w')\nplt.title('Lasso Regression')","e753177f":"## Summary","cf27a2f7":"First, let's review the content of linear regression. In linear regression, we need to solve the following function:","99dab68a":"<br>\n$$H_{{ij}}={\\frac  {1}{i+j-1}} \\tag5$$","779805ee":"In the sample dataset corresponding to the Hilbert matrix above, assume that the known $x_{1}$ to $x_{10}$ obey the linear distribution:<br>\n<br>\n$$ y= w_{1} * x_{1} + w_{2} * x_{2} +\u2026\u2026+w_{10} * x_{10}\\tag7$$","5dd37e3d":"When we use the ordinary least squares (OLS) to learn (set the loss function as the square loss function), it becomes the minimum objective function:","70a9fc8a":"We conclude that, when the value of `alpha` is larger, the regular term dominates the convergence process, and every $w$ coefficient approaches to $0$. When `alpha` is small, the fluctuations of the $w$ coefficients become larger.","91a79845":"In the previous sections, we learned to solve the linear regression and polynomial regression problems using the least squares method. At the same time, in the chapter of polynomial regression, we learned that multiple regression can be regarded as a special form of linear regression, that is to say, the core of the regression method is linear regression.","07b41407":"#### Pearson Correlation Coefficient","4cd64e64":"A Hilbert matrix of $5$x$5$ is shown by the formula $(6)$:<br>\n<br>\n$$\nH_{5}={\\begin{bmatrix}1&{\\frac  {1}{2}}&{\\frac  {1}{3}}&{\\frac  {1}{4}}&{\\frac  {1}{5}}\\\\[4pt]{\\frac  {1}{2}}&{\\frac  {1}{3}}&{\\frac  {1}{4}}&{\\frac  {1}{5}}&{\\frac  {1}{6}}\\\\[4pt]{\\frac  {1}{3}}&{\\frac  {1}{4}}&{\\frac  {1}{5}}&{\\frac  {1}{6}}&{\\frac  {1}{7}}\\\\[4pt]{\\frac  {1}{4}}&{\\frac  {1}{5}}&{\\frac  {1}{6}}&{\\frac  {1}{7}}&{\\frac  {1}{8}}\\\\[4pt]{\\frac  {1}{5}}&{\\frac  {1}{6}}&{\\frac  {1}{7}}&{\\frac  {1}{8}}&{\\frac  {1}{9}}\\end{bmatrix}}\\tag6\n$$","d12579d2":"In $(3)$, $X$ is a matrix of $(x_{1},x_{2},\u2026\u2026,x_{n})^{T}$ data, and $y$ is a column vector of $(y_{1},y_{2},\u2026\u2026,y_{n})^{T}$. At the same time, the analytical solution of the formula $(3)$ is:","5e26f1b1":"In this section, we will complete the data fitting in section 1.2 through the ridge regression method `Ridge()` provided by `scikit-learn`:","e5d47f84":"## LASSO Regression","8f0467fb":"$$F_{Ridge}={\\left \\| y-Xw \\right \\|_{2}}^{2} + \\lambda{\\left \\| w \\right \\|_{2}}^{2} \\tag9$$","f1b106a7":"$$F_{LASSO} = {\\left \\| y-Xw \\right \\|_{2}}^{2} + \\lambda{\\left \\| w \\right \\|_{1}} \\tag{12}$$","8f5ea10c":"When the paraboloid is constrained by $w_{1}^2+w_{2}^2 \\leq t^2$, it is equivalent to a circle in a two-dimensional plane (blue below). At this time, the point where the contour is tangent to the circle is the best advantage under the constraint condition, as shown in the following figure:","f1f8fb3a":"### Derivation of Ridge Regression","eea939c8":"![7%20Ridge%20Regression%20and%20LASSO%20Regression%201.jpeg](attachment:7%20Ridge%20Regression%20and%20LASSO%20Regression%201.jpeg)","bef69062":"### Key Points","46616118":"The formula $(8)$ is a vector representation of the objective function  under ridge regression optimization. In fact, the formula $(8)$ is also equivalent to:<br>\n<br>\n$$F_{Ridge}={\\left \\| y-Xw \\right \\|_{2}}^{2} $$\n$$ s.t. {\\left \\| w \\right \\|_{2}} \\leqslant t \\tag{11}$$","108ce9dc":"```python\nsklearn.linear_model.Lasso(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n```\n- `alpha`: 1 by default. Constant that is multiplied with the L1 term.<br>\n- `fit_intercept`: True by default. Whether to calculate the intercept for this model.<br>\n- `normalize`: False by default. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm.<br>\n- `precompute`: Whether to use a pre-computed Gram matrix to speed calculations up.<br>\n- `copy_X`: If True (default), X will be copied; else, it may be overwritten.<br>\n- `max_iter`: The maximum number of iterations.<br>\n- `tol`: The tolerance for the optimization.<br>\n- `warm_start`: When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution.<br>\n- `positive`: When set to True, forces the coefficients to be positive.<br>\n- `random_state`: The seed of the pseudo random number generator that selects a random feature to update.<br>\n- `selection`: If set to \u2018random\u2019, a random coefficient is updated every iteration rather than looping over features sequentially by default.","343b0d68":"We can see that, when the value of `alpha` is larger, the regular term dominates the convergence process and every $w$ coefficient approaches $0$. When `alpha` is small, the fluctuations of the $w$ coefficients become larger.","7af1c84d":"The condition for formula $(4)$ is that $\\left | X^{T}X \\right |$ cannot be $0$. And, when the correlation between variables is strong (multicollinearity) or $m$ is greater than $n$, $X$ in $(4)$ is not a full-rank ($rank(A)\\neq dim( x)$) matrix. Thus, the result of $\\left | X^{T}X \\right |$ approaches $0$, leading to an increase in the numerical instability of the fitting parameters, which is exactly one limitation of the ordinary least squares method.","0c94ad9f":"Here, we use the `hilbert()` method provided by SciPy to create a Hilbert matrix of $10$x$10$ directly:","7a375894":"Where $t$ is a constant corresponding to $\\lambda$. We avoid overfitting by limiting the size of ${\\left \\| w \\right \\|_{2}}$ here. So, suppose we have two variables. Then the residual sum of squares $(y_{1}-w_{1}^Tx)^2 + (y_{2}-w_{2}^Tx)^2$ is a quadratic function, which represents a paraboloid in three-dimensional space, and it can be geometrically represented by a contour (red below).","045fce12":"We first use NumPy to randomly generate the $w$ parameter and get the actual corresponding $y$ value:","c898ac76":"Similar to the ridge regression, the LASSO regression also improves the ordinary least squares by adding regular terms, but here the $L_{1}$ regular term is added, which is:","2518783b":"From the difference between the formula $(4)$ and the formula $(10)$, it can be seen that by adding an identity matrix to $X^TX$, the matrix becomes a full-rank matrix, perfecting the deficiency of the ordinary least squares method.","09e4da40":"## Ridge Regression and LASSO Regression ","942d9e41":"When we use the ordinary least squares method for regression fitting, if the correlation between the characteristic variables is strong, it may cause some $w$ coefficients to be large, while others become small negative numbers. For this consideration, we add the $L_{2}$ regular term to the ridge regression above.","48a69902":"In the regression prediction, in addition to the linear regression and the polynomial regression mentioned before, there are many other regression methods as well. For example, Ridge Regression, LASSO Regression and various other regression methods derived from the next week's classification method. In this section, we first introduce the ridge regression and the LASSO regression methods, both of which belong to regularization methods, which can solve several problems that linear regression cannot cover.","dfc49b81":"```python\nsklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)<br>\n```\n- `alpha`: 1 by default. Regularization strength, corresponding to $\\lambda$ in $(8)$.<br>\n- `fit_intercept`: True by default. Whether to calculate the intercept for this model.<br>\n- `normalize`: False by default. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm.<br>\n- `copy_X`: If True (default), X will be copied; else, it may be overwritten.<br>\n- `max_iter`: Maximum number of iterations for conjugate gradient solver.<br>\n- `tol`: Precision of the solution.<br>\n- `solver`: Solvers to use for different type of data.<br>\n- `random_state`: The seed of the pseudo random number generator to use when shuffling the data.","b8c8d3d0":"![7%20Ridge%20Regression%20and%20LASSO%20Regression%202.jpeg](attachment:7%20Ridge%20Regression%20and%20LASSO%20Regression%202.jpeg)","f53028b6":"There is a video about Lasso Regression. We hope it can deepen your understanding of Supervised Learning: Regularisation with Lasso Regression\n\n[Video] Lasso Regression | Source: [Statquest.org](Statquest.org](https:\/\/statquest.org\/video-index\/)","5bd79aba":"## Limitations of Ordinary Least Squares Method","f78ae4f1":"### LASSO Regression Fitting","1ed1d8cd":"Pandas provides a way to directly calculate the correlation coefficient. Thus it's quite convenient to calculate the correlation coefficient of the Hilbert matrix above:","c765d6cd":"You may ask: Why do we introduce Hilbert matrix here? That's because now we are intended to perform a least squares linear fitting experiment using the values of the Hilbert matrix. Since there is a strong linear correlation among the values of every column of the Hilbert matrix, it coincides with the result of $X^{T}X$ mentioned in the section 1.1, which is close to $0$.","d7ffce6f":"### OLS Linear Fitting for Hilbert Matrix","5235e616":"It can be seen that, although there is a gap with the model's true parameter, the parameters are much better compared to the OLS method.","20bc1d47":"As you can see, the result of $X^{T}X$ here is indeed close to $0$.","ff30d255":"- Principles of ridge regression<br>\n- Ridge regression fitting<br>\n- Geometric meaning of ridge regression<br>\n- Principles of LASSO regression<br>\n- LASSO regression fitting<br>\n- Geometric meaning of LASSO regression<br>\n- Regularization<br>\n- Correlation coefficient","7e09771d":"---","067729c8":"---","c89c5c63":"<h1 style=\"color:brown\">Supervised Learning: Regression","f497c380":"Similarly, we fit the data in section 1.2 through the LASSO regression method `Lasso()` provided by `scikit-learn`. This time we no longer write it by steps, but in the same cell:","b183da7e":"### Ridge Regression Fitting","3c09b202":"The analytical solution for the regression coefficient $w$ in the formula $(9)$ is:<br>\n<br>\n$$\\hat w_{Ridge} = (X^TX + \\lambda I)^{-1} X^TY \\tag{10}$$","526734cc":"$$F={\\left \\| y-Xw \\right \\|_{2}}^{2}  \\tag3$$","a6acb28a":"### Matrix Representation of Ordinary Least Squares Method","56308a24":"$$F=\\sum_{i=1}^{n}(y_{i}-w^Tx)^2  \\tag2$$","4ff1a81c":"Where $t$ is a constant corresponding to $\\lambda$. We avoid overfitting by limiting the size of $ {\\left \\| w \\right \\|_{2}}$ here. So, suppose we have two variables. Then the residual sum of squares $(y_{1}-w_{1}^Tx)^2 + (y_{2}-w_{2}^Tx)^2$ is a quadratic function, which represents a paraboloid in three-dimensional space, and it can be geometrically represented by a contour (red below).<br>\n","568eaabc":"When the paraboloid is constrained by the $\\left \\| w_{1} \\right \\|_{1}+\\left \\| w_{2} \\right \\|_{1} \\leq t$, it is equivalent to a rectangle under the dimension plane (blue below). At this time, the point where the contour is tangent to the rectangular fixed point is the most advantageous under the constraint condition, as shown in the following figure:","eb5a44a2":"In this chapter, we introduced two regularization methods, namely ridge regression and LASSO regression. There are many similarities between them, but there are differences also. This chapter contains many mathematical formulas and principles, which require you to have a certain knowledge of linear algebra. The main points of this chapter are:<br>\n- Principles of ridge regression<br>\n- Ridge regression fitting<br>\n- Geometric meaning of ridge regression<br>\n- Principles of LASSO regression<br>\n- LASSO regression fitting<br>\n- Geometric meaning of LASSO regression<br>\n- Regularization<br>\n- Correlation coefficient","8434b898":"You will find that the actual parameter $w$ and the fitted parameter $w$ are very different. In fact, this confirms the limitations of the ordinary least squares method described in Section 1.1.","6a3532fe":"## LASSO Regression","5441d611":"### Introduction","897a24a3":"Next, we use the least squares method learned before to linearly fit the dataset and find the parameters obtained by fitting:","8d236338":"- Python 3.6.6<br>\n- NumPy 1.18.1<br>\n- Matplotlab 3.0.3<br>\n- Pandas 0.23.0.3<br>\n- `scikit-learn` 0.22.1","71c0c6fd":"**\u261e Exercise:**","ee8b0b2a":"In order to solve the problems in the above two cases, Ridge Regression came into being. Ridge regression can be seen as an improved least squares estimation method, which effectively prevents the model from being overfitted by adding the $L_{2}$ regular term to the loss function. It solves the problem of difficulty in inverting under non-full-rank conditions, thus improving the explanatory ability of the model as well. In a mathematical expression, the loss function corresponding to the above formula $(2)$ becomes:","842ef55b":"<div style=\"color: #999;font-size: 12px;font-style: italic;\"><br>\n**Question**: If you are careful enough when using `sklearn.linear_model.LinearRegression()` to construct a linear regression model, you will find that the obtained parameters are close to the ridge regression and quite different from the least squares method. Why?<br>\n<br><br><br>\n**Answer**: In fact, this actually shows that `scikit-learn` provides a packaged linear regression class `LinearRegression()` that does not just implement a simple least squares method. It may be for convenience of use, thus regularization and other means are used by default under certain conditions. So, the result of the ordinary least squares method is different. `scikit-learn` is a very mature module. Every class supports a lot of parameters and implementation functions. You can learn more through the official documentation.<br>\n<\/div>","369f8a1d":"### Geometric Meaning of Ridge Regression (Elective)","03cfe691":"This is the geometric meaning of the ridge regression.","8df1515c":"---","7ccdf5a9":"$$F_{Ridge}=\\sum_{i=1}^{n}(y_{i}-w^Tx)^2 + \\lambda \\sum_{i=1}^{n}(w_{i})^2  \\tag8$$","76a93c35":"Similar to ridge regression, the objective function of LASSO regression optimization is also equivalent to:<br>\n<br>\n$$F_{LASSO}={\\left \\| y-Xw \\right \\|_{2}}^{2} $$<br>\n<br>\n$$ s.t. {\\left \\| w \\right \\|_{1}} \\leqslant t \\tag{13}$$","bd73f278":"### Geometric Meaning of LASSO Regression (Elective)","74422b9d":"$$\\hat{w} = (X^{T}X)^{-1}X^{T}y \\tag4$$","1bc44ce6":"In linear algebra, the Hilbert matrix is a block matrix in which the coefficients are unit fractions. Specifically, the coefficient of the $i$ row and the $j$ column in a Hilbert matrix $H$ is:<br>\n","82d2c72a":"We can rewrite $(8)$ to a vector representation:","8d3b056c":"There is a video about Ridge Regression. We hope it can deepen your understanding of Supervised Learning: Regularisation with Ridge Regression\n\n[Video] Ridge Regression | Source: [Statquest.org](https:\/\/statquest.org\/video-index\/)","291cf1b2":"### Environment","9f180b5a":"<div style=\"color: #999;font-size: 12px;font-style: italic;\">* $\\left \\| y-Xw \\right \\|_{2}$ represents the l2-norm, which is the root of the sum of the squares of the absolute values of all the vector elements.<\/div>","d038148a":"The Pearson Correlation Coefficient is usually used to measure the degree of linear correlation between two variables $X$ and $Y$. The value of Pearson Correlation Coefficient is between $-1$ and $1$. Closer the value is to $1$, the higher the correlation degree is; and closer to $-1$, the lower the linear correlation degree is.","e37c1009":"#### Least Squares Linear Fitting","3106abdc":"We can rewrite $(2)$ as a vector representation:","4857c0ee":"As mentioned above, the limitations imposed by the ordinary least squares method have led to the inability to directly use linear regression fittings for many cases. In particular, the following two situations attract attention:<br>\n<br>\n-  **The number of columns (features) in the dataset is larger than the amount of data (the number of rows), that is, $X$ is not a full-rank matrix.**<br>\n- **There is a strong linear correlation in the dataset column (feature) data, that is, the model is prone to overfitting.**","ad23c0ef":"$$ f(x)=\\sum_{i=1}^{m}w_i x_i =w^Tx \\tag1$$","4c4f0a26":"The `alpha` parameter in the `Ridge()` model controls the regularization strength. Below, we try to adjust this parameter to get different fitting results:","1850180f":"**\u261e Exercise:**","931f015c":"As shown above, in a $10$x$10$ Hilbert matrix, there is a high numerical correlation in every column of data."}}