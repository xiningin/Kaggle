{"cell_type":{"1fe49b3b":"code","3a37f24c":"code","3b191b28":"code","57989047":"code","4a80b30d":"code","45963ea0":"code","b8748cd4":"code","4cf31ad8":"code","5055c86f":"code","92bc2cb2":"code","931f77ce":"code","86353475":"code","e21efcff":"code","cfa51a8a":"markdown"},"source":{"1fe49b3b":"# Importing the required libraries\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport os\nimport glob\nimport pickle\nimport time\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.preprocessing import image","3a37f24c":"# Version\n\nimport tensorflow as tf\nprint('Tensorflow version:', tf.__version__)\nprint('OpenCV version:', cv2.__version__)","3b191b28":"# Initializing the paths\n\ninput_path = '\/kaggle\/input\/deepfake-detection-challenge\/'\ntrain_dir = glob.glob(input_path + 'train_sample_videos\/*.mp4')","57989047":"# Reading the labels of training data\n\ndf_train = pd.read_json(input_path + 'train_sample_videos\/metadata.json').transpose()\ndf_train.head()","4a80b30d":"# Plotting the count of labels\n\n# Fake class is in majority\ndf_train.label.value_counts().plot.bar()","45963ea0":"df_train.head()","b8748cd4":"# Undersampling\n\ni = 0\nfor ind in df_train.index: \n    if(i > 243):\n        break\n    if(df_train['label'][ind] == 'FAKE'):\n        i += 1\n        train_dir.remove(input_path + 'train_sample_videos\/' + ind)","4cf31ad8":"len(train_dir)","5055c86f":"# Taking the base model as Inception V3 and initializing its weight with imagenet\n\n# Enable internet on kernel settings\ninput_tensor = Input(shape = (229, 229, 3))\ncnn_model = InceptionV3(input_tensor = input_tensor, weights = 'imagenet', include_top = False, pooling = 'avg')\ncnn_model.summary()","92bc2cb2":"# Number of frames per video\n\nfpv = 5","931f77ce":"# Creating 40 frames per video, resizing it to (229, 229, 3) and feeding it to Pretrained Inception-V3 to extract features\n# Extracted features are stored in cnn_output\n\nt = time.time()\ncnn_output = {}\n# count1 = 0\nfor v in train_dir:\n    t1 = time.time()\n    folder_name = v.split('\/')[5]\n    cap = cv2.VideoCapture(v)\n    count = 0\n    while count < fpv:\n        cap.set(cv2.CAP_PROP_POS_MSEC,((count * 30) + 5000))   \n        ret, frame = cap.read()\n        frame = cv2.resize(frame, (229, 229))\n        x = image.img_to_array(frame)\n        x = np.expand_dims(x, axis = 0)\n        x = preprocess_input(x)\n        result = cnn_model.predict(x)\n        if folder_name not in cnn_output.keys():\n            cnn_output[folder_name] = []\n        cnn_output[folder_name].append(list(result))\n        count = count + 1\n#     count1 += 1\n#     print('Elapsed: ', time.time() - t1, ' | ', count1, '\/', len(train_dir), ' | ', v)\nprint('Total elapsed: ', time.time() - t)","86353475":"# Saving the cnn_output\n\nos.mknod('cnn_output.txt')\nwith open('cnn_output.txt', 'wb') as f:\n    pickle.dump(cnn_output, f)","e21efcff":"# # Retrieving the cnn_output\n\n# with open('cnn_output.txt', 'rb') as f:\n#     cnn_output = pickle.load(f)","cfa51a8a":"This is a demonstration of how to capture frames from training videos and feed into the Pretrained Inception-V3 to extract 2048 dimensional feature vector. This is just the preprocessing step done on all the (77 (REAL) + 79 (FAKE)) training videos. The dataset undersampled.\n\n**Note**: Enable internet on kernel settings to download the weights for Inception-V3"}}