{"cell_type":{"be92076c":"code","ae8f876e":"code","2d644ca0":"code","cda25633":"code","08d41060":"code","84e9c5ef":"code","3b9a2e7b":"code","5ca545e4":"code","ce1bf4cf":"code","7a48f97b":"code","5ab4b57e":"code","94947ff0":"code","19fdd97b":"markdown","3a141a11":"markdown","e545346e":"markdown","1639a94b":"markdown","d41cd1a8":"markdown"},"source":{"be92076c":"import numpy as np\nimport pandas as pd\n\nimport os\n\nimport torch\nfrom torch import optim\nfrom torch import nn\n\nimport torchaudio\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\nfrom sklearn.utils import shuffle\nimport matplotlib.pyplot as plt","ae8f876e":"class MusicDS(Dataset):\n    def __init__(self, path):\n        labels = os.listdir(path)\n        self.idx_to_labels = {k:v for k,v in enumerate(labels)}\n        self.labels_to_idx = {v:k for k,v in enumerate(labels)}\n        \n        songs_lists = [os.listdir(path + l) for l in labels]\n        songs_lists = [list(map(list, zip([path + labels[i] + '\/' for a in range(len(sl))],sl))) for i,sl in enumerate(songs_lists)]\n        labels = np.array([[s[0].split('\/')[-2] for s in l] for l in songs_lists])\n        labels = labels.reshape(labels.shape[0] * labels.shape[1])\n        \n        songs_lists = np.array([[s[0] + s[1] for s in l] for l in songs_lists])\n        songs_lists = songs_lists.reshape(songs_lists.shape[0] * songs_lists.shape[1])\n        self.labels, self.songs_lists = shuffle(labels, songs_lists)\n        \n    def plot_specgram(self, waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n        waveform = waveform.numpy()\n\n        num_channels, num_frames = waveform.shape\n        time_axis = torch.arange(0, num_frames) \/ sample_rate\n\n        figure, axes = plt.subplots(num_channels, 1)\n        if num_channels == 1:\n            axes = [axes]\n        for c in range(num_channels):\n            axes[c].specgram(waveform[c], Fs=sample_rate)\n            if num_channels > 1:\n                axes[c].set_ylabel(f'Channel {c+1}')\n            if xlim:\n                axes[c].set_xlim(xlim)\n        figure.suptitle(title)\n        plt.show(block=False)\n        \n    def print_stats(self, waveform, sample_rate=None, src=None):\n        if src:\n            print(\"-\" * 10)\n            print(\"Source:\", src)\n            print(\"-\" * 10)\n        if sample_rate:\n            print(\"Sample Rate:\", sample_rate)\n        print(\"Shape:\", tuple(waveform.shape))\n        print(\"Dtype:\", waveform.dtype)\n        print(f\" - Max:     {waveform.max().item():6.3f}\")\n        print(f\" - Min:     {waveform.min().item():6.3f}\")\n        print(f\" - Mean:    {waveform.mean().item():6.3f}\")\n        print(f\" - Std Dev: {waveform.std().item():6.3f}\")\n        print()\n        print(waveform)\n        print()\n        \n    def get_sample(self, path, sample_rate=4000):\n        effects = [\n          [\"lowpass\", \"-1\", \"150\"], # apply single-pole lowpass filter\n          [\"speed\", \"0.9\"],  # reduce the speed\n                             # This only changes sample rate, so it is necessary to\n                             # add `rate` effect with original sample rate after this.\n          [\"rate\", f\"{sample_rate}\"],\n          [\"reverb\", \"-w\"],  # Reverbration gives some dramatic feeling\n        ]\n        return torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n    \n    def __len__(self):\n        return len(self.songs_lists)\n    \n    def __getitem__(self, idx):\n        song_path = self.songs_lists[idx]\n        try:\n            waveform, frame_num = self.get_sample(song_path)\n        except:\n            idx += 1\n            song_path = self.songs_lists[idx]\n            waveform, frame_num = self.get_sample(song_path)\n        waveform = torch.unsqueeze(waveform, 0)\n        waveform = F.interpolate(waveform, size=(300134))\n        waveform = torch.squeeze(waveform, 0)\n        return waveform, frame_num, self.labels_to_idx[self.labels[idx]]","2d644ca0":"path = '..\/input\/gtzan-dataset-music-genre-classification\/Data\/genres_original\/'\n\nmusic_ds = MusicDS(path)\nwaveform, frame_num, label = music_ds[100]\nmusic_ds.plot_specgram(waveform, frame_num)\nmusic_ds.print_stats(waveform, frame_num)","cda25633":"test_size = int(len(music_ds) * 0.2)\ntrain_size = int(len(music_ds) - test_size)\n\ntrain_dataset, test_dataset = torch.utils.data.random_split(music_ds, [train_size, test_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, num_workers=0, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=1, num_workers=0, shuffle=False)","08d41060":"class Net(nn.Module):\n    def __init__(self, n_input=2, n_output=10, stride=16, n_channel=32):\n        super().__init__()\n        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n        self.bn1 = nn.BatchNorm1d(n_channel)\n        self.pool1 = nn.MaxPool1d(4)\n        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n        self.bn2 = nn.BatchNorm1d(n_channel)\n        self.pool2 = nn.MaxPool1d(4)\n        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n        self.pool3 = nn.MaxPool1d(4)\n        self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n        self.pool4 = nn.MaxPool1d(4)\n        self.fc1 = nn.Linear(2 * n_channel, n_output)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(self.bn1(x))\n        x = self.pool1(x)\n        x = self.conv2(x)\n        x = F.relu(self.bn2(x))\n        x = self.pool2(x)\n        x = self.conv3(x)\n        x = F.relu(self.bn3(x))\n        x = self.pool3(x)\n        x = self.conv4(x)\n        x = F.relu(self.bn4(x))\n        x = self.pool4(x)\n        x = F.avg_pool1d(x, x.shape[-1])\n        x = x.permute(0, 2, 1)\n        x = self.fc1(x)\n        return F.log_softmax(x, dim=2)","84e9c5ef":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nnet = Net().to(device)","3b9a2e7b":"optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=0.0001)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)","5ca545e4":"def train(model, epoch):\n\n    losses = []\n    for batch_idx, (data, num, target) in enumerate(train_loader):\n\n        data = data.to(device)\n        target = target.to(device)\n\n        output = model(data).to(device)\n\n        loss = F.nll_loss(output.squeeze(), target)\n        losses.append(loss.item())\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n    return losses","ce1bf4cf":"def number_of_correct(pred, target):\n    return pred.squeeze().eq(target).sum().item()\n\n\ndef get_likely_index(tensor):\n    return tensor.argmax(dim=-1)\n\n\ndef test(model, epoch):\n    model.eval()\n    correct = 0\n\n    for data, num, target in test_loader:\n\n        data = data.to(device)\n        target = target.to(device)\n\n        output = model(data).to(device)\n\n        pred = get_likely_index(output)\n        correct += number_of_correct(pred, target)\n\n    accuracy = 100. * correct \/ len(test_loader.dataset)\n    return accuracy","7a48f97b":"n_epoch = 80\n\nlosses = []\naccuracies = []\n\nfor epoch in range(1, n_epoch + 1):\n    loss = train(net, epoch)\n    losses.append(sum(loss) \/ len(loss))\n\n    accuracy = test(net, epoch)\n    accuracies.append(accuracy)\n    scheduler.step()","5ab4b57e":"plt.figure(figsize=(14,8))\nplt.title('Accuracies')\nplt.plot(accuracies)","94947ff0":"plt.figure(figsize=(14,8))\nplt.title('Losses')\nplt.plot(losses)","19fdd97b":"<div>\n    <img src='https:\/\/storage.googleapis.com\/kaggle-datasets-images\/568973\/1032238\/7ff23ec0b526773506bd5964d4f100d1\/dataset-cover.jpg' \/>\n<\/div>","3a141a11":"<h1 id=\"analysis\" style=\"color:#c7ced6; background:#dc9231; border:0.5px dotted;\"> \n    <center>Analysis\n        <a class=\"anchor-link\" href=\"#analysis\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","e545346e":"<h1 id=\"training\" style=\"color:#c7ced6; background:#dc9231; border:0.5px dotted;\"> \n    <center>Training\n        <a class=\"anchor-link\" href=\"#training\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","1639a94b":"<h1 id=\"dataset\" style=\"color:#c7ced6; background:#dc9231; border:0.5px dotted;\"> \n    <center>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","d41cd1a8":"<h1 id=\"network\" style=\"color:#c7ced6; background:#dc9231; border:0.5px dotted;\"> \n    <center>Network\n        <a class=\"anchor-link\" href=\"#network\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>"}}