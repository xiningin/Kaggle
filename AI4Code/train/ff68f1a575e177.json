{"cell_type":{"b622d21b":"code","2f8bfcde":"code","82ffb2e9":"code","5b616762":"code","12d454f1":"code","e5bd8420":"code","f33f72d8":"code","146f5075":"code","7d6ba9af":"code","e3ed2ec1":"code","a1de47e1":"code","aae29c0e":"code","ca3749c3":"code","f94db5f1":"code","6c0976d0":"code","ec015e55":"code","f56c0355":"code","a49e72f0":"code","aa290530":"code","48177550":"code","7bd8c2a2":"code","f2f052ad":"markdown","0cc583cd":"markdown","33433b50":"markdown","5246fc41":"markdown","4b9e1707":"markdown","3282fbc8":"markdown","1c3698a9":"markdown","ee109e2c":"markdown","88b21539":"markdown","0700b5e5":"markdown","c6a352c6":"markdown","f92ef9d3":"markdown"},"source":{"b622d21b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport shutil\nimport os\nimport tensorflow as tf\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# BASE_DATASET_DIR = \"..\/input\/rockpaperscissors\/\"\nif os.path.exists('\/kaggle\/working\/rockpaperscissors'):\n    shutil.rmtree('\/kaggle\/working\/rockpaperscissors')\nshutil.copytree(\"..\/input\/rockpaperscissors\/\", '\/kaggle\/working\/rockpaperscissors')\n\nBASE_DATASET_DIR = '\/kaggle\/working\/rockpaperscissors'\n# Folder are created because such type of structure are useful when using KERAS ImageDataGenerator, which takes care of\n# data augmentation like rotation, scaling, shearing, flipping etc. which we don't have to do seperately. \n# We will keep this option of data augmentation in case our model overfits. Using ImageDataGenerator we can create\n# more data on the fly while training and saving disk space. We will experience this when we use ImageDataGenerator.\nimport os\nif not os.path.exists('\/kaggle\/working\/validation'):\n    os.mkdir('\/kaggle\/working\/validation')\n    os.mkdir('\/kaggle\/working\/validation\/rock')\n    os.mkdir('\/kaggle\/working\/validation\/paper')\n    os.mkdir('\/kaggle\/working\/validation\/scissors')\n    \n# under testing dataset folder we don't need any folder structure. because out trained model will predict the\n# images store belongs to which class\nif not os.path.exists('\/kaggle\/working\/testing'):\n    os.mkdir('\/kaggle\/working\/testing')\n\nVALIDATION_DATASET_DIR = '\/kaggle\/working\/validation\/'\nTESTING_DATASET_DIR = '\/kaggle\/working\/testing\/'","2f8bfcde":"print(tf.__version__)","82ffb2e9":"\nshutil.rmtree(BASE_DATASET_DIR+'\/rps-cv-images')\nprint(BASE_DATASET_DIR)\nprint(os.listdir(VALIDATION_DATASET_DIR))\nprint(os.listdir(BASE_DATASET_DIR))\n# print(os.listdir(TESTING_DATASET_DIR))\n","5b616762":"# Get total count of dataset\nprint(len(os.listdir(\"\/kaggle\/working\/rockpaperscissors\/rock\")))\nprint(len(os.listdir(\"\/kaggle\/working\/rockpaperscissors\/scissors\")))\nprint(len(os.listdir(\"\/kaggle\/working\/rockpaperscissors\/paper\")))","12d454f1":"# Extracting the file names in a list, shuffling and dividing into test and validation set.\n# We will keep the split as 20% for validation and 10% for testing.\nimport random\nfrom tqdm.notebook import trange\nfrom shutil import move\n\ndef clear_dir(folder):\n    \"\"\" Deletes the content of the folder to avoid adding files when the code is run multiple times\"\"\"\n    if len(os.listdir(folder)) != 0:\n        shutil.rmtree(folder)\n        os.mkdir(folder)\n\ndef split_data(src_folder_path, dst_folder_path, split_range):\n    \"\"\"\n    src_folder_path: Folder path we want to split\n    dst_folder_path: Destination Path\n    split_range: percentage of data to split\n    \"\"\"\n    if src_folder_path is not None and dst_folder_path is not None:\n        if os.path.exists(src_folder_path) and os.path.exists(dst_folder_path):\n            print(\"Splitting data for {}\".format(dst_folder_path))\n            \n            file_names = [f_name for f_name in os.listdir(src_folder_path)]\n            shuffled_files = random.sample(file_names, len(file_names))\n            \n            if dst_folder_path != TESTING_DATASET_DIR:\n                clear_dir(dst_folder_path)\n            \n            print(\"Total Files Found: {}\".format(len(file_names)))\n            print(\"Files to split {}\".format(round(len(shuffled_files)*split_range)))\n            for each_img in trange(round(len(shuffled_files)*split_range)):\n                src_full_file_path = os.path.join(src_folder_path, shuffled_files[each_img])\n                dst_full_path = os.path.join(dst_folder_path, shuffled_files[each_img])      \n                \n                move(src_full_file_path, dst_full_path)\n                \n        else:\n            print(\"Source {} or destination {} folder doesnot exists.\".format(src_folder_path, dst_folder_path))\n    else:\n        print(\"Source or destination folder arguments missing.\")\n\nclasses = ['rock', 'paper', 'scissors']\nBASE_DATASET_DIR = '\/kaggle\/working\/rockpaperscissors'\nfor each_labels in classes:    \n    label_src = os.path.join(BASE_DATASET_DIR, each_labels)\n    label_dst_val = os.path.join(VALIDATION_DATASET_DIR, each_labels)\n    label_dst_test = TESTING_DATASET_DIR\n    split_data(label_src, label_dst_val, 0.2)\n    split_data(label_src, label_dst_test, 0.1)\n    \n\n","e5bd8420":"train_rock = os.path.join(BASE_DATASET_DIR, classes[0])\ntrain_paper = os.path.join(BASE_DATASET_DIR, classes[1])\ntrain_scissor = os.path.join(BASE_DATASET_DIR, classes[2])\n\nvalid_rock = os.path.join(VALIDATION_DATASET_DIR, classes[0])\nvalid_paper = os.path.join(VALIDATION_DATASET_DIR, classes[1])\nvalid_scissor = os.path.join(VALIDATION_DATASET_DIR, classes[2])\n","f33f72d8":"# VERIFY WHETHER THE FOLDERS ARE CREATED OR NOT\nprint(\"Training Dataset\")\n\nprint(len(os.listdir(train_rock)))\nprint(len(os.listdir(train_paper)))\nprint(len(os.listdir(train_scissor)))\n\nprint(\"Validation dataset count\")\n\nprint(len(os.listdir(valid_rock)))\nprint(len(os.listdir(valid_paper)))\nprint(len(os.listdir(valid_scissor)))\n\nprint(\"Testing dataset count\")\nprint(len(os.listdir(TESTING_DATASET_DIR)))","146f5075":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfig = plt.gcf()\nn_rows, n_cols = 3, 5\n\n# setting the canvas size to fit all the images\nfig.set_size_inches(n_cols*5, n_rows*5)\nrock_fnames = [os.path.join(train_rock,images) for images in os.listdir(train_rock)][0:5]\npaper_fnames = [os.path.join(train_paper,images) for images in os.listdir(train_paper)][0:5]\nscissor_fnames = [os.path.join(train_scissor,images) for images in os.listdir(train_scissor)][0:5]\n\nfor idx, img_path  in enumerate(rock_fnames + paper_fnames + scissor_fnames):\n    # Set up subplot; subplot indices start at 1\n    sp = plt.subplot(n_rows, n_cols, idx + 1)\n    fig.suptitle(\"RPS: Training Images\", fontsize=48)\n    sp.axis('Off') # Don't show axes (or gridlines)\n    img = mpimg.imread(img_path)\n    plt.imshow(img)\nplt.show()\n","7d6ba9af":"fig = plt.gcf()\nn_rows, n_cols = 3, 5\n\n# setting the canvas size to fit all the images\nfig.set_size_inches(n_cols*5, n_rows*5)\nrock_fnames = [os.path.join(valid_rock,images) for images in os.listdir(valid_rock)][0:5]\npaper_fnames = [os.path.join(valid_paper,images) for images in os.listdir(valid_paper)][0:5]\nscissor_fnames = [os.path.join(valid_scissor,images) for images in os.listdir(valid_scissor)][0:5]\n\nfor idx, img_path  in enumerate(rock_fnames + paper_fnames + scissor_fnames):\n    # Set up subplot; subplot indices start at 1\n    sp = plt.subplot(n_rows, n_cols, idx + 1)\n    fig.suptitle(\"RPS: Validation Images\", fontsize=48)\n    sp.axis('Off') # Don't show axes (or gridlines)\n    img = mpimg.imread(img_path)\n    plt.imshow(img)\nplt.show()","e3ed2ec1":"fig = plt.gcf()\nn_rows, n_cols = 5, 5\nfig.set_size_inches(n_cols*5, n_rows*5)\n\ntst_fnames = [os.path.join(TESTING_DATASET_DIR,images) for images in os.listdir(TESTING_DATASET_DIR)][0:25]\ntst_fnames = random.sample(tst_fnames, len(tst_fnames))\n\nfor idx, img_path  in enumerate(tst_fnames):\n    # Set up subplot; subplot indices start at 1\n    sp = plt.subplot(n_rows, n_cols, idx + 1)\n    fig.suptitle(\"RPS: TESTING Images\", fontsize=40)\n    sp.axis('Off') # Don't show axes (or gridlines)\n    img = mpimg.imread(img_path)\n    plt.imshow(img)\nplt.show()","a1de47e1":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Just normalizing the data now.\ntrain_datagen = ImageDataGenerator(rescale=1\/255,\n                                   width_shift_range = 0.1,\n                                   height_shift_range = 0.1,\n                                   rotation_range=0.1,\n                                   zoom_range=0.1,\n                                   fill_mode=\"nearest\",\n                                   vertical_flip=True,\n                                   horizontal_flip=True\n                                  )\n\ntrain_generator = train_datagen.flow_from_directory('\/kaggle\/working\/rockpaperscissors',\n                                                    target_size=(150, 150),\n                                                    class_mode=\"categorical\",\n                                                    batch_size=32)\nvalid_datagen = ImageDataGenerator(rescale=1\/255)\nvalid_generator = valid_datagen.flow_from_directory(VALIDATION_DATASET_DIR,\n                                                    class_mode=\"categorical\",\n                                                    target_size=(150,150),\n                                                    batch_size=32)","aae29c0e":"import tensorflow as tf\nrps_model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\", input_shape=(150,150,3)),    \n    tf.keras.layers.MaxPool2D(2,2),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\"),\n    tf.keras.layers.MaxPool2D(2,2),\n    tf.keras.layers.Conv2D(32, (3,3), activation=\"relu\"),\n    tf.keras.layers.MaxPool2D(2,2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(3, activation='softmax')   \n])\nrps_model.summary()","ca3749c3":"class myCallBack(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        print(logs)\n        if(logs.get('accuracy') > 0.97):\n            print(\"\\nReached 97% accuracy so cancelling training!\")\n            self.model.stop_training = True\n","f94db5f1":"from tensorflow.keras.optimizers import RMSprop\n\nrps_model.compile(optimizer=RMSprop(lr=0.0001),\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])","6c0976d0":"epochs = 30\ntraining_len = 1576\nbatch_size = 32\nsteps_per_epoch=int(training_len\/batch_size)\n\ncallbacks = myCallBack()\nhistory = rps_model.fit_generator(train_generator,\n                                  steps_per_epoch=steps_per_epoch,\n                                  epochs=epochs,\n                                  validation_data=valid_generator,\n                                  callbacks=[callbacks],\n                                  verbose=1)","ec015e55":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend(loc=0)\nplt.figure()\n\n\nplt.show()","f56c0355":"classes = ['paper', 'rock', 'scissors']\nlabel_map = train_generator.class_indices\nprint(label_map)\n\ndef test_model(image):\n    import cv2\n    img = image\n    resized = cv2.resize(img, (150,150), interpolation = cv2.INTER_AREA)\n    exp_img = np.expand_dims(resized, axis=0)\n    y_prob = rps_model.predict(exp_img)\n    _cls = y_prob.argmax(axis=-1)\n#     label_map = train_generator.class_indices\n#     print(y_prob, _cls)\n#     print(label_map)\n#     sp = plt.subplot(1, 1, 1, title=\"Label: \"+ classes[_cls[0]])\n#     fig.suptitle(\"RPS: Self-Captured data to test\", fontsize=40)\n#     sp.axis('Off') # Don't show axes (or gridlines)     \n#     plt.imshow(img)\n    return classes[_cls[0]]\n","a49e72f0":"new_dataset = '..\/input\/rpsselfcaptureddata\/'\nstst_fnames = [os.path.join(new_dataset,images) for images in os.listdir(new_dataset)]\nstst_fnames = random.sample(stst_fnames, len(stst_fnames))","aa290530":"fig = plt.gcf()\nn_rows, n_cols = 5,5\nfig.set_size_inches(n_cols*5, n_rows*5)\nfor idx, img in enumerate(tst_fnames):\n    img = mpimg.imread(img)\n    label_ = str(test_model(img))\n    sp = plt.subplot(n_cols, n_rows, idx + 1, title=\"Label: \"+ label_)\n    fig.suptitle(\"Inference\", fontsize=35)\n    sp.axis('Off') # Don't show axes (or gridlines)  \n    plt.imshow(img)\nplt.show()\n\n","48177550":"fig = plt.gcf()\nn_rows, n_cols = 2,5\nfig.set_size_inches(n_cols*3, n_rows*3)\nfor idx, img in enumerate(stst_fnames):\n    img = mpimg.imread(img)\n    label_ = str(test_model(img))\n    sp = plt.subplot(n_rows, n_cols, idx + 1, title=\"Label: \"+ label_)\n    fig.suptitle(\"Inference\", fontsize=35)\n    sp.axis('Off') # Don't show axes (or gridlines)  \n    plt.imshow(img)\nplt.show()","7bd8c2a2":"import numpy as np\nimport random\nfrom   tensorflow.keras.preprocessing.image import img_to_array, load_img\n\n# Let's define a new Model that will take an image as input, and will output\n# intermediate representations for all layers in the previous model after\n# the first.\nsuccessive_outputs = [layer.output for layer in rps_model.layers[1:]]\n\n#visualization_model = Model(img_input, successive_outputs)\nvisualization_model = tf.keras.models.Model(inputs = rps_model.input, outputs = successive_outputs)\n\ntr_imgs = [rock_fnames[:2], paper_fnames[:2], scissor_fnames[0:2]]\n\nimg_path = random.choice(tr_imgs)\nimg = load_img(img_path[0], target_size=(150, 150))  # this is a PIL image\n\nx   = img_to_array(img)                           # Numpy array with shape (150, 150, 3)\nx   = x.reshape((1,) + x.shape)                   # Numpy array with shape (1, 150, 150, 3)\n\n# Rescale by 1\/255\nx \/= 255.0\n\n# Let's run our image through our network, thus obtaining all\n# intermediate representations for this image.\nsuccessive_feature_maps = visualization_model.predict(x)\n\n# These are the names of the layers, so can have them as part of our plot\nlayer_names = [layer.name for layer in rps_model.layers]\n\n# -----------------------------------------------------------------------\n# Now let's display our representations\n# -----------------------------------------------------------------------\nfor layer_name, feature_map in zip(layer_names, successive_feature_maps):\n  \n  if len(feature_map.shape) == 4:\n    \n    #-------------------------------------------\n    # Just do this for the conv \/ maxpool layers, not the fully-connected layers\n    #-------------------------------------------\n    n_features = feature_map.shape[-1]  # number of features in the feature map\n    size       = feature_map.shape[ 1]  # feature map shape (1, size, size, n_features)\n    \n    # We will tile our images in this matrix\n    display_grid = np.zeros((size, size * n_features))\n    \n    #-------------------------------------------------\n    # Postprocess the feature to be visually palatable\n    #-------------------------------------------------\n    for i in range(n_features):\n      x  = feature_map[0, :, :, i]\n      x -= x.mean()\n      x \/= x.std ()\n      x *=  64\n      x += 128\n      x  = np.clip(x, 0, 255).astype('uint8')\n      display_grid[:, i * size : (i + 1) * size] = x # Tile each filter into a horizontal grid\n\n    #-----------------\n    # Display the grid\n    #-----------------\n\n    scale = 20. \/ n_features\n    plt.figure( figsize=(scale * n_features, scale) )\n    plt.title ( layer_name )\n    plt.grid  ( False )\n    plt.imshow( display_grid, aspect='auto', cmap='viridis' ) ","f2f052ad":"Now since we have used ImageDataGenerator for managing our dataset, so to train our network instead of the regular API **model.fit** we have to use **model.fit_generator**","0cc583cd":"# ImageDataGenerator\nOnce our dataset is ready let's setup the image generator and see if all the things are in place. \n\nAdvantages of ImageDataGenerator are: \n\n* If no labels are present, just arrange the dataset in class-wise folder and pass the root directory of the dataset to the ImageDataGenerator object.  The ImageDataGenerator will label the images based on the directory names.\n* ImageDataGenerator will make our task of data augmentation easy.\n* ImageDataGenerator will apply the transformation to the images and apply them to the training on the fly. It doesnot save any transformed images to the disk. Therefore, the training might take some extra time for applying transformation.\n\nFirst will check if by just normalizaing the dataset produces a good result or not. Otherwise will experiment with other data pre-processing things with ImageDataGenerator. \n\nFoll the link for more detail. [ImageDataGenerator](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator)","33433b50":"# Call back function to monitor the training\n* Save the best model\n* Stop the training if accuracy is > 90%\n* If model is not training then ","5246fc41":"INFERENCING ON THE SELF CAPTURED DATASET","4b9e1707":"Result is 40% on the self captured dataset. here it is recognizing everything as paper.\nFew observations:\n\n* The image captured by me could be in a different way than the original dataset.\n* In scissor images, my nuckles and fingers are not aligned. \n* Background color could be an issue.","3282fbc8":"Inference on the provided test images ","1c3698a9":"ImageDataGenerator has itself arranged all the images w.r.t to the labels. So, therea re 1576 training images with 3 classes and 437 validation images with 3 classes. That what we were looking for.\n\nNext will create our model, train and test the performance","ee109e2c":"Lets test this on some self-captured dataset\n\n","88b21539":"There is an easier way to split the dataset is to download and split manually in your local PC and then upload back in here and start using it.\n\nBut in case if some one wants to directly work on this data structure they can use this Kernel.","0700b5e5":"# Let's see how our images look like. ","c6a352c6":"Lets shuffle the images and divide the images into testing and validation. \n\nPaper Images: 712\n\nRock Images: 726\n\nScissors Images: 750\n\nTotal: 2188","f92ef9d3":"COMPILING OUR MODEL"}}