{"cell_type":{"97d68a0c":"code","c266fceb":"code","3e69c1f2":"code","ce7c452a":"code","8a6dcda9":"code","7aa94d26":"code","ead91aec":"code","a3ab3a09":"code","22c60e36":"code","1a48aafe":"code","1b3c091c":"code","c6927748":"code","d0344bd5":"code","f642d4a4":"code","a3268850":"code","ae1d2989":"code","191e8df8":"code","b3e56956":"code","72a9e55f":"code","65f4ffd5":"code","32f45117":"code","f46ca0c5":"code","ca7a9787":"code","5594a4fb":"code","f886fe24":"code","9c47313e":"code","9b37c91d":"code","fc704fa2":"code","696623db":"code","b0a26dfd":"code","58decb55":"code","dfed63e8":"code","90a667e4":"code","327a5c62":"code","cfa91936":"code","0c3498d9":"code","8e8031a2":"code","5b3401b2":"code","7500e847":"code","d5394aa9":"code","cb4a93f9":"code","e79a8020":"code","31ce7457":"code","89f1af9b":"code","c5c8b784":"code","6a0913e3":"code","cbc2ddb8":"code","72d39c8a":"code","f4eb2d93":"code","360e4c02":"code","4f9d5864":"markdown","4d45d4c1":"markdown","d18cb9fe":"markdown","54220758":"markdown","4bfeaf88":"markdown","dfb6af4e":"markdown","7ed58ee1":"markdown","c2470b48":"markdown","4b460eca":"markdown","899e07ce":"markdown","81fe5711":"markdown","e0ba27bd":"markdown","62d99231":"markdown","22bac558":"markdown","c2751ec6":"markdown","84156585":"markdown","af1cf1c2":"markdown","accbe6fd":"markdown","d3dbec2c":"markdown","4870835b":"markdown","c36d1578":"markdown","0cbedfeb":"markdown","b1c3ac15":"markdown","ff51774f":"markdown","907e3acc":"markdown","df7da047":"markdown","d8bbde8e":"markdown","c55509ed":"markdown","de9a8746":"markdown","faa72b3f":"markdown","be969e63":"markdown","a27a7bbb":"markdown","e840424e":"markdown","1bc839ab":"markdown","c087fc0c":"markdown","2893f8d9":"markdown","295d3f2c":"markdown","a3c56f1c":"markdown","df5ee105":"markdown","e1997b02":"markdown","05a1b315":"markdown","0ebe3606":"markdown","612fac23":"markdown"},"source":{"97d68a0c":"import os;\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename));\n","c266fceb":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import __version__\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\nimport pylab \nimport scipy.stats as stats\nimport statsmodels.api as sm\n\nfrom numpy import mean\nfrom numpy import median\nfrom numpy import percentile\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\n\nfrom pandas import read_csv\nfrom pandas import datetime\nfrom pandas import DataFrame\n\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score","3e69c1f2":"df = pd.read_csv('\/kaggle\/input\/daily_values_ee.csv')","ce7c452a":"df.info()","8a6dcda9":"df.head(3)","7aa94d26":"print('Size of df data', df.shape)","ead91aec":"plt.figure(figsize=(16,4))\nsns.heatmap(df.isnull()) # plot the missing data\nplt.title('Missing Data')\n\ntotal = df.isnull().sum().sort_values(ascending = False) # count list of missing data per column\npercent = (df.isnull().sum()\/df.isnull().count()*100).sort_values(ascending = False) # percentage list of missing data per column\nmissing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent']) # create a df of the missing data\nmissing_data.head(6)","a3ab3a09":"df=df.tail(593) # this way we will exclude the first 7 rows\ndf.head()","22c60e36":"## Function to reduce the DF size\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ndf = reduce_mem_usage(df);\n","1a48aafe":"fig, saxis = plt.subplots(3, 2,figsize=(16,14))\n\nsns.distplot(df['53_kW_mean'].dropna(), bins=20, ax = saxis[0,0])\nsns.distplot(df['71_kW_mean'].dropna(), bins=20, ax = saxis[1,0])\nsns.distplot(df['71A_kW_mean'].dropna(), bins=20, ax = saxis[0,1])\nsns.distplot(df['83_kW_mean'].dropna(), bins=20, ax = saxis[1,1])\nsns.distplot(df['71_71AkW_mean'].dropna(),bins=20, ax= saxis[2,0])\nsns.distplot(df['totalkW_mean'].dropna(), bins=20, ax= saxis[2,1]);","1b3c091c":"plt.figure(figsize=[16,12])\n\nplt.subplot(321)\nplt.plot(df['53_kW_mean'])\nplt.ylabel('kW ')\nplt.title('53')\n\nplt.subplot(322)\nplt.plot(df['71_kW_mean'])\nplt.ylabel('kW')\nplt.title('71')\n\nplt.subplot(323)\nplt.plot(df['71A_kW_mean'])\nplt.ylabel('kW')\nplt.title('71A')\n\nplt.subplot(324)\nplt.plot(df['83_kW_mean'])\nplt.ylabel('kW')\nplt.title('83')\n\nplt.subplot(325)\nplt.plot(df['71_71AkW_mean'])\nplt.ylabel('kW')\nplt.title('71 + 71A')\n\nplt.subplot(326)\nplt.plot(df['totalkW_mean'])\nplt.ylabel('kW')\nplt.title('Total');\n\n","c6927748":"# Since we will work initially with the total values, let's filter it\ntotal_cols = ['Date','totalkW_mean','totalkW_max','totalkW_time_max','totalkW_d-1','totalkW_w-1','totalkW_d\/1','totalkW_w\/1','Month','Year','Day','WD','CDD_15','HDD_25','NWD','temp_max','insolation','temp_mean','RH']\ntotal_df = df[total_cols]\ntotal_df.info() # review the variables","d0344bd5":"# let's analyze the correlation of the dataset\nplt.figure(figsize=(16,10))\nsns.heatmap(total_df.corr(), mask=False, annot=True, cmap='viridis')\nplt.title('Correlations');","f642d4a4":"# focusing on the dependent variable\nplt.figure(figsize=(16,1))\nsns.heatmap(total_df.corr()[-17:1], mask=False, annot=True, cmap='viridis')\nplt.title('Correlations - Total kW');","a3268850":"plt.figure(figsize=(14,6))\nplt.plot(total_df['HDD_25'], color='red', label='HDD')\nplt.plot(total_df['CDD_15'], color='blue', label='CDD')\nplt.plot(total_df['temp_mean'], color='black', label='Mean Temperature')\nplt.legend()\nplt.xlabel('day')\nplt.title('Weather Data - HDD, CDD and Mean Temperature (\u00baC)')\n\nplt.figure(figsize=(14,6))\nplt.plot(total_df['RH'], color='purple',label='Relative Hudimity')\nplt.xlabel('day')\nplt.title('Weather Data - Relative Humidity (%)');","ae1d2989":"# here we will plot the rolling mean with a 7 window day period to analyze the weekly frequency\nplt.figure(figsize=(16,6))\nplt.plot(total_df['totalkW_mean'].rolling(window=7,center=False).mean(),label='Rolling Mean') # analyze the rolling mean considering a 7 days window\nplt.plot(total_df['totalkW_mean'].rolling(window=7,center=False).std(),label='Rolling Std Dev')# analyze the rolling std dev considering a 7 days window\nplt.legend()\nplt.xlabel('day')\nplt.title('7 day window - Mean & Std. Deviation (kW)');\n","191e8df8":"autocorr=plot_acf(total_df['totalkW_mean'].dropna(), lags=30); # autocorrelation with a 30 days window","b3e56956":"Consumo_WD_M = total_df[['WD','Month','totalkW_mean']].groupby(by=['WD','Month']).mean()['totalkW_mean'].unstack()\/1000\nplt.figure(figsize=(14,6))\nsns.heatmap(Consumo_WD_M,cmap='viridis', annot=True)\nplt.title(\"Mean (MW)\");","72a9e55f":"plt.figure(figsize=(10,5))\nsns.boxplot(x='WD', y='totalkW_mean', data=total_df)\nplt.title(\"Day of the Week Mean (kW)\");\n\nplt.figure(figsize=(10,5))\nsns.boxplot(x='Month', y='totalkW_mean', data=total_df)\nplt.title(\"Monthly Mean (kW)\");","65f4ffd5":" # this plot shows the time during the day where it has the greatest power\ncount=total_df[['WD','totalkW_time_max','totalkW_max']].groupby(by=['WD','totalkW_time_max']).count()['totalkW_max'].unstack()\ncount[np.isnan(count)] = 0\nplt.figure(figsize=(13,5))\nsns.heatmap(count.dropna())\nplt.title(\"Count of maximum power per hour\");\n","32f45117":"total = total_df['totalkW_time_max'].value_counts()\ncount  = pd.concat([total], axis=1, keys=['Count'])\ncount.reset_index(drop=False, inplace=True)\ncount.head(8)\ncount.sort_values(by=['index'], inplace=True, ascending=True)\nplt.figure(figsize=(14,4))\nsns.barplot(x='index', y='Count', data=count, color='black')\nplt.title(\"Count of maximum power per hour\");\n\n\ncount.sort_values(by=['Count'], inplace=True, ascending=False)\nplt.figure(figsize=(14,4))\nsns.barplot(x='index', y='Count', data=count, color='black');\nplt.title(\"Count of maximum power per hour - descendant\");\n","f46ca0c5":"# X is the independent variable - aka INPUT\n# y is the dependent variable - aka OUTPUT\nX = total_df.drop(['Date','Year','Day','totalkW_mean','totalkW_max','totalkW_time_max','insolation'], axis=1).values\ny = total_df['totalkW_mean'].values\n# the model only works with arrays, not DF","ca7a9787":"# splitting the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=102)\n# since there are not too many data, I thought it was better to have a bigger training set\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# normalizing it\nscaler = MinMaxScaler()\nscaler.fit(X_train) # scale the X_train\nX_train = scaler.transform(X_train)  # apply for the train data\nX_test = scaler.transform(X_test) # apply for the test data\n","5594a4fb":"X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1])) # need to add a column for using LSTM Models\nX_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1])) # need to add a column for using LSTM Models\n\nprint('X_train shape = ',X_train.shape)\nprint('X_test shape = ' ,X_test.shape)","f886fe24":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.optimizers import Adam\n\n\n# Here I create a Long Short-Term Memory artificial recurrent neural network as a model\nmodel = Sequential()\nmodel.add(LSTM( units=4, input_shape=(1, 12), activation='sigmoid', return_sequences=True))\n# first layer receiving the input. Sigmoid was choosing as an activation funcion in order to work properly for LSTM\nmodel.add(LSTM( units=2, input_shape=(1, 12), activation='linear',recurrent_activation='linear', return_sequences=False))\n# the second layer work with a linear activation function to behave more in tune with our input data\nmodel.add(Dense(units=1,activation='linear'))\n# finally a last layer with only one output, which is the respective mean power of the day\nmodel.compile(loss='mse', optimizer='adam')\n# for the optimization model, the Minimum Squared Error is minimized as the objective function","9c47313e":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=250)\n# added an early stop that is trigger when the difference of the validation loss at each epoch is minimum.","9b37c91d":"model.fit(x = X_train, \n          y = y_train, \n          epochs = 5000, \n          validation_data = (X_test, y_test), \n          batch_size = 128,\n          callbacks=[early_stop],\n          verbose=0\n         );\n\n# 5000 epochs to train the model\n# 128 cases for updating the model parameters, which is the batch size","fc704fa2":"model_loss = pd.DataFrame(model.history.history)\nprint(model_loss.tail(1))\nmodel_loss[['loss','val_loss']].plot()\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.title(\"Losses\");","696623db":"predictions = model.predict(X_test)\n\ny_test_std_dev=y_test.astype(float).std()\nprint(\"Real Mean = %.2f\" % total_df['totalkW_mean'].astype(float).mean())\nprint(\"y_test Mean = %.2f\" % y_test.astype(float).mean())\nprint(\"y_test Std Dev = %.2f\" % y_test.astype(float).std())\n\nalpha = 5.0\nlower_p = alpha \/ 2.0 # calculate lower percentile (e.g. 2.5)\nlower = max(0.0, percentile(y_test, lower_p)) # retrieve observation at lower percentile\nprint('%.1fth percentile = %.2f' % (lower_p, lower)) # calculate upper percentile (e.g. 97.5)\nupper_p = (100 - alpha) + (alpha \/ 2.0) # retrieve observation at upper percentile\nupper = max(1.0, percentile(y_test, upper_p))\nprint('%.1fth percentile = %.2f' % (upper_p, upper))\nprint(\"predictions Mean = %.2f\" % predictions.astype(float).mean())\nprint(\"predictions Std Dev = %.2f\" % predictions.astype(float).std())\n\nprint(\"MAE = %.2f\" % mean_absolute_error(y_test,predictions))\nprint(\"MSE = %.2f\" % np.sqrt(mean_squared_error(y_test,predictions)))\nprint(\"EVS = %.2f\" % explained_variance_score(y_test,predictions));","b0a26dfd":"plt.figure(figsize=(14,5))\nplt.scatter(y_test,predictions, color='blue') # Our predictions\n\nplt.plot(y_test,y_test,'red') # y=x line\nplt.plot(y_test,y_test+y_test_std_dev,'grey', linewidth=.1) # y=x line\nplt.plot(y_test,y_test-y_test_std_dev,'grey', linewidth=.1) # y=x line\n\n\nplt.xlabel(\"kW\")\nplt.title(\"Scatter - y_test vs. predicted values (kW)\");\n\nplt.figure(figsize=(14,5))\nplt.plot(y_test, 'red', linewidth=1)\nplt.plot(predictions, 'blue',linewidth=.5)\nplt.title(\"y_test vs. predicted values (kW)\")\nplt.xlabel(\"y_test\");","58decb55":"y_test_pd = DataFrame(y_test)\npredictions_pd = DataFrame(predictions)\ncomparisson  = pd.concat([y_test_pd, predictions_pd], axis=1, keys=['y_test', 'predictions'])\ncomparisson['resid'] = comparisson['y_test'] -  comparisson['predictions']\ncomparisson['resid_ratio'] = (comparisson['predictions'] \/ comparisson['y_test'])\ncomparisson['residsqr'] = comparisson['resid']*comparisson['resid']\n\nplt.figure(figsize=[12,4])\n\ncomp_resid = plt.plot(comparisson['resid'])\nplt.title('Residuals (kW)')\nplt.figure(figsize=[12,4])\ncomp_resid_ratio = plt.plot(comparisson['resid_ratio'])\nplt.title('Residuals Ratio')\nplt.figure(figsize=(12,4))\nsns.distplot((comparisson['resid']), bins=30)\nplt.title('Residuals Distribution (kW)')\nplt.xlabel(\"\");","dfed63e8":"autocorr_resid = plot_acf(comparisson['resid']);\nplt.title('Autocorrelation - Residuals');\n\nautocorr_resid_sqr = plot_acf(comparisson['residsqr']);\nplt.title('Autocorrelation - Residuals Squared');","90a667e4":"X_df = total_df.drop(['Date','Day','Year','totalkW_max','totalkW_mean','insolation','totalkW_time_max'], axis=1)\nX_df = X_df.reset_index(drop=True)\n\ny_df = total_df['totalkW_mean']\ny_df = pd.concat([y_df], axis=1, keys=\"total\")\ny_df = y_df.reset_index(drop=True)\n\ninitial_values = 450\n\nnl1 = y_df.head(initial_values)\nnl2 = X_df.head(initial_values)","327a5c62":"i=len(nl1)-1\nj=1\npred=0\nsteps=144\nfor j in range(1,steps):\n    nl2 = nl2.append(pd.Series([nl1.loc[i][0]-nl1.loc[i-1][0]] + [nl1.loc[i][0]-nl1.loc[i-7][0]] + [nl1.loc[i][0]\/nl1.loc[i-1][0]] + [nl1.loc[i][0]\/nl1.loc[i-7][0]] + [nl2.loc[i-364][4]] + [nl2.loc[i-6][5]] + [X_df.loc[i][6]] + [X_df.loc[i][7]] + [X_df.loc[i][8]] + [X_df.loc[i][9]] + [X_df.loc[i][10]] + [X_df.loc[i][11]],  index=nl2.columns), ignore_index=True)\n    #print(nl2)\n    nly = nl2.tail(1).values\n    nly=scaler.transform(nly)\n    nly = np.reshape(nly, (nly.shape[0], 1, nly.shape[1]))\n    pred = float(model.predict(nly))\n    #print(pred)\n    nl1 = nl1.append(pd.Series(pred, index=nl1.columns), ignore_index=True)\n    j=j+1\n    i=i+1\n\n","cfa91936":"# Plotting the real and the predicted values\nplt.figure(figsize=(10,4))\nplt.plot(nl1.head(initial_values),label='Real Series')\nplt.plot(nl1.tail(steps), label='Predicted Values')\nplt.legend()\nplt.title(\"Model 1 - Application (kW)\")\nplt.xlabel(\"day\");","0c3498d9":"plt.figure(figsize=(10,4))\nplt.plot(nl2['totalkW_d-1'], label='d-1')\nplt.plot(nl2['totalkW_w-1'], label='w-1')\nplt.legend()\nplt.title(\"Model 1 - 'totalkW_mean' [d-1] & [w-1] (kW)\")\nplt.xlabel(\"day\");","8e8031a2":"# comparing the distribution of the following values\n\nplt.figure(figsize=[18,8])\nplt.subplot(221)\nsns.distplot(nl2['totalkW_d\/1'].head(initial_values), label='Real Series')\nsns.distplot(nl2['totalkW_d\/1'].tail(steps), label='Predictions')\nplt.legend()\nplt.title(\"Model 1 - Total kW [d\/1]\")\nplt.xlabel(\"\");\n\nplt.subplot(223)\nsns.distplot(nl2['totalkW_w\/1'].head(initial_values), label='Real Series')\nsns.distplot(nl2['totalkW_w\/1'].tail(steps), label='Predictions')\nplt.legend()\nplt.title(\"Model 1 - Total kW [w\/1]\")\nplt.xlabel(\"\");\n\nplt.subplot(222)\nsns.distplot(nl2['totalkW_d-1'].head(initial_values), label='Real Series')\nsns.distplot(nl2['totalkW_d-1'].tail(steps), label='Predictions')\nplt.legend()\nplt.title(\"Model 1 - Total kW [d-1] (kW)\")\nplt.xlabel(\"\");\n\nplt.subplot(224)\nsns.distplot(nl2['totalkW_w-1'].head(initial_values), label='Real Series')\nsns.distplot(nl2['totalkW_w-1'].tail(steps), label='Predictions')\nplt.legend()\nplt.title(\"Model 1 - Total kW [w-1] (kW)\")\nplt.xlabel(\"\");","5b3401b2":"df_real = y_df['t']\ndf_real = df_real.tail(steps)\ndf_pred = nl1.tail(steps)\n\ny_test_std_dev=y_test.astype(float).std()\nprint(\"Real Mean = %.2f\" % df_real.astype(float).mean())\nprint(\"Real Std Dev = %.2f\" % df_real.astype(float).std())\n\nalpha = 5.0\nlower_p = alpha \/ 2.0 # calculate lower percentile (e.g. 2.5)\nlower = max(0.0, percentile(df_real, lower_p)) # retrieve observation at lower percentile\nprint('%.1fth percentile = %.2f' % (lower_p, lower)) # calculate upper percentile (e.g. 97.5)\nupper_p = (100 - alpha) + (alpha \/ 2.0) # retrieve observation at upper percentile\nupper = max(1.0, percentile(df_real, upper_p))\nprint('%.1fth percentile = %.2f' % (upper_p, upper))\nprint(\"predictions Mean = %.2f\" % df_pred.astype(float).mean())\nprint(\"predictions Std Dev = %.2f\" % df_pred.astype(float).std())\n\nprint(\"MAE = %.2f\" % mean_absolute_error(df_real,df_pred))\nprint(\"MSE = %.2f\" % np.sqrt(mean_squared_error(df_real,df_pred)))\nprint(\"EVS = %.2f\" % explained_variance_score(df_real,df_pred))\nprint(\"Real Mean = %.2f\" % df_real.mean())\nprint(\"Prediction Mean = %.2f\" % df_pred['t'].mean())","7500e847":"plt.figure(figsize=(12,5)) \nplt.scatter(df_real,df_pred) # Model 1 predictions\nplt.plot(df_real,df_real,'red')  # y=x line\nplt.plot(y_test,y_test+y_test_std_dev,'grey', linewidth=.1) # y=x line\nplt.plot(y_test,y_test-y_test_std_dev,'grey', linewidth=.1) # y=x line\nplt.title(\"Scatter Model 1 - Real Series vs. predicted values (kW)\")\nplt.xlabel(\"kW\");\n\nplt.figure(figsize=(12,5)) # Plot predictions and the real values together\nplt.plot(df_real, 'blue', label= 'Real Series')\nplt.plot(df_pred, 'red', label= 'Predictions')\nplt.title(\"Model 1 - Real Series vs. Predicted Values (kW)\")\nplt.xlabel(\"days\")\nplt.legend();","d5394aa9":"X_df = total_df.drop(['Date','Day','Year','totalkW_max','totalkW_mean','insolation','totalkW_time_max'], axis=1)\nX_df = X_df.reset_index(drop=True)\n\ny_df = total_df['totalkW_mean']\ny_df = pd.concat([y_df], axis=1, keys=\"total\")\ny_df = y_df.reset_index(drop=True)\n\ninitial_values = 30\n\nnl3 = y_df.head(initial_values)\nnl4 = X_df.head(initial_values)","cb4a93f9":"i=len(nl3)-1\nj=1\npred=0\nsteps=564\nfor j in range(1,steps):\n    nl4 = nl4.append(pd.Series([nl3.loc[i][0]-nl3.loc[i-1][0]] + [nl3.loc[i][0]-nl3.loc[i-7][0]] + [nl3.loc[i][0]\/nl3.loc[i-1][0]] + [nl3.loc[i][0]\/nl3.loc[i-7][0]] + [X_df.loc[i][4]] + [nl4.loc[i-6][5]] + [X_df.loc[i][6]] + [X_df.loc[i][7]] + [X_df.loc[i][8]] + [X_df.loc[i][9]] + [X_df.loc[i][10]] + [X_df.loc[i][11]],  index=nl4.columns), ignore_index=True)\n    #print(nl4)\n    nly = nl4.tail(1).values\n    nly=scaler.transform(nly)\n    nly = np.reshape(nly, (nly.shape[0], 1, nly.shape[1]))\n    pred = float(model.predict(nly))\n    #print(pred)\n    nl3 = nl3.append(pd.Series(pred, index=nl3.columns), ignore_index=True)\n    j=j+1\n    i=i+1\n","e79a8020":"plt.figure(figsize=(10,4))\nplt.plot(nl3.head(initial_values),label='Real Series')\nplt.plot(nl3.tail(steps), label='Predicted Values')\nplt.legend()\nplt.title(\"Model 2 - Application (kW)\")\nplt.xlabel(\"day\");","31ce7457":"# comparing the distribution of the following values\n\nplt.figure(figsize=[18,8])\nplt.subplot(221)\nsns.distplot(nl4['totalkW_d\/1'].head(initial_values), label='Real Series')\nsns.distplot(nl4['totalkW_d\/1'].tail(steps), label='Predictions')\nplt.legend()\nplt.title(\"Model 2 - Total kW [d\/1]\")\nplt.xlabel(\"\");\n\nplt.subplot(223)\nsns.distplot(nl4['totalkW_w\/1'].head(initial_values), label='Real Series')\nsns.distplot(nl4['totalkW_w\/1'].tail(steps), label='Predictions')\nplt.legend()\nplt.title(\"Model 2 - Total kW [w\/1]\")\nplt.xlabel(\"\");\n\nplt.subplot(222)\nsns.distplot(nl4['totalkW_d-1'].head(initial_values), label='Real Series')\nsns.distplot(nl4['totalkW_d-1'].tail(steps), label='Predictions')\nplt.legend()\nplt.title(\"Model 2 - Total kW [d-1] (kW)\")\nplt.xlabel(\"\");\n\nplt.subplot(224)\nsns.distplot(nl4['totalkW_w-1'].head(initial_values), label='Real Series')\nsns.distplot(nl4['totalkW_w-1'].tail(steps), label='Predictions')\nplt.legend()\nplt.title(\"Model 2 - Total kW [w-1] (kW)\")\nplt.xlabel(\"\");","89f1af9b":"df_real = y_df['t']\ndf_real = df_real.tail(steps)\ndf_pred = nl3.tail(steps)\n\ny_test_std_dev=y_test.astype(float).std()\nprint(\"Real Mean = %.2f\" % df_real.astype(float).mean())\nprint(\"Real Std Dev = %.2f\" % df_real.astype(float).std())\n\nalpha = 5.0\nlower_p = alpha \/ 2.0 # calculate lower percentile (e.g. 2.5)\nlower = max(0.0, percentile(df_real, lower_p)) # retrieve observation at lower percentile\nprint('%.1fth percentile = %.2f' % (lower_p, lower)) # calculate upper percentile (e.g. 97.5)\nupper_p = (100 - alpha) + (alpha \/ 2.0) # retrieve observation at upper percentile\nupper = max(1.0, percentile(df_real, upper_p))\nprint('%.1fth percentile = %.2f' % (upper_p, upper))\nprint(\"predictions Mean = %.2f\" % df_pred.astype(float).mean())\nprint(\"predictions Std Dev = %.2f\" % df_pred.astype(float).std())\n\nprint(\"MAE = %.2f\" % mean_absolute_error(df_real,df_pred))\nprint(\"MSE = %.2f\" % np.sqrt(mean_squared_error(df_real,df_pred)))\nprint(\"EVS = %.2f\" % explained_variance_score(df_real,df_pred))\nprint(\"Real Mean = %.2f\" % df_real.astype(float).mean())\nprint(\"Prediction Mean = %.2f\" % df_pred['t'].mean())","c5c8b784":"df_real = y_df['t']\ndf_real = df_real.tail(steps)\ndf_pred = nl3.tail(steps)\nmodel_2_df = nl4\nmodel_2_df['predictions'] = df_pred\nmodel_2_df['real'] = df_real\n\nplt.figure(figsize=(12,5)) \nplt.scatter(df_real,df_pred) # Model 1 predictions\nplt.plot(df_real,df_real,'red')  # y=x line\nplt.plot(y_test,y_test+y_test_std_dev,'grey', linewidth=.1) # y=x line\nplt.plot(y_test,y_test-y_test_std_dev,'grey', linewidth=.1) # y=x line\nplt.title(\"Scatter Model 2 - Real Series vs. predicted values (kW)\")\nplt.xlabel(\"kW\");\n\nplt.figure(figsize=(18,6)) # Plot predictions and the real values together\nplt.plot(df_real, 'blue', label= 'Real Series')\nplt.plot(df_pred, 'red', label= 'Predictions')\nplt.title(\"Model 2 - Real Series vs. Predicted Values (kW)\")\nplt.xlabel(\"days\")\nplt.legend();","6a0913e3":"df_model_2 = nl4\ndf['real'] = df_real\ndf['predictions'] = df_pred\n# join the predictions and the real values to the used DF, so we can correlate all the dependent and independent variables","cbc2ddb8":"plt.figure(figsize=(12,4))\nsns.lineplot(x='WD', y='real', data=df_model_2, label='real')\nsns.lineplot(x='WD', y='predictions', data=df_model_2, label='predictions');\nplt.title(\"Model 2 - Real Series vs. Predicted Values (kW)\")\nplt.ylabel(\"\")\nplt.legend();\n","72d39c8a":"g = sns.FacetGrid(df_model_2, col='WD')\ng = g.map(plt.hist, 'real')\nh = sns.FacetGrid(df_model_2, col='WD')\nh = h.map(plt.hist, 'predictions');","f4eb2d93":"plt.figure(figsize=(14,1))\nrwd = df_model_2.pivot_table(values='real',columns='WD')\nsns.heatmap(rwd\/1000, annot=True,linecolor='white',linewidths=1, vmin=0.0, vmax=0.4)\nplt.title(\"Model 2 - Real Series (MW)\")\n\nplt.figure(figsize=(14,1));\npwd = df_model_2.pivot_table(values='predictions',columns='WD')\nsns.heatmap(pwd\/1000, annot=True,linecolor='white',linewidths=1, vmin=0.0, vmax=0.4)\nplt.title(\"Model 2 - Predictions (MW)\")\n\nplt.figure(figsize=(14,1));\nratio_wd = pwd.values\/rwd.values\nsns.heatmap(ratio_wd, annot=True,linecolor='white',linewidths=1, vmin=0.8, vmax=1.4)\nplt.title(\"Model 2 - Predictions \/ Real Series\");","360e4c02":"plt.figure(figsize=(14,2))\nsns.heatmap(df_model_2.corr()[-2:17], annot=True,linecolor='white',linewidths=1);","4f9d5864":"### When analyzing the power distribution on each day of the week there are some differences as well.","4d45d4c1":"### Below, there are 2 plots: the first illustrating the HDD, CDD and Mean Temperature during the internal.\n### On the second, it is shown the Relative Humidity\n### The weather variables have a considerable impact on the 'totalkW_mean', as it was presented right before.","d18cb9fe":"Let's check the missing values of the gathered data!","54220758":"### The variables shown above have a significant correlation with out dependent variable - 'totalkW_mean', so we will keep them in the model, with excecption for 'Day' and 'Insolation'.","4bfeaf88":"# Step 9 - Analyze the results - Model 2","dfb6af4e":"### The plot above illustrates that on Saturdays the model isn't predicting the values as it was supposed to.","7ed58ee1":"## Creating the model!","c2470b48":"# Step 7 - Analyze the results - Model 1","4b460eca":"### There are differences between the distribution of the relations of kW (d-1, d\/1, w-1 and w\/1) when comparing the prediction model and the real series. It might be a problem, because these 4 variables are input, which will attribute certain causal behaviours in the model, and consequently, on the predictions.","899e07ce":"### There are 2 main clusters of data, which represents the days during the week and the weekends. It is shown in the second graph as well (the valleys are the weekends).","81fe5711":"# Step 3 - Analyze the data","e0ba27bd":"### The residuals should be white noize for an effective model. THey were plotted before, but now it is analyzed for its autocorrelation. There are no significant correlation values for either the residuals and the residuals squared.","62d99231":"# Step 1 - Uploading the data","22bac558":"# Step 8 - Prediction Model 2!\n### Model 2 will predict 564 days\n\n","c2751ec6":"### The autocorrelation plot illustrates that our dataset has a correlation with its 7th day in a row. Basically, it means that the same week day has similar properties through time.","84156585":"### Above, we can clearly see that there is a major difference between the weekends (5 and 6) and the working days.\n### Let's analyze another relations between the dependent variable (output - 'totalkW_mean') and other dependent variables (inputs - Day of the week, month, ...).","af1cf1c2":"# The End!","accbe6fd":"# Step 4 - Create the model","d3dbec2c":"### It seems that the model captured a good behaviour of the system, but let's analyze it deeply!","4870835b":"### It seems that the model is a little bit better than the Model 1, but since there are less 'Real Series' values, the distribution is not that representative.","c36d1578":"### There are a lot of missing values on insolation, so we will discard it later.\n### Additionally, since we are missing the **first** 7 rows of 'totalkW_w\/1' and 'totalkW_w-1', we will discard them as well.","0cbedfeb":"### It is needed to split the gathered data to train and test our model. Therefore, it will use 75% of the data for training and 25% for testing.","b1c3ac15":"### When comparing the applied model with the model itself, there are a few differences. The applied model (Model 1) has larger errors than the trained model, as shown in the scattered plot. The same can be seen during the Sundays on the second plot. ","ff51774f":"### Above we analyzed the residuals (errors between the predicted and the real value). It is good to have a normal distribution, with low kurtosis and skewness.","907e3acc":"### Since we dropped the columns cited above, we remain with the following INPUT variables:\n* totalkW_d-1\n* totalkW_w-1\n* totalkW_d\/1\n* totalkW_w\/1\n* Month\n* WD\n* CDD_15\n* HDD_25\n* NWD\n* temp_max\n* temp_mean\n* RH","df7da047":"### First, let's see the distribution of the data","d8bbde8e":"### Finally, there are some differences between the correlations of the INPUT variables with the Real Series and the Predictions.","c55509ed":"### Adding an Early Stop! \n","de9a8746":"### The scatter plot shows some weekend data (the lower kW values) outside the 1 std. deviation line, as compared with the week days. The values through time also shows the predicted values on weekend below the real series.\n### The model is not predicting too well on weekends.","faa72b3f":"## Splitting and normalizing the data!","be969e63":"# Step 2 - Reduce the file size\n\n### This is a code that I didn't create. It is just used to reduce the file size, when working with a big amount of data","a27a7bbb":"### Here, we will plot the 4 substations (53, 71, 7A and 83), plus the joint values of SEs 71 & 71A and finally the Total, which is the sum of the 4 SEs.","e840424e":"### I did the same as Model 1, but now there are much more predicted values.","1bc839ab":"## Initially (in this worksheet, at least) we will work only with the total data, so we can filter it to a new DataFrame called \"total_df\"","c087fc0c":"### Thank you for reading my notebook. I know that there are some errors and missing informations for a complete and robust model creation and analyzis, but it will soon be released on v2.0.\n### On v2.0 it will segregate week days from weekends, so there will be 2 ANN models applied.\n\n### If there are any questions or feedback, please don't hesitate on contacting me.","2893f8d9":"### The ideal model will have the same results for training and validation losses, as well as lower values, indicating a great fitting of the model. In this case, the validation is a 5% higher than the training losses, indicating an underfitting.","295d3f2c":"### When comparing the mean values of the Predictions and the Real Series, it becomes clear that from Tuesday to Friday the model is tunned. On Saturdays there is a big difference, on Sundays and Mondays too, but not that significant.","a3c56f1c":"### The last 3 charts presents how the power peak behave during the day. Since we only have information about the peak, we don't have daily consumption curve, but we know that it peaks aroun 14-15 during the week and there is a distribution during the day for weekends.","df5ee105":"# Data description\n\n* Date = Date (dd\/mm\/yyyy)\n* 53_kW_mean = Mean electrical energy power from substation # 53 during the day (kW)\n* 53_kW_max  = Maximum electrical energy power from substation # 53 during the day (kW)\n* N_kW_mean = Mean electrical energy power from substation # N during the day (kW)\n* N_kW_max  = Maximum electrical energy power from substation # N during the day (kW)\n\n\n\n\n* totalkW_mean = Mean electrical energy power from all substations during the day (kW)\n* totalkW_max  = Maximum electrical energy power from all substations during the day (kW)\n\n\n\n* totalkW_d-1 = totalkW_mean from day [i] minus totalkW_mean from day [i-1] (kW)\n* totalkW_w-1  = totalkW_mean from day [i] minus totalkW_mean from day [i-7] (kW)\n* totalkW_d\/1 = totalkW_mean from day [i] over totalkW_mean from day [i-1] (kW)\n* totalkW_w\/1  = totalkW_mean from day [i] over totalkW_mean from day [i-7] (kW)\n\n\n\n* 71_71A_kW_mean = Mean electrical energy power from substations # 71 & 71A during the day (kW)\n* 71_71A_kW_max  = Maximum electrical energy power from substations # 71 & 71A during the day (kW)\n\n\n\n* Month = Month of the data\n* Year = Year of the data\n* Day = Day of the month of the data\n* WD = Day of the week - 0 is Monday, 6 is Sunday\n\n\n\n* N_kW_time_max = Hour of the day for the power peak from substation N (hh:mm)\n\n\n\n* CDD_15 = Cooling degree day of a 15\u00baC base (\u00baC)\n* HDD_25 = Heating degree day of a 25\u00baC base (\u00baC)\n\n\n\n* NWD = Non Working Day - 0 is False, 1 is True\n\n\n\n* temp_max = Maximum temperature during the day (\u00baC)\n* insolation = Average insolation during the day (Wh\/m\u00b2)\n* temp_mean = Mean temperature during the day (\u00baC)\n* RH = Relative Humidity (%)\n","e1997b02":"# Step 6 - Prediction Model","05a1b315":"### Then, we can see how it behaves through time (from day 0 to day 593)","0ebe3606":"# Step 5 - Fit the model!\n","612fac23":"### The ideia of the code below is to run a loop for each day, to consider the info from the last day. Simply, it collects all the INPUTs needed, which are the mean power of the day before and the weather data (temperature, relative humidity, HDD, ...)."}}