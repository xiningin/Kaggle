{"cell_type":{"518d81d0":"code","218bd660":"code","1993fa8e":"code","6d330574":"code","f08bc6d7":"code","4bca8fb6":"code","5c62206b":"code","90f9335e":"code","9fd418d0":"code","21e0cf84":"code","2782dc14":"code","e8729c78":"code","b6076fe7":"code","2dfcff43":"code","8936d235":"code","70f6d00b":"code","4f54699d":"code","9f078a7f":"code","453a356d":"code","09a069ef":"code","59146320":"code","db8fbed9":"markdown","08f8819b":"markdown","ebd9908e":"markdown","b1bb4367":"markdown","47fd1f4a":"markdown","76226261":"markdown","0c2739dc":"markdown","2997f9d8":"markdown","90af77f5":"markdown","60501849":"markdown","3b17d544":"markdown","50e3937b":"markdown","de150dac":"markdown","4074bbf3":"markdown","4d6a8e89":"markdown","f6cd575e":"markdown","8141ad66":"markdown","78eb6808":"markdown","5b4a06dd":"markdown","0f23c225":"markdown","b6c5c400":"markdown","b3c0700d":"markdown","b3fa5354":"markdown","3fbd5b85":"markdown","82e40964":"markdown","280ccfa0":"markdown","c586c8a3":"markdown","5ae1bf78":"markdown","cd8ef090":"markdown"},"source":{"518d81d0":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport transformers\nfrom transformers import AutoModel, BertTokenizerFast\n\n# specify GPU\ndevice = torch.device(\"cuda\")","218bd660":"df = pd.read_csv(\"..\/input\/spamdatatest\/spamdata_v2.csv\")\ndf.head()","1993fa8e":"# check class distribution\ndf['label'].value_counts(normalize = True)","6d330574":"# split train dataset into train, validation and test sets\ntrain_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n                                                                    random_state=2018, \n                                                                    test_size=0.3, \n                                                                    stratify=df['label'])\n\n\nval_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n                                                                random_state=2018, \n                                                                test_size=0.5, \n                                                                stratify=temp_labels)","f08bc6d7":"# import BERT-base pretrained model\nbert = AutoModel.from_pretrained('bert-base-uncased')\n\n# Load the BERT tokenizer\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')","4bca8fb6":"# get length of all the messages in the train set\nseq_len = [len(i.split()) for i in train_text]\n\npd.Series(seq_len).hist(bins = 30)","5c62206b":"# tokenize and encode sequences in the training set\ntokens_train = tokenizer.batch_encode_plus(\n    train_text.tolist(),\n    max_length = 25,\n    pad_to_max_length=True,\n    truncation=True\n)\n\n# tokenize and encode sequences in the validation set\ntokens_val = tokenizer.batch_encode_plus(\n    val_text.tolist(),\n    max_length = 25,\n    pad_to_max_length=True,\n    truncation=True\n)\n\n# tokenize and encode sequences in the test set\ntokens_test = tokenizer.batch_encode_plus(\n    test_text.tolist(),\n    max_length = 25,\n    pad_to_max_length=True,\n    truncation=True\n)","90f9335e":"## convert lists to tensors\n\ntrain_seq = torch.tensor(tokens_train['input_ids'])\ntrain_mask = torch.tensor(tokens_train['attention_mask'])\ntrain_y = torch.tensor(train_labels.tolist())\n\nval_seq = torch.tensor(tokens_val['input_ids'])\nval_mask = torch.tensor(tokens_val['attention_mask'])\nval_y = torch.tensor(val_labels.tolist())\n\ntest_seq = torch.tensor(tokens_test['input_ids'])\ntest_mask = torch.tensor(tokens_test['attention_mask'])\ntest_y = torch.tensor(test_labels.tolist())","9fd418d0":"\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n#define a batch size\nbatch_size = 32\n\n# wrap tensors\ntrain_data = TensorDataset(train_seq, train_mask, train_y)\n\n# sampler for sampling the data during training\ntrain_sampler = RandomSampler(train_data)\n\n# dataLoader for train set\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# wrap tensors\nval_data = TensorDataset(val_seq, val_mask, val_y)\n\n# sampler for sampling the data during training\nval_sampler = SequentialSampler(val_data)\n\n# dataLoader for validation set\nval_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)","21e0cf84":"# freeze all the parameters\nfor param in bert.parameters():\n    param.requires_grad = False","2782dc14":"class BERT_Arch(nn.Module):\n\n    def __init__(self, bert):\n        super(BERT_Arch, self).__init__()\n        \n        self.bert = bert \n        \n        # dropout layer\n        self.dropout = nn.Dropout(0.1)\n      \n        # relu activation function\n        self.relu =  nn.ReLU()\n\n        # dense layer 1\n        self.fc1 = nn.Linear(768,512)\n      \n        # dense layer 2 (Output layer)\n        self.fc2 = nn.Linear(512,2)\n\n        #softmax activation function\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    #define the forward pass\n    def forward(self, sent_id, mask):\n        \n        #pass the inputs to the model  \n        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n      \n        x = self.fc1(cls_hs)\n\n        x = self.relu(x)\n\n        x = self.dropout(x)\n\n        # output layer\n        x = self.fc2(x)\n      \n        # apply softmax activation\n        x = self.softmax(x)\n\n        return x","e8729c78":"# pass the pre-trained BERT to our define architecture\nmodel = BERT_Arch(bert)\n\n# push the model to GPU\nmodel = model.to(device)","b6076fe7":"# optimizer from hugging face transformers\nfrom transformers import AdamW\n\n# define the optimizer\noptimizer = AdamW(model.parameters(),lr = 1e-5) ","2dfcff43":"from sklearn.utils.class_weight import compute_class_weight\n\n#compute the class weights\nclass_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n\nprint(\"Class Weights:\",class_weights)","8936d235":"\n# converting list of class weights to a tensor\nweights= torch.tensor(class_weights,dtype=torch.float)\n\n# push to GPU\nweights = weights.to(device)\n\n# define the loss function\ncross_entropy  = nn.NLLLoss(weight=weights) \n\n# number of training epochs\nepochs = 10","70f6d00b":"# function to train the model\ndef train():\n    \n    model.train()\n    total_loss, total_accuracy = 0, 0\n  \n    # empty list to save model predictions\n    total_preds=[]\n  \n    # iterate over batches\n    for step,batch in enumerate(train_dataloader):\n        \n        # progress update after every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n        \n        # push the batch to gpu\n        batch = [r.to(device) for r in batch]\n \n        sent_id, mask, labels = batch\n        \n        # clear previously calculated gradients \n        model.zero_grad()        \n\n        # get model predictions for the current batch\n        preds = model(sent_id, mask)\n\n        # compute the loss between actual and predicted values\n        loss = cross_entropy(preds, labels)\n\n        # add on to the total loss\n        total_loss = total_loss + loss.item()\n\n        # backward pass to calculate the gradients\n        loss.backward()\n\n        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # update parameters\n        optimizer.step()\n\n        # model predictions are stored on GPU. So, push it to CPU\n        preds=preds.detach().cpu().numpy()\n\n    # append the model predictions\n    total_preds.append(preds)\n\n    # compute the training loss of the epoch\n    avg_loss = total_loss \/ len(train_dataloader)\n  \n      # predictions are in the form of (no. of batches, size of batch, no. of classes).\n      # reshape the predictions in form of (number of samples, no. of classes)\n    total_preds  = np.concatenate(total_preds, axis=0)\n\n    #returns the loss and predictions\n    return avg_loss, total_preds","4f54699d":"# function for evaluating the model\ndef evaluate():\n    \n    print(\"\\nEvaluating...\")\n  \n    # deactivate dropout layers\n    model.eval()\n\n    total_loss, total_accuracy = 0, 0\n    \n    # empty list to save the model predictions\n    total_preds = []\n\n    # iterate over batches\n    for step,batch in enumerate(val_dataloader):\n        \n        # Progress update every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            \n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n\n        # push the batch to gpu\n        batch = [t.to(device) for t in batch]\n\n        sent_id, mask, labels = batch\n\n        # deactivate autograd\n        with torch.no_grad():\n            \n            # model predictions\n            preds = model(sent_id, mask)\n\n            # compute the validation loss between actual and predicted values\n            loss = cross_entropy(preds,labels)\n\n            total_loss = total_loss + loss.item()\n\n            preds = preds.detach().cpu().numpy()\n\n            total_preds.append(preds)\n\n    # compute the validation loss of the epoch\n    avg_loss = total_loss \/ len(val_dataloader) \n\n    # reshape the predictions in form of (number of samples, no. of classes)\n    total_preds  = np.concatenate(total_preds, axis=0)\n\n    return avg_loss, total_preds","9f078a7f":"# set initial loss to infinite\nbest_valid_loss = float('inf')\n\n# empty lists to store training and validation loss of each epoch\ntrain_losses=[]\nvalid_losses=[]\n\n#for each epoch\nfor epoch in range(epochs):\n     \n    print('\\n Epoch {:} \/ {:}'.format(epoch + 1, epochs))\n    \n    #train model\n    train_loss, _ = train()\n    \n    #evaluate model\n    valid_loss, _ = evaluate()\n    \n    #save the best model\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'saved_weights.pt')\n    \n    # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    \n    print(f'\\nTraining Loss: {train_loss:.3f}')\n    print(f'Validation Loss: {valid_loss:.3f}')","453a356d":"#load weights of best model\npath = 'saved_weights.pt'\nmodel.load_state_dict(torch.load(path))","09a069ef":"# get predictions for test data\nwith torch.no_grad():\n    preds = model(test_seq.to(device), test_mask.to(device))\n    preds = preds.detach().cpu().numpy()","59146320":"# model's performance\npreds = np.argmax(preds, axis = 1)\nprint(classification_report(test_y, preds))","db8fbed9":"<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">1. Import Required Libraries & Dataset<\/h1>","08f8819b":"<center><h1 style=\"font-size:200%; color:green;\">Please give this kernel an UPVOTE to show your appreciation, if you find it useful.<\/h1><\/center>","ebd9908e":"<center><h1 style=\"font-size:200%; font-family:cursive; color:navy; height:65px;\">The model is trained with both Masked LM and Next Sentence Prediction together. This is to minimize the combined loss function of the two strategies \u2014 \u201ctogether is better\u201d.<\/h1><\/center>\n","b1bb4367":"<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">3. Import Bert - base- uncased<\/h1>","47fd1f4a":"<p style=\"font-size:150%; font-family:verdana;\">What is language modeling really about? Which problem are language models trying to solve? Basically, their task is to \u201cfill in the blank\u201d based on context. For example, given<\/p>\n<br>\n<center><h1 style=\"font-size:150%; font-family:cursive;\">\u201cThe woman went to the store and bought a _____ of shoes.\u201d<\/h1><\/center>\n<br>\n<br>\n<p style=\"font-size:150%; font-family:verdana;\">a language model(One-Directional Approach) might complete this sentence by saying that the word \u201ccart\u201d would fill the blank 20% of the time and the word \u201cpair\u201d 80% of the time.<\/p>\n\n<p style=\"font-size:150%; font-family:verdana;\"><b>Now enters BERT<\/b>, a language model which is bidirectionally trained (this is also its key technical innovation). This means we can now have a deeper sense of language context and flow compared to the single-direction language models. Unlike the previous language models, it takes both the previous and next tokens into account at the same time.<\/p>\n\n<p style=\"font-size:150%; font-family:verdana;\">Moreover, BERT is based on the Transformer model architecture, instead of LSTMs.<\/p>","76226261":"<center><p style=\"font-size:200%; font-family:cursive; background:skyblue; padding:20px; border:solid;\">\u201cBERT stands for Bidirectional Encoder Representations from Transformers. It is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of NLP tasks.\u201d<\/p><\/center>","0c2739dc":"<center><h1 style=\"font-size:200%; font-family:cursive;\">1. <u>Text - Preprocessing<\/u><\/h1><\/center>\n<br>\n<p style=\"font-size:150%; font-family:verdana;\">BERT relies on a Transformer (the attention mechanism that learns contextual relationships between words in a text). A basic Transformer consists of an encoder to read the text input and a decoder to produce a prediction for the task. Since BERT\u2019s goal is to generate a language representation model, it only needs the encoder part. The input to the encoder for BERT is a sequence of tokens, which are first converted into vectors and then processed in the neural network. For starters, every input embedding is a combination of 3 embeddings:<\/p>\n\n<center><img src=\"https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2019\/09\/bert_emnedding.png\"><\/center>\n<center><h1 style=\"font-size:150%; font-family:verdana;\">The input representation for BERT: The input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings.<\/h1><\/center>\n<br>\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n<ul>\n    <li style=\"font-size:150%; font-family:verdana;\"><b>Token embeddings:<\/b> A [CLS] token is added to the input word tokens at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.<\/li>\n    <li style=\"font-size:150%; font-family:verdana;\"><b>Segment embeddings:<\/b>A marker indicating Sentence A or Sentence B is added to each token. This allows the encoder to distinguish between sentences.<\/li>\n    <li style=\"font-size:150%; font-family:verdana;\"><b>Positional embeddings:<\/b>A positional embedding is added to each token to indicate its position in the sentence.<\/li>\n<\/ul>\n<br>\n<center><h1 style=\"font-size:200%; font-family:cursive;\">2. <u>Pre-training Tasks<\/u><\/h1><\/center>\n<br>\n<p style=\"font-size:190%; font-family:verdana;\">BERT is pre-trained on two NLP tasks:<\/p>\n\n<h1 style=\"font-size:170%; font-family:cursive; color:navy;\"> 1. Masked Language Modelling<\/h1>\n<ul>\n    <li style=\"font-size:150%; font-family:verdana;\">Language Modeling is the task of predicting the next word given a sequence of words. In masked language modeling instead of predicting every next token, a percentage of input tokens is masked at random and only those masked tokens are predicted.<\/li>\n    <li style=\"font-size:150%; font-family:verdana;\"> [MASK] Token - This is a token to denote that the token is missing<\/li>\n    <li style=\"font-size:150%; font-family:verdana;\"> The masked words are not always replaced with the masked token \u2013 [MASK] because then the masked tokens would never be seen before fine-tuning. Therefore, 15% of the tokens are chosen at random. And out of the 15% of the tokens selected for masking:<\/li>\n    <br>\n    <center><h1 style=\"font-size:150%; font-family:cursive;\">80% of the tokens are actually replaced with the token [MASK].<\/h1><\/center>\n    <center><h1 style=\"font-size:150%; font-family:cursive;\">10% of the time tokens are replaced with a random token.<\/h1><\/center>\n    <center><h1 style=\"font-size:150%; font-family:cursive;\">10% of the time tokens are left unchanged.<\/h1><\/center>\n<\/ul>\n<br>\n<h1 style=\"font-size:170%; font-family:cursive; color:navy;\"> 2. Next Sentence Prediction<\/h1>\n<ul>\n    <li style=\"font-size:150%; font-family:verdana;\">Next sentence prediction task is a binary classification task in which, given a pair of sentences, it is predicted if the second sentence is the actual next sentence of the first sentence.<\/li>\n    <br>\n    <center><img src=\"https:\/\/yashuseth.files.wordpress.com\/2019\/06\/fig5.png?w=442&h=231\"><\/center>\n    <br>\n    <li style=\"font-size:150%; font-family:verdana;\">This task can be easily generated from any monolingual corpus. It is helpful because many downstream tasks such as Question and Answering and Natural Language Inference require an understanding of the relationship between two sentences.<\/li>\n<\/ul>","2997f9d8":"<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">2. Split the Dataset into train \/ test<\/h1>","90af77f5":"<center><h1 style=\"font-size:300%; font-family:cursive; color:black; background:skyblue; padding:15px; border:solid;\">BERT's ARCHITECTURE<\/h1><\/center>","60501849":"<ol>\n    <li style=\"font-size:150%;\"><a href=\"https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/ao23cp\/p_how_to_use_bert_in_kaggle_competitions_a\/\">How to use BERT in Kaggle competitions - Reddit Thread<\/a><\/li>\n    <li style=\"font-size:150%;\"><a href=\"http:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/\">A visual guide to using BERT by Jay Alammar<\/a><\/li>\n    <li style=\"font-size:150%;\"><a href=\"https:\/\/www.analyticsvidhya.com\/blog\/2019\/09\/demystifying-bert-groundbreaking-nlp-framework\/\">Demystifying BERT: Groundbreaking NLP Framework by Mohd Sanad Zaki Rizvi<\/a><\/li>\n    <li style=\"font-size:150%;\"><a href=\"https:\/\/towardsdatascience.com\/bert-for-dummies-step-by-step-tutorial-fb90890ffe03\">BERT for Dummies step by step tutorial by Michel Kana<\/a><\/li>\n<\/ol>","3b17d544":"<ul>\n    <li style=\"font-size:150%; font-family:verdana;\">Proper language representation is key for general-purpose language understanding by machines. Context-free models such as <b>word2vec or GloVe<\/b> generate a single word embedding representation for each word in the vocabulary. For example, the word \u201cbank\u201d would have the same representation in \u201cbank deposit\u201d and in \u201criverbank\u201d. Contextual models instead generate a representation of each word that is based on the other words in the sentence. <b>BERT<\/b>, as a contextual model, captures these relationships in a bidirectional way.<\/li>\n    <br>\n    <li style=\"font-size:150%; font-family:verdana;\">BERT was built upon recent work and clever ideas in pre-training contextual representations including Semi-supervised Sequence Learning, Generative Pre-Training, ELMo, the OpenAI Transformer, ULMFit and the Transformer. Although these models are all unidirectional or shallowly bidirectional, BERT is fully bidirectional.<\/li>\n    <br>\n    <li style=\"font-size:150%; font-family:verdana;\">The best part about <b>BERT<\/b> is that it can be download and used for free \u2014  we can either use the  BERT models to extract high quality language features from our text data, or we can fine-tune these models on a specific task, like sentiment analysis and question answering, with our own data to produce state-of-the-art predictions<\/li>\n<\/ul>","50e3937b":"<center><h1 style=\"font-size:300%; font-family:cursive; color:navy;\">How does it work? - Let's Dive into it<\/h1><\/center>","de150dac":"<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">8. Fine - Tune<\/h1>","4074bbf3":"<center><h1 style=\"font-size:300%; font-family:cursive; color:black; background:skyblue; padding:15px; border:solid;\">REFERENCES & CREDITS<\/h1><\/center>","4d6a8e89":"<center><img src=\"https:\/\/towardsml.files.wordpress.com\/2019\/09\/bert.png?w=810&h=580&crop=1\" height=100px width=500px><\/center>","f6cd575e":"<center><h1 style=\"font-size:300%; font-family:cursive; color:navy;\">Why do we need BERT ?<\/h1><\/center>","8141ad66":"<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">9. Make Predictions<\/h1>","78eb6808":"<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">5. List to Tensors<\/h1>","5b4a06dd":"<center><h1 style=\"font-size:300%; font-family:cursive; color:navy;\">What is the core idea behind it?<\/h1><\/center>","0f23c225":"<br>\n<center><h1 style=\"font-size:300%; font-family:cursive; color:black; background:skyblue; padding:15px; border:solid;\">CONCLUSION<\/h1><\/center>","b6c5c400":"<br>\n<center><h1 style=\"font-size:300%; font-family:cursive; color:black; background:skyblue; padding:15px; border:solid;\">IMPLEMENTATION OF BERT<\/h1><\/center>","b3c0700d":"<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">7. Model Architecture<\/h1>","b3fa5354":"<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">4. Tokenize & Encode the Sequences<\/h1>","3fbd5b85":"<p style=\"font-size:150%; font-family:verdana;\">There are four types of pre-trained versions of BERT depending on the scale of the model architecture:<\/p>\n<ol>\n    <li style=\"font-size:150%; font-family:verdana;\">BERT-Base (Cased \/ Un-Cased): 12-layer, 768-hidden-nodes, 12-attention-heads, 110M parameters<\/li>\n    <li style=\"font-size:150%; font-family:verdana;\">BERT-Large (Cased \/ Un-Cased): 24-layer, 1024-hidden-nodes, 16-attention-heads, 340M parameters<\/li>\n<\/ol>\n\n<center><img src=\"https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2019\/09\/bert_encoder.png\"><\/center>\n\n<p style=\"font-size:150%; font-family:verdana;\">We need to choose <b>which BERT pre-trained weights we want<\/b>. For example, if we don\u2019t have access to a Google TPU, we\u2019d rather stick with the Base models. And then the choice of \u201ccased\u201d vs \u201cuncased\u201d depends on whether we think letter casing will be helpful for the task at hand.<\/p>","82e40964":"<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">6. Data Loader<\/h1>","280ccfa0":"<u><h2 style=\"font-size:170%; font-family:cursive;\">Which Tokenization strategy is used by BERT?<\/h2><\/u>\n\n<p style=\"font-size:150%; font-family:verdana;\">BERT uses WordPiece tokenization. The vocabulary is initialized with all the individual characters in the language, and then the most frequent\/likely combinations of the existing words in the vocabulary are iteratively added.<\/p>\n<br>\n<u><h2 style=\"font-size:170%; font-family:cursive;\">What is the maximum sequence length of the input?<\/h2><\/u>\n\n<p style=\"font-size:150%; font-family:verdana;\">The maximum sequence length of the input = 512<\/p>","c586c8a3":"<p style=\"font-size:150%; font-family:verdana;\">BERT is undoubtedly a breakthrough in the use of Machine Learning for Natural Language Processing. The fact that it\u2019s approachable and allows fast fine-tuning will likely allow a wide range of practical applications in the future. In this Notebook we have discussed about BERT (Theoritical + Practical Part).<\/p>","5ae1bf78":"<p style=\"font-size:150%; font-family:verdana;\"><b>Problem Statement:<\/b> We have a collection of SMS messages. Some of these messages are spam and the rest are genuine. Our task is to build a system that would automatically detect whether a message is spam or not.<\/p>","cd8ef090":"<ul>\n    <li style=\"font-size:150%;\">The dataset consists of two columns \u2013 \u201clabel\u201d and \u201ctext\u201d. The column \u201ctext\u201d contains the message body and the \u201clabel\u201d is a binary variable where 1 means spam and 0 means the message is not a spam.<\/li>\n<\/ul>"}}