{"cell_type":{"baf390a1":"code","ff2ae494":"code","f56cd70c":"code","10d86259":"code","745f8120":"code","55677fcd":"code","696f2f52":"code","277b0e36":"code","8d2a2ce4":"code","f3ceaebb":"code","ca98796f":"code","25f426b2":"code","7df8cdb3":"code","9542527e":"code","572b9beb":"code","28515f05":"code","b72c6fd5":"code","e433f7e9":"code","85e0c40d":"code","fac25ecb":"code","e98e74be":"code","b1323e43":"code","b6432ea9":"code","5c90e5c5":"code","74197a5f":"code","b895f0ef":"code","8c3f7eea":"code","744dfb12":"code","564abf08":"code","3a8acaa3":"code","a5a16537":"code","5d8a645a":"code","466f3477":"code","1a3ef256":"code","0740d8a6":"code","545a2b00":"code","afca4bea":"code","7efd2d28":"code","290b4cf5":"code","5cd05cbc":"code","c22f7ec4":"code","2682d3fa":"code","0abd0735":"code","3ca4f5d1":"code","8da91cb4":"code","54de660f":"code","0ff819c6":"code","336db6ce":"code","b600ff02":"code","b985af7d":"code","d5642036":"code","9afbc5ad":"code","fa03c1c7":"code","3ee456a0":"code","a913192a":"code","d9cf0af5":"code","6ad37d6f":"code","27a40b95":"code","b5cdf9b8":"code","1a049df5":"code","39ed1843":"code","3b11123b":"code","0708e0f8":"code","53b291fa":"code","8a76637c":"code","a53814c4":"code","ba66f6aa":"code","5d30d5d6":"code","ec2fcafd":"code","0a8adc84":"code","8c65777f":"code","0e63efdd":"code","98c31c77":"code","e75b3565":"code","3f0d02ee":"code","42a6ccb7":"code","616d6a1b":"code","f3e15463":"code","695298c2":"code","aa9b2773":"code","775fdca6":"code","c701c109":"code","0db21dad":"code","d0767d48":"code","10a71fee":"code","13e09389":"code","96f37690":"code","7b283e0b":"code","3ca0e0fc":"code","718939ec":"code","6df2efd3":"code","bf2f628d":"code","66391d6b":"code","56aa3bd5":"code","0d140c74":"code","e15d8aa6":"markdown","ccb2c6a6":"markdown","f546b7a7":"markdown","bd3961e6":"markdown","eb9edaec":"markdown","dde72ebb":"markdown","83a6850d":"markdown","e99063fd":"markdown","9dcc9d31":"markdown","d157e16c":"markdown","c076fbe9":"markdown","aa9a81a2":"markdown","e434dd4f":"markdown","bd35afb4":"markdown","05ac2cf4":"markdown","1255bafe":"markdown","cc337b15":"markdown","3f28140e":"markdown","aa50f715":"markdown","ea17d18d":"markdown","ec7176e5":"markdown","d8e7906f":"markdown","2e2bae02":"markdown","178ac54a":"markdown","e8ddc25a":"markdown","8e0df868":"markdown","79aade05":"markdown","575ced36":"markdown","8a69b6c8":"markdown","7a758ee1":"markdown","dc1e4996":"markdown","10fbd674":"markdown","9565568f":"markdown","29048a56":"markdown","094e4c1c":"markdown","0b779112":"markdown","446ec537":"markdown","76347daa":"markdown","2ef03ad3":"markdown","a11cfcd9":"markdown","0b3c82d8":"markdown","5a8abb61":"markdown","8270106a":"markdown","7640ab2b":"markdown","b0e45dfb":"markdown","86fb771d":"markdown","36a0fccf":"markdown","273449bc":"markdown","65c633ba":"markdown","38c77f00":"markdown","db06fa59":"markdown","7e56db66":"markdown"},"source":{"baf390a1":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport re\nimport emoji\n\nfrom IPython.display import Markdown as md\nplt.style.use('ggplot')","ff2ae494":"from sklearn.metrics import classification_report,accuracy_score,confusion_matrix","f56cd70c":"train_path = \"..\/input\/nlp-getting-started\/train.csv\"\ntest_path = \"..\/input\/nlp-getting-started\/test.csv\"\nsample_submission_path = \"..\/input\/nlp-getting-started\/sample_submission.csv\"","10d86259":"df_train = pd.read_csv(train_path)\ndf_test = pd.read_csv(test_path)\nsubmission = pd.read_csv(sample_submission_path)","745f8120":"df_train.head()","55677fcd":"df_train.info()","696f2f52":"print(df_train.info())","277b0e36":"df_test.info()","8d2a2ce4":"df_train = df_train[['text','target']]\ndf_test = df_test[['text']]","f3ceaebb":"y = np.array(df_train.target.value_counts())\nsns.barplot(x = [0,1],y = y,palette='gnuplot2_r')\ndifference = y[0]-y[1]\nprint(\"Difference between target 0 and 1: \",y[0]-y[1])","ca98796f":"df_train.text.describe()","25f426b2":"df_train['Char_length'] = df_train['text'].apply(len)","7df8cdb3":"df_train[df_train['target']==0].Char_length.head()","9542527e":"f, axes = plt.subplots(1, 2, figsize=(14, 4), sharex=True)\nf.suptitle(\"Histogram of char length of text\",fontsize=20)\nsns.distplot(df_train[df_train['target']==0].Char_length.values,kde=False,bins=20,hist=True,ax=axes[0],label=\"Histogram of 20 bins of label 0\",\n            kde_kws={\"color\": \"r\", \"lw\": 2, \"label\": \"KDE 0\"},\n                           hist_kws={ \"linewidth\": 2,\n                                     \"alpha\": 1, \"color\": \"y\"})\naxes[0].legend(loc=\"best\")\naxes[0].set_ylabel(\"Rows Count\")\nsns.distplot(df_train[df_train['target']==1].Char_length.values,kde=False,bins=20,hist=True,ax=axes[1],label=\"Histogram of 20 bins of label 1\",\n            kde_kws={\"color\": \"g\", \"lw\": 2, \"label\": \"KDE 1\"},\n                           hist_kws={ \"linewidth\": 2,\n                                     \"alpha\": 1, \"color\": \"pink\"})\naxes[1].legend(loc=\"best\")\n\nplt.figure(figsize=(14,4))\nsns.distplot(df_train[df_train['target']==0].Char_length.values,kde=True,bins=20,hist=True,label=\"Histogram of 20 bins of label 0\",\n            kde_kws={\"color\": \"r\", \"lw\": 2,\"label\": \"KDE 0\"},\n                           hist_kws={ \"linewidth\": 2,\n                                     \"alpha\": 1, \"color\": \"y\"})\n\nsns.distplot(df_train[df_train['target']==1].Char_length.values,kde=True,bins=20,hist=True,label=\"Histogram of 20 bins of label 1\",\n            kde_kws={\"color\": \"g\", \"lw\": 2,\"label\": \"KDE 1\"},\n                           hist_kws={ \"linewidth\": 2,\n                                     \"alpha\": 1, \"color\": \"pink\"})\nplt.ylabel(\"density\")\nplt.legend(loc=\"best\")","572b9beb":"def word_count(sent):\n    return len(sent.split())\ndf_train['word_count'] = df_train.text.apply(word_count)","28515f05":"df_train.head()","b72c6fd5":"f, axes = plt.subplots(1, 2, figsize=(14, 4), sharex=True)\nf.suptitle(\"Histogram of word count\",fontsize=20)\nsns.distplot(df_train[df_train['target']==0].word_count.values,kde=False,bins=20,hist=True,ax=axes[0],label=\"Histogram of label 0\",\n            kde_kws={\"color\": \"r\", \"lw\": 2, \"label\": \"KDE 0\"},\n                           hist_kws={ \"linewidth\": 2,\n                                     \"alpha\": 1, \"color\": \"y\"})\naxes[0].legend(loc=\"best\")\naxes[0].set_ylabel(\"Rows Count\")\nsns.distplot(df_train[df_train['target']==1].word_count.values,kde=False,bins=20,hist=True,ax=axes[1],label=\"Histogram of label 1\",\n            kde_kws={\"color\": \"g\", \"lw\": 2, \"label\": \"KDE 1\"},\n                           hist_kws={ \"linewidth\": 2,\n                                     \"alpha\": 1, \"color\": \"pink\"})\naxes[1].legend(loc=\"best\")\n\nplt.figure(figsize=(14,4))\nsns.distplot(df_train[df_train['target']==0].word_count,kde=True,bins=20,hist=True,label=\"Histogram of 20 bins of label 0\",\n            kde_kws={\"color\": \"r\", \"lw\": 2,\"label\": \"KDE 0\"},\n                           hist_kws={ \"linewidth\": 2,\n                                     \"alpha\": 1, \"color\": \"y\"})\n\nsns.distplot(df_train[df_train['target']==1].word_count,kde=True,bins=20,hist=True,label=\"Histogram of 20 bins of label 1\",\n            kde_kws={\"color\": \"g\", \"lw\": 2,\"label\": \"KDE 1\"},\n                           hist_kws={ \"linewidth\": 2,\n                                     \"alpha\": 1, \"color\": \"pink\"})\nplt.ylabel(\"Density\")\nplt.legend(loc=\"best\")","e433f7e9":"def urls(sent):\n    return re.findall('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',sent)\ndef url_counts(sent):\n    return len(re.findall('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',sent))\ndef remove_urls(sent):\n    return re.sub('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',sent)","85e0c40d":"s ='Working on Nlp. So much fun  - https:\/\/www.helloworld.com, https:\/\/www.worldhello.com'\nprint(urls(s))\nprint(url_counts(s))\nprint(remove_urls(remove_urls(s)))","fac25ecb":"%%time\n\ndf_train['url_count'] = df_train.text.apply(url_counts)\ndf_train['urls'] = df_train.text.apply(urls)","e98e74be":"# An overview of dataframe after above transformations\ndf_train.head()","b1323e43":"print(\"Total Urls : \",sum(df_train.url_count))","b6432ea9":"f, axes = plt.subplots(1, 2, figsize=(14, 4), sharex=True)\nf.suptitle(\"Histogram of url_counts\",fontsize=20)\nsns.distplot(df_train[df_train['target']==0].url_count,kde=False,bins=10,hist=True,ax=axes[0],label=\"Histogram of label 0\",\n                           hist_kws={ \"linewidth\": 2,\n                                     \"alpha\": 1, \"color\": \"y\"})\naxes[0].legend(loc=\"best\")\naxes[0].set_ylabel(\"Rows Count\")\nsns.distplot(df_train[df_train['target']==1].url_count,kde=False,bins=10,hist=True,ax=axes[1],label=\"Histogram of label 1\",\n                           hist_kws={ \"linewidth\": 2,\n                                     \"alpha\": 1, \"color\": \"pink\"})\naxes[1].legend(loc=\"best\")\n\nplt.figure(figsize=(14,4))\nsns.distplot(df_train[df_train['target']==0].url_count,kde=False,bins=10,hist=True,label=\"Histogram of 10 bins of label 0\",\n                           hist_kws={ \"linewidth\": 2,\n                                     \"alpha\": 1, \"color\": \"y\"})\n\nsns.distplot(df_train[df_train['target']==1].url_count,kde=False,bins=10,hist=True,label=\"Histogram of 10 bins of label 1\",\n                           hist_kws={ \"linewidth\": 2,\n                                     \"alpha\": 0.8, \"color\": \"pink\"})\nplt.ylabel(\"Rows count\")\nplt.legend(loc=\"best\")","5c90e5c5":"# Actual Url Count  and differnece\nprint(\"Actual Url Count in 0 : \",df_train[df_train['target']==0].url_count.sum())\nprint(\"Actual Url Count in 1 : \",df_train[df_train['target']==1].url_count.sum())\nprint(\"Actual Url Count  differnece : \"\n      ,df_train[df_train['target']==1].url_count.sum() - df_train[df_train['target']==0].url_count.sum())","74197a5f":"# Unique Urls\n\ntotal_uniques = np.unique(np.ravel(df_train.urls.values)).shape[0]\nuniques_in_1 = np.unique(np.ravel(df_train[df_train['target']==1].urls.values)).shape[0]\nuniques_in_0 = np.unique(np.ravel(df_train[df_train['target']==0].urls.values)).shape[0]\nprint(\"Total uniques url : \", total_uniques)\nprint(\"Uniques in 0 : \",uniques_in_0)\nprint(\"Uniques in 1 : \",uniques_in_1)","b895f0ef":"df_train['text'] = df_train.text.apply(remove_urls)","8c3f7eea":"# Only for emojis not work for special content\ndef emoji_extraction(s):\n    return ''.join(c for c in s if c in emoji.UNICODE_EMOJI)\ndef emoji_count(s):\n    return len(''.join(c for c in s if c in emoji.UNICODE_EMOJI))","744dfb12":"# Example\n\ns = \"Working on Nlp \ud83d\ude42. So much \ud83d\ude00 fun \ud83d\ude00 \"\nprint(\"emoji_text                      : \", emoji_extraction(s))\nprint(\"Count of emojis                 : \",emoji_count(s))","564abf08":"# Work for both emojis and special content.\n\ndef emoji_extraction(sent):\n    e_sent = emoji.demojize(sent)\n    \n    return re.findall(':(.*?):',e_sent)\ndef emoji_count(sent):\n    e_sent = emoji.demojize(sent)\n    return len(re.findall(':(.*?):',e_sent))\n\ndef emoji_to_text(sent):\n    e_sent = emoji.demojize(sent)\n    emo = re.findall(':(.*?):',e_sent)\n    for e in emo:\n        e_sent = e_sent.replace(':{}:'.format(e),'{}'.format(e))\n    return e_sent","3a8acaa3":"# Example\n\ns = \"Working on Nlp \ud83d\ude42. So much \ud83d\ude00 fun \ud83d\ude00 \"\nprint(\"emoji_text                      : \", emoji_extraction(s))\nprint(\"Count of emojis                 : \",emoji_count(s))\n\nprint(\"Placing text in place of emojis : \",emoji_to_text(s))","a5a16537":"%%time\ndf_train['emoji_count'] = df_train.text.apply(emoji_count)\ndf_train['emojis'] = df_train.text.apply(emoji_extraction)","5d8a645a":"df_train[85:90]","466f3477":"emoji_count0  = df_train[df_train.target==0].emoji_count.value_counts()\nemoji_count0 = emoji_count0.sort_index()\n\nemoji_count1  = df_train[df_train.target==1].emoji_count.value_counts()\nemoji_count1 = emoji_count1.sort_index()","1a3ef256":"f,axes = plt.subplots(1,2,figsize=(14, 4))\nf.suptitle(\"# of Emojis\",fontsize=20)\nsns.barplot(emoji_count0.index,emoji_count0.values,ax = axes[0], label = \"# of emojis in 0\")\naxes[0].set(ylim=(0, 4500))\nplt.legend()\naxes[0].legend(loc=\"best\")\n\nsns.barplot(emoji_count1.index,emoji_count1.values,ax = axes[1], label = \"# of emojis in 1\")\naxes[1].set(ylim=(0, 4500))\naxes[1].legend(loc=\"best\")","0740d8a6":"print(\"Actual emoji Count in 0 : \",df_train[df_train['target']==0].emoji_count.sum())\nprint(\"Actual emoji Count in 1 : \",df_train[df_train['target']==1].emoji_count.sum())\nprint(\"Actual Url Count  differnece : \"\n      ,df_train[df_train['target']==1].emoji_count.sum() - df_train[df_train['target']==0].emoji_count.sum())","545a2b00":"# Converting list of Emojis_text to Single full_text\n\ndef concatlists(lists):\n    full_text =\"\"\n    for l in lists:\n        full_text = full_text + \" \"+l[0]\n    return full_text    \n\n\nlists0 = df_train[np.logical_and(df_train.emoji_count>0,df_train.target==0)].emojis.values\n\nemoji_text_for_target0 = concatlists(lists0)\n\n\nlists1 = df_train[np.logical_and(df_train.emoji_count>0,df_train.target==1)].emojis.values\n\nemoji_text_for_target1 = concatlists(lists1)","afca4bea":"plt.figure(figsize = (18,12))\ncloud = WordCloud(background_color='black',max_font_size =80).generate(emoji_text_for_target0)\nplt.imshow(cloud)\nplt.axis('off')\nplt.title(\"EMOJIS TEXT FOR TARGET 0\",fontsize=35)\nplt.show()","7efd2d28":"plt.figure(figsize = (18,12))\ncloud = WordCloud(background_color='black',max_font_size =50).generate(emoji_text_for_target1)\nplt.imshow(cloud)\nplt.axis('off')\nplt.title(\"EMOJIS TEXT FOR TARGET 1\",fontsize=35)\nplt.show()","290b4cf5":"%%time\ndf_train['text'] = df_train['text'].apply(emoji_to_text)","5cd05cbc":"def get_text(df):\n    astext = '. '.join(list(df_train.text.values))\n    text_file = open(\"tweets.txt\", \"w\")\n    text_file.write(astext)\n    text_file.close()\n    \nget_text(df_train)","c22f7ec4":"def find_hashtags(text):\n    gethashtags = re.findall('#\\w*[a-zA-Z]\\w*',text)\n    return gethashtags\n\ndef count_hashtags(text):\n    gethashtags = re.findall('#\\w*[a-zA-Z]\\w*',text)\n    return len(gethashtags)\n\ndef remove_hashtags(text):\n    return re.sub('#\\w*[a-zA-Z]\\w*','',text)","2682d3fa":"# Example\n\ns = \"Working on Nlp #Nlp. So much fun #getfun awesome #tag89tag  #99999\" \nprint(\"Hashtags    : \",find_hashtags(s))\nprint(\"HashCount   : \",count_hashtags(s))\nprint(\"withouthash : \",remove_hashtags(s))","0abd0735":"%%time\ndf_train['hash_count'] = df_train.text.apply(count_hashtags)\ndf_train['hashtags'] = df_train.text.apply(find_hashtags)","3ca4f5d1":"df_train.head()","8da91cb4":"hash_count0  = df_train[df_train.target==0].hash_count.value_counts()\nhash_count0 = hash_count0.sort_index()\n\nhash_count1  = df_train[df_train.target==1].hash_count.value_counts()\nhash_count1 = hash_count1.sort_index()","54de660f":"# Dropping Count 0 as it will unbalance our plot\n\nhash_count0 = hash_count0.drop(0)\nhash_count1 = hash_count1.drop(0)","0ff819c6":"f,axes = plt.subplots(1,2,figsize=(14, 4))\nf.suptitle(\"# of HashTags\",fontsize=20)\nsns.barplot(hash_count0.index,hash_count0.values,ax = axes[0], label = \"# of HashTags in 0\")\naxes[0].set(ylim=(0, 3500))\nplt.legend()\naxes[0].legend(loc=\"best\")\n\nsns.barplot(hash_count1.index,hash_count1.values,ax = axes[1], label = \"# of HashTags in 1\")\naxes[1].set(ylim=(0, 3500))\naxes[1].legend(loc=\"best\")","336db6ce":"# Converting list of Hashtags to Single full_text\n\nlists0 = df_train[np.logical_and(df_train.hash_count>0,df_train.target==0)].hashtags.values\n\nhash_for_target0 = concatlists(lists0)\n\n\nlists1 = df_train[np.logical_and(df_train.hash_count>0,df_train.target==1)].hashtags.values\n\nhash_for_target1 = concatlists(lists1)","b600ff02":"plt.figure(figsize = (18,12))\ncloud = WordCloud(background_color='black',max_font_size =80).generate(hash_for_target0)\nplt.imshow(cloud)\nplt.axis('off')\nplt.title(\"HASHTAGS FOR TARGET 0\",fontsize=35)\nplt.show()","b985af7d":"plt.figure(figsize = (18,12))\ncloud = WordCloud(background_color='black',max_font_size =50).generate(hash_for_target1)\nplt.imshow(cloud)\nplt.axis('off')\nplt.title(\"HASHTAGS FOR TARGET 1\",fontsize=35)\nplt.show()","d5642036":"df_train['text'] = df_train.text.apply(remove_hashtags)","9afbc5ad":"def extract_username(sent):\n    usernames = re.findall('@[A-Za-z0-9_$]*',sent)\n    return usernames\n\ndef count_username(sent):\n    return len(re.findall('@[A-Za-z0-9_$]*',sent))\n\ndef replace_username(sent):\n    usernames = extract_username(sent)\n    for un in usernames:\n        un = re.sub('@','',un)\n        sent = sent.replace('@{}'.format(un),'{}'.format(un))\n    return sent","fa03c1c7":"# Example\n\ns = \"hello this is @Aman. Wanna talk to @some_one99 urgently.\"\n\nprint(\"usernames       : \", extract_username(s))\nprint(\"Count username  : \", count_username(s))\nprint(\"replace text    : \", replace_username(s))","3ee456a0":"%%time\ndf_train['text'] = df_train.text.apply(replace_username)","a913192a":"def find_number(text):\n    getnumber = re.findall('#[0-9]+',text)\n    return getnumber\n\ndef count_number(text):\n    getnumber = re.findall('#[0-9]+',text)\n    return len(getnumber)\n\ndef remove_number(text):\n    return re.sub('#[0-9]+','',text)","d9cf0af5":"# Example\n\ns = \"Working on Nlp #Nlp. So much fun #getfun awesome #tag89tag  #99999\" \nprint(\"Number        : \",find_number(s))\nprint(\"Numbercount   : \",count_number(s))\nprint(\"withoutNumber : \",remove_number(s))","6ad37d6f":"%%time\ndf_train['count_number'] = df_train.text.apply(count_number)\ndf_train['number'] = df_train.text.apply(find_number)","27a40b95":"print(\"Total number found : \",df_train.count_number.sum())","b5cdf9b8":"df_train['text'] = df_train.text.apply(remove_number)","1a049df5":"def find_punctuations(text):\n    getpunctuation = re.findall('[.?\"\\'`\\,\\-\\!:;\\(\\)\\[\\]\\\\\/\u201c\u201d]+?',text)\n    return getpunctuation\n\ndef count_punctuations(text):\n    getpunctuation = re.findall('[.?\"\\'`\\,\\-\\!:;\\(\\)\\[\\]\\\\\/\u201c\u201d]+?',text)\n    return len(getpunctuation)\n\ndef remove_punctuations(text):\n    return re.sub('[.?\"\\'`\\,\\-\\!:;\\(\\)\\[\\]\\\\\/\u201c\u201d]+?','',text)","39ed1843":"s = 'Aman : \u201cIt is a historic moment ,\u201d What about! ... your thoughts? 100\/100' \nprint(\"Punctuation        : \",find_punctuations(s))\nprint(\"Punctuationcount   : \",count_punctuations(s))\nprint(\"withoutPunctuation : \",remove_punctuations(s))","3b11123b":"%%time\ndf_train['count_punctuation'] = df_train.text.apply(count_punctuations)\ndf_train['punctuation'] = df_train.text.apply(find_punctuations)","0708e0f8":"punct_count = df_train.count_punctuation.value_counts()\npunct_count = punct_count.sort_index()","53b291fa":"plt.figure(figsize=(18,6))\nplt.title(\"# of punctuations\",fontsize=20)\nsns.barplot(punct_count.index,punct_count.values)\nplt.xlabel(\"# of punctuation per row\",fontsize=20)\nfor row,col in zip([i for i in range(len(punct_count))],punct_count.values):\n    plt.text(row,col,col,ha = 'center')\nplt.ylabel(\"# of rows \",fontsize=20)","8a76637c":"def remove_symbols(text):\n    return re.sub('[~:*\u00db\u00d3_\u00e5\u00a8\u00c8$#\u0089&%^\u00aa|+-]+?','',text)","a53814c4":"s = 'abcd \u00db sd\u00d3_da\u00e5fs%^\u00aa|+-fgdas' \n\nprint(\"withoutsymbols : \",remove_symbols(s))","ba66f6aa":"df_train['text'] = df_train.text.apply(remove_punctuations)","5d30d5d6":"df_train['text'] = df_train.text.apply(remove_symbols)","ec2fcafd":"get_text(df_train)","0a8adc84":"!pip install pyspellchecker\n!pip install textblob","8c65777f":"from spellchecker import SpellChecker\nfrom textblob import TextBlob\n\ndef find_typo(sent):\n    spell = SpellChecker()\n    words = sent.split()\n    words = spell.unknown(words)\n    return words\n\ndef count_typo(sent):\n    return len(find_typo(sent))\n\ndef correct_typo(sent):\n    spell = SpellChecker()\n    words = sent.split()\n    words = spell.unknown(words)\n    find = []\n    for word in words:\n        find.append(spell.correction(word))\n    return find    \n\ndef correct_byspellchecker(sent):\n    ic, c = list(find_typo(sent)), list(correct_typo(sent))\n    for i in range(len(ic)):\n        sent = sent.replace(ic[i],c[i])\n    return sent\n\ndef correct_bytextblob(sent):\n    return str(TextBlob(sent).correct())","0e63efdd":"# Example\n\ns = \"Good to workk with naturl laguage procesing Can yoou tell me more about your work how well you doneee it\"\nprint(\"orginal_text            :  \",s)\nprint(\"typos                   :  \",find_typo(s))\nprint(\"count typos             :  \",count_typo(s))\nprint(\"correct typos           :  \",correct_typo(s))\nprint(\"correct_by spellchecker :  \",correct_byspellchecker(s))\nprint(\"correct_bytextblob      :  \",correct_bytextblob(s))","98c31c77":"# %%time\n# df_train['count_typo'] = df_train.text.apply(count_typo)\n# df_train['typo'] = df_train.text.apply(find_typo)\n# try:\n#     df_train['correct_typo'] = df_train.text.apply(correct_typo)\n# except:\n#     pass\n\n# df_train.to_csv('getdata.csv',index=False)","e75b3565":"df_train = pd.read_csv(\"..\/input\/real-nlp-disaster-tweets-processed-dataframe\/getdata.csv\")","3f0d02ee":"plt.figure(figsize = (14, 4))\nplt.title(\"Histogram of typo_count\",fontsize=20)\nsns.distplot(df_train.count_typo,kde=False,bins=30,hist=True,\n                           hist_kws={ \"linewidth\": 2,\n                                     \"alpha\": 1, \"color\": \"y\"})\nplt.ylabel(\"Rows Count\")","42a6ccb7":"from sklearn.model_selection import train_test_split\n\ntrain,valid = train_test_split(df_train,test_size = 0.2,random_state=0,stratify = df_train.target.values)\n\nprint(\"train shape : \", train.shape)\nprint(\"valid shape : \", valid.shape)","616d6a1b":"from nltk.corpus import stopwords\nstop = list(stopwords.words('english'))","f3e15463":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(decode_error = 'replace',stop_words = stop)\n\nX_train = vectorizer.fit_transform(train.text.values)\nX_valid = vectorizer.transform(valid.text.values)\n\ny_train = train.target.values\ny_valid = valid.target.values\n\nprint(\"X_train.shape : \", X_train.shape)\nprint(\"X_train.shape : \", X_valid.shape)\nprint(\"y_train.shape : \", y_train.shape)\nprint(\"y_valid.shape : \", y_valid.shape)","695298c2":"from sklearn.naive_bayes import MultinomialNB\n\nbaseline_clf = MultinomialNB()\nbaseline_clf.fit(X_train,y_train)","aa9b2773":"baseline_prediction = baseline_clf.predict(X_valid)\nbaseline_accuracy = accuracy_score(y_valid,baseline_prediction)\nprint(\"training accuracy Score    : \",baseline_clf.score(X_train,y_train))\nprint(\"Validdation accuracy Score : \",baseline_accuracy )","775fdca6":"plt.figure(figsize = (4,4))\nclass_label = [0,1]\nfig = sns.heatmap(confusion_matrix(y_valid,baseline_prediction),cmap= \"coolwarm\",annot=True,vmin=0,cbar = False,\n            center = True,xticklabels=class_label,yticklabels=class_label, fmt='d' )\nfig.set_xlabel(\"Prediction\",fontsize=30)\nfig.xaxis.set_label_position('top')\nfig.set_ylabel(\"True\",fontsize=30)\nfig.xaxis.tick_top()","c701c109":"md(\"### Our Baseline model validation accuracy is {}% (overfitting)\".format(round(baseline_accuracy,2)))","0db21dad":"from sklearn.linear_model import SGDClassifier\nlinear_model_sgd = SGDClassifier(loss = 'hinge', penalty = 'l2', random_state=0)\nlinear_model_sgd.fit(X_train,y_train)","d0767d48":"linear_model_sgd_prediction = linear_model_sgd.predict(X_valid)\nlinear_model_sgd_accuracy = accuracy_score(y_valid,linear_model_sgd_prediction)\nprint(\"training accuracy Score    : \",linear_model_sgd.score(X_train,y_train))\nprint(\"Validdation accuracy Score : \",linear_model_sgd_accuracy )","10a71fee":"md(\"### Our sgd model validation accuracy is {}% (heavily overfitting)\".format(round(linear_model_sgd_accuracy,2)))","13e09389":"plt.figure(figsize = (4,4))\nclass_label = [0,1]\nfig = sns.heatmap(confusion_matrix(y_valid,linear_model_sgd_prediction),cmap= \"coolwarm\",annot=True,vmin=0,cbar = False,\n            center = True,xticklabels=class_label,yticklabels=class_label, fmt='d' )\nfig.set_xlabel(\"Prediction\",fontsize=30)\nfig.xaxis.set_label_position('top')\nfig.set_ylabel(\"True\",fontsize=30)\nfig.xaxis.tick_top()","96f37690":"from sklearn.model_selection import GridSearchCV\n\nparams = {\n     'max_iter': (100,500,1000),\n     'alpha': (1e-1,1e-2,1e-4),\n      'learning_rate': ('optimal','invscaling'),\n    'eta0' : (0.1,0.05),\n                     \n}\n\ngridcv = GridSearchCV(linear_model_sgd,param_grid = params, cv = 5)\n\ngridcv.fit(X_train,y_train)\nprint(\"best parameter : \")\nfor param_name in sorted(params.keys()):\n    print(\"      %s: %r\" % (param_name, gridcv.best_params_[param_name]))","7b283e0b":"linear_model_sgd_prediction = gridcv.predict(X_valid)\nlinear_model_sgd_accuracy = accuracy_score(y_valid,linear_model_sgd_prediction)\nprint(\"Tune training accuracy Score    : \",gridcv.score(X_train,y_train))\nprint(\"Tune Validation accuracy Score : \",linear_model_sgd_accuracy )","3ca0e0fc":"plt.figure(figsize = (4,4))\nclass_label = [0,1]\nfig = sns.heatmap(confusion_matrix(y_valid,linear_model_sgd_prediction),cmap= \"coolwarm\",annot=True,vmin=0,cbar = False,\n            center = True,xticklabels=class_label,yticklabels=class_label, fmt='d' )\nfig.set_xlabel(\"Prediction\",fontsize=30)\nfig.xaxis.set_label_position('top')\nfig.set_ylabel(\"True\",fontsize=30)\nfig.xaxis.tick_top()","718939ec":"print(classification_report(y_valid,linear_model_sgd_prediction))","6df2efd3":"md(\"### Our tune sgd model validation accuracy is {}% (less overfitting)\".format(round(linear_model_sgd_accuracy,2)))","bf2f628d":"def preprocessing(df):\n    df['Char_length']       = df['text'].apply(len)\n    df['word_count']        = df.text.apply(word_count)\n    df['url_count']         = df.text.apply(url_counts)\n    df['urls']              = df.text.apply(urls)\n    df['text']              = df.text.apply(remove_urls)\n    df['emoji_count']       = df.text.apply(emoji_count)\n    df['emojis']            = df.text.apply(emoji_extraction)\n    df['text']              = df['text'].apply(emoji_to_text)\n    df['hash_count']        = df.text.apply(count_hashtags)\n    df['hashtags']          = df.text.apply(find_hashtags)\n    df['text']              = df.text.apply(remove_hashtags)\n    df['text']              = df.text.apply(replace_username)\n    df['count_number']      = df.text.apply(count_number)\n    df['number']            = df.text.apply(find_number)\n    df['text']              = df.text.apply(remove_number)\n    df['count_punctuation'] = df.text.apply(count_punctuations)\n    df['punctuation']       = df.text.apply(find_punctuations)\n    df['text']              = df.text.apply(remove_punctuations)\n    df['text']              = df.text.apply(remove_symbols)\n                                            \n    return df","66391d6b":"%%time\nprocessed_test_df = preprocessing(df_test)","56aa3bd5":"def bagofword(df):\n    X = vectorizer.transform(df.text.values)\n    return X    \n\ndef predict_test(model,x):\n    return model.predict(x)","0d140c74":"X_test = bagofword(processed_test_df)\ntarget = predict_test(gridcv,X_test)\nsubmission['target'] = target\nsubmission.to_csv('submission.csv',index=False)\nsubmission.head()","e15d8aa6":"# Hashtags Cloud","ccb2c6a6":"# Loading Data","f546b7a7":"### Looking for text and target data only","bd3961e6":"# Cleaning text \n - by removing urls link from text as we have extract urls to other column feature.","eb9edaec":"#### by birdview of textfile. Some of the insights are-\n* textfile contains Hashtags\n* textfile contains @\n* contains date and numbers\n* punctuation specially ( ? , ' . - \" : )\n* Special Character $ * %\n* Misspelt word(typos) and abbreviations","dde72ebb":"# Dealing with @username","83a6850d":"# Cleaning Text\nby removing numbers","e99063fd":"# In Progress ...","9dcc9d31":"<h3 style=\"color:red;\" > \u2620\ufe0f Histogram is plot on continuous variable but we will see analyse variable discreate variable. Before analysing this we suppose that these variables are continuous rather than discreate. Because of higher bins(buckets) some of histogram more likely seems as barplot. <\/h3>","d157e16c":"# Cleaning Text\n\n- by removing hashtags from text","c076fbe9":"#### To Get more familier with text. We collect the text to a textfile and then point out some more features in it.","aa9a81a2":"# Work Flow\n<img src=\"https:\/\/www.mdpi.com\/ijgi\/ijgi-06-00368\/article_deploy\/html\/images\/ijgi-06-00368-g001.png\" width=\"500\">","e434dd4f":"# EDA ON TWEETS\n![](data:image\/jpeg;base64,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)","bd35afb4":"# Looking for emojis\n* Emojis are very few in the content text.\n* So We are not only looking for emojis but some special content as :{special content}:\n* We simply call emojis to both emojis and special content.","05ac2cf4":"# BagOfWords Implementation","1255bafe":"# Preprocessing Test Set","cc337b15":"## Some other symbols","3f28140e":"# Some Other Refrences\n- [Spellchecker](https:\/\/pypi.org\/project\/pyspellchecker\/)\n- [textblob](https:\/\/www.pythonprogramming.in\/how-to-correct-spelling-using-textblob-in-python.html)","aa50f715":"# Misspelt word(typo)","ea17d18d":"#### It will only find or replace hastag with empty string s with at least one letter. It will not touch any standard phone keyboards as describe above in some common symbol and uses.","ec7176e5":"# Linear Model (SGD Classifier)","d8e7906f":"# Calculating and analyzing No. of words in each text","2e2bae02":"**Problem To Solve** - Predict which Tweets are about real disasters and which ones are not\n<img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcR7UC1R_V7RUvB2P140YPT5N70EwAOCjb893UauMd_Gp0RQoBM&s\" width=\"500\">","178ac54a":"# Linear Model Prediction on test set","e8ddc25a":"# Extracting urls from the text","8e0df868":"# Getting Phone Numbers","79aade05":"### Text length almost same for both targets","575ced36":"### Our First Submission Touch the benchmark score 0.78179","8a69b6c8":"#### We will not going to get username as it might be unique for every text so best to remove or replace it. Removing may issue and looks as  misleading text. So we replace it by removing @ symbol. Later we will replace with one common name.","7a758ee1":"# Loading Libraries","dc1e4996":"# Baseline Model (Naive Bayes)","10fbd674":"### Classification Report of tune sgd model","9565568f":"# Calculating and analyzing Char length of each text","29048a56":"# Cleaning Text\n- by removing @","094e4c1c":"### Parameter Tuning of SGDClassifier","0b779112":"# Getting textfile","446ec537":"# Disaster Tweets and its Consequences","76347daa":"**#\\w*[a-zA-Z]\\w* &nbsp; &nbsp; -> &nbsp; &nbsp; # after that string(any letter or number) with at least one letter**\n* \\w: Any letter or number, *: 0 or more times\n* [a-zA-Z]: Any letter, A-Z, caps A-Z or lowercase a-z\n* \\w: Any letter or number, *: 0 or more times","2ef03ad3":"# Emojis Text Cloud","a11cfcd9":"# Acknowledge to Other Sources\n1. [Technology org](https:\/\/www.technology.org\/2020\/02\/28\/real-or-not-nlp-with-disaster-tweets\/#:~:text=Twitter%20has%20become%20an%20important,re%20observing%20in%20real%2Dtime.)\n2. [Basic EDA,Cleaning and GloVe](https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove)\n3. [emojicopy](https:\/\/www.emojicopy.com\/)\n4. [Cheatsheet - Text Helper Functions](https:\/\/www.kaggle.com\/raenish\/cheatsheet-text-helper-functions)\n4. [englishclub](https:\/\/www.englishclub.com\/writing\/symbols.php#:~:text=%40,addresses%20and%20social%20media%20handles.)","0b3c82d8":"# Cleaning text \n- by replacing emoji to emoji_text","5a8abb61":"#### Twitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programmatically monitoring Twitter  (i.e. disaster relief organizations and news agencies).","8270106a":"# Splitting dataset","7640ab2b":"# Punctuations","b0e45dfb":"## Some common symbol and uses\n\n* '&'   \n        -> The ampersand is short-hand for the word \"and\". It is typically seen in company names and various abbreviations.\n* '*' \n        -> We use the asterisk to point to an annotation or footnote.\n        -> It can also be used as a substitute for letters in a swear word (\"Oh f***!\") or to make a name anonymous (Mr M***).\n* '@' \n        -> Originally an accounting shorthand meaning \"at the rate of\".\n        -> Today the at sign is most commonly seen in email addresses and social media handles.\n       \n* '#' \n        -> Since 2007 it has been used to create social media hashtags.\n        -> The symbol # is commonly used with numbers, especially in American English.\n        -> It is also used on standard phone keyboards.\n\n#### Visit [site](https:\/\/www.englishclub.com\/writing\/symbols.php#:~:text=%40,addresses%20and%20social%20media%20handles.) to get familier with typing symbols.","86fb771d":"# Version History\n* **Version 1 ---------------------------------------------------------------  22-08-2020**\n* * Old version\n* **Version 2 ---------------------------------------------------------------- 22-08-2020**\n* * *Modifying Some Results*\n* **Version 3 ---------------------------------------------------------------- 22-08-2020**\n* * *Modifying Some Results*\n* **Version 4 ---------------------------------------------------------------- 23-08-2020**\n* * Loading Libraries\n* * Loading Data\n* * EDA ON TWEETS\n* * Target Counts\n* * Calculating and analyzing Char length of each text\n* * Calculating and analyzing No. of words in each text\n* * Extracting urls from the text\n* * Cleaning Text ( by removing urls )\n* * Looking for emojis\n* * Emojis Text Cloud\n* * Cleaning Text ( by replacing emojis to emojis text )\n* **Version 5 ---------------------------------------------------------------- 24-08-2020**\n* * Getting textfile\n* * Hashtags\n* * Hashtags Cloud\n* * Cleaning Text ( by removing Hashtags )\n* **Version 6 ---------------------------------------------------------------- 25-08-2020**\n* * Dealing with @username \n* * Cleaning Text ( by removing @)\n* **Version 7 ---------------------------------------------------------------- 26-08-2020**\n* * Getting Phone Numbers\n* * Cleaning Text ( by removing Phone Number )\n* * Punctuations\n* * Cleaning Text ( by removing Punctuations and special symbols)\n* **Version 8 ----------------------------------------------------------------- 27-08-2020**\n* * *Modifying Some Results*\n* **Version 9 ----------------------------------------------------------------- 27-08-2020**\n* * Misspelt word(typo)\n* **Version 10 ---------------------------------------------------------------- 27-08-2020**\n* * *Modifying Some Results*\n* **Version 11 ---------------------------------------------------------------- 27-08-2020**\n* * *Modifying Some Results*\n* **Version 12 ---------------------------------------------------------------- 28-08-2020**\n* * Splitting dataset\n* * BagOfWords Implementation\n* * Baseline Model (Naive Bayes)\n* * Linear Model (SGD Classifier)\n* **Version 13 ---------------------------------------------------------------- 30-08-2020**\n* * *Modifying Some Results*\n* **Version 14 ---------------------------------------------------------------- 30-08-2020**\n* * Preprocessing Test Set\n* * Linear Model Prediction on test set\n* **Version 15 --------------------------------------------------------------- *In Progress***\n* * Acknowledement to other source","36a0fccf":"# Target Counts","273449bc":"### Above comment code tooks so much time\n- CPU times: user 3h 44min 24s\n- sys: 8min 14s, total: 3h 52min 39s\n- Wall time: 3h 52min 43s*\n\nWe use the data that run first time in version 11 of the same notebook. So That we have not to wait.","65c633ba":"# Hashtags","38c77f00":"### Looks more url counts in disaster Tweets","db06fa59":"# Cleaning Text\n- by removing Punctuations and special symbols","7e56db66":"### Looks almost same for both targets"}}