{"cell_type":{"dd5063df":"code","e5acd877":"code","2c67b65b":"code","0f379b8c":"code","59e24bd0":"code","7bf103d1":"code","a823bb83":"code","1563c716":"code","f0ccf5ee":"code","ecc72f69":"code","b853e84e":"code","2e2f90a4":"code","4cb8c41a":"code","b8ef7b20":"code","690d14af":"code","008fa71f":"code","10f7b4dd":"code","62c081d2":"code","594e14bf":"code","f93d1119":"code","4da1af3c":"code","c5b137eb":"code","0f9b06bd":"code","e3f295ea":"code","37595fb3":"code","4c022e68":"code","6b872e6a":"code","808785dc":"code","8ab21551":"code","181dc22c":"code","962ba49c":"code","a77feb5f":"code","2941fcdc":"code","357276d9":"code","d5518545":"code","bcbac5c4":"code","7cd13001":"code","3c3080d6":"code","45d835b7":"code","4d210c4d":"code","df567950":"code","a15eeaba":"code","107b2ac9":"code","7840e46f":"code","b6a40557":"code","7a96bebe":"code","9eb7896e":"code","8b68b32d":"code","17ff5aa4":"code","d29320a9":"code","a866fa6e":"code","8d0de284":"code","bbd667d8":"code","c66c58a2":"code","f2aa7784":"code","0ac20c0a":"code","d586679e":"code","3c34f8b8":"code","5732785c":"code","3e8a7fb4":"code","79b082f1":"code","5340db75":"code","15d159c4":"code","25cb541c":"code","1e27ca75":"code","5c5eaa3e":"code","8cbb60f7":"code","30796418":"code","818130d8":"code","44b02aca":"code","2496bc88":"code","6f285946":"code","5518c92b":"code","6446412b":"code","3552ea2c":"code","b3f00095":"code","bb73435e":"code","80b8a503":"code","d4f17b00":"code","aa91d051":"markdown","3d8a7a69":"markdown","968d81eb":"markdown","c00652b5":"markdown","57615973":"markdown","2e4a3f96":"markdown","324f1081":"markdown","8601fd98":"markdown","49e0777b":"markdown","5ad7c96b":"markdown","89211912":"markdown","b098abe7":"markdown","c43cb153":"markdown","9315f1bb":"markdown","b5c19df9":"markdown","fa3848c7":"markdown","438f63e0":"markdown","65192d66":"markdown","1b45a460":"markdown","ec8ca364":"markdown","3d2c637e":"markdown","e98ba259":"markdown","b0482fc2":"markdown","a2024980":"markdown","cce93eb9":"markdown","52edbca8":"markdown","248f50d6":"markdown","87535710":"markdown","66e08085":"markdown","7b4540bb":"markdown","59d86d0b":"markdown","af882f5e":"markdown","1bc048ed":"markdown","391ccbdb":"markdown","3e16017e":"markdown","8a67a575":"markdown","b9c2509c":"markdown","cb01572b":"markdown","fa3ede20":"markdown","09f73f61":"markdown","ef97a4a4":"markdown","d6877872":"markdown","a82f24f4":"markdown","d0efcf9f":"markdown","a67a91e0":"markdown","144e27ad":"markdown","5c0e23f1":"markdown","93cd0b88":"markdown","e9d51068":"markdown","837728d6":"markdown","f0ac343d":"markdown","9d8579bc":"markdown","265ab9c8":"markdown","98c49afd":"markdown","74381c1b":"markdown","b6c221ac":"markdown","f7f737b3":"markdown","7f7d8c96":"markdown","a389b34e":"markdown","59ad6622":"markdown","3034851a":"markdown","1281c400":"markdown","318abbc5":"markdown","62856c65":"markdown"},"source":{"dd5063df":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e5acd877":"import seaborn as sns\nprint('Seaborn version : {}'.format(sns.__version__))\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nprint('Pandas version : {}'.format(pd.__version__))\nprint('Numpy version : {}'.format(np.__version__))\nprint('Matplotlib version : {}'.format(None))\nplt.style.use('ggplot')\n","2c67b65b":"sns.set_palette('Pastel2')","0f379b8c":"df_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\nprint('Train data import sucessfully. : df_train')\nprint('Test data import sucessfully. : df_test')\n\ntrain = df_train.copy()\ntest = df_test.copy()\n\nprint('Copy of Train data created sucessfully. : train')\nprint('Copy of Test data created sucessfully. : test')","59e24bd0":"print('Train data : {}'.format(train.shape))\nprint('Test data : {}'.format(test.shape))","7bf103d1":"print(train.info(),'\\n')\nprint(train.describe(),'\\n')","a823bb83":"print(test.info(),'\\n')\nprint(test.describe(),'\\n')","1563c716":"train","f0ccf5ee":"test","ecc72f69":"train_na = train.isnull().sum()\ntest_na = test.isnull().sum()\n\nprint('Missing value\\'s count for each column from train and test data :')\nprint(pd.concat([train_na, test_na], axis = 1, keys=['Train', 'Test']))","b853e84e":"print('Unique values for Categorical features :','\\n')\n\nprint('Train')\nfor col in train.drop(['PassengerId', 'Name','Age', 'Ticket', 'Fare', 'Cabin'],axis = 1):\n    print(col + ' : {}'.format(train[col].unique()))\nprint('\\n')    \nprint('Test')\nfor col in test.drop(['PassengerId', 'Name','Age', 'Ticket', 'Fare', 'Cabin'],axis = 1):\n    print(col + ' : {}'.format(test[col].unique()))","2e2f90a4":"from pandas_profiling import ProfileReport","4cb8c41a":"ProfileReport(train, title=\"Pandas Profiling Report\")","b8ef7b20":"train.Survived.unique()","690d14af":"def univariate_countplot(data, x,xlabel,ylabel = 'No. of Passengers',title = '',fontsize = 14, legend = '',label = '',ax = None):\n    '''Plot countplot for a single variable.'''\n    sns.countplot(data = data, x = x,ax = ax)\n    plt.title(title, fontsize = fontsize)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n\ndef univariate_count(on, norm = False):\n    '''Print the Pmf for one variable.'''\n    print('Proportion of {} :'.format(on))\n    print(train[on].value_counts(normalize = norm ).sort_index())\n    \ndef bivariate_countplot(data, x,hue,xlabel,ylabel,title,legend = '',label = '',ax = ''):\n    '''Plot countplot for a single variable with hue as second variable.'''\n    sns.countplot(data = train, x = x, hue = hue, ax=ax)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(title, fontsize = 14)\n    if legend == True:\n        plt.legend(loc = 'best',labels = label, title = hue)\n\n    \ndef bivariate_groupby(by,on,normalize):\n    '''Print the Pmf for two variable.'''\n    print('Proportion of {} by {} :'.format(by,on))\n    print(train.groupby(by)[on].value_counts(normalize = normalize).sort_index())","008fa71f":"print('Survival rate :')\nprint(train.Survived.value_counts(normalize=True))","10f7b4dd":"fig,ax = plt.subplots(figsize = (5,5))\n\nunivariate_countplot(train,'Survived','Survived',ylabel = 'No. of Passengers',title = 'Survived',fontsize = 14, legend = '',label = '',ax = ax)\nplt.xticks([0,1], ['No','Yes'])\nplt.yticks(np.arange(0,600,50))","62c081d2":"univariate_count('Pclass', True)","594e14bf":"fig,ax = plt.subplots()\nunivariate_countplot(train,'Pclass','Pclass', 'No. of passengers', 'Ticket Class',ax=ax)","f93d1119":"bivariate_groupby('Pclass','Survived', True)","4da1af3c":"bivariate_groupby(['Pclass','Sex'],'Survived', True)","c5b137eb":"fig = plt.figure(figsize = (10,6))\n\nax1 = fig.add_subplot(121)\nbivariate_countplot(train,'Pclass', 'Survived','Class', 'No. of passengers','Class by Survival',True, ['No','Yes'],ax1)\n\nax2 = fig.add_subplot(122)\nbivariate_countplot(train,'Pclass', 'Sex','Class', 'No. of passengers','Class by Sex',ax=ax2)","0f9b06bd":"g = sns.catplot(data = train, x = 'Pclass', hue = 'Survived',col = 'Sex', kind = 'count')","e3f295ea":"train['Title'] = train.Name.apply(lambda x : x.split(',')[1].split('.')[0].strip())\n\ndef title_type(row):\n    if row in ['Don', 'Mme',\n       'Major', 'Lady', 'Sir', 'Mlle', 'Col', 'Capt', 'the Countess',\n       'Jonkheer','Dona','Dr','Rev']:\n        # label as rare for titles that are low in counts\n        return 'Rare'\n    elif row == 'Miss':\n        return 'Ms'\n    else:\n        return row\n    \ntrain['Title'] = train.Title.apply(title_type)","37595fb3":"print('Proportion of Title :')\nprint(train.Title.value_counts(normalize = True).sort_values(ascending = False))","4c022e68":"fig,ax = plt.subplots()\nunivariate_countplot(train, 'Title','Title of Passengers', title = 'Title of Passenger',ax=ax )\nplt.xticks(rotation = 90)","6b872e6a":"fig =plt.figure(figsize = (20,15))\nax1 = fig.add_subplot(221)\ntrain.groupby('Title')['Pclass'].value_counts(normalize = True).unstack().plot(kind = 'bar', stacked = True,ax=ax1)\nax1.set(title = 'Proportion of Ticket class by Title')\n\nax2 = fig.add_subplot(222)\ntrain.groupby('Title')['Embarked'].value_counts(normalize = True).unstack().plot(kind = 'bar', stacked = True,ax=ax2)\nax2.set(title = 'Proportion of destination by Title')\n\nax3 = fig.add_subplot(223)\nsns.boxplot(data = train, x = 'Title', y = 'Fare',ax=ax3)\nax3.set(title = 'Fare of ticket by Title')\n\nax4 = fig.add_subplot(224)\ntrain.groupby('Title')['Survived'].value_counts(normalize = True).unstack().plot(kind = 'bar', stacked = True,ax=ax4)\nax4.set(title = 'Proportion of Survival by Title')\nplt.legend(title = 'Survived', labels = ['No','Yes'])\n\nplt.xticks(rotation = 90)","808785dc":"univariate_count('Sex',True)","8ab21551":"bivariate_groupby('Sex', 'Survived', True)","181dc22c":"fig = plt.figure(figsize = (10,5))\nax1 = fig.add_subplot(121)\nunivariate_countplot(train,'Sex',xlabel = 'Sex', title = 'Sex of Passengers', ax=ax1)\n\nax2 = fig.add_subplot(122)\nbivariate_countplot(train,'Sex','Survived','Sex', 'No.of passengers', 'Survival by Sex', True, ['No','Yes'],ax=ax2)","962ba49c":"univariate_count('Age',True)","a77feb5f":"train.Age.describe()","2941fcdc":"# explore min and max age\ntrain[(train.Age == train.Age.max()) | (train.Age == train.Age.min())]","357276d9":"fig = plt.figure(figsize = (15,8))\nax1 = fig.add_subplot(221)\nsns.distplot(train['Age'],ax=ax1)\nax1.set(title = 'Age\\'s PDF')\n\nax2 = fig.add_subplot(222)\nsns.kdeplot(train.Age, cumulative = True, shade = True,ax=ax2)\nax2.set(title = 'Age\\'s CDF',xlabel = 'Age')\nax2.axhline(0.5, color = 'b', label = 'median')\nplt.legend()\n\nax3 = fig.add_subplot(223)\nfrom scipy.stats import norm\nx = np.arange(0,80)\ny = norm(train.Age.mean(), train.Age.std()).pdf(x)\nax3.plot(x,y, label = 'theorical')\nsns.kdeplot(train['Age'],ax=ax3, label = 'sample')\nplt.legend(loc = 'upper left')\n\nax4 = fig.add_subplot(224)\ny = norm(train.Age.mean(), train.Age.std()).cdf(x)\nax4.plot(x,y,label = 'theorical')\nsns.kdeplot(train.Age, cumulative = True, shade = True,ax=ax4, label = 'sample')\nplt.legend()","d5518545":"# Children , adult and senior citizens\ndef age_diff(row):\n    if row < 18:\n        return 'Child'\n    elif (row < 60) & (row >=18):\n        return 'Adult'\n    else:\n        return 'Elderly'\n\ntrain['Age_cat'] = train.Age.apply(age_diff)","bcbac5c4":"univariate_count('Age_cat',True)","7cd13001":"fig,ax = plt.subplots()\nunivariate_countplot(train,'Age_cat','Age',title = 'Age category', ax=ax)","3c3080d6":"fig = plt.figure(figsize = (10,12))\nax1 = fig.add_subplot(221)\nsns.boxplot(data = train, x = 'Sex', y = 'Age',ax=ax1)\nsns.stripplot(data = train, x = 'Sex', y = 'Age',ax=ax1,size = 2)\nax1.set(title = 'Age by Sex')\n\nax2 = fig.add_subplot(222)\nsns.regplot(data = train, x = 'Age', y = 'Fare',ax=ax2,scatter_kws = {'alpha':0.5, 's' : 5})\nax2.set(title = 'Age vs Fare')\n\nax3 = fig.add_subplot(223)\nsns.boxplot(data = train, x = 'Survived', y = 'Age',ax=ax3)\nsns.stripplot(data = train, x = 'Survived', y = 'Age',ax=ax3,size = 2)\nax3.set(title = 'Age by Survived')\nplt.xticks([0,1], ['No','Yes'])\n\nax4 = fig.add_subplot(224)\nsns.boxplot(data = train, x = 'Pclass', y = 'Age',ax=ax4)\nsns.stripplot(data = train, x = 'Pclass', y = 'Age',ax=ax4,size = 2)\nax4.set(title = 'Age by class')\n","45d835b7":"sns.catplot(data = train, x = 'Sex', y = 'Age', col = 'Survived', kind = 'box')","4d210c4d":"univariate_count('SibSp', True)","df567950":"fig = plt.figure(figsize = (15,8))\nax1 = fig.add_subplot(131)\n\nsns.countplot(data = train, x = 'SibSp',ax=ax1)\nax1.set(title='Siblings and Spouse', ylabel = 'No. of passengers')\n\nax2 = fig.add_subplot(132)\ntrain.groupby('SibSp')['Survived'].value_counts(normalize = True).unstack().plot(kind = 'bar', stacked = True,ax=ax2)\nax2.set(title = 'Siblings and Spouse by Survival')\nplt.xticks(rotation = False)\nplt.legend(loc = 'upper right', title = 'Survived', labels = ['No','Yes'])\n\nax3 = fig.add_subplot(133)\ntrain.groupby('SibSp')['Sex'].value_counts(normalize = True).unstack().plot(kind = 'bar', stacked = True,ax=ax3)\nax3.set(title = 'Siblings and Spouse by Sex')\nplt.xticks(rotation = False)\nplt.legend(loc = 'upper right', title = 'Sex')","a15eeaba":"univariate_count('Parch', True)","107b2ac9":"fig = plt.figure(figsize = (15,8))\nax1 = fig.add_subplot(131)\n\nsns.countplot(data = train, x = 'Parch',ax=ax1)\nax1.set(title='Parent and Children', ylabel = 'No. of passengers')\n\nax2 = fig.add_subplot(132)\ntrain.groupby('Parch')['Survived'].value_counts(normalize = True).unstack().plot(kind = 'bar', stacked = True,ax=ax2)\nax2.set(title = 'Parent and Children by Survival')\nplt.xticks(rotation = False)\nplt.legend(loc = 'upper right', title = 'Survived', labels = ['No','Yes'])\n\nax3 = fig.add_subplot(133)\ntrain.groupby('Parch')['Sex'].value_counts(normalize = True).unstack().plot(kind = 'bar', stacked = True,ax=ax3)\nax3.set(title = 'Parent and Children by Sex')\nplt.xticks(rotation = False)\nplt.legend(loc = 'upper right', title = 'Sex')","7840e46f":"train['Family'] = train.SibSp + train.Parch + 1\ntrain['Family_type'] = pd.cut(train.Family, [0,1,4,7,11], labels = ['Single', 'Small', 'Medium', 'Large'])","b6a40557":"univariate_count('Family_type', True)","7a96bebe":"fig,ax = plt.subplots()\nunivariate_countplot(train,'Family_type','Family',title = 'Type of family size', ax=ax)","9eb7896e":"train.Fare.describe()","8b68b32d":"train[(train.Fare == train.Fare.min()) |(train.Fare == train.Fare.max())]","17ff5aa4":"fig = plt.figure(figsize = (15,8))\nax1 = fig.add_subplot(221)\nsns.distplot(train['Fare'],ax=ax1)\nax1.set(title = 'Fare\\'s PDF')\n\nax2 = fig.add_subplot(222)\nsns.kdeplot(train.Fare, cumulative = True, shade = True,ax=ax2)\nax2.set(title = 'Fare\\'s CDF',xlabel = 'Age')\nplt.legend()\n","d29320a9":"fig = plt.figure(figsize = (12,8))\nax1 = fig.add_subplot(121)\nsns.boxplot(data = train, x = 'Survived', y = 'Fare',ax=ax1)\nsns.stripplot(data = train, x = 'Survived', y = 'Fare',ax=ax1,size = 2)\nax1.set(title = 'Fare by Survived')\nplt.xticks([0,1], labels = ['No','Yes'])\n\nax2 = fig.add_subplot(122)\nsns.boxplot(data = train, x = 'Pclass', y = 'Fare',ax=ax2)\nsns.stripplot(data = train, x = 'Pclass', y = 'Fare',ax=ax2,size = 2)\nax2.set(title = 'Fare by Ticket Class')\n","a866fa6e":"fig,ax = plt.subplots()\n\nsns.kdeplot(train.query('Pclass == 1').Fare, shade = True,ax=ax, label = '1')\nsns.kdeplot(train.query('Pclass == 2').Fare, shade = True,ax=ax, label = '2')\nsns.kdeplot(train.query('Pclass == 3').Fare, shade = True,ax=ax, label = '3')\nax.set(xlabel = 'Fare', title = 'Fare by Ticket Class')\nplt.legend(title = 'Pclass', labels = [1,2,3])","8d0de284":"train['Cabin_floor'] = train.Cabin.apply(lambda x: list(str(x))[0])\ntrain['Cabin_floor'] = train.Cabin_floor.replace('n', np.nan)","bbd667d8":"univariate_count('Cabin_floor', True)","c66c58a2":"fig = plt.figure(figsize = (15,8))\n\nax1 = fig.add_subplot(131)\nsns.countplot(data = train, x = 'Cabin_floor', order = ['A','B','C','D','E','F','T'],ax=ax1)\nax1.set(title = 'Cabin floor', xlabel = 'Cabin floor', ylabel = 'no. of passengers')\n\nax2 = fig.add_subplot(132)\ntrain.groupby('Cabin_floor')['Survived'].value_counts(normalize = True).unstack().plot(kind = 'bar', stacked = True,ax=ax2)\nax2.set(title = 'Cabin floor by Survival')\nplt.xticks(rotation = False)\nplt.legend(loc = 'upper right', title = 'Survived')\n\nax3 = fig.add_subplot(133)\ntrain.groupby('Cabin_floor')['Pclass'].value_counts(normalize = True).unstack().plot(kind = 'bar', stacked = True,ax=ax3)\nax3.set(title = 'Cabin floor by Ticket class')\nplt.xticks(rotation = False)\nplt.legend(loc = 'upper right', title = 'Pclass')","f2aa7784":"univariate_count('Embarked', True)","0ac20c0a":"fig = plt.figure(figsize = (10,6))\n\nax1 = fig.add_subplot(121)\nsns.countplot(data = train, x = 'Embarked',ax=ax1)\nax1.set(xlabel = 'Embarked', ylabel = 'No. of passengers', title = 'Embarked')\n\nax2 = fig.add_subplot(122)\nsns.countplot(data = train, x = 'Embarked',hue = 'Survived',ax=ax2)\nax1.set(xlabel = 'Embarked', ylabel = 'No. of passengers', title = 'Embarked by Survived')\nplt.legend(title = 'Survived', labels = ['No', 'Yes'])","d586679e":"fig,ax = plt.subplots(figsize = (8,8))\nsns.heatmap(train.corr(),annot = True,ax=ax)","3c34f8b8":"import missingno as msno\nmsno.bar(train)","5732785c":"msno.matrix(train)","3e8a7fb4":"train.Age.fillna(train.Age.median(), inplace = True)\ntrain.Cabin_floor.fillna(train.Cabin_floor.mode().values[0], inplace= True)\n\ntrain.drop(['PassengerId', 'Name', 'Ticket','Cabin'], axis = 1, inplace = True)\n\ntrain.isna().sum()","79b082f1":"train.info()","5340db75":"fig = plt.figure(figsize = (10,8))\nax1 = fig.add_subplot(121)\nsns.boxplot(data = train, y = 'Age',ax=ax1)\nax1.set( title = 'Age')\n\nax2 = fig.add_subplot(122)\nsns.boxplot(data = train, y = 'Fare',ax=ax2)\nax2.set( title = 'Fare')","15d159c4":"# Age, Fare\n# use IQR approach\nQ1_age = train.Age.quantile(0.25)\nQ3_age = train.Age.quantile(0.75)\nIQR_age = Q3_age - Q1_age\n\ntrain = train[~((train.Age < (Q1_age - 1.5 * IQR_age)) | (train.Age > (Q3_age + 1.5 * IQR_age )))]\n\nQ1_fare = train.Fare.quantile(0.25)\nQ3_fare= train.Fare.quantile(0.75)\nIQR_fare = Q3_fare - Q1_fare\n\ntrain = train[~((train.Fare < (Q1_fare - 1.5 * IQR_fare)) | (train.Fare > (Q3_fare + 1.5 * IQR_fare )))]","25cb541c":"fig = plt.figure(figsize = (10,8))\nax1 = fig.add_subplot(121)\nsns.boxplot(data = train, y = 'Age',ax=ax1)\nax1.set( title = 'Age')\n\nax2 = fig.add_subplot(122)\nsns.boxplot(data = train, y = 'Fare',ax=ax2)\nax2.set( title = 'Fare')","1e27ca75":"from sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\n\nfor col in train.select_dtypes('object').columns:\n    train[col] = encoder.fit_transform(train[col])","5c5eaa3e":"y = train['Survived']\nX = train.drop(['Survived','SibSp','Parch','Age','Family','Cabin_floor'],axis = 1)\n\nprint('Feature\\'s shape : {}'.format(X.shape))\nprint('Target\\'s shape : {}'.format(y.shape))","8cbb60f7":"X.Family_type = encoder.fit_transform(X.Family_type)","30796418":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import plot_tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import VotingClassifier\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nimport xgboost as xgb\n\nseed = 225\n\nmodel_result = dict()","818130d8":"steps = [('Scaler', StandardScaler()), ('KNN', KNN())]\npipeline = Pipeline(steps)\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = seed)\n\nparams = {'KNN__n_neighbors': np.arange(1,20)}\nknn_cv = GridSearchCV(pipeline,params, cv = 5, verbose = 1, n_jobs = -1 )\n\nknn_cv.fit(X_train,y_train)\n\nprint('Best params : {}'.format(knn_cv.best_params_))\nprint('Best score : {:.2f}'.format(knn_cv.best_score_))\n\ny_pred_train = knn_cv.predict(X_train)\ny_pred_test = knn_cv.predict(X_test)\n\nprint('KNN\\'s train score : {:.3f}'.format(accuracy_score(y_train,y_pred_train)))\nprint('KNN\\'s test score : {:.3f}'.format(accuracy_score(y_test,y_pred_test)))\nprint(confusion_matrix(y_test, y_pred_test))\nprint(classification_report(y_test,y_pred_test))\nprint('KNN\\'s roc score : {:.3f}'.format(roc_auc_score(y_test,y_pred_test)))\nConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_test),[1,0]).plot()\n\nmodel_result['KNN'] = accuracy_score(y_test,y_pred_test)","44b02aca":"steps = [('Scaler', StandardScaler()), ('LR', LogisticRegression(random_state = seed))]\npipeline = Pipeline(steps)\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = seed)\n\nparams = {'LR__C': [0.001,0.01,0.1,1,10,100,1000]}\n\nlr_cv = GridSearchCV(pipeline,params, cv = 5, verbose = 1, n_jobs = -1 )\n\nlr_cv.fit(X_train,y_train)\n\nprint('Best params : {}'.format(lr_cv.best_params_))\nprint('Best score : {:.2f}'.format(lr_cv.best_score_))\n\ny_pred_train = lr_cv.predict(X_train)\ny_pred_test = lr_cv.predict(X_test)\n\nprint('LR\\'s train score : {:.3f}'.format(accuracy_score(y_train,y_pred_train)))\nprint('LR\\'s test score : {:.3f}'.format(accuracy_score(y_test,y_pred_test)))\nprint(confusion_matrix(y_test, y_pred_test))\nprint(classification_report(y_test,y_pred_test))\nprint('LR\\'s roc score : {:.3f}'.format(roc_auc_score(y_test,y_pred_test)))\nConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_test),[1,0]).plot()\n\nmodel_result['LR'] = accuracy_score(y_test,y_pred_test)","2496bc88":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = seed)\n\nrf = RandomForestClassifier(random_state = seed)\nparams = {'n_estimators' : [200,300,400], 'max_depth':[10,12,14], 'max_features':['auto','sqrt','log2']}\n\nrf_cv = GridSearchCV(rf,params, cv = 3, verbose = 1, n_jobs = -1 )\n\nrf_cv.fit(X_train,y_train)\n\nprint('Best params : {}'.format(rf_cv.best_params_))\nprint('Best score : {:.2f}'.format(rf_cv.best_score_))\n\ny_pred_train = rf_cv.predict(X_train)\ny_pred_test = rf_cv.predict(X_test)\n\nprint('RF\\'s train score : {:.3f}'.format(accuracy_score(y_train,y_pred_train)))\nprint('RF\\'s test score : {:.3f}'.format(accuracy_score(y_test,y_pred_test)))\nprint(confusion_matrix(y_test, y_pred_test))\nprint(classification_report(y_test,y_pred_test))\nprint('RF\\'s roc score : {:.3f}'.format(roc_auc_score(y_test,y_pred_test)))\nConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_test),[1,0]).plot()\n\nmodel_result['RF'] = accuracy_score(y_test,y_pred_test)","6f285946":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = seed)\n\nsteps = [('Scaler', StandardScaler()), ('SVC', SVC(random_state = seed))]\npipeline = Pipeline(steps)\n\n\nparameters = {'SVC__C':[0.1, 1, 10,100, 1000], 'SVC__gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1,1,10]}\nsearcher = GridSearchCV(pipeline, parameters, cv = 5, n_jobs = -1, verbose = 1)\n\nsearcher.fit(X_train,y_train)\n\nprint('Best params : {}'.format(searcher.best_params_))\nprint('Best score : {:.2f}'.format(searcher.best_score_))\n\ny_pred_train = searcher.predict(X_train)\ny_pred_test = searcher.predict(X_test)\n\nprint('SVC\\'s train score : {:.3f}'.format(accuracy_score(y_train,y_pred_train)))\nprint('SVC\\'s test score : {:.3f}'.format(accuracy_score(y_test,y_pred_test)))\nprint(confusion_matrix(y_test, y_pred_test))\nprint(classification_report(y_test,y_pred_test))\nprint('SVC\\'s roc score : {:.3f}'.format(roc_auc_score(y_test,y_pred_test)))\nConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_test),[1,0]).plot()\n\nmodel_result['SVC'] = accuracy_score(y_test,y_pred_test)","5518c92b":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = seed)\n\nsteps = [('Scaler', StandardScaler()), ('LinearSVC', LinearSVC(random_state = seed))]\npipeline = Pipeline(steps)\n\n\nparameters = {'LinearSVC__C':[0.1, 1, 10,100, 1000], 'LinearSVC__penalty':['l1','l2'], 'LinearSVC__loss' : ['hinge', 'squared_hinge']}\nsearcher = GridSearchCV(pipeline, parameters, cv = 5, n_jobs = -1, verbose = 1)\n\nsearcher.fit(X_train,y_train)\n\nprint('Best params : {}'.format(searcher.best_params_))\nprint('Best score : {:.2f}'.format(searcher.best_score_))\n\ny_pred_train = searcher.predict(X_train)\ny_pred_test = searcher.predict(X_test)\n\nprint('LinearSVC\\'s train score : {:.3f}'.format(accuracy_score(y_train,y_pred_train)))\nprint('LinearSVC\\'s test score : {:.3f}'.format(accuracy_score(y_test,y_pred_test)))\nprint(confusion_matrix(y_test, y_pred_test))\nprint(classification_report(y_test,y_pred_test))\nprint('LinearSVC\\'s roc score : {:.3f}'.format(roc_auc_score(y_test,y_pred_test)))\nConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_test),[1,0]).plot()\n\nmodel_result['LinearSVC'] = accuracy_score(y_test,y_pred_test)","6446412b":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = seed)\n\nsteps = [('Scaler', StandardScaler()), ('SGD', SGDClassifier(random_state = seed))]\npipeline = Pipeline(steps)\n\n\nparameters = {'SGD__alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100], 'SGD__loss':['hinge', 'log'], 'SGD__penalty':['l1', 'l2']}\nsearcher = GridSearchCV(pipeline, parameters, cv = 5, n_jobs = -1, verbose = 1)\n\nsearcher.fit(X_train,y_train)\n\nprint('Best params : {}'.format(searcher.best_params_))\nprint('Best score : {:.2f}'.format(searcher.best_score_))\n\ny_pred_train = searcher.predict(X_train)\ny_pred_test = searcher.predict(X_test)\n\nprint('SGD\\'s train score : {:.3f}'.format(accuracy_score(y_train,y_pred_train)))\nprint('SGD\\'s test score : {:.3f}'.format(accuracy_score(y_test,y_pred_test)))\nprint(confusion_matrix(y_test, y_pred_test))\nprint(classification_report(y_test,y_pred_test))\nprint('SGD\\'s roc score : {:.3f}'.format(roc_auc_score(y_test,y_pred_test)))\nConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_test),[1,0]).plot()\n\nmodel_result['SGD'] = accuracy_score(y_test,y_pred_test)","3552ea2c":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = seed)\n\ndt = DecisionTreeClassifier(max_depth = 4,random_state = seed)\nada = AdaBoostClassifier(dt,random_state = seed)\n\nparams = {'n_estimators' : [200,300,400], 'learning_rate' : [0.1,0.2,0.4,1]}\n\nsearcher = GridSearchCV(ada,params, cv = 5, verbose = 1, n_jobs = -1 )\n\nsearcher.fit(X_train,y_train)\n\nprint('Best params : {}'.format(searcher.best_params_))\nprint('Best score : {:.2f}'.format(searcher.best_score_))\ny_pred_train = searcher.predict(X_train)\ny_pred_test = searcher.predict(X_test)\n\nprint('Adaboost\\'s train score : {:.3f}'.format(accuracy_score(y_train,y_pred_train)))\nprint('Adaboost\\'s test score : {:.3f}'.format(accuracy_score(y_test,y_pred_test)))\nprint(confusion_matrix(y_test, y_pred_test))\nprint(classification_report(y_test,y_pred_test))\nprint('Adaboost\\'s roc score : {:.3f}'.format(roc_auc_score(y_test,y_pred_test)))\nConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_test),[1,0]).plot()\n\nmodel_result['Adaboost'] = accuracy_score(y_test,y_pred_test)","b3f00095":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = seed)\n\ngb = GradientBoostingClassifier(random_state = seed, subsample = 0.8)\n\nparams = {'learning_rate' : [0.1,0.2], 'n_estimators' : [200,300,400],'max_depth' : [2,3,4,6]}\n\nsearcher = GridSearchCV(gb,params, cv = 5, verbose = 1, n_jobs = -1 )\n\nsearcher.fit(X_train,y_train)\n\nprint('Best params : {}'.format(searcher.best_params_))\nprint('Best score : {:.2f}'.format(searcher.best_score_))\n\ny_pred_train = searcher.predict(X_train)\ny_pred_test = searcher.predict(X_test)\n\nprint('Gradient Boosting\\'s train score : {:.3f}'.format(accuracy_score(y_train,y_pred_train)))\nprint('Gradient Boosting\\'s test score : {:.3f}'.format(accuracy_score(y_test,y_pred_test)))\nprint(confusion_matrix(y_test, y_pred_test))\nprint(classification_report(y_test,y_pred_test))\nprint('Gradient Boosting\\'s roc score : {:.3f}'.format(roc_auc_score(y_test,y_pred_test)))\nConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_test),[1,0]).plot()\n\nmodel_result['Gradient Boosting'] = accuracy_score(y_test,y_pred_test)","bb73435e":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = seed)\n\nsteps = [('Scaler', StandardScaler()), ('XG', xgb.XGBClassifier(random_state = seed))]\npipeline = Pipeline(steps)\n\nparams = {'XG__learning_rate' : [0.1,0.2,0.4], 'XG__gamma' : [0.0001,0.001,0.01,1,10],'XG__max_depth' : [2,3,4,6]}\n\nsearcher = GridSearchCV(pipeline,params, cv = 5, verbose = 1, n_jobs = -1 )\n\nsearcher.fit(X_train,y_train)\n\nprint('Best params : {}'.format(searcher.best_params_))\nprint('Best score : {:.2f}'.format(searcher.best_score_))\n\ny_pred_train = searcher.predict(X_train)\ny_pred_test = searcher.predict(X_test)\n\nprint('XGBoost\\'s train score : {:.3f}'.format(accuracy_score(y_train,y_pred_train)))\nprint('XGBoost\\'s test score : {:.3f}'.format(accuracy_score(y_test,y_pred_test)))\nprint(confusion_matrix(y_test, y_pred_test))\nprint(classification_report(y_test,y_pred_test))\nprint('XGBoost\\'s roc score : {:.3f}'.format(roc_auc_score(y_test,y_pred_test)))\nConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_test),[1,0]).plot()\n\nmodel_result['XGBoost'] = accuracy_score(y_test,y_pred_test)","80b8a503":"print(model_result)","d4f17b00":"# Voting Classifier\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = seed)\n\nlr_pipeline = Pipeline([('scale', StandardScaler()), ('LR',LogisticRegression(random_state=seed, C = 1))])\n\nsgd = SGDClassifier(alpha = 0.01, loss = 'hinge', penalty = 'l2', random_state = seed)\n\ngb = GradientBoostingClassifier(learning_rate = 0.1, max_depth = 2, n_estimators = 200, random_state = seed, subsample = 0.8)\n\nxg = xgb.XGBClassifier(random_state = seed, gamma = 0.01, learning_rate = 0.2, max_depth = 3)\n\nclassifiers = [('Logistic Regression', lr_pipeline), ('SGD', sgd), ('Gradient Boosting', gb), ('XGBoost',xg)]\n\nfor clf_name, clf in classifiers:    \n \n    # Fit clf to the training set\n    clf.fit(X_train, y_train)    \n   \n    # Predict y_pred\n    y_pred = clf.predict(X_test)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_test, y_pred) \n   \n    # Evaluate clf's accuracy on the test set\n    print('{:s} : {:.3f}'.format(clf_name, accuracy))\n\n# Import VotingClassifier from sklearn.ensemble\nfrom sklearn.ensemble import VotingClassifier\n\n# Instantiate a VotingClassifier vc\nvc = VotingClassifier(estimators=classifiers)     \n\n# Fit vc to the training set\nvc.fit(X_train, y_train)   \n\n# Evaluate the test set predictions\ny_pred = vc.predict(X_test)\n\n# Calculate accuracy score\naccuracy = accuracy_score(y_test, y_pred)\nprint('Voting Classifier: {:.3f}'.format(accuracy))","aa91d051":"Most of the passengers will stop at Southampton (72%)","3d8a7a69":"**KNN**","968d81eb":" **Pclass**\n \n No. of passengers for each ticket class","c00652b5":"* Most of the passengers did not have any siblings and spouse onboard in the ship (68%)\n* the 2nd is with 1 spouse or siblings with them onboard (23%)","57615973":"* Columns ( age,fare,cabin, embarked ) have missing values.\n* Cabin has the most missing values\n","2e4a3f96":"# Import libraries","324f1081":"# Summary :","8601fd98":"**Cabin**","49e0777b":"We can see that the for all 3 class the male passengers is relatively higher.","5ad7c96b":"* 76% of the passengers did not have parents or chilren on board with them\n* 13% of the passengers have 1 parents or children on board with them","89211912":"# Impute missing values","b098abe7":"**Main problem :**\nWho will be more likely to survived in the tragedy?\n\n**Assumption :**\n* Passengers with higher age \n* Children\n* Female\n* Female that are elderly\n* Passengers with royal title\n* Class 1\n* Higher fare\n* Cabin that are in lower deck\n* Fare that is more than median","c43cb153":"**Color palette**","9315f1bb":"**Summary**\n* Survived which is the target feature not in the test data\n* Passenger ID and Ticket is a column which will not assits in our model and can be drop.\n* We can extract the title of the passengers from the Name column","b5c19df9":"**Sex**","fa3848c7":"**Name**","438f63e0":"We can see that most of passengers are Mr and very less passengers has a title 'Rare'","65192d66":"**Logistic regression**","1b45a460":"* We can see that most of the fare are around price 0 and 100, which leads to left-skewed.\n* It seems that the distribution is came from a log-normal distributions\n* Passengers with higher fare are most likely to be survived\n* The fare of ticket class 1 are relatively higher\n* for fare of class 1 it has wide range from 0 to 500\n* for fare of class 3, the fare tend to focus between range 0 to 100.","ec8ca364":"# Description on this notebook\n\n* This notebook is mainly for exploring the datasets and applying various algorithmn to find which of those predicted most accurate.\n* The actual modelling notebook is : https:\/\/www.kaggle.com\/samuelwy\/machine-learning\/notebook\n* This notebook analyse every features ( univariate, bivariate and multivariate analysis )","3d2c637e":"Extract the title from the name column","e98ba259":"* C's cabin has the most passengers (29%)\n* Cabin A,B,C are for passengers class 1\n* Passengers from Cabin B,D,E are more likely to survived","b0482fc2":"**Age and Fare's boxplot before outlier treatment**","a2024980":"* Survival rate is 38.3% ( around 350 passengers )\n* Most of the people did not survive in the tragedy.","cce93eb9":"We can see that most of the passengers are adult and children is the lowest in count.","52edbca8":"**Multivariate**","248f50d6":"**Age**","87535710":"# EDA","66e08085":"# Read training data and testing data","7b4540bb":"**Embarked**\n*  C = Cherbourg\n*  Q = Queenstown\n*  S = Southampton","59d86d0b":"**Drop columns not used in modelling**\n* SibSp and Parch and Family_type because repetitive due to exists of family\n* Age because exits of Age_cat\n* Cabin floor and because less helpful","af882f5e":"* Pclass 1 has the highest survival rate which is 63%\n* Pclass 3 has the highest death rate which is 75.7% \n* We can see that the assumption of ticket class 1 has the highest survival rate compare to class 2 and 3","1bc048ed":"* Most of the passengers are Adult (65%) and lesser is children (13%)\n* most of the passenger is around age 18 to 30+\n* median age is 28 and mean is around age 30 \n* max age is 80 and min age is 0.42\n* for the age 0.42 passenger, we can conclude that he is a child and not entry error because it has 1 parents onboard with him and title Master which indicates he was an underage male.\n* for age 80 passenger, we can believe that it is reasonable because age 80 is consider veteren and is possible to on board on the ship.\n\nIs Age comes from a normal distribution?\n* according to the pdf of Age, we hard to compare the age distribution to a theorical normal distribution \n* but if we looks at its cdf, we can see that both distribution is smiliar and we can temproary conclude that age cames from a normal distirbution but need to do further inferential analysis\n\n* The age of male are significant higher than female\n* the age of passengers in class 3 are lower than others","391ccbdb":"* The top 3 title of passengers are Mr, Ms, Master where Mr has around 72%.\n* Most of the title Rare are in class 1 \n* Most of passengers that have a known title are in class 1\n* Most of the master title's passenger are in class 3 ( > 60% )\n* Most of the passenger are going to embarked at Southampton\n* Passengers with title Mrs and Ms are likely to survived","3e16017e":"There are fare with values 0 and 512","8a67a575":"**Features details**\n* survival : Survival\t0 = No, 1 = Yes\n* pclass : Ticket class\t1 = 1st, 2 = 2nd, 3 = 3rd\n* sex\t: Sex\t\n* Age\t: Age in years\t\n* sibsp\t: # of siblings \/ spouses aboard the Titanic\t\n* parch\t: # of parents \/ children aboard the Titanic\t\n* ticket\t: Ticket number\t\n* fare\t: Passenger fare\t\n* cabin\t: Cabin number\t\n* embarked\t: Port of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton\n\n**Variable Notes**\n* pclass: A proxy for socio-economic status (SES)\n* 1st = Upper\n* 2nd = Middle\n* 3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way\n* Sibling = brother, sister, stepbrother, stepsister\n* Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\n* Parent = mother, father\n* Child = daughter, son, stepdaughter, stepson\n* Some children travelled only with a nanny, therefore parch=0 for them.","b9c2509c":"# EDA","cb01572b":"# Check for missing values","fa3ede20":"\t\u2022\tMost of the passengers did not survive in the tragedy\n\t\u2022\tSurvival rate is 38.3% \n\t\u2022\tMost of the passengers holding ticket class 3 ( >= 500 passengers )\n\t\u2022\tPassenger with ticket class 1 has the highest survival rate ( 63%) while ticket class 3 has the highest death rate ( 75.8%)\n\t\u2022\tFemale and male in ticket class 3 has the highest survival rate ( 96.8% , 63%)\n\t\u2022\tMost of the passenger are with title Mr or Ms\n\t\u2022\tMost of the passengers are going to embarked at Southampton\n\t\u2022\tPassengers with special title are likely to survived\n\t\u2022\tMost of the passengers are Male \n\t\u2022\tFemale are more likely to survived ( 74.2% )\n\t\u2022\tAge are most likely came from normal distribution\n\t\u2022\tMost of the passengers are Adult \n\t\u2022\tThe age of male are higher than female\n\t\u2022\tMost of the passengers did not have siblings and spouse onboard\n\t\u2022\tMost of the passengers did not have parent and children onboard\n\t\u2022\tFare are most likey came from a lognormal distribution\n\t\u2022\tPassengers with higher fare are most likely to be survived\n\t\u2022\tMost of the passengers are in Cabin C ","09f73f61":"# Explore missing data","ef97a4a4":"# Description of the features","d6877872":"# Modelling","a82f24f4":"**Fare**","d0efcf9f":"**Family**","a67a91e0":"Master is a title for an underage male","144e27ad":"**Survival ( Target feature )**\n\nSurvival rate of the overall","5c0e23f1":"**SVC**","93cd0b88":"**Siblings and Spouse**","e9d51068":"**Age and Fare's boxplot after outlier treatment**","837728d6":"**Test data info**","f0ac343d":"# Train & test data exploring","9d8579bc":"* All Ticket class for male has a high death count compare to survived\n* All Ticket class for female has a very high survival count with almost zero death count for class 1 2\n\n* we can say that regardless of the class, the death rate of male is higher than female","265ab9c8":"# Hypothesis to be verified\n\n**Main Question**\nWho will survived in the Titanic tragedy ?\n\n**Question**\n1. What is the survival rate of the tragedy?\n2. Who is more likely to survived?\n3. What will affect the survival rate?\n\n**Assumption**\n* Female is more likely to survived\n* Children and Senior cititzen are likely to survived\n* Passenger from ticket class 1 are more likely to survived\n* Passenger with Royal or noble title are more likely to survived","98c49afd":"# Outlier","74381c1b":"Most of the passengers is single onboard without any family member","b6c221ac":"We can conclude that the hypothesis male are less likely to survive and female or more lightly to survived","f7f737b3":"* We can see that for fare 512.3292, all of the class are 1st. \n* for the fare 0, the class occur througout each class, therefore maybe the ticket is FOC, thats why the fare is 0.","7f7d8c96":"# Feature engineer","a389b34e":"* Ticket class 3 has the highest no. of passengers with almost 500 of it and a difference of 300 compare to 1, 2.","59ad6622":"**Random Forest**","3034851a":"Testing normality","1281c400":"# Functions that are mostly used","318abbc5":"**Train dataset info**","62856c65":"**Parch**"}}