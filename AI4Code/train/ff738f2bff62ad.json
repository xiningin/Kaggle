{"cell_type":{"6ea802ea":"code","838e94bb":"code","d924299a":"code","c264c9e8":"code","43fe0854":"code","3aba9cff":"code","16eef073":"code","e1bd3d73":"code","98df2e95":"code","c0530dbd":"code","17af9180":"code","57b43c2c":"code","25fbc14f":"code","b43053fa":"code","6b91b325":"code","bb367d2f":"code","a34c5e55":"code","0624a109":"code","df4a61e1":"code","e1905e5e":"code","335254c8":"code","8bd1a101":"code","0abf211a":"code","7ce54ef9":"code","19969ad6":"code","9e138a1a":"code","a5cb3f93":"code","82c15267":"code","4fc128ad":"code","1d6bb705":"code","2edf9bd4":"code","d5db47ef":"code","614b5547":"code","353ce21c":"code","273bd3a1":"code","f0044dd9":"code","479586cf":"code","2a53773c":"code","b42d574e":"code","7d7260a7":"code","36c2ff8b":"code","1e40f6c6":"code","0a50e1e6":"code","b81dc3ad":"code","76b9a16a":"code","272ceab4":"code","889c3a9f":"code","885fd1cb":"code","23ad8fd2":"code","b41ced76":"code","dc31d76d":"code","bbe97748":"code","3e5c5479":"code","5b5b14dd":"code","38199633":"code","a1cdffdd":"code","59380625":"code","7a6100f1":"code","856fa8d8":"code","e4adfd45":"markdown","fb668643":"markdown","477dbc55":"markdown","9ff67e20":"markdown","fe119de5":"markdown","0162c8c1":"markdown","d7c9f880":"markdown","0e4c0bf2":"markdown","a71190f8":"markdown","4e4cfa92":"markdown","a78843e4":"markdown","fe1dddf0":"markdown","ade953ac":"markdown","b121ac44":"markdown","3dd0abd5":"markdown","ec7ec9a4":"markdown","40c18342":"markdown","31fe7ea7":"markdown","2e1e8478":"markdown"},"source":{"6ea802ea":"# Import all the tools we need\n\n# Regular EDA(exploratory data analysis) and plotting libraries\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# we want our plots to apper inside the notebook\n%matplotlib inline \n\n#Models from Scikit-Learn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Model Ecaluation\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, f1_score\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import plot_confusion_matrix","838e94bb":"df = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\ndf.shape","d924299a":"df.head()","c264c9e8":"df[\"target\"].value_counts()","43fe0854":"df[\"target\"].value_counts().plot(kind=\"bar\",color=[\"salmon\", \"green\"])","3aba9cff":"df.info()","16eef073":"# is there any null value\ndf.isna().sum()","e1bd3d73":"df.describe()","98df2e95":"df.sex.value_counts()","c0530dbd":"pd.crosstab(df.target, df.sex)","17af9180":"# Create a plot of crosstab\npd.crosstab(df.target, df.sex).plot(kind ='bar', color=[\"salmon\", \"green\"],figsize = (10,6))\nplt.title(\"Heart Disease Frequency for Sex\")\nplt.xlabel(\"0 = no disease, 1 = disease\")\nplt.legend([\"Female\", \"Male\"])\nplt.xticks(rotation =0);","57b43c2c":"# Create another figure\nplt.figure(figsize=(10,6))\n\n# Scatter with postivie examples\nplt.scatter(df.age[df.target ==1],\n           df.thalach[df.target ==1],\n           c =\"salmon\")\n\n# Scatter with negative examples\nplt.scatter(df.age[df.target ==0],\n           df.thalach[df.target ==0],\n           c = \"lightgreen\")\n\nplt.title(\"Heart Disease in function of age max heart rate\")\nplt.ylabel(\"Max Heart Rate\")\nplt.legend([\"Disease\", \"No Disease\"])\n","25fbc14f":"# check the distribution of the age column with a histogram\ndf.age.plot.hist()","b43053fa":"pd.crosstab(df.cp, df.target)","6b91b325":"# Make the crosstab more visual\npd.crosstab(df.cp, df.target).plot(kind=\"bar\",\n                                  figsize =(10,6),\n                                  color =[\"salmon\", \"green\"])\nplt.title(\"Heart Disease Frequency Per Chest Pain Type\")\nplt.xlabel(\"Chest Pain Type\")\nplt.ylabel(\"Amount\")\nplt.legend([\"No Disease\", \"Disease\"])\nplt.xticks(rotation=0);","bb367d2f":"# Make a correlation matrix\ndf.corr()","a34c5e55":"#  make our correlation matrix a little prettier\ncorr_matrix = df.corr()\nfig, ax = plt.subplots(figsize =(15,10))\nax = sns.heatmap(corr_matrix,\n                annot = True,\n                linewidths =0.5,\n                fmt =\".2f\",\n                cmap = \"YlGnBu\")\n\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top -0.5); ","0624a109":"df","df4a61e1":"df.head()","e1905e5e":"# Split data into X and Y\nX = df.drop(\"target\",axis=1)\nY = df[\"target\"]","335254c8":"# Split data into train and test sets\nnp.random.seed(42)\n\n# Split into train and test set \nx_train, x_test,y_train,y_test = train_test_split(X, Y, test_size = 0.2)","8bd1a101":" models = {\"Logistic Regression\" : LogisticRegression(),\n          \"KNM\": KNeighborsClassifier(),\n          \"Random Forest\": RandomForestClassifier( )}\n    \n# Create a function to fit and score models\ndef fit_and_score(models, x_train, x_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given machine learningg models.\n    models : a dict of different scikit-learn machine learning models\n    x_train : training data\n    x_test : testing data\n    y_train : training labels\n    y_test : test labels\n    \"\"\"\n    \n    # Set random seed\n    np.random.seed(42)\n    # Make a dictionary to keep model scores\n    model_scores = {}\n    # Loop through models\n    for name, model in models.items():\n        # Fit the model to the data\n        model.fit(x_train, y_train)\n        # Evaluate the model and append its score to model_scores\n        model_scores[name] = model.score(x_test, y_test)\n    return model_scores\n    ","0abf211a":"model_scores = fit_and_score(models=models,\n                             x_train= x_train,\n                             x_test = x_test,\n                             y_train = y_train,\n                             y_test = y_test\n                            )\nmodel_scores","7ce54ef9":"model_compare = pd.DataFrame(model_scores,index=[\"accuracy\"])\nmodel_compare.T.plot.bar();","19969ad6":"# Let's tune KNN\ntrain_scores = []\ntest_scores = []\n\n# Create a list of different values for n_neighbors\nneighbors = range(1,21);\n\n# Setup KNN instance\nKnn = KNeighborsClassifier()\n\n# Loop through different n_neighbors\nfor i in neighbors:\n    Knn.set_params(n_neighbors =i)\n    \n    #fit the algorithm\n    Knn.fit(x_train, y_train)\n    \n    #update the rraining scores list\n    train_scores.append(Knn.score(x_train,y_train))\n    \n    #update the test scores list\n    test_scores.append(Knn.score(x_test, y_test))","9e138a1a":"train_scores","a5cb3f93":"test_scores","82c15267":"plt.plot(neighbors, train_scores, label = \"Train score\")\nplt.plot(neighbors, test_scores, label = \"Test score\")\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\nplt.xticks(np.arange(1,21,1))\n\nprint(f\"Maximum KNN score on the test data :{max(test_scores)*100 :.2f}%\")","4fc128ad":"# Create a hyperparameter grid for LogisticRegression\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n               \"solver\": [\"liblinear\"]}\n\n# Create a hyperparameter grid for RandomForestClassifier\nrf_grid = {\"n_estimators\": np.arange(10, 1000,50),\n          \"max_depth\": [None, 3, 5, 10],\n          \"min_samples_split\": np.arange(2,20,2),\n          \"min_samples_leaf\": np.arange(1,20,2)}","1d6bb705":"# Tune LogisticRegression\n\nnp.random.seed(42)\n\n# Setup random hyperparameter search for LogisticRegression\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                               param_distributions=log_reg_grid,\n                               cv =5,\n                                n_iter=20,\n                               verbose = True)\n\n# Fit ramdom hyperparameter search model for LogisticRegression\nrs_log_reg.fit(x_train, y_train)","2edf9bd4":"rs_log_reg.best_params_","d5db47ef":"rs_log_reg.score(x_test, y_test)","614b5547":"# Setup random seed\nnp.random.seed(42)\n\n#Setup random hyperparameter search for RandomForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                          param_distributions=rf_grid,\n                          cv =5,\n                           n_iter=20,\n                          verbose =True)\n\n# Fit random hyperparameter serch model for RandomForestClassifier()\nrs_rf.fit(x_train, y_train)","353ce21c":"# Find the best hyperparameters\nrs_rf.best_params_","273bd3a1":" # Evaluate the randomized search RandomFores\n rs_rf.score(x_test, y_test)","f0044dd9":"# Diffrent hyperparameters for out LogisticRegression model\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 30),\n               \"solver\":[\"liblinear\"]}\n\n# Setup grid hyperparameter search for LogisticRegression\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                         param_grid= log_reg_grid,\n                         cv =5,\n                         verbose =True)\n\n# Fit grid hyperparameter search model\ngs_log_reg.fit(x_train, y_train);\n\n","479586cf":"gs_log_reg.best_params_","2a53773c":"gs_log_reg.score(x_test, y_test)","b42d574e":"# Make predictions with tuned model\ny_preds = gs_log_reg.predict(x_test)","7d7260a7":"y_preds","36c2ff8b":"y_test","1e40f6c6":"# Plot ROC curve and calculate and calculate AUC metric\nplot_roc_curve(gs_log_reg, x_test, y_test)","0a50e1e6":"# Confusion matrix\nprint(confusion_matrix(y_test, y_preds))","b81dc3ad":"sns.set(font_scale =1.5)\n\ndef plot_conf_mat(y_test, y_preds):\n    \"\"\"\n    Plots a nice looking confusion matrix using seaborn's heatmap()\n    \"\"\"\n    \n    fig, ax = plt.subplots(figsize = (3,3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                    annot = True,\n                    cbar=False)\n    plt.xlabel(\"True label\")\n    plt.ylabel(\"Predicted label\")\n    \nplot_conf_mat(y_test, y_preds)","76b9a16a":"plot_confusion_matrix(gs_log_reg,x_test,y_test,cmap=\"nipy_spectral\")\nsns.set(font_scale =2.5)\n","272ceab4":"print(classification_report(y_test,y_preds))","889c3a9f":"# check best hypeerparameters\ngs_log_reg.best_params_","885fd1cb":"# Create a new classifier with best parameters\nclf = LogisticRegression(C =0.20433597178569418,\n                        solver=\"liblinear\")","23ad8fd2":"# Cross-validatied accuracy\ncv_acc = cross_val_score(clf,X, Y ,cv =5, scoring =\"accuracy\")\ncv_acc","b41ced76":"cv_acc = np.mean(cv_acc)\ncv_acc","dc31d76d":"# Cross-validatied precision\n\ncv_precision = cross_val_score(clf,X, Y ,cv =5, scoring =\"precision\")\ncv_precision = np.mean(cv_precision)\ncv_precision","bbe97748":"# Cross-validation recall\n\ncv_recall = cross_val_score(clf,X, Y ,cv =5, scoring =\"recall\")\ncv_recall = np.mean(cv_recall)\ncv_recall","3e5c5479":"# f-1 score\n\ncv_f1 = cross_val_score(clf,X, Y ,cv =5, scoring =\"f1\")\ncv_f1 = np.mean(cv_f1)\ncv_f1","5b5b14dd":"# Visualize cross-valdidated metrics\ncv_metrics = pd.DataFrame({\"Accuracy\": cv_acc,\n                          \"Precision\":cv_precision,\n                          \"recall\": cv_recall,\n                          \"f1\": cv_f1},\n                         index=[0])\ncv_metrics\ncv_metrics.T.plot.bar(title = \"Cross-validated classification metrics\", legend=False)","38199633":"# Fit an instance of LogisticRegression\n\nclf = LogisticRegression(C = 0.20433597178569418,\n                        solver=\"liblinear\")\n\nclf.fit(x_train,y_train)","a1cdffdd":"# check coef_\nclf.coef_","59380625":"df.head()","7a6100f1":"feature_dict = dict(zip(df.columns, list(clf.coef_[0])))\nfeature_dict","856fa8d8":"# Visualize feature importance\nfeature_df = pd.DataFrame(feature_dict, index=[0])\nfeature_df.T.plot.bar(title = \"Feature Importance\", legend = False);","e4adfd45":"### Heart Disease frequency per Chest pain type\n\n* 0: Typical angina: chest pain related decrease blood supply to the heart\n* 1: Atypical angina: chest pain not related to heart\n* 2: Non-anginal pain: typically esophageal spasms (non heart related)\n* 3: Asymptomatic: chest pain not showing signs of disease","fb668643":"### Feature Importance \n\nFeature importance is another as asking, \"which features contributed most to the outcome of the model and how did they contribut?\"\n\nFinding feature importance is different for each machine learning model.","477dbc55":"## Data Exploration (explogatory data analysis or EDA)\n\nThe goel here is to find out more about the data and become a subject matter export on the dataset you're working with.\n\n1. what questions(s) are we trying to solve?\n2. what kind of data do we have and how we treat different types?\n3. what's missing from the data and how do you deal with it?\n4. where are the outliers and why should you care about them?\n5. how can you add, change or remove features to get more out of you data?","9ff67e20":"### Model Comparison","fe119de5":"Now we'va got a baseline model... and we know a model's first predictions aren't always we should based our next steps off.\n\nLet's look at the following:\n* Hypyterparameter tuning\n* Feature importance\n* Confusion matrix\n* Cross-validation\n* Precision\n* Recall\n* F1 score\n* Classification report\n* ROC curve\n* Area under the curve (AUC)\n\n### Hyperparameter tuning(By hands)","0162c8c1":"### Calculate evalution metrics using cross-validation\n\nwe're going to calculate precision, recall and f1-score of our model using cross-validation and to do so we'll be using `cross_val_score()`","d7c9f880":"### Age vs max heart rate for heart disease","0e4c0bf2":"Now we've got ROC curve, an AUC metric and a confusion matrix, let's get a classification report as well as cross-validated precision,recall and f1-score","a71190f8":"## Hyperparameter tuning with RandomizedSearchCV\n\nwe're going to tune:\n* LogisticRegression()\n* RandomForestClassifier()\n\n.... using RandomizedSearchCV","4e4cfa92":"Now we've tuned LogisticRegression(), let's do the same for RandomForestClassifer()","a78843e4":"### Heart Disease Frequency according to sex","fe1dddf0":"## Hyperparameter Tuning with GrideSearchCV\n\nSince our LogisticRegression model provides the best scores so far, we'll try and improve them again using GridSearhCV........","ade953ac":"## Load data","b121ac44":"Now we'have got hyperparameter grids setup for each of our models. let's tune them using RandomizedSearchCV.......","3dd0abd5":"Now we've got our data split into training and test sets, it's time to build a machine learning model.\n\nwe'll train it(find the patterns) on the training set.\n\nAnd we'll test it (Use the patterns) on the test set.\n\nWe're going to try 3 different machine learning model\n1. Logistic Regression\n2. K-Nearest Neighbours classifier\n3. Random Forest classifier","ec7ec9a4":"## Preparing the tools\nwe're going to use pandas, matplotlib and numpy for data analysis and manipulation","40c18342":"# Predicting heart disease using machine learning\n\n\nThis notebook looks into using various Python-based machine learning and data science libraries in an attempt to build a machine learning model capable of predicting whether or not someone has heart diesase based on their medical attributes.\n\nWe're going to take the following approch:\n1. Problem definition\n2. Data\n3. Evaluaion \n4. Features\n5. Modeling\n6. Experimentation\n\n## 1. Proble Defenation\n\nIn a statement,\n> Given clinical parameters about a patient, can we predict whether or not they have disease?\n\n## 2. Data \n\nThe original data came from the cleavland data from the UCI Machine Learning Repository. https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\n\n## 3. Evaluation\n\n> If we can reach 95% at predicting whether or not a patient has heart disease during the proof of concept, we'll pursue the project.\n\n\n## 4 .Features\n\n1. age - age in years\n\n2. sex - (1 = male; 0 = female)\n\n3. cp - chest pain type\n\n    * 0: Typical angina: chest pain related decrease blood supply to the heart\n    * 1: Atypical angina: chest pain not related to heart\n    * 2: Non-anginal pain: typically esophageal spasms (non heart related)\n    * 3: Asymptomatic: chest pain not showing signs of disease\n4. trestbps - resting blood pressure (in mm Hg on admission to the hospital) anything above 130-140 is typically cause for concern\n\n5. chol - serum cholestoral in mg\/dl\n    * serum = LDL + HDL + .2 * triglycerides\n    * above 200 is cause for concern\n    \n6. fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n    * '>126' mg\/dL signals diabetes\n    \n7. restecg - resting electrocardiographic results\n    * 0: Nothing to note\n    * 1: ST-T Wave abnormality\n        * can range from mild symptoms to severe problems\n        * signals non-normal heart beat\n    * 2: Possible or definite left ventricular hypertrophy\n        * Enlarged heart's main pumping chamber\n        \n8. thalach - maximum heart rate achieved\n\n9. exang - exercise induced angina (1 = yes; 0 = no)\n\n10. oldpeak - ST depression induced by exercise relative to rest looks at stress of heart during excercise unhealthy heart will stress more\n\n11. slope - the slope of the peak exercise ST segment\n    * 0: Upsloping: better heart rate with excercise (uncommon)\n    * 1: Flatsloping: minimal change (typical healthy heart)\n    * 2: Downslopins: signs of unhealthy heart\n\n12. ca - number of major vessels (0-3) colored by flourosopy\n    * colored vessel means the doctor can see the blood passing through\n    * the more blood movement the better (no clots)\n\n13. thal - thalium stress result\n    * 1,3: normal\n    * 6: fixed defect: used to be defect but ok now\n    * 7: reversable defect: no proper blood movement when excercising\n\n14. target - have disease or not (1=yes, 0=no) (= the predicted attribute)","31fe7ea7":"## Evaluting our tuned machine learning classifier, beyound accuracy\n\n* ROC curve and AUC score\n* Confusion matrix\n* Classification report\n* Precision\n* Recall\n* F1-score\n\n....and it would be great if cross-validation was used where possible\n\nTo make comparison and evalute our trained model, first we need to make predictions.","2e1e8478":"## Modelling"}}