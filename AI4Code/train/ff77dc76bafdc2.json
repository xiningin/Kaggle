{"cell_type":{"eabf1e70":"code","61249f41":"code","c7cf9d77":"code","20939a0f":"code","b3577617":"code","6f218f5e":"code","ff6d8455":"code","606945e6":"code","54771542":"code","f8a490c5":"code","3fc4a8a7":"code","bcdc6f52":"code","30c98a2d":"code","e86f4732":"code","9342ee06":"code","6e098c1b":"code","3f309577":"code","2a371bc5":"markdown","19b63772":"markdown","bc877bb7":"markdown","cd08f9bc":"markdown","68cfe7e8":"markdown","08732ceb":"markdown","1216f808":"markdown","15c877b2":"markdown","8159ed1d":"markdown","4c64cc28":"markdown","0a18bd3d":"markdown","74ff0e28":"markdown","9282bc27":"markdown","6ccb1bfc":"markdown"},"source":{"eabf1e70":"#I will begin with the standard imports:\n\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom scipy import stats\n\n# use seaborn plotting defaults\nimport seaborn as sns; sns.set()\n\n#default theme\nsns.set(context='notebook', style='darkgrid', palette='colorblind', font='sans-serif', font_scale=1, rc=None)\nmatplotlib.rcParams['figure.figsize'] =[25,8]\nmatplotlib.rcParams.update({'font.size': 15})\nmatplotlib.rcParams['font.family'] = 'sans-serif'\n","61249f41":"from sklearn.datasets import make_blobs\nX, y = make_blobs(n_samples=50, centers=2,\n                  random_state=0, cluster_std=0.60)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')","c7cf9d77":"xfit = np.linspace(-1, 3.5)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\nplt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\nfor m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n    plt.plot(xfit, m * xfit + b, '-k')\nplt.xlim(-1, 3.5)","20939a0f":"xfit = np.linspace(-1, 3.5)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\nfor m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n    yfit = m * xfit + b\n    plt.plot(xfit, yfit, '-k')\n    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n                     color='#AAAAAA', alpha=0.4)\nplt.xlim(-1, 3.5)","b3577617":"from sklearn.svm import SVC # \"Support vector classifier\"\nmodel = SVC(kernel='linear', C=1E10)\nmodel.fit(X, y)","6f218f5e":"def plot_svc_decision_function(model, ax=None, plot_support=True):\n    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n    if ax is None:\n        ax = plt.gca()\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n    \n    # create grid to evaluate model\n    x = np.linspace(xlim[0], xlim[1], 30)\n    y = np.linspace(ylim[0], ylim[1], 30)\n    Y, X = np.meshgrid(y, x)\n    xy = np.vstack([X.ravel(), Y.ravel()]).T\n    P = model.decision_function(xy).reshape(X.shape)\n    \n    # plot decision boundary and margins\n    ax.contour(X, Y, P, colors='k',\n               levels=[-1, 0, 1], alpha=0.5,\n               linestyles=['--', '-', '--'])\n    \n    # plot support vectors\n    if plot_support:\n        ax.scatter(model.support_vectors_[:, 0],\n                   model.support_vectors_[:, 1],\n                   s=300, linewidth=1, facecolors='none');\n    ax.set_xlim(xlim)\n    ax.set_ylim(ylim)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\nplot_svc_decision_function(model)\n\n","ff6d8455":"model.support_vectors_\n","606945e6":"def plot_svm(N=10, ax=None):\n    X, y = make_blobs(n_samples=200, centers=2,\n                      random_state=0, cluster_std=0.60)\n    X = X[:N]\n    y = y[:N]\n    model = SVC(kernel='linear', C=1E10)\n    model.fit(X, y)\n    \n    ax = ax or plt.gca()\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n    ax.set_xlim(-1, 4)\n    ax.set_ylim(-1, 6)\n    plot_svc_decision_function(model, ax)\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 6))\nfig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\nfor axi, N in zip(ax, [60, 120]):\n    plot_svm(N, axi)\n    axi.set_title('N = {0}'.format(N))","54771542":"from sklearn.datasets import fetch_lfw_people\nfaces = fetch_lfw_people(min_faces_per_person=60)\nprint(faces.target_names)\nprint(faces.images.shape)","f8a490c5":"fig, ax = plt.subplots(3, 5)\nfor i, axi in enumerate(ax.flat):\n    axi.imshow(faces.images[i], cmap='bone')\n    axi.set(xticks=[], yticks=[],\n            xlabel=faces.target_names[faces.target[i]])","3fc4a8a7":"from sklearn.svm import SVC\nfrom sklearn.decomposition import PCA as RandomizedPCA\nfrom sklearn.pipeline import make_pipeline\n\npca = RandomizedPCA(n_components=150, whiten=True, random_state=42)\nsvc = SVC(kernel='rbf', class_weight='balanced')\nmodel = make_pipeline(pca, svc)","bcdc6f52":" from sklearn.model_selection import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target,\n                                                random_state=42)","30c98a2d":"from sklearn.model_selection import GridSearchCV\nparam_grid = {'svc__C': [1, 5, 10, 50],\n              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\ngrid = GridSearchCV(model, param_grid)\n\n%time grid.fit(Xtrain, ytrain)\nprint(grid.best_params_)","e86f4732":"model = grid.best_estimator_\nyfit = model.predict(Xtest)","9342ee06":"fig, ax = plt.subplots(4, 6)\nfor i, axi in enumerate(ax.flat):\n    axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')\n    axi.set(xticks=[], yticks=[])\n    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],\n                   color='black' if yfit[i] == ytest[i] else 'red')\nfig.suptitle('Predicted Names; Incorrect Labels in Red', size=14)","6e098c1b":"from sklearn.metrics import classification_report\nprint(classification_report(ytest, yfit,\n                            target_names=faces.target_names))","3f309577":"from sklearn.metrics import confusion_matrix\nmat = confusion_matrix(ytest, yfit)\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n            xticklabels=faces.target_names,\n            yticklabels=faces.target_names)\nplt.xlabel('true label')\nplt.ylabel('predicted label')","2a371bc5":"The optimal values fall toward the middle of our grid; if they fell at the edges, we would want to expand the grid to make sure we have found the true optimum.\n\nNow with this cross-validated model, we can predict the labels for the test data, which the model has not yet seen:","19b63772":"With those traits in mind, I generally only turn to Support Vector Machines once another simpler, faster, and less tuning-intensive methods have been shown to be insufficient for my needs.","bc877bb7":"## Example: Face Recognition\nAs an example of support vector machines in action, let\u2019s take a look at the facial recognition problem. We will use the Labeled Faces in the Wild dataset, which consists of several thousand collated photos of various public figures. A fetcher for the dataset is built into Scikit-Learn:","cd08f9bc":"This is the dividing line that maximizes the margin between the two sets of points. Notice that a few of the training points just touch the margin: the black circles in this figure indicate them.\n\nThese points are the pivotal elements of this fit, and are known as the support vectors, and give the algorithm its name. In Scikit-Learn, the identity of these points are stored in the support_vectors_ the attribute of the classifier:","68cfe7e8":"A key to this classifier\u2019s success is that for the fit, only the position of the support vectors matter; any points further from the margin which are on the right side do not modify the fit!\n\nTechnically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin.\n\nWe can see this, for example, if we plot the model learned from the first 60 points and first 120 points of this dataset:","08732ceb":"Support vector machines (SVMs) are a particularly powerful and flexible class of supervised algorithms for both classification and regression. In this article, I will develop the intuition behind support vector machines and their use in classification problems.\n![](https:\/\/i.ytimg.com\/vi\/efR1C6CvhmE\/maxresdefault.jpg)","1216f808":"These are three very different separators, which, nevertheless, correctly discriminate between these samples. Depending on which you choose, a new data point (e.g., the one marked by the \u201cX\u201d in this plot) will be assigned a different label! Our simple intuition of \u201cdrawing a line between classes\u201d is not enough, and we need to think a bit deeper.","15c877b2":"To better visualize what\u2019s happening here, let\u2019s create a quick convenience function that will plot SVM decision boundaries for us:","8159ed1d":"Out of this small sample, our optimal estimator mislabeled only a single face (Bush\u2019s face in the bottom row was mislabeled as Blair).\n\nWe can get a better sense of our estimator\u2019s performance using the classification report, which lists recovery statistics label by label:","4c64cc28":"## Fitting a Support Vector Machine\nLet\u2019s see the result of an exact fit for this data: we will use Scikit-Learn\u2019s support vector classifier to train an SVM model on this data. For the time being, we will use a linear kernel and set the C parameter to a very large number:","0a18bd3d":"Each image contains [62\u00d747] or nearly 3,000 pixels. We could proceed by merely using each pixel value as a feature, but often it is more effective to use some sort of preprocessor to extract more meaningful features; here, we will use a principal component analysis to remove 150 fundamental components to feed into our support vector machine classifier.\n\nWe can do this most straightforwardly by packaging the preprocessor and the classifier into a single pipeline:","74ff0e28":"## upport Vector Machines: Maximizing the Margin\nSupport vector machines offer one way to improve on this. The intuition is this: rather than merely drawing a zero-width line between the classes, we can bring around each edge a margin of some width, up to the nearest point. Here is an example of how this might look:","9282bc27":"A linear discriminative classifier would attempt to draw a straight line separating the two sets of data and thereby create a model for classification.\n\nFor two dimensional data like that shown here, this is a task we could do by hand. But immediately we see a problem: there is more than one possible dividing line that can correctly discriminate between the two classes.\n\nI will draw them as follows:","6ccb1bfc":"## Motivating Support Vector Machines\nLet\u2019s consider the simple case of a classification task, in which the two classes of points are well separated:"}}