{"cell_type":{"b48ad8b7":"code","9fb4aec3":"code","109f8b89":"code","418a6c3b":"code","867e6ec9":"code","f4e1c6b8":"code","c04e74e0":"code","64c2cceb":"code","ecfb900f":"code","24ad85a1":"code","2ab0aa33":"code","91051b25":"code","73c4fe3c":"code","2d4a8ff9":"code","33127cee":"code","bdafb330":"code","e8dc58d3":"code","67da334b":"code","e1a44d9e":"code","468bce2b":"code","f72b0fdf":"code","107ac7cf":"code","90c53f5f":"code","6caafef2":"code","23201a1e":"code","6b7fa336":"code","e608c253":"code","a1b0e154":"code","13ee17b7":"code","173f8988":"code","1c6dcf3e":"code","9631dba8":"code","ac9ba097":"code","6f9cdefd":"code","2d44733c":"markdown","e89c1245":"markdown","a668d4ad":"markdown","32e9fd47":"markdown","6a2c4b13":"markdown","ca0424b2":"markdown","c719d3a6":"markdown","6bf57527":"markdown","66f5778c":"markdown","39037f5e":"markdown","972d0b0c":"markdown","980d08d4":"markdown","4a05ef17":"markdown"},"source":{"b48ad8b7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9fb4aec3":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntrain_df.head()","109f8b89":"test_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ntest_df.head()","418a6c3b":"train_df.shape, test_df.shape","867e6ec9":"train_df.dtypes","f4e1c6b8":"train_df.describe().transpose()","c04e74e0":"train_df.isnull().sum()","64c2cceb":"X = train_df.drop([\"id\"],axis=1)\nX.head()","ecfb900f":"print(train_df[\"id\"].nunique())\nprint(train_df[\"keyword\"].nunique())\nprint(train_df[\"location\"].nunique())\nprint(train_df[\"target\"].unique())\nprint(train_df[\"text\"].nunique())","24ad85a1":"key = X[\"keyword\"].value_counts().index[0]\nprint(\"most frequent word in keyword is :\",key)\nloc = X[\"location\"].value_counts().index[0]\nprint(\"most frequent word in location is :\",loc)","2ab0aa33":"train_df['keyword'] = train_df['keyword'].fillna(train_df['keyword'].value_counts().idxmax())\ntrain_df['location'] = train_df['location'].fillna(train_df['location'].value_counts().idxmax())\ntrain_df.isnull().sum()","91051b25":"X_new = train_df.drop([\"target\"],axis=1)\nX_new.head()","73c4fe3c":"y = train_df[\"target\"]\ny.shape","2d4a8ff9":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nimport csv\nfrom sklearn.feature_extraction.text import CountVectorizer,  TfidfVectorizer, HashingVectorizer\nfrom sklearn.ensemble import RandomForestClassifier \nimport nltk\nfrom sklearn.metrics import accuracy_score, confusion_matrix,classification_report\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud","33127cee":"#not a disaster tweet\nplt.figure(figsize = (20,20))\nWc = WordCloud(max_words = 500 , width = 1600 , height = 800).generate(\" \".join(train_df[train_df.target == 0].text))\nplt.axis(\"off\")\nplt.imshow(Wc , interpolation = 'bilinear');","bdafb330":"# Disaster Tweet\nplt.figure(figsize = (20,20))\nWc = WordCloud(max_words = 500 , width = 1600 , height = 800).generate(\" \".join(train_df[train_df.target == 1].text))\nplt.axis(\"off\")\nplt.imshow(Wc , interpolation = 'bilinear');","e8dc58d3":"test_df.isnull().sum()","67da334b":"test_df['keyword'] = test_df['keyword'].fillna(test_df['keyword'].value_counts().idxmax())\ntest_df['location'] = test_df['location'].fillna(test_df['location'].value_counts().idxmax())\ntest_df.isnull().sum()","e1a44d9e":"X_new.replace(\"[^a-zA-Z]\", \" \",regex = True, inplace = True)\nX_new.head()","468bce2b":"X_new = X_new.drop([\"id\"],axis=1)\nX_new.head()","f72b0fdf":"# Converting into lower case\nfor i in X_new.columns:\n    X_new[i] = X_new[i].str.lower()\nX_new.head(1)","107ac7cf":"from nltk.corpus import stopwords\nstop = stopwords.words('english')","90c53f5f":"X_new['keyword'].apply(lambda x: [item for item in x if item not in stop])\nX_new['location'].apply(lambda x: [item for item in x if item not in stop])\nX_new['text'].apply(lambda x: [item for item in x if item not in stop])\nprint(X_new.shape)","6caafef2":"ori_test = test_df.drop([\"id\"],axis=1)","23201a1e":"# Removing extra punctuations like , ? \/ @ # ! * ~ etc all\nori_test.replace(\"[^a-zA-Z]\", \" \",regex = True, inplace = True)\n\n# Converting Upper case to Lower case so all string data are same\nfor i in ori_test.columns:\n    ori_test[i] = ori_test[i].str.lower()\n\n# Remove StopWords\nori_test['keyword'].apply(lambda x: [item for item in x if item not in stop])\nori_test['location'].apply(lambda x: [item for item in x if item not in stop])\nori_test['text'].apply(lambda x: [item for item in x if item not in stop])\nprint(ori_test.shape)","6b7fa336":"# Adding Keyword and Text because both are important for our prediction \n\n# IN TRAIN DATASET\nX_new[\"sentence\"] = X_new['keyword'] + \" \" + X_new['text']\ntrain_text = np.array(X_new[\"sentence\"])\nprint(train_text[0])\nprint(f\" train_text type : '{type(train_text)}' \")\n\n# IN TEST DATASET \nori_test[\"sentence\"] = ori_test['keyword'] + \" \" + ori_test['text']\ntest_text = np.array(ori_test[\"sentence\"])\nprint(test_text[0])","e608c253":"print(train_text[0])","a1b0e154":"test_text[0]","13ee17b7":"x_train, x_test, y_train, y_test = train_test_split(train_text,y,test_size=0.25,random_state=0)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","173f8988":"# encoding essay attribute using count vectorizer\nvectorizer = TfidfVectorizer()\nvectorizer.fit_transform(x_train)\n\nkeyword = vectorizer.get_feature_names()\nX_train = vectorizer.transform(x_train)\nX_test = vectorizer.transform(x_test)\nX_test_new = vectorizer.transform(test_text)\n\nprint(\" after encoding in bow the size of keyword:\",len(keyword))\nprint(\" train feature --\",X_train.shape,y_train.shape)\nprint(\"test feature --\",X_test.shape,y_test.shape)\nprint(\"new test feature --\",X_test_new.shape)","1c6dcf3e":"print(\"data before vectorization :\\n\",x_train[0])\nprint(\"\\n\")\nprint(\"data after vectorization in vector form : \\n\",X_train[0])","9631dba8":"rc = RandomForestClassifier(max_depth = 400 ,random_state=0, n_estimators = 300)\nrc.fit(X_train,y_train) # fit in model\n\npred1 = rc.predict(X_test) # predict data ","ac9ba097":"# Metric\nmatrix = confusion_matrix(y_test,pred1)\nprint(matrix)\nscore = accuracy_score(y_test,pred1)\nprint(score)\nreport = classification_report(y_test,pred1)\nprint(report)\nprediction1 = rc.predict(X_test_new)\nprediction1","6f9cdefd":"data = {'id':test_df[\"id\"],'target':prediction1}\noutput = pd.DataFrame(data, columns = ['id','target'])\noutput.index = test_df.index\n\noutput.to_csv(\"submission.csv\", index = False)    \n\n# the csv file will be saved locally on the same location where this notebook is located.\na = pd.read_csv(\"submission.csv\")\na","2d44733c":"## Dataset Properties","e89c1245":"## Text PreProcessing in Test Data","a668d4ad":"## Predict","32e9fd47":"## Text PreProcessing","6a2c4b13":"##  Data PreProcessing","ca0424b2":"### Remove StopWords","c719d3a6":"## Save Result","6bf57527":"###  Converting Upper case to Lower case so all string data are same","66f5778c":"### Splitting data","39037f5e":"### Filling Missing Value with Most Frequent Value","972d0b0c":"### Test Data PreProcessing","980d08d4":"### Removing extra punctuations like , ? \/ @ # ! * ~ etc all","4a05ef17":"## CountVectorizer change text to vector and help to use BagOfWords"}}