{"cell_type":{"bbf7c059":"code","f1f7d5d4":"code","388fb9bc":"code","39129d02":"code","0c0c8efc":"code","b275623e":"code","689397c3":"code","a27718ee":"code","515f7c91":"code","490769e2":"code","247bd779":"code","a05bc5d0":"code","825974e7":"code","853dd30d":"code","09cd3e03":"code","acd91155":"code","beed7387":"code","5a6a65f9":"code","c8f42f29":"code","97d3274f":"code","165f4b63":"code","d07804cf":"code","900f0f83":"code","45c56706":"code","148c9542":"code","fe4ad8c5":"code","8de45c64":"code","706d5ae9":"code","aa43ba79":"code","190a8c68":"code","25d93284":"code","649bcc4e":"code","8d7b7f25":"code","ce619391":"code","4351055d":"code","c1b6d528":"code","5f9f09c8":"code","2e76ff17":"code","f7f02549":"code","8af3f3c2":"code","8797b1a8":"code","695da9b0":"code","be8b8590":"code","a0c1c0db":"code","ce15c7b5":"code","4b4356ac":"code","d871030f":"code","81992107":"code","e5e35e65":"code","223fbf07":"code","7a08462e":"code","f478f0fb":"code","0222edb9":"code","fb9ea32e":"code","fb7e1920":"code","739fb17a":"code","67b0023e":"code","2f844b47":"code","42ed69d5":"code","e08d0e80":"code","f0fd2204":"code","c327c1dd":"code","d3cb407d":"code","54c6954e":"markdown","d4d120eb":"markdown","09aad8de":"markdown","91279b88":"markdown","540825e3":"markdown","7488be7f":"markdown","5014d9b5":"markdown","89bcf2b8":"markdown","12ef9ccf":"markdown","49be8459":"markdown","a4b0c476":"markdown","325d9cfb":"markdown","5fdb169a":"markdown","7a5297b0":"markdown","99f6c930":"markdown","1e3fdcab":"markdown","1c8c980a":"markdown","6ab6f63a":"markdown","9e3d9803":"markdown","afb13c70":"markdown","eae3e482":"markdown","cfa93416":"markdown","4c54275b":"markdown","abfa8ebd":"markdown","7fc868c2":"markdown","6cf7f1c1":"markdown","8275031a":"markdown","8872df7b":"markdown","b17f12f5":"markdown","6dcf5154":"markdown","e146c226":"markdown","6c6a6b26":"markdown","a8d20f85":"markdown","8cc28075":"markdown","ce633333":"markdown","eb8790ad":"markdown","5b01adcb":"markdown","419f5a22":"markdown","9a472383":"markdown","4730198e":"markdown","ad214ae3":"markdown","d0532d28":"markdown","a5f090b3":"markdown","93f26466":"markdown","6efe0d8d":"markdown","0d1482ce":"markdown","fc154a14":"markdown","38f22d6b":"markdown","b9f39139":"markdown","93021318":"markdown","c4d3ca90":"markdown","9f69ce50":"markdown","30ab7eac":"markdown","7debb528":"markdown","909984ca":"markdown","db2d0d96":"markdown","6dbf2b2f":"markdown","3345f7db":"markdown","d4533214":"markdown","8519106a":"markdown","b1badb0b":"markdown","1b623fbf":"markdown","e8bb5019":"markdown","331240e4":"markdown","1092d186":"markdown"},"source":{"bbf7c059":"# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n# data visualisation and manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\nimport missingno as msno\n\n#configure\n# sets matplotlib to inline and displays graphs below the corressponding cell.\n%matplotlib inline  \nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid',color_codes=True)\n\n#import the necessary modelling algos.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn import datasets\nfrom sklearn.naive_bayes import GaussianNB\n\n#model selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\n\n#preprocess.\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,Imputer,LabelEncoder,OneHotEncoder","f1f7d5d4":"train=pd.read_csv(r'..\/input\/voice.csv')","388fb9bc":"train.head(10)","39129d02":"df=train.copy()","0c0c8efc":"df.head(10)","b275623e":"df.shape","689397c3":"df.index   ","a27718ee":"df.columns # give a short description of each feature.","515f7c91":"# check for null values.\ndf.isnull().any()   ","490769e2":"msno.matrix(df)  # just to visualize. no missing value.","247bd779":"df.describe()","a05bc5d0":"def calc_limits(feature):\n    q1,q3=df[feature].quantile([0.25,0.75])\n    iqr=q3-q1\n    rang=1.5*iqr\n    return(q1-rang,q3+rang)","825974e7":"def plot(feature):\n    fig,axes=plt.subplots(1,2)\n    sns.boxplot(data=df,x=feature,ax=axes[0])\n    sns.distplot(a=df[feature],ax=axes[1],color='#ff4125')\n    fig.set_size_inches(15,5)\n    \n    lower,upper = calc_limits(feature)\n    l=[df[feature] for i in df[feature] if i>lower and i<upper] \n    print(\"Number of data points remaining if outliers removed : \",len(l))\n\n","853dd30d":"plot('meanfreq')","09cd3e03":"plot('sd')","acd91155":"plot('median')","beed7387":"plot('Q25')","5a6a65f9":"plot('IQR')","c8f42f29":"plot('skew')","97d3274f":"plot('kurt')","165f4b63":"plot('sp.ent')","d07804cf":"plot('sfm')","900f0f83":"plot('meanfun')","45c56706":"sns.countplot(data=df,x='label')","148c9542":"df['label'].value_counts()","fe4ad8c5":"temp = []\nfor i in df.label:\n    if i == 'male':\n        temp.append(1)\n    else:\n        temp.append(0)\ndf['label'] = temp","8de45c64":"#corelation matrix.\ncor_mat= df[:].corr()\nmask = np.array(cor_mat)\nmask[np.tril_indices_from(mask)] = False\nfig=plt.gcf()\nfig.set_size_inches(30,12)\nsns.heatmap(data=cor_mat,mask=mask,square=True,annot=True,cbar=True)","706d5ae9":"df.drop('centroid',axis=1,inplace=True)","aa43ba79":"# drawing features against the target variable.\n\ndef plot_against_target(feature):\n    sns.factorplot(data=df,y=feature,x='label',kind='box')\n    fig=plt.gcf()\n    fig.set_size_inches(7,7)","190a8c68":"plot_against_target('meanfreq') # 0 for females and 1 for males.","25d93284":"plot_against_target('sd')","649bcc4e":"plot_against_target('median')","8d7b7f25":"plot_against_target('Q25')","ce619391":"plot_against_target('IQR')","4351055d":"plot_against_target('sp.ent')","c1b6d528":"plot_against_target('sfm')","5f9f09c8":"plot_against_target('meanfun')  ","2e76ff17":"g = sns.PairGrid(df[['meanfreq','sd','median','Q25','IQR','sp.ent','sfm','meanfun','label']], hue = \"label\")\ng = g.map(plt.scatter).add_legend()","f7f02549":"# removal of any data point which is an outlier for any fetaure.\nfor col in df.columns:\n    lower,upper=calc_limits(col)\n    df = df[(df[col] >lower) & (df[col]<upper)]","8af3f3c2":"df.shape","8797b1a8":"df.head(10)","695da9b0":"temp_df=df.copy()\n\ntemp_df.drop(['skew','kurt','mindom','maxdom'],axis=1,inplace=True) # only one of maxdom and dfrange.\ntemp_df.head(10)\n#df.head(10)","be8b8590":"temp_df['meanfreq']=temp_df['meanfreq'].apply(lambda x:x*2)\ntemp_df['median']=temp_df['meanfreq']+temp_df['mode']\ntemp_df['median']=temp_df['median'].apply(lambda x:x\/3)","a0c1c0db":"temp_df.head(10) ","ce15c7b5":"sns.boxplot(data=temp_df,y='median',x='label') # seeing the new 'median' against the 'label'.","4b4356ac":"temp_df['pear_skew']=temp_df['meanfreq']-temp_df['mode']\ntemp_df['pear_skew']=temp_df['pear_skew']\/temp_df['sd']\ntemp_df.head(10)","d871030f":"sns.boxplot(data=temp_df,y='pear_skew',x='label') # plotting new 'skewness' against the 'label'.","81992107":"scaler=StandardScaler()\nscaled_df=scaler.fit_transform(temp_df.drop('label',axis=1))\nX=scaled_df\nY=df['label'].as_matrix()","e5e35e65":"x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.20,random_state=42)","223fbf07":"clf_lr=LogisticRegression()\nclf_lr.fit(x_train,y_train)\npred=clf_lr.predict(x_test)\nprint(accuracy_score(pred,y_test))","7a08462e":"clf_knn=KNeighborsClassifier()\nclf_knn.fit(x_train,y_train)\npred=clf_knn.predict(x_test)\nprint(accuracy_score(pred,y_test))","f478f0fb":"clf_svm=SVC()\nclf_svm.fit(x_train,y_train)\npred=clf_svm.predict(x_test)\nprint(accuracy_score(pred,y_test))","0222edb9":"clf_dt=DecisionTreeClassifier()\nclf_dt.fit(x_train,y_train)\npred=clf_dt.predict(x_test)\nprint(accuracy_score(pred,y_test))","fb9ea32e":"clf_rf=RandomForestClassifier()\nclf_rf.fit(x_train,y_train)\npred=clf_rf.predict(x_test)\nprint(accuracy_score(pred,y_test))","fb7e1920":"clf_gb=GradientBoostingClassifier()\nclf_gb.fit(x_train,y_train)\npred=clf_gb.predict(x_test)\nprint(accuracy_score(pred,y_test))","739fb17a":"models=[LogisticRegression(),LinearSVC(),SVC(kernel='rbf'),KNeighborsClassifier(),RandomForestClassifier(),\n        DecisionTreeClassifier(),GradientBoostingClassifier(),GaussianNB()]\nmodel_names=['LogisticRegression','LinearSVM','rbfSVM','KNearestNeighbors','RandomForestClassifier','DecisionTree',\n             'GradientBoostingClassifier','GaussianNB']\n\nacc=[]\nd={}\n\nfor model in range(len(models)):\n    clf=models[model]\n    clf.fit(x_train,y_train)\n    pred=clf.predict(x_test)\n    acc.append(accuracy_score(pred,y_test))\n     \nd={'Modelling Algo':model_names,'Accuracy':acc}","67b0023e":"acc_frame=pd.DataFrame(d)\nacc_frame","2f844b47":"sns.barplot(y='Modelling Algo',x='Accuracy',data=acc_frame)","42ed69d5":"params_dict={'C':[0.001,0.01,0.1,1,10,100],'gamma':[0.001,0.01,0.1,1,10,100],'kernel':['linear','rbf']}\nclf=GridSearchCV(estimator=SVC(),param_grid=params_dict,scoring='accuracy',cv=10)\nclf.fit(x_train,y_train)","e08d0e80":"clf.best_score_","f0fd2204":"clf.best_params_","c327c1dd":"print(accuracy_score(clf.predict(x_test),y_test))","d3cb407d":"print(precision_score(clf.predict(x_test),y_test))","54c6954e":"**You can also try some other coefficient also and see how it comapres with the target i.e. the 'label' column.**","d4d120eb":"In this section I have analyzed the corelation between different features. To do it I have plotted a 'heat map' which clearly visulizes the corelation between different features.","09aad8de":"<a id=\"content5\"><\/a>\n## 5 ) Preparing the Data","91279b88":"## 5.1 ) Normalizing the Features.","540825e3":"**#A short description as on 'Data' tab on kaggle is :**","7488be7f":"#### INFERENCES--\n\n1) Firstly note that 0->'female' and 1->'male'.\n\n2) Note that the boxpot depicts that the females in genral have higher mean frequencies than their male counterparts and which is a generally accepted fact.","5014d9b5":"[ **6 ) Modelling**](#content6)","89bcf2b8":"**** ..........................................................Coefficent = (Mean - Mode )\/StandardDeviation......................................................****","12ef9ccf":"###  After tuning SVM gives an amazing accuracy of around 99.1 %. Similarly tuning other algorithms parameters might give even greater accuracy !!!","49be8459":"[ **7 ) Parameter Tuning with GridSearchCV**](#content7)","a4b0c476":"### The precision is almost 99.5 % which is quite high.","325d9cfb":"#### kNN","5fdb169a":"#### SOME INFERENCES FROM THE ABOVE HEATMAP--\n\n1) Mean frequency is  moderately related to label.\n\n2) IQR and label tend to have a strong positive corelation.\n\n3) Spectral entropy is also quite highly corelated with the label while sfm is moderately related with label.\n\n4) skewness and kurtosis aren't much related with label.\n\n5) meanfun is highly negatively corelated with  the label.\n\n6) Centroid and median have a high positive corelationas expected from their formulae.\n\n7) ALSO NOTE THAT MEANFREQ AND CENTROID ARE EXACTLY SAME FEATURES AS PER FORMULAE AND VALUES ALSO. HENCE THEIR CORELATION IS PERFCET 1. IN THAT CASE WE CAN DROP ANY COLUMN. note that centroid in general has a high degree of corelation with most of the other features. \n\nSO I WILL DROP THE 'CENTROID' COLUMN.\n\n8) sd is highly positively related to sfm and so is sp.ent to sd.\n\n9) kurt and skew are also highly corelated.\n\n10) meanfreq is highly related to medaina s well as Q25.\n\n11) IQR is highly corelated to sd.\n\n12) Finally self relation ie of a feature to itself is equal to 1 as expected.","7a5297b0":"## 2.3 ) Univariate Analysis","99f6c930":"Here I have just written a small utility function that plots the 'label' column vs the provided feature on a boxplot. In this way I have plotted some of the features against our target variable. This makes it easier to see the effect of the corressponding feature on the 'label'.","1e3fdcab":"####  Note that  we have equal no of observations for the 'males' and the 'females'. Hence it is a balanced class problem.","1c8c980a":"# Gender Recognition by Voice Kaggle [ Test Accuracy : 99.08 % ]","6ab6f63a":"#### \n\n**meanfreq**: mean frequency (in kHz)\n\n**sd**: standard deviation of frequency\n\n**median**: median frequency (in kHz)\n\n**Q25**: first quantile (in kHz)\n\n**Q75**: third quantile (in kHz)\n\n**IQR**: interquantile range (in kHz)\n\n**skew**: skewness (see note in specprop description)\n\n**kurt**: kurtosis (see note in specprop description)\n\n**sp.ent**: spectral entropy\n\n**sfm**: spectral flatness\n\n**mode**: mode frequency\n\n**centroid**: frequency centroid (see specprop)\n\n**peakf**: peak frequency (frequency with highest energy)\n\n**meanfun**: average of fundamental frequency measured across acoustic signal\n\n**minfun**: minimum fundamental frequency measured across acoustic signal\n\n**maxfun**: maximum fundamental frequency measured across acoustic signal\n\n**meandom**: average of dominant frequency measured across acoustic signal\n\n**mindom**: minimum of dominant frequency measured across acoustic signal\n\n**maxdom**: maximum of dominant frequency measured across acoustic signal\n\n**dfrange**: range of dominant frequency measured across acoustic signal\n\n**modindx**: modulation index. Calculated as the accumulated absolute difference between adjacent measurements of fundamental frequencies divided by the frequency range\n\n**label**: male or female","9e3d9803":"#### We can now move onto comparing the results of various modelling algorithms. for tthis I shall combine the results of all models in a data frame and then plot using  a barplot .","afb13c70":"#### To detect the outliers I have used the standard 1.5 InterQuartileRange (IQR) rule which states that any observation lesser than  'first quartile - 1.5 IQR' or greater than 'third quartile +1.5 IQR' is an outlier.","eae3e482":"## 4.1 ) Dropping the features","cfa93416":"I have dropped some columns which according to my analysis proved to be less useful or redundant.","4c54275b":"[ **3 ) OutlierTreatment**](#content3)","abfa8ebd":"<a id=\"content2\"><\/a>\n## 2 )  Exploratory Data Analysis (EDA)","7fc868c2":"## CONTENTS::","6cf7f1c1":"#### For now I shall be removing all the observations or data points which are outlier to 'any' feature. Note that this substantially reduces the dataset size.","8275031a":"#### RANDOM FOREST","8872df7b":"## 2.4 ) Bivariate Analysis","b17f12f5":"#### Note that we have 3168 voice samples  and for each of sample 20 different acoustic properties are recorded. Finally the 'label' column is the target variable which we have to predict which is the gender of the person.","6dcf5154":"#### Again similar inferences can be drawn.","e146c226":"#### DECISION TREE  ","6c6a6b26":"#### Again high difference in females and males mean fundamental frequency. This is evident from the heat map which clearly shows the high corelation between meanfun and the 'label'.","a8d20f85":"## 1.2 ) Loading the Dataset","8cc28075":"## 4.2 ) Creating new features","ce633333":"<a id=\"content3\"><\/a>\n## 3 ) Outlier Treatment","eb8790ad":"#### Now we move onto analyzing different features pairwise. Since all the features are continuous the most reasonable way to do this is plotting the scatter plots for each feature pair. I have also distinguished males and feamles on the same plot which makes it a bit easier to compare the variation of features within the two classes.","5b01adcb":"## 2.2 ) Missing Values Treatment","419f5a22":"<a id=\"content7\"><\/a>\n## 7 ) Parameter Tuning with GridSearchCV","9a472383":"[ **2 ) Exploratory Data Analysis (EDA)**](#content2)","4730198e":"<a id=\"content6\"><\/a>\n## 6 ) Modelling","ad214ae3":"In this section I have performed the univariate analysis. Note that since all of the features are 'numeric' the most reasonable way to plot them would either be a 'histogram' or a 'boxplot'.\n\nAlso note that univariate analysis is useful for outlier detection. Hence besides plotting a boxplot and a histogram for each column or feature, I have written a small utility function which tells the remaining no of observations for each feature if  we remove its outliers.","d0532d28":"## 1.1 ) Importing Various  Modules","a5f090b3":"In this section I have dealt with the outliers.  Note that we discovered the potential outliers in the **'univariate analysis' ** section. Now to remove those outliers we can either remove the corressponding data points or impute them with some other statistical quantity like median (robust to outliers) etc..","93f26466":"## 2.4.2 ) Plotting the Features against the 'Target' variable","6efe0d8d":"#### Note that we can drop some highly corelated features as they add redundancy to the model  but  let us keep all the features for now. In case of highly corelated features we can use dimensionality reduction techniques like Principal Component Analysis(PCA) to reduce our feature space.","0d1482ce":"[ **4 ) Feature Engineering**](#content4)","fc154a14":"#### INFERENCES FROM THE PLOT--\n\n1) First of all note that the values are in compliance with that observed from describe method data frame..\n\n2) Note that we have a couple of outliers w.r.t. to 1.5 quartile rule (reprsented by a 'dot' in the box plot).Removing  these data points or outliers leaves us with around 3104 values.\n\n3) Also note from the distplot that the distribution seems to be a bit -ve skewed hence we can normalize to make the distribution a bit more symmetric. \n \n4) LASTLY NOTE THAT A LEFT TAIL DISTRIBUTION HAS MORE OUTLIERS ON THE SIDE BELOW TO Q1 AS EXPECTED AND A RIGHT TAIL HAS ABOVE THE Q3.","38f22d6b":"## 5.2 ) Splitting into Training and Validation sets.","b9f39139":"#### Similar other plots can be inferenced.","93021318":"<a id=\"content4\"><\/a>\n## 4 ) Feature Engineering.","c4d3ca90":"The second new feature that I have added is  a new feature to mesure the 'skewness'. ","9f69ce50":"1. I have tuned only SVM Similarly other algorithms can be tuned.","30ab7eac":"[ **5 ) Preparing the Data**](#content5)","7debb528":"## THE END!!!","909984ca":"## 2.4.1 ) Corealtion b\/w Features","db2d0d96":"####  For this I have adjusted values in the 'median' column as shown below. You can alter values in any of the other column say the 'meanfreq' column.","6dbf2b2f":"[ **1 )  Importing Various Modules and Loading the Dataset**](#content1)","3345f7db":"#### For this I have used the 'Karl Pearson Coefficent' which is calculated as shown below->","d4533214":"#### Note here that  there is a remarkable difference b\/w the inter quartile ranges of males and females.This is evident from the strong relation between 'label' and the 'IQR' in the heatmap plotted above.","8519106a":"I have done two new things. Firstly I have made 'meanfreq','median' and 'mode' to comply by the standard relation->","b1badb0b":"####     ......................................................................................3*Median=2*Mean +Mode.........................................................................","1b623fbf":"#### GRADIENT BOOSTING","e8bb5019":"#### LOGISTIC REGRESSSION","331240e4":"## 2.1 ) The Features and the 'Target' variable","1092d186":"#### Support Vector Machine (SVM)"}}