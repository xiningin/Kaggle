{"cell_type":{"71e25cf7":"code","1bc97c65":"code","0acb949a":"code","94591bb1":"code","1e79a9b0":"code","967d6417":"code","ee7b3ed1":"code","8a87de56":"code","dbb34fa6":"code","383e6977":"code","a0af9ccf":"code","43cb7592":"code","a7207a4d":"code","1670711d":"code","d922c146":"code","6962c0a0":"code","4fe8f6f6":"code","ef2d95a1":"code","7ab2f20b":"code","c63b70e1":"code","8f2e2804":"code","82dd9435":"code","4f713e42":"code","0cc1df91":"code","48f9cb30":"code","80010fe6":"code","3d6c3752":"code","0f3bcfa1":"code","83bd87a7":"code","c75a7240":"code","917349f1":"code","d0f25c6b":"code","e81852cd":"code","e8637661":"code","d0e64157":"code","bef2e8fc":"code","ef338064":"code","9460c1c2":"code","f368aceb":"code","676a82d2":"code","6f0a69f3":"markdown","06e3ae60":"markdown","0fb359be":"markdown","f9afdebc":"markdown","396e2e56":"markdown","b71236f9":"markdown","2f3ffc0a":"markdown","e6e1c6b6":"markdown","ef16041f":"markdown","a2ff2c41":"markdown","133b9afd":"markdown","05276158":"markdown","7e38dc97":"markdown"},"source":{"71e25cf7":"#Importing packages\nimport numpy as np\nimport pandas as pd\nimport re","1bc97c65":"# Read the dataset\ndisaster = pd.read_csv('\/kaggle\/input\/disaster-tweets\/tweets.csv')\ndisaster.head()","0acb949a":"# Loading required features\ndisaster = disaster[['text','target']]\ndisaster.head()","94591bb1":"# Checking the Null Values\ndisaster.isna().sum()","1e79a9b0":"# Checking the DF size\ndisaster.shape","967d6417":"# Lets see how Target Values labled\ndisaster['target'].value_counts()","ee7b3ed1":"disaster['text'] = [entry.lower() for entry in disaster['text']]\ndisaster['text'].head()","8a87de56":"# Loading packages for Tokenzation\nimport nltk\nfrom nltk.tokenize import word_tokenize","dbb34fa6":"disaster['text'] = [word_tokenize(entry) for entry in disaster['text']]","383e6977":"disaster['text'].head()","a0af9ccf":"disaster['text'].head()","43cb7592":"# loading packages \nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom collections import defaultdict\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import wordnet","a7207a4d":"tag_map = defaultdict(lambda : wn.NOUN)\ntag_map['j'] = wn.ADJ\ntag_map['v'] = wn.VERB\ntag_map['v'] = wn.ADV","1670711d":"# Storing all the stopwords into variables\nstop_words = set(stopwords.words(\"english\"))\nprint(stop_words)","d922c146":"for index,entry in enumerate(disaster['text']):\n    Final_words = []\n    word_lemmstized = WordNetLemmatizer()\n    for word,tag in pos_tag(entry):\n        if word not in stopwords.words('english') and word.isalpha():\n            word_final = word_lemmstized.lemmatize(word,tag_map[tag[0]])\n            Final_words.append(word_final)\n    disaster.loc[index,'text_final'] = str(Final_words)","6962c0a0":"disaster.head()","4fe8f6f6":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt","ef2d95a1":"# splitting all the words from the data and assining those to Comment-words\ncomment_words = ' '\nstopwords = set(STOPWORDS)","7ab2f20b":"for val in disaster.text_final:\n    val = str(val)\n    tokens = val.split()\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n    for words in tokens:\n        comment_words = comment_words + words + ' '\n","c63b70e1":"wordcloud = WordCloud(width = 1000, height =1000, background_color = 'white',\n                      stopwords = stopwords,min_font_size = 10).generate(comment_words)\n\n\nplt.figure(figsize = (4,4), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.tight_layout(pad = 0)\nplt.show()","8f2e2804":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import model_selection,naive_bayes\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder","82dd9435":"Train_X,Test_X,Train_Y,Test_Y = model_selection.train_test_split(disaster['text_final'],disaster['target'],test_size = 0.3)\nencoder = LabelEncoder()\nTrain_Y = encoder.fit_transform(Train_Y)\nTest_Y = encoder.fit_transform(Test_Y)","4f713e42":"y = Train_Y.tolist()\n#Y","0cc1df91":"Tfidf_vect = TfidfVectorizer(max_features = 5000)\nTfidf_vect.fit(disaster['text_final'])\nTrain_X_Tfidf = Tfidf_vect.transform(Train_X)\nTest_X_Tfidf = Tfidf_vect.transform(Test_X)","48f9cb30":"data = Train_X_Tfidf.toarray()","80010fe6":"data","3d6c3752":"naive = naive_bayes.MultinomialNB()\nnaive.fit(Train_X_Tfidf,Train_Y)","0f3bcfa1":"predictions_NB = naive.predict(Test_X_Tfidf)","83bd87a7":"print(\"Naive Bayes Model Accuracy : \",accuracy_score(predictions_NB,Test_Y)*100)","c75a7240":"Train_X,Test_X,Train_Y,Test_Y = model_selection.train_test_split(disaster['text_final'],disaster['target'],test_size = 0.3)\nencoder = LabelEncoder()\nTrain_Y = encoder.fit_transform(Train_Y)\nTest_Y = encoder.fit_transform(Test_Y)\n\nTfidf_vect = TfidfVectorizer(max_features = 5000)\nTfidf_vect.fit(disaster['text_final'])\nTrain_X_Tfidf = Tfidf_vect.transform(Train_X)\nTest_X_Tfidf = Tfidf_vect.transform(Test_X)\n\n","917349f1":"#loading random forest\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Setting random forest \nRandomForest = RandomForestClassifier(n_estimators = 19, bootstrap = True, random_state=9)\n\nRandomForest.fit(Train_X_Tfidf,Train_Y)\n\n#making predcitons with randome forest model\nlabel_predict = RandomForest.predict(Test_X_Tfidf)\n\naccuracy = accuracy_score( label_predict,Test_Y)\nprint((\"Accuracy of RandomForest: \", accuracy*100))\n","d0f25c6b":"Train_X,Test_X,Train_Y,Test_Y = model_selection.train_test_split(disaster['text_final'],disaster['target'],test_size = 0.3)\nencoder = LabelEncoder()\nTrain_Y = encoder.fit_transform(Train_Y)\nTest_Y = encoder.fit_transform(Test_Y)\n\nTfidf_vect = TfidfVectorizer(max_features = 5000)\nTfidf_vect.fit(disaster['text_final'])\nTrain_X_Tfidf = Tfidf_vect.transform(Train_X)\nTest_X_Tfidf = Tfidf_vect.transform(Test_X)\n\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(solver='liblinear')\nlogreg.fit(Train_X_Tfidf,Train_Y)\n\nlabel_predict = logreg.predict(Test_X_Tfidf)\n\n\naccuracy = accuracy_score(label_predict,Test_Y)\nprint((\"Accuracy of logreg: \", accuracy*100))","e81852cd":"from keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom tensorflow.keras.optimizers import RMSprop\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\n%matplotlib inline","e8637661":"Train_X,Test_X,Train_Y,Test_Y = model_selection.train_test_split(disaster['text_final'],disaster['target'],test_size = 0.3)\nencoder = LabelEncoder()\nTrain_Y = encoder.fit_transform(Train_Y)\nTest_Y = encoder.fit_transform(Test_Y)\n\nTfidf_vect = TfidfVectorizer(max_features = 5000)\nTfidf_vect.fit(disaster['text_final'])\nTrain_X_Tfidf = Tfidf_vect.transform(Train_X)\nTest_X_Tfidf = Tfidf_vect.transform(Test_X)\nmax_words = 1000\nmax_len = 150\ntok = Tokenizer(num_words=max_words)\ntok.fit_on_texts(Train_X)\nsequences = tok.texts_to_sequences(Train_X)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","d0e64157":"def RNN():\n    inputs = Input(name='inputs',shape=[max_len])\n    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n    layer = LSTM(64)(layer)\n    layer = Dense(256,name='FC1')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(1,name='out_layer')(layer)\n    layer = Activation('sigmoid')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model","bef2e8fc":"model = RNN()\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])","ef338064":"model.fit(sequences_matrix,Train_Y,batch_size=128,epochs=20,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])","9460c1c2":"test_sequences = tok.texts_to_sequences(Test_X)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)","f368aceb":"accr = model.evaluate(test_sequences_matrix,Test_Y)\n","676a82d2":"print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","6f0a69f3":"##  <font color='purple'> Lower casing<\/font>\n\n* Converting a word to lower case (NLP -> nlp)","06e3ae60":"# <font color='purple'>Part of Speech tagging and Stemming Words [lemmatization]<\/font> \n","0fb359be":"#  <font color='Orange'>Data Exploration<\/font>\n","f9afdebc":"### _Explaining each and every step for better understanding for even beginners_","396e2e56":"# <font color='Orange'> MODEL : LogisticRegression <\/font>","b71236f9":"##  <font color='purple'> Tokenization<\/font> \n\n","2f3ffc0a":"### _Tokenization in NLP is the process by which a large quantity of text is divided into smaller parts called tokens_","e6e1c6b6":"# <font color='Orange'>DATA CLEANING<\/font>\n","ef16041f":"* Part-of-Speech (PoS) tagging, then it may be defined as the process of assigning one of the parts of speech to the given word\n\n* Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language and gives its dictionary word\n\n\n * _Each row has to go through these both so we are going to create a function and add those values into another feature [Final Text]_","a2ff2c41":"# <font color='Orange'> MODEL : RandomForest <\/font>","133b9afd":"# <font color='Orange'> WORDCLOUD<\/font>","05276158":"# <font color='Orange'>MODEL : LSTM <\/font>","7e38dc97":"# <font color='Orange'> MODEL : Naive Bayes<\/font>"}}