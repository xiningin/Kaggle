{"cell_type":{"4c5e4e48":"code","f878ce69":"code","5c2a6e93":"code","1ab71751":"code","65f17d8c":"code","0f03c200":"code","e718e395":"code","04b273e6":"code","cabd587e":"code","02a3c8f5":"code","bfe6efa5":"code","122e3a32":"markdown","bafa113f":"markdown","efab3be1":"markdown","cf3d1e20":"markdown","b451fc5b":"markdown","68d58c49":"markdown","c9068429":"markdown"},"source":{"4c5e4e48":"import tensorflow as tf\nimport tensorflow.keras as keras\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom IPython import display\nfrom tqdm.auto import trange","f878ce69":"def plot_results(images, n_cols=None, title=None):\n    \n    n_cols = n_cols or len(images)\n    n_rows = (len(images) - 1) \/\/ n_cols + 1\n\n    if images.shape[-1] == 1:\n        images = np.squeeze(images, axis=-1)\n    \n    fig = plt.figure(figsize=(n_cols, n_rows))\n    \n    for index, image in enumerate(images):\n        plt.subplot(n_rows, n_cols, index + 1)\n        plt.imshow(image, cmap=\"binary\")\n        plt.axis(\"off\")\n        \n    plt.suptitle(title)","5c2a6e93":"BATCH_SIZE = 128\nCODINGS_SIZE = 32\nN_EPOCHS = 150","1ab71751":"def prepare_data(label, batch_size):\n    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n    X_all = np.concatenate([X_train, X_test])\n    y_all = np.concatenate([y_train, y_test])\n    \n    X_all = X_all.astype(np.float32) \/ 255\n    X_all = X_all.reshape(-1, 28, 28, 1) * 2. - 1.\n    X_train = X_all[np.where(y_all == LABEL)]\n\n    dataset = tf.data.Dataset.from_tensor_slices(X_train)\n    dataset = dataset.shuffle(1024)\n    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True).prefetch(1)\n    \n    return dataset\n\ndef prepare_images(label):\n    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n    X_all = np.concatenate([X_train, X_test])\n    y_all = np.concatenate([y_train, y_test])\n    \n    X_all = X_all.astype(np.float32) \/ 255\n    X_all = X_all.reshape(-1, 28, 28, 1) * 2. - 1.\n    X_train = X_all[np.where(y_all == label)]\n    \n    return X_train","65f17d8c":"def build_generator():\n    inputs = keras.Input(shape=[CODINGS_SIZE])\n    x = keras.layers.Dense(7 * 7 * 128)(inputs)\n    x = keras.layers.Reshape([7, 7, 128])(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Conv2DTranspose(64, kernel_size=3, strides=1, padding=\"SAME\", activation=\"selu\")(x)\n    x = keras.layers.BatchNormalization()(x)\n    skip = keras.layers.Conv2DTranspose(64, kernel_size=3, strides=1, padding=\"SAME\", activation=\"selu\")(x)\n    skip = keras.layers.BatchNormalization()(skip)\n    skip = keras.layers.Conv2DTranspose(64, kernel_size=3, strides=1, padding=\"SAME\", activation=\"selu\")(skip)\n    skip = keras.layers.BatchNormalization()(skip)\n    x = keras.layers.add([x, skip])\n    x = keras.layers.Conv2DTranspose(64, kernel_size=3, strides=2, padding=\"SAME\",activation=\"selu\")(x)\n    skip = keras.layers.Conv2DTranspose(64, kernel_size=3, strides=1, padding=\"SAME\", activation=\"selu\")(x)\n    skip = keras.layers.BatchNormalization()(skip)\n    skip = keras.layers.Conv2DTranspose(64, kernel_size=3, strides=1, padding=\"SAME\", activation=\"selu\")(skip)\n    skip = keras.layers.BatchNormalization()(skip)\n    x = keras.layers.add([x, skip])\n    outputs = keras.layers.Conv2DTranspose(1, kernel_size=3, strides=2, padding=\"SAME\",activation=\"tanh\")(x)\n    return keras.models.Model(inputs, outputs, name='generator')","0f03c200":"def build_discriminator():\n    return keras.models.Sequential([\n    keras.layers.Conv2D(64, kernel_size=3, strides=1, padding=\"SAME\", activation=keras.layers.LeakyReLU(0.2), input_shape=[28, 28, 1]),\n    keras.layers.Conv2D(128, kernel_size=3, strides=1, padding=\"SAME\", activation=keras.layers.LeakyReLU(0.2)),\n    keras.layers.Conv2D(128, kernel_size=3, strides=2, padding=\"SAME\", activation=keras.layers.LeakyReLU(0.2)),\n    keras.layers.Dropout(0.4),\n    keras.layers.Conv2D(128, kernel_size=3, strides=1, padding=\"SAME\", activation=keras.layers.LeakyReLU(0.2)),\n    keras.layers.Conv2D(128, kernel_size=3, strides=1, padding=\"SAME\", activation=keras.layers.LeakyReLU(0.2)),\n    keras.layers.Conv2D(128, kernel_size=3, strides=2, padding=\"SAME\", activation=keras.layers.LeakyReLU(0.2)),\n    keras.layers.Dropout(0.4),\n    keras.layers.Flatten(),\n    keras.layers.Dense(1, activation=\"sigmoid\")\n], name='discriminator')","e718e395":"class DCGAN(keras.Model):\n    def __init__(self, discriminator, generator, latent_dim):\n        super().__init__()\n        self.discriminator = discriminator\n        self.generator = generator\n        self.latent_dim = latent_dim\n\n    def compile(self, d_optimizer, g_optimizer, loss_fn):\n        super().compile()\n        self.d_optimizer = d_optimizer\n        self.g_optimizer = g_optimizer\n        self.loss_fn = loss_fn\n        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n\n    @property\n    def metrics(self):\n        return [self.d_loss_metric, self.g_loss_metric]\n\n    def train_step(self, real_images):\n        batch_size = tf.shape(real_images)[0]\n        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n        \n        generated_images = self.generator(random_latent_vectors)\n        combined_images = tf.concat([generated_images, real_images], axis=0)\n        labels = tf.concat(\n            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n        )\n        # Add random noise to the labels - important trick!\n        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n\n        # Train the discriminator\n        with tf.GradientTape() as tape:\n            predictions = self.discriminator(combined_images)\n            d_loss = self.loss_fn(labels, predictions)\n        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n        self.d_optimizer.apply_gradients(\n            zip(grads, self.discriminator.trainable_weights)\n        )\n\n        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n        misleading_labels = tf.zeros((batch_size, 1))\n\n        # Train the generator \n        with tf.GradientTape() as tape:\n            predictions = self.discriminator(self.generator(random_latent_vectors))\n            g_loss = self.loss_fn(misleading_labels, predictions)\n        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n\n        # Update metrics\n        self.d_loss_metric.update_state(d_loss)\n        self.g_loss_metric.update_state(g_loss)\n        return {\n            \"d_loss\": self.d_loss_metric.result(),\n            \"g_loss\": self.g_loss_metric.result(),\n        }\n","04b273e6":"generator = build_generator()\ndiscriminator = build_discriminator()\nprint('Generator Summary\\n\\n')\ngenerator.summary()\nprint('\\n\\nDiscriminator Summary\\n\\n')\ndiscriminator.summary()\nkeras.utils.plot_model(generator, show_shapes=True, expand_nested=True, to_file='generator.png')\nkeras.utils.plot_model(discriminator, show_shapes=True, expand_nested=True, to_file='discriminator.png')\nfig, ax = plt.subplots(1, 2, figsize=(20, 12))\nax[0].imshow(plt.imread('generator.png'))\nax[0].set_title('Generator', fontsize=18)\nax[1].imshow(plt.imread('discriminator.png'))\nax[1].set_title('Discriminator', fontsize=18)\nax[0].axis(\"off\")\nax[1].axis(\"off\")\nplt.show()","cabd587e":"for i in range(10):\n    LABEL = i\n    dataset = prepare_data(LABEL, BATCH_SIZE)\n\n    generator = build_generator()\n    discriminator = build_discriminator()\n    \n    gan = DCGAN(\n        discriminator=discriminator, generator=generator, \n        latent_dim=CODINGS_SIZE\n    )\n    gan.compile(\n        d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n        g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n        loss_fn=keras.losses.BinaryCrossentropy(),\n    )\n    \n    fig, ax = plt.subplots(figsize=(20, 6))\n    ax.set_title(f'Learning Curve-{LABEL}', fontsize=18)\n    history = gan.fit(dataset, epochs=N_EPOCHS, verbose=1)\n    pd.DataFrame(history.history).plot(ax=ax)\n    ax.grid()\n       \n    generator.save(f'MNIST-AUG-DCGAN-{LABEL}.h5')","02a3c8f5":"from scipy.linalg import sqrtm\n\ndef frechet_distance(act1, act2):\n    mu1, sigma1 = np.mean(act1, axis=0), np.cov(act1, rowvar=False)\n    mu2, sigma2 = np.mean(act2, axis=0), np.cov(act2, rowvar=False)\n    ssdiff = np.sum((mu1 - mu2)**2.0)\n    covmean = sqrtm(sigma1.dot(sigma2))\n    if np.iscomplexobj(covmean):\n        covmean = covmean.real\n    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n    return fid\n\nevaluator = keras.models.Sequential(keras.models.load_model('..\/input\/mnist-net\/mnist_net.h5').layers[:-1])\nscores = []\n\nfor i in range(10):\n    generator = keras.models.load_model(f'MNIST-AUG-DCGAN-{i}.h5')\n    fake_images = generator(tf.random.normal([128, CODINGS_SIZE]))\n    embeddings_real = evaluator(prepare_images(i))\n    embeddings_fake = evaluator(fake_images)\n    scores.append(frechet_distance(embeddings_real, embeddings_fake))\n    plot_results(fake_images, 16, f'Images Generated for class {i}')\n    plt.show()","bfe6efa5":"pd.Series(scores, name=\"Frechet Distance\")","122e3a32":"# GAN in Action","bafa113f":"# Build the Model\n\nIn DCGANs, convolutional layers are predominantly used to build the generator and discriminator. You will see how the layers are stacked as well as the best practices shown below.\n\n### Generator\n\nFor the generator, we take in random noise and eventually transform it to the shape of the MNIST images. The general steps are:\n\n* Feed the input noise to a dense layer.\n* Reshape the output to have three dimensions. This stands for the (length, width, number of filters).\n* Perform a deconvolution (with Conv2DTranspose), reducing the number of filters by half and using a stride of `2`.\n* The final layer upsamples the features to the size of the training images. In this case 28 x 28 x 1.\n\nNotice that batch normalization is performed except for the final deconvolution layer. As best practice, `selu` is the activation used for the intermediate deconvolution while `tanh` is for the output.\n\n### Discriminator\n\nThe discriminator will use strided convolutions to reduce the dimensionality of the input images. As best practice, these are activated by LeakyRELU. The output features will be flattened and fed to a 1-unit dense layer activated by `sigmoid`.","efab3be1":"## Utilities","cf3d1e20":"# Generated Images","b451fc5b":"# Evaluation","68d58c49":"# Prepare the Dataset","c9068429":"#  Deep Convolutional GAN (DCGAN) -MNIST Augmentation"}}