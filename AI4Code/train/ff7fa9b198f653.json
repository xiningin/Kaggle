{"cell_type":{"93c00063":"code","01148b53":"code","8d8237fc":"code","d498101e":"code","b252b7bc":"code","9bb55bcc":"code","3d6d1975":"code","b627c6fa":"code","895dce67":"code","bd12d79c":"code","98d72027":"code","be40276a":"code","1b52db0f":"code","4c7c3a73":"code","d53a3683":"code","b0818373":"code","625cb85e":"code","247947dc":"code","7cd9f9bc":"code","ae360754":"code","0fed01ed":"markdown","4d8e2502":"markdown","5768ff2f":"markdown","52835b92":"markdown","cab3505a":"markdown","aa74b922":"markdown","fe4f8268":"markdown","634ab4e7":"markdown","324c0141":"markdown","44b123fb":"markdown"},"source":{"93c00063":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","01148b53":"import pandas as pd\n\ndf = pd.read_csv('\/kaggle\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv')","8d8237fc":"df.head()","d498101e":"df = df.drop(['id', 'name', 'host_id', 'neighbourhood_group', 'latitude', 'longitude', 'room_type', 'host_name', 'neighbourhood', 'last_review'], axis=1)","b252b7bc":"df.info()","9bb55bcc":"print(\"Dimension of the data: \", df.shape)\n\nno_of_rows = df.shape[0]\nno_of_columns = df.shape[1]\n\nprint(\"No. of Rows: %d\" % no_of_rows)\nprint(\"No. of Columns: %d\" % no_of_columns)","3d6d1975":"# Import Matplotlib for plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf.hist(bins=50, figsize=(20,15))","b627c6fa":"df.describe()","895dce67":"df = df[~df['price'].isin([0, 1])]b\n","bd12d79c":"#drop row with nan value\ndf = df.dropna()","98d72027":"# Distribution of Grades\n\ndf['price'].describe()","be40276a":"# Variable Correlations with Final Grade\n\ndf.corr()['price'].sort_values(ascending=False)","1b52db0f":"from pandas.plotting import scatter_matrix\n\n\nattributes = [\"availability_365\", \"calculated_host_listings_count\", \"minimum_nights\", \"number_of_reviews\", \"reviews_per_month\"]\n\nscatter_matrix(df[attributes], figsize=(15, 12))","4c7c3a73":"df = df.dropna()\nallData = df\n\n\ny = df['price'] # 1D targer vector\nX = df.drop(columns='price')  # Data Matrix containing all features excluding the target\n\nallData.head()","d53a3683":"from sklearn.model_selection import train_test_split\n\n\n# Split into training\/testing sets with 20% split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=42)","b0818373":"# Calculate mae and rmse\ndef evaluate_predictions(predictions, true):\n    mae = np.mean(abs(predictions - true))\n    rmse = np.sqrt(np.mean((predictions - true) ** 2))\n    \n    return mae, rmse","625cb85e":"# Naive baseline is the median\n\nimport numpy as np\n\n\nmedian_pred = y_train.median()\n\nprint(median_pred)\n\n\n\nmedian_preds = [median_pred for _ in range(len(y_test))]\ntrue = y_test\n\n\n# Display the naive baseline metrics\nmb_mae, mb_rmse = evaluate_predictions(median_preds, true)\nprint('Median Baseline  MAE: {:.4f}'.format(mb_mae))\nprint('Median Baseline RMSE: {:.4f}'.format(mb_rmse))","247947dc":"X = X.values\ny = y.values","7cd9f9bc":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Create linear regression object\nmodel = LinearRegression()\n\n# Train the model using the traing data and label\nmodel.fit(X_train, y_train)\n\n# The intercept\nb = model.intercept_\nprint('Intercept: \\n',b )\n\n# The feature weights (coefficients) in an array\nm = model.coef_\nprint('Coefficients: \\n', m)\n\n\n# Show the prediction formula with the intercept and feature weight values\nprint(\"\\nPrediction Formula: \")\nprint(' y(predicted) = {0} +  x_1 * {1} +  x_2 * {2} + x_3 * {3} + x_4 * {4} + x_5 * {5} + x_6'\n      .format(b, m[0], m[1], m[2], m[3], m[4]))\n\n\n# Make predictions using the test data\ny_predicted = model.predict(X_test)\n\n\n# RMSE of the model\nprint(\"\\nMean squared error: %.2f\"\n      % mean_squared_error(y_test, y_predicted))\n\n\n# Explained variance score: 1 is perfect prediction\nprint('\\nCoefficient of determination r^2 variance score [1 is perfect prediction]: %.2f' % r2_score(y_test, y_predicted))","ae360754":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Create linear regression object\nmodel = LogisticRegression(random_state=0)\n\n# Train the model using the traing data and label\nmodel.fit(X_train, y_train)\n\n# # The intercept\n# b = model.intercept_\n# print('Intercept: \\n',b )\n\n# # The feature weights (coefficients) in an array\n# m = model.coef_\n# print('Coefficients: \\n', m)\n\n\n# # Show the prediction formula with the intercept and feature weight values\n# print(\"\\nPrediction Formula: \")\n# print(' y(predicted) = {0} +  x_1 * {1} +  x_2 * {2} + x_3 * {3} + x_4 * {4} + x_5 * {5} + x_6'\n#       .format(b, m[0], m[1], m[2], m[3], m[4]))\n\n\n# Make predictions using the test data\ny_predicted = model.predict(X_test)\n\n\n# RMSE of the model\nprint(\"\\nMean squared error: %.2f\"\n      % mean_squared_error(y_test, y_predicted))\n\n\n# Explained variance score: 1 is perfect prediction\nprint('\\nCoefficient of determination r^2 variance score [1 is perfect prediction]: %.2f' % r2_score(y_test, y_predicted))","0fed01ed":"\n## 14. Looking for Numerical Correlations with the Target Column\n\nSince the dataset is not too large, we can easily compute the standard correlation coefficient (also called Pearson\u2019s r) between every pair of attributes using DataFrame's corr() method.","4d8e2502":"\n## 5. Dimension the Data\n\nWe need to know the dimension (number of rows and columns) of the data using DataFrame's shape method.","5768ff2f":"### Logistic","52835b92":"### Drop non-NA or 0 data ","cab3505a":"\n3. Quick Check of the Data\nLet\u2019s take a look at the top five rows using the DataFrame\u2019s head() method.","aa74b922":"## 7. Explore the Data","fe4f8268":"## Explore target column","634ab4e7":"### Visualize numerical column","324c0141":"### Naive baseline","44b123fb":"\n## 4. Description of the Data\n\nDataFrame\u2019s info() method is useful to get a quick description of the data, in particular the total number of rows, and each attribute\u2019s type and number of non-null values"}}