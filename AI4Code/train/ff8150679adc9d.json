{"cell_type":{"52b4fb3d":"code","599b0c1b":"code","b6e51b78":"code","01cdd2ce":"code","36db627d":"code","1d651dcf":"code","dfdb39e1":"code","354f904f":"code","1a22a6bd":"code","42879a6b":"code","221bad71":"code","d9ebf2eb":"code","7bcca538":"code","324c05d4":"code","69b7b36c":"code","17b9b6f7":"code","7827adaf":"code","1436844d":"code","2eafc421":"code","3ecd286f":"code","c8894a1b":"code","af88dcba":"code","fa1bb024":"code","dafdd567":"code","bd0bf1b0":"code","ee5f47f3":"code","0ca51ce0":"code","6d289844":"code","cb6f19ed":"code","861f55ec":"code","1403220c":"code","9677f5ef":"code","40bd7264":"code","b9555d26":"code","69c7e46e":"code","043db83a":"code","f67cf60e":"code","5e1bab8c":"code","0e3e6745":"code","05a83630":"code","c9e3cd2c":"code","24a604a4":"code","d4b20b3d":"markdown","871cabb9":"markdown","75667f89":"markdown","ae9967c6":"markdown","6f02b99f":"markdown","6cddf66f":"markdown","76999293":"markdown","9749d301":"markdown","ee48f91b":"markdown","d965c746":"markdown","c6605573":"markdown","5182be74":"markdown","c0f61336":"markdown","ecdeaa2a":"markdown","b3fa74e4":"markdown","9071e864":"markdown","9e839712":"markdown","bd918fa9":"markdown","da66375a":"markdown","537f3536":"markdown","ff563c88":"markdown","fc386523":"markdown","64cc3b6d":"markdown","001ebf37":"markdown","b3c8a6e9":"markdown","6a971b15":"markdown","12eb343a":"markdown","5f48e381":"markdown","8f23dddd":"markdown","ab567e84":"markdown","c040cb7b":"markdown","1b17c58e":"markdown","4b296994":"markdown","8342e921":"markdown","b1309400":"markdown","13200962":"markdown"},"source":{"52b4fb3d":"import sys\nimport os\nimport glob\nimport random\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nimport cv2\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid import ImageGrid\n\nplt.style.use(\"dark_background\")","599b0c1b":"# Path to all data\nDATA_PATH = \"\/kaggle\/input\/lgg-mri-segmentation\/kaggle_3m\/\"\n\n# File path line length images for later sorting\nBASE_LEN = 89 # len(\/kaggle\/input\/lgg-mri-segmentation\/kaggle_3m\/TCGA_DU_6404_19850629\/TCGA_DU_6404_19850629_ <-!!!43.tif)\nEND_IMG_LEN = 4 # len(\/kaggle\/input\/lgg-mri-segmentation\/kaggle_3m\/TCGA_DU_6404_19850629\/TCGA_DU_6404_19850629_43 !!!->.tif)\nEND_MASK_LEN = 9 # (\/kaggle\/input\/lgg-mri-segmentation\/kaggle_3m\/TCGA_DU_6404_19850629\/TCGA_DU_6404_19850629_43 !!!->_mask.tif)\n\n# img size\nIMG_SIZE = 512","b6e51b78":"# Raw data\ndata_map = []\nfor sub_dir_path in glob.glob(DATA_PATH+\"*\"):\n    if os.path.isdir(sub_dir_path):\n        dirname = sub_dir_path.split(\"\/\")[-1]\n        for filename in os.listdir(sub_dir_path):\n            image_path = sub_dir_path + \"\/\" + filename\n            data_map.extend([dirname, image_path])\n    else:\n        print(\"This is not a dir:\", sub_dir_path)\n        \n        \ndf = pd.DataFrame({\"dirname\" : data_map[::2],\n                  \"path\" : data_map[1::2]})\ndf.head()","01cdd2ce":"# Masks\/Not masks\ndf_imgs = df[~df['path'].str.contains(\"mask\")]\ndf_masks = df[df['path'].str.contains(\"mask\")]\n\n# Data sorting\nimgs = sorted(df_imgs[\"path\"].values, key=lambda x : int(x[BASE_LEN:-END_IMG_LEN]))\nmasks = sorted(df_masks[\"path\"].values, key=lambda x : int(x[BASE_LEN:-END_MASK_LEN]))\n\n# Sorting check\nidx = random.randint(0, len(imgs)-1)\nprint(\"Path to the Image:\", imgs[idx], \"\\nPath to the Mask:\", masks[idx])","36db627d":"# Final dataframe\ndf = pd.DataFrame({\"patient\": df_imgs.dirname.values,\n                       \"image_path\": imgs,\n                   \"mask_path\": masks})\n\n\n# Adding A\/B column for diagnosis\ndef positiv_negativ_diagnosis(mask_path):\n    value = np.max(cv2.imread(mask_path))\n    if value > 0 : return 1\n    else: return 0\n\ndf[\"diagnosis\"] = df[\"mask_path\"].apply(lambda m: positiv_negativ_diagnosis(m))\ndf","1d651dcf":"# Plot\nax = df.diagnosis.value_counts().plot(kind='bar',\n                                      stacked=True,\n                                      figsize=(10, 6),\n                                     color=[\"violet\", \"lightseagreen\"])\n\n\nax.set_xticklabels([\"Positive\", \"Negative\"], rotation=45, fontsize=12);\nax.set_ylabel('Total Images', fontsize = 12)\nax.set_title(\"Distribution of data grouped by diagnosis\",fontsize = 18, y=1.05)\n\n# Annotate\nfor i, rows in enumerate(df.diagnosis.value_counts().values):\n    ax.annotate(int(rows), xy=(i, rows-12), \n                rotation=0, color=\"white\", \n                ha=\"center\", verticalalignment='bottom', \n                fontsize=15, fontweight=\"bold\")\n    \nax.text(1.2, 2550, f\"Total {len(df)} images\", size=15,\n        color=\"black\",\n         ha=\"center\", va=\"center\",\n         bbox=dict(boxstyle=\"round\",\n                   fc=(\"lightblue\"),\n                   ec=(\"black\"),\n                   )\n         );","dfdb39e1":"# Data\npatients_by_diagnosis = df.groupby(['patient', 'diagnosis'])['diagnosis'].size().unstack().fillna(0)\npatients_by_diagnosis.columns = [\"Positive\", \"Negative\"]\n\n# Plot\nax = patients_by_diagnosis.plot(kind='bar',stacked=True,\n                                figsize=(18, 10),\n                                color=[\"mediumvioletred\", \"springgreen\"], \n                                alpha=0.9)\nax.legend(fontsize=20);\nax.set_xlabel('Patients',fontsize = 20)\nax.set_ylabel('Total Images', fontsize = 20)\nax.set_title(\"Distribution of data grouped by patient and diagnosis\",fontsize = 25, y=1.005)\n\n# Annotations\n\"\"\"for i, rows in enumerate(patients_by_diagnosis.values):\n    plt.annotate(int(rows[0]), xy=(i, rows[0]+1), rotation=90, color=\"white\")\n    plt.annotate(int(rows[1]), xy=(i, rows[1]+1), rotation=90, color=\"aqua\")\"\"\";","354f904f":"# Data\nsample_yes_df = df[df[\"diagnosis\"] == 1].sample(5).image_path.values\nsample_no_df = df[df[\"diagnosis\"] == 0].sample(5).image_path.values\n\nsample_imgs = []\nfor i, (yes, no) in enumerate(zip(sample_yes_df, sample_no_df)):\n    yes = cv2.resize(cv2.imread(yes), (IMG_SIZE, IMG_SIZE))\n    no = cv2.resize(cv2.imread(no), (IMG_SIZE, IMG_SIZE))\n    sample_imgs.extend([yes, no])\n\n\nsample_yes_arr = np.vstack(np.array(sample_imgs[::2]))\nsample_no_arr = np.vstack(np.array(sample_imgs[1::2]))\n\n# Plot\nfig = plt.figure(figsize=(25., 25.))\ngrid = ImageGrid(fig, 111,  # similar to subplot(111)\n                 nrows_ncols=(1, 4),  # creates 2x2 grid of axes\n                 axes_pad=0.1,  # pad between axes in inch.\n                 )\n\n\ngrid[0].imshow(sample_yes_arr)\ngrid[0].set_title(\"Positive\", fontsize=15)\ngrid[0].axis(\"off\")\ngrid[1].imshow(sample_no_arr)\ngrid[1].set_title(\"Negative\", fontsize=15)\ngrid[1].axis(\"off\")\n\ngrid[2].imshow(sample_yes_arr[:,:,0], cmap=\"hot\")\ngrid[2].set_title(\"Positive\", fontsize=15)\ngrid[2].axis(\"off\")\ngrid[3].imshow(sample_no_arr[:,:,0], cmap=\"hot\")\ngrid[3].set_title(\"Negative\", fontsize=15)\ngrid[3].axis(\"off\")#set_title(\"No\", fontsize=15)\n\n# annotations\nplt.figtext(0.36,0.90,\"Original\", va=\"center\", ha=\"center\", size=20)\nplt.figtext(0.66,0.90,\"With hot colormap\", va=\"center\", ha=\"center\", size=20)\nplt.suptitle(\"Brain MRI Images for Brain Tumor Detection\\nLGG Segmentation Dataset\", y=.95, fontsize=30, weight=\"bold\")\n\n# save and show\nplt.savefig(\"dataset.png\", bbox_inches='tight', pad_inches=0.2, transparent=True)\nplt.show()","1a22a6bd":"# Data\nsample_df = df[df[\"diagnosis\"] == 1].sample(5).values\nsample_imgs = []\nfor i, data in enumerate(sample_df):\n    #print(data)\n    img = cv2.resize(cv2.imread(data[1]), (IMG_SIZE, IMG_SIZE))\n    mask = cv2.resize(cv2.imread(data[2]), (IMG_SIZE, IMG_SIZE))\n    sample_imgs.extend([img, mask])\n\n\nsample_imgs_arr = np.hstack(np.array(sample_imgs[::2]))\nsample_masks_arr = np.hstack(np.array(sample_imgs[1::2]))\n\n# Plot\nfig = plt.figure(figsize=(25., 25.))\ngrid = ImageGrid(fig, 111,  # similar to subplot(111)\n                 nrows_ncols=(2, 1),  # creates 2x2 grid of axes\n                 axes_pad=0.1,  # pad between axes in inch.\n                 )\n\n\ngrid[0].imshow(sample_imgs_arr)\ngrid[0].set_title(\"Images\", fontsize=15)\ngrid[0].axis(\"off\")\ngrid[1].imshow(sample_masks_arr)\ngrid[1].set_title(\"Masks\", fontsize=15, y=0.9)\ngrid[1].axis(\"off\")\nplt.show()","42879a6b":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensor, ToTensorV2\n\nfrom sklearn.model_selection import train_test_split","221bad71":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","d9ebf2eb":"class BrainMriDataset(Dataset):\n    def __init__(self, df, transforms):\n        \n        self.df = df\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        image = cv2.imread(self.df.iloc[idx, 1])\n        mask = cv2.imread(self.df.iloc[idx, 2], 0)\n\n        augmented = self.transforms(image=image, \n                                    mask=mask)\n \n        image = augmented['image']\n        mask = augmented['mask']   \n        \n        return image, mask\n    \n        # unnormilize mask\n        #mask = torch.clamp(mask.float(), min=0, max=1)\n        #mask = torch.ceil(mask)       ","7bcca538":"PATCH_SIZE = 128#256\n\nstrong_transforms = A.Compose([\n    A.RandomResizedCrop(width = PATCH_SIZE, height = PATCH_SIZE, p=1.0),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.RandomRotate90(p=0.5),\n    A.Transpose(p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.01, scale_limit=0.04, rotate_limit=0, p=0.25),\n    \n    # Pixels\n    A.RandomBrightnessContrast(p=0.5),\n    A.RandomGamma(p=0.25),\n    A.IAAEmboss(p=0.25),\n    A.Blur(p=0.01, blur_limit = 3),\n    \n    # Affine\n    A.OneOf([\n        A.ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n        A.GridDistortion(p=0.5),\n        A.OpticalDistortion(p=1, distort_limit=2, shift_limit=0.5)                  \n    ], p=0.8),\n    \n    \n    A.Normalize(p=1.0),\n    #https:\/\/albumentations.readthedocs.io\/en\/latest\/api\/pytorch.html?highlight=ToTensor#albumentations.pytorch.transforms.ToTensor\n    ToTensor(),\n])\n\n\ntransforms = A.Compose([\n    A.Resize(width = PATCH_SIZE, height = PATCH_SIZE, p=1.0),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.RandomRotate90(p=0.5),\n    A.Transpose(p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.01, scale_limit=0.04, rotate_limit=0, p=0.25),\n\n    \n    \n    A.Normalize(p=1.0),\n    ToTensor(),\n])","324c05d4":"# Split df into train_df and val_df\ntrain_df, val_df = train_test_split(df, stratify=df.diagnosis, test_size=0.1)\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\n\n# Split train_df into train_df and test_df\ntrain_df, test_df = train_test_split(train_df, stratify=train_df.diagnosis, test_size=0.15)\ntrain_df = train_df.reset_index(drop=True)\n\n#train_df = train_df[:1000]\nprint(f\"Train: {train_df.shape} \\nVal: {val_df.shape} \\nTest: {test_df.shape}\")","69b7b36c":"# train\ntrain_dataset = BrainMriDataset(df=train_df, transforms=transforms)\ntrain_dataloader = DataLoader(train_dataset, batch_size=26, num_workers=4, shuffle=True)\n\n# val\nval_dataset = BrainMriDataset(df=val_df, transforms=transforms)\nval_dataloader = DataLoader(val_dataset, batch_size=26, num_workers=4, shuffle=True)\n\n#test\ntest_dataset = BrainMriDataset(df=test_df, transforms=transforms)\ntest_dataloader = DataLoader(test_dataset, batch_size=26, num_workers=4, shuffle=True)","17b9b6f7":"def show_aug(inputs, nrows=5, ncols=5, image=True):\n    plt.figure(figsize=(10, 10))\n    plt.subplots_adjust(wspace=0., hspace=0.)\n    i_ = 0\n    \n    if len(inputs) > 25:\n        inputs = inputs[:25]\n        \n    for idx in range(len(inputs)):\n    \n        # normalization\n        if image is True:           \n            img = inputs[idx].numpy().transpose(1,2,0)\n            mean = [0.485, 0.456, 0.406]\n            std = [0.229, 0.224, 0.225] \n            img = (img*std+mean).astype(np.float32)\n        else:\n            img = inputs[idx].numpy().astype(np.float32)\n            img = img[0,:,:]\n        \n        #plot\n        #print(img.max(), len(np.unique(img)))\n        plt.subplot(nrows, ncols, i_+1)\n        plt.imshow(img); \n        plt.axis('off')\n \n        i_ += 1\n        \n    return plt.show()\n\n    \nimages, masks = next(iter(train_dataloader))\nprint(images.shape, masks.shape)\n\nshow_aug(images)\nshow_aug(masks, image=False)","7827adaf":"def double_conv(in_channels, out_channels):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n        nn.ReLU(inplace=True))","1436844d":"class UNet(nn.Module):\n\n    def __init__(self, n_classes):\n        super().__init__()\n                \n        self.conv_down1 = double_conv(3, 64)\n        self.conv_down2 = double_conv(64, 128)\n        self.conv_down3 = double_conv(128, 256)\n        self.conv_down4 = double_conv(256, 512)        \n\n        self.maxpool = nn.MaxPool2d(2)\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n        \n        self.conv_up3 = double_conv(256 + 512, 256)\n        self.conv_up2 = double_conv(128 + 256, 128)\n        self.conv_up1 = double_conv(128 + 64, 64)\n        \n        self.last_conv = nn.Conv2d(64, n_classes, kernel_size=1)\n        \n        \n    def forward(self, x):\n        # Batch - 1d tensor.  N_channels - 1d tensor, IMG_SIZE - 2d tensor.\n        # Example: x.shape >>> (10, 3, 256, 256).\n        \n        conv1 = self.conv_down1(x)  # <- BATCH, 3, IMG_SIZE  -> BATCH, 64, IMG_SIZE..\n        x = self.maxpool(conv1)     # <- BATCH, 64, IMG_SIZE -> BATCH, 64, IMG_SIZE 2x down.\n        conv2 = self.conv_down2(x)  # <- BATCH, 64, IMG_SIZE -> BATCH,128, IMG_SIZE.\n        x = self.maxpool(conv2)     # <- BATCH, 128, IMG_SIZE -> BATCH, 128, IMG_SIZE 2x down.\n        conv3 = self.conv_down3(x)  # <- BATCH, 128, IMG_SIZE -> BATCH, 256, IMG_SIZE.\n        x = self.maxpool(conv3)     # <- BATCH, 256, IMG_SIZE -> BATCH, 256, IMG_SIZE 2x down.\n        x = self.conv_down4(x)      # <- BATCH, 256, IMG_SIZE -> BATCH, 512, IMG_SIZE.\n        x = self.upsample(x)        # <- BATCH, 512, IMG_SIZE -> BATCH, 512, IMG_SIZE 2x up.\n        \n        #(Below the same)                                 N this       ==        N this.  Because the first N is upsampled.\n        x = torch.cat([x, conv3], dim=1) # <- BATCH, 512, IMG_SIZE & BATCH, 256, IMG_SIZE--> BATCH, 768, IMG_SIZE.\n        \n        x = self.conv_up3(x) #  <- BATCH, 768, IMG_SIZE --> BATCH, 256, IMG_SIZE. \n        x = self.upsample(x)  #  <- BATCH, 256, IMG_SIZE -> BATCH,  256, IMG_SIZE 2x up.   \n        x = torch.cat([x, conv2], dim=1) # <- BATCH, 256,IMG_SIZE & BATCH, 128, IMG_SIZE --> BATCH, 384, IMG_SIZE.  \n\n        x = self.conv_up2(x) # <- BATCH, 384, IMG_SIZE --> BATCH, 128 IMG_SIZE. \n        x = self.upsample(x)   # <- BATCH, 128, IMG_SIZE --> BATCH, 128, IMG_SIZE 2x up.     \n        x = torch.cat([x, conv1], dim=1) # <- BATCH, 128, IMG_SIZE & BATCH, 64, IMG_SIZE --> BATCH, 192, IMG_SIZE.  \n        \n        x = self.conv_up1(x) # <- BATCH, 128, IMG_SIZE --> BATCH, 64, IMG_SIZE.\n        \n        out = self.last_conv(x) # <- BATCH, 64, IMG_SIZE --> BATCH, n_classes, IMG_SIZE.\n        out = torch.sigmoid(out)\n        \n        return out","2eafc421":"unet = UNet(n_classes=1).to(device)\noutput = unet(torch.randn(1,3,256,256).to(device))\nprint(\"\",output.shape)","3ecd286f":"class ConvReluUpsample(nn.Module):\n    def __init__(self, in_channels, out_channels, upsample=False):\n        super().__init__()\n        self.upsample = upsample\n        self.make_upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        \n        self.block = nn.Sequential(\n            nn.Conv2d(\n                in_channels, out_channels, (3, 3), stride=1, padding=1, bias=False\n            ),\n            nn.GroupNorm(32, out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        x = self.block(x)\n        if self.upsample:\n            x = self.make_upsample(x)\n        return x\n\n\n\nclass SegmentationBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, n_upsamples=0):\n        super().__init__()\n\n        blocks = [ConvReluUpsample(in_channels, out_channels, upsample=bool(n_upsamples))]\n\n        if n_upsamples > 1:\n            for _ in range(1, n_upsamples):\n                blocks.append(ConvReluUpsample(out_channels, out_channels, upsample=True))\n\n        self.block = nn.Sequential(*blocks)\n\n    def forward(self, x):\n        return self.block(x)","c8894a1b":"class FPN(nn.Module):\n\n    def __init__(self, n_classes=1, \n                 pyramid_channels=256, \n                 segmentation_channels=256):\n        super().__init__()\n         \n        # Bottom-up layers\n        self.conv_down1 = double_conv(3, 64)\n        self.conv_down2 = double_conv(64, 128)\n        self.conv_down3 = double_conv(128, 256)\n        self.conv_down4 = double_conv(256, 512)        \n        self.conv_down5 = double_conv(512, 1024)   \n        self.maxpool = nn.MaxPool2d(2)\n        \n        # Top layer\n        self.toplayer = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)  # Reduce channels\n\n        # Smooth layers\n        self.smooth1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        self.smooth3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n\n        # Lateral layers\n        self.latlayer1 = nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0)\n        self.latlayer2 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)\n        self.latlayer3 = nn.Conv2d(128, 256, kernel_size=1, stride=1, padding=0)\n        \n        # Segmentation block layers\n        self.seg_blocks = nn.ModuleList([\n            SegmentationBlock(pyramid_channels, segmentation_channels, n_upsamples=n_upsamples)\n            for n_upsamples in [0, 1, 2, 3]\n        ])\n        \n        # Last layer\n        self.last_conv = nn.Conv2d(256, n_classes, kernel_size=1, stride=1, padding=0)\n        \n    def upsample_add(self, x, y):\n        _,_,H,W = y.size()\n        upsample = nn.Upsample(size=(H,W), mode='bilinear', align_corners=True) \n        \n        return upsample(x) + y\n    \n    def upsample(self, x, h, w):\n        sample = nn.Upsample(size=(h, w), mode='bilinear', align_corners=True)\n        return sample(x)\n        \n    def forward(self, x):\n        \n        # Bottom-up\n        c1 = self.maxpool(self.conv_down1(x))\n        c2 = self.maxpool(self.conv_down2(c1))\n        c3 = self.maxpool(self.conv_down3(c2))\n        c4 = self.maxpool(self.conv_down4(c3))\n        c5 = self.maxpool(self.conv_down5(c4)) \n        \n        # Top-down\n        p5 = self.toplayer(c5) \n        p4 = self.upsample_add(p5, self.latlayer1(c4)) \n        p3 = self.upsample_add(p4, self.latlayer2(c3))\n        p2 = self.upsample_add(p3, self.latlayer3(c2)) \n        \n        # Smooth\n        p4 = self.smooth1(p4)\n        p3 = self.smooth2(p3)\n        p2 = self.smooth3(p2)\n        \n        # Segmentation\n        _, _, h, w = p2.size()\n        feature_pyramid = [seg_block(p) for seg_block, p in zip(self.seg_blocks, [p2, p3, p4, p5])]\n        \n        out = self.upsample(self.last_conv(sum(feature_pyramid)), 4 * h, 4 * w)\n        \n        out = torch.sigmoid(out)\n        return out","af88dcba":"fpn = FPN().to(device)\noutput = fpn(torch.randn(1,3,256,256).to(device))\nprint(output.shape)","fa1bb024":"from torchvision.models import resnext50_32x4d\n\nclass ConvRelu(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel, padding):\n        super().__init__()\n\n        self.convrelu = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.convrelu(x)\n        return x\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        \n        self.conv1 = ConvRelu(in_channels, in_channels \/\/ 4, 1, 0)\n        \n        self.deconv = nn.ConvTranspose2d(in_channels \/\/ 4, in_channels \/\/ 4, kernel_size=4,\n                                          stride=2, padding=1, output_padding=0)\n        \n        self.conv2 = ConvRelu(in_channels \/\/ 4, out_channels, 1, 0)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.deconv(x)\n        x = self.conv2(x)\n\n        return x","dafdd567":"class ResNeXtUNet(nn.Module):\n\n    def __init__(self, n_classes):\n        super().__init__()\n        \n        self.base_model = resnext50_32x4d(pretrained=True)\n        self.base_layers = list(self.base_model.children())\n        filters = [4*64, 4*128, 4*256, 4*512]\n        \n        # Down\n        self.encoder0 = nn.Sequential(*self.base_layers[:3])\n        self.encoder1 = nn.Sequential(*self.base_layers[4])\n        self.encoder2 = nn.Sequential(*self.base_layers[5])\n        self.encoder3 = nn.Sequential(*self.base_layers[6])\n        self.encoder4 = nn.Sequential(*self.base_layers[7])\n\n        # Up\n        self.decoder4 = DecoderBlock(filters[3], filters[2])\n        self.decoder3 = DecoderBlock(filters[2], filters[1])\n        self.decoder2 = DecoderBlock(filters[1], filters[0])\n        self.decoder1 = DecoderBlock(filters[0], filters[0])\n\n        # Final Classifier\n        self.last_conv0 = ConvRelu(256, 128, 3, 1)\n        self.last_conv1 = nn.Conv2d(128, n_classes, 3, padding=1)\n                       \n        \n    def forward(self, x):\n        # Down\n        x = self.encoder0(x)\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(e1)\n        e3 = self.encoder3(e2)\n        e4 = self.encoder4(e3)\n\n        # Up + sc\n        d4 = self.decoder4(e4) + e3\n        d3 = self.decoder3(d4) + e2\n        d2 = self.decoder2(d3) + e1\n        d1 = self.decoder1(d2)\n        #print(d1.shape)\n\n        # final classifier\n        out = self.last_conv0(d1)\n        out = self.last_conv1(out)\n        out = torch.sigmoid(out)\n        \n        return out","bd0bf1b0":"rx50 = ResNeXtUNet(n_classes=1).to(device)\noutput = rx50(torch.randn(1,3,256,256).to(device))\nprint(output.shape)","ee5f47f3":"def dice_coef_metric(inputs, target):\n    intersection = 2.0 * (target * inputs).sum()\n    union = target.sum() + inputs.sum()\n    if target.sum() == 0 and inputs.sum() == 0:\n        return 1.0\n\n    return intersection \/ union\n\n# Metric check\ndice_coef_metric(np.array([0., 0.9]), \n                 np.array([0., 1]))","0ca51ce0":"def dice_coef_loss(inputs, target):\n    smooth = 1.0\n    intersection = 2.0 * ((target * inputs).sum()) + smooth\n    union = target.sum() + inputs.sum() + smooth\n\n    return 1 - (intersection \/ union)\n\n\ndef bce_dice_loss(inputs, target):\n    dicescore = dice_coef_loss(inputs, target)\n    bcescore = nn.BCELoss()\n    bceloss = bcescore(inputs, target)\n\n    return bceloss + dicescore\n\n# loss check\nbce_dice_loss(torch.tensor([0.7, 1., 1.]), \n              torch.tensor([1.,1.,1.]))","6d289844":"def train_model(model_name, model, train_loader, val_loader, train_loss, optimizer, lr_scheduler, num_epochs):  \n    \n    print(model_name)\n    loss_history = []\n    train_history = []\n    val_history = []\n\n    for epoch in range(num_epochs):\n        model.train() # Enter train mode\n        \n        losses = []\n        train_iou = []\n                \n        if lr_scheduler:\n            \n            warmup_factor = 1.0 \/ 100\n            warmup_iters = min(100, len(train_loader) - 1)\n            lr_scheduler = warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n        \n        \n        for i_step, (data, target) in enumerate(train_loader):\n            data = data.to(device)\n            target = target.to(device)\n                      \n            outputs = model(data)\n            \n            out_cut = np.copy(outputs.data.cpu().numpy())\n            out_cut[np.nonzero(out_cut < 0.5)] = 0.0\n            out_cut[np.nonzero(out_cut >= 0.5)] = 1.0\n            \n            train_dice = dice_coef_metric(out_cut, target.data.cpu().numpy())\n            \n            loss = train_loss(outputs, target)\n            \n            losses.append(loss.item())\n            train_iou.append(train_dice)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n    \n            if lr_scheduler:\n                lr_scheduler.step()\n \n        #torch.save(model.state_dict(), f'{model_name}_{str(epoch)}_epoch.pt')\n        val_mean_iou = compute_iou(model, val_loader)\n        \n        loss_history.append(np.array(losses).mean())\n        train_history.append(np.array(train_iou).mean())\n        val_history.append(val_mean_iou)\n        \n        print(\"Epoch [%d]\" % (epoch))\n        print(\"Mean loss on train:\", np.array(losses).mean(), \n              \"\\nMean DICE on train:\", np.array(train_iou).mean(), \n              \"\\nMean DICE on validation:\", val_mean_iou)\n        \n    return loss_history, train_history, val_history\n\n\ndef compute_iou(model, loader, threshold=0.3):\n    \"\"\"\n    Computes accuracy on the dataset wrapped in a loader\n    \n    Returns: accuracy as a float value between 0 and 1\n    \"\"\"\n    #model.eval()\n    valloss = 0\n    \n    with torch.no_grad():\n\n        for i_step, (data, target) in enumerate(loader):\n            \n            data = data.to(device)\n            target = target.to(device)\n            #prediction = model(x_gpu)\n            \n            outputs = model(data)\n           # print(\"val_output:\", outputs.shape)\n\n            out_cut = np.copy(outputs.data.cpu().numpy())\n            out_cut[np.nonzero(out_cut < threshold)] = 0.0\n            out_cut[np.nonzero(out_cut >= threshold)] = 1.0\n\n            picloss = dice_coef_metric(out_cut, target.data.cpu().numpy())\n            valloss += picloss\n\n        #print(\"Threshold:  \" + str(threshold) + \"  Validation DICE score:\", valloss \/ i_step)\n\n    return valloss \/ i_step","cb6f19ed":"# Optimizers\nunet_optimizer = torch.optim.Adamax(unet.parameters(), lr=1e-3)\nfpn_optimizer = torch.optim.Adamax(fpn.parameters(), lr=1e-3)\nrx50_optimizer = torch.optim.Adam(rx50.parameters(), lr=5e-4)\n\n# lr_scheduler\ndef warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):\n    def f(x):\n        if x >= warmup_iters:\n            return 1\n        alpha = float(x) \/ warmup_iters\n        return warmup_factor * (1 - alpha) + alpha\n\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)","861f55ec":"%%time\nnum_ep = 10                                                                                                  \n# Train UNet\n#unet_lh, unet_th, unet_vh = train_model(\"Vanila_UNet\", unet, train_dataloader, val_dataloader, bce_dice_loss, unet_optimizer, False, 20) \n\n# Train FPN\n#fpn_lh, fpn_th, fpn_vh = train_model(\"FPN\", fpn, train_dataloader, val_dataloader, bce_dice_loss, fpn_optimizer, False, 20)#\n\n# Train ResNeXt50\nrx50_lh, rx50_th, rx50_vh = train_model(\"ResNeXt50\", rx50, train_dataloader, val_dataloader, bce_dice_loss, rx50_optimizer, False, num_ep)","1403220c":"def plot_model_history(model_name,\n                        train_history, val_history, \n                        num_epochs):\n    \n    x = np.arange(num_epochs)\n\n    fig = plt.figure(figsize=(10, 6))\n    plt.plot(x, train_history, label='train dice', lw=3, c=\"springgreen\")\n    plt.plot(x, val_history, label='validation dice', lw=3, c=\"deeppink\")\n\n    plt.title(f\"{model_name}\", fontsize=15)\n    plt.legend(fontsize=12)\n    plt.xlabel(\"Epoch\", fontsize=15)\n    plt.ylabel(\"DICE\", fontsize=15)\n\n    fn = str(int(time.time())) + \".png\"\n    plt.show()\n    #plt.savefig(fn, bbox_inches='tight', pad_inches=0.2)\n    #plt.close()\n    ","9677f5ef":"#plot_model_history(\"Vanilla UNet\", unet_th, unet_vh, 20)\n#plot_model_history(\"FPN\", fpn_th, fpn_vh, 20)\nplot_model_history(\"UNet with ResNeXt50 backbone\", rx50_th, rx50_vh, num_ep)","40bd7264":"#test_iou = compute_iou(unet, test_dataloader)\n#print(f\"\"\"Vanilla UNet\\nMean IoU of the test images - {np.around(test_iou, 2)*100}%\"\"\")\n\n#test_iou = compute_iou(fpn, test_dataloader)\n#print(f\"\"\"FPN\\nMean IoU of the test images - {np.around(test_iou, 2)*100}%\"\"\")\n\ntest_iou = compute_iou(rx50, test_dataloader)\nprint(f\"\"\"ResNext50\\nMean IoU of the test images - {np.around(test_iou, 2)*100}%\"\"\")","b9555d26":"# image\ntest_sample = test_df[test_df[\"diagnosis\"] == 1].sample(1).values[0]\nimage = cv2.resize(cv2.imread(test_sample[1]), (128, 128))\n\n#mask\nmask = cv2.resize(cv2.imread(test_sample[2]), (128, 128))\n\n# pred\npred = torch.tensor(image.astype(np.float32) \/ 255.).unsqueeze(0).permute(0,3,1,2)\npred = rx50(pred.to(device))\npred = pred.detach().cpu().numpy()[0,0,:,:]\n\n# pred with tshd\npred_t = np.copy(pred)\npred_t[np.nonzero(pred_t < 0.3)] = 0.0\npred_t[np.nonzero(pred_t >= 0.3)] = 255.#1.0\npred_t = pred_t.astype(\"uint8\")\n\n# plot\nfig, ax = plt.subplots(nrows=2,  ncols=2, figsize=(10, 10))\n\nax[0, 0].imshow(image)\nax[0, 0].set_title(\"image\")\nax[0, 1].imshow(mask)\nax[0, 1].set_title(\"mask\")\nax[1, 0].imshow(pred)\nax[1, 0].set_title(\"prediction\")\nax[1, 1].imshow(pred_t)\nax[1, 1].set_title(\"prediction with threshold\")\nplt.show()","69c7e46e":"test_samples = test_df[test_df[\"diagnosis\"] == 1].sample(105).values\n\n\ndef batch_preds_overlap(model, samples):\n    \"\"\"\n    Computes prediction on the dataset\n    \n    Returns: list with images overlapping with predictions\n    \n    \"\"\"\n    prediction_overlap = []\n    #model.eval():\n    for test_sample in samples:\n\n         # sample\n        image = cv2.resize(cv2.imread(test_sample[1]),(128, 128))\n        image =  image \/ 255.\n\n        # gt\n        ground_truth = cv2.resize(cv2.imread(test_sample[2], 0), (128, 128)).astype(\"uint8\")\n\n        # pred\n        prediction = torch.tensor(image).unsqueeze(0).permute(0,3,1,2)\n        prediction = model(prediction.to(device).float())\n        prediction = prediction.detach().cpu().numpy()[0,0,:,:]\n\n        prediction[np.nonzero(prediction < 0.3)] = 0.0\n        prediction[np.nonzero(prediction >= 0.3)] = 255.#1.0\n        prediction = prediction.astype(\"uint8\")\n\n        # overlap \n        original_img = cv2.resize(cv2.imread(test_sample[1]),(128, 128))\n\n        _, thresh_gt = cv2.threshold(ground_truth, 127, 255, 0)\n        _, thresh_p = cv2.threshold(prediction, 127, 255, 0)\n        contours_gt, _ = cv2.findContours(thresh_gt, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n        contours_p, _ = cv2.findContours(thresh_p, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n        overlap_img = cv2.drawContours(original_img, contours_gt, 0, (0, 255, 0), 1)\n        overlap_img = cv2.drawContours(overlap_img, contours_p, 0, (255,36,0), 1)#255,0,0\n        prediction_overlap.append(overlap_img)\n\n    return prediction_overlap\n    \n    \n#prediction_overlap_u = batch_preds_overlap(unet, test_samples)\n#prediction_overlap_f = batch_preds_overlap(fpn, test_samples)\nprediction_overlap_r = batch_preds_overlap(rx50, test_samples)","043db83a":"# DATA\n\n# unet plates\n#pred_overlap_5x1_u = []\n#pred_overlap_5x3_u = []\n\n# fpn plates\n#pred_overlap_5x1_f = []\n#pred_overlap_5x3_f = []\n\n# rx plates\npred_overlap_5x1_r = []\npred_overlap_5x3_r = []\n\nfor i in range(5, 105+5, 5):\n    #pred_overlap_5x1_u.append(np.hstack(np.array(prediction_overlap_u[i-5:i])))\n    #pred_overlap_5x1_f.append(np.hstack(np.array(prediction_overlap_f[i-5:i])))\n    pred_overlap_5x1_r.append(np.hstack(np.array(prediction_overlap_r[i-5:i])))\n\nfor i in range(3, 21+3, 3):\n   #pred_overlap_5x3_u.append(np.vstack(pred_overlap_5x1_u[i-3:i]))\n   #pred_overlap_5x3_f.append(np.vstack(pred_overlap_5x1_f[i-3:i]))\n    pred_overlap_5x3_r.append(np.vstack(pred_overlap_5x1_r[i-3:i]))","f67cf60e":"# PLOT\n\ndef plot_plate_overlap(batch_preds, title, num):\n    plt.figure(figsize=(15, 15))\n    plt.imshow(batch_preds)\n    plt.axis(\"off\")\n\n    plt.figtext(0.76,0.75,\"Green - Ground Truth\", va=\"center\", ha=\"center\", size=20,color=\"lime\");\n    plt.figtext(0.26,0.75,\"Red - Prediction\", va=\"center\", ha=\"center\", size=20, color=\"#ff0d00\");\n    plt.suptitle(title, y=.80, fontsize=20, weight=\"bold\", color=\"#00FFDE\");\n\n    fn = \"_\".join((title+str(num)).lower().split()) + \".png\"\n    plt.savefig(fn, bbox_inches='tight', pad_inches=0.2, transparent=False, facecolor='black')\n    plt.close()\n\n    \n\ntitle1 = \"Predictions of Vanilla UNet\"\ntitle2 = \"Predictions of FPN\"\ntitle3 = \"Predictions of UNet with ResNeXt50 backbone\"\n\nfor num, batch in enumerate(pred_overlap_5x3_r):\n    plot_plate_overlap(batch,title3, num)\n    \n\n\"\"\"for num, (batch1, batch2, batch3) in enumerate(zip(\n    pred_overlap_5x3_u, pred_overlap_5x3_f, pred_overlap_5x3_r)):\n    \n    plot_plate_overlap(batch1,title1, num)   \n    plot_plate_overlap(batch2,title2, num)\n    plot_plate_overlap(batch3,title3, num)\"\"\";","5e1bab8c":"from PIL import Image\n\ndef make_gif(title):\n    base_name = \"_\".join(title.lower().split())\n\n    base_len = len(base_name) \n    end_len = len(\".png\")\n    fp_in = f\"{base_name}*.png\"\n    fp_out = f\"{base_name}.gif\"\n\n    img, *imgs = [Image.open(f) \n                  for f in sorted(glob.glob(fp_in), \n                                  key=lambda x : int(x[base_len:-end_len]))]\n\n    img.save(fp=fp_out, format='GIF', append_images=imgs,\n             save_all=True, duration=1000, loop=0)\n    \n    return fp_out\n\n#fn1 = make_gif(title1)\n#fn2 = make_gif(title2)\nfn3 = make_gif(title3)","0e3e6745":"from IPython.display import Image as Image_display","05a83630":"\"\"\"with open(fn1,'rb') as f:\n    display(Image_display(data=f.read(), format='png'))\"\"\";","c9e3cd2c":"\"\"\"with open(fn3,'rb') as f:\n    display(Image_display(data=f.read(), format='png'))\"\"\";","24a604a4":"with open(fn3,'rb') as f:\n    display(Image_display(data=f.read(), format='png'))","d4b20b3d":"## Data Distribution","871cabb9":"## Creating a DataFrame","75667f89":"Samples of images and masks with a positive diagnosis","ae9967c6":"+ https:\/\/www.semanticscholar.org\/paper\/Comparison-of-Jaccard%2C-Dice%2C-Cosine-Similarity-To-Thada-Jaglan\/8575e8beef47bd2880c92f54a749f933db983e56\n\nIntuitive interpretation  |  Formula depending on the type of coefficient\n:------------------------:|:-------------------------:\n![](https:\/\/www.pyimagesearch.com\/wp-content\/uploads\/2016\/09\/iou_equation.png) | ![](https:\/\/d3i71xaburhd42.cloudfront.net\/8575e8beef47bd2880c92f54a749f933db983e56\/2-Table1-1.png)\n\n","6f02b99f":"Split data on train val test","6cddf66f":"## Feature Pyramid Network + UNet","76999293":"Final dataframe","9749d301":"# Test Prediction","ee48f91b":"# DataGenerator and Data Augmentation","d965c746":"### Predictive overlapping on test images","c6605573":"# What's next?\n\nExperiments:\n+  More epochs for training iterations(train longer, without overfitting)\n+  Experimenting with different input image size.\n+  Experimenting with different batch size.\n+  Experimenting with different learning rate.\n+  Strong image transforms.\n+  Experiments with different UNet backbone\n+ Overfits\n\nFine tuning:\n+ Lovasz-Loss\n+ lr Schedulers\n","5182be74":"## Data Visualization","c0f61336":"# Gifs","ecdeaa2a":"# Data","b3fa74e4":"<p> \n    <img src=\"https:\/\/miro.medium.com\/max\/1400\/1*f7YOaE4TWubwaFF7Z1fzNw.png\" width=\"650\" height=\"300\">\n<\/p>","9071e864":"DataGenerator","9e839712":"# Segmentation Quality Metric","bd918fa9":"Distribution of positive\/negative diagnosis","da66375a":"### Test IoU","537f3536":"## Unet with ResNeXt50 backbone.","ff563c88":"### Display gifs\n","fc386523":"transforms","64cc3b6d":"## UNet","001ebf37":"## Plates of images of predictive overlapping","b3c8a6e9":"# Model Zoo","6a971b15":"# Segmentation Loss","12eb343a":"# Image segmentation using Vanilla UNet, UNet with ResNeXt50 backbone, and Feature Pyramid Network.\n\n**Steps**:\n+ **Data Preparation**\n\n+ **Data Visualization**\n\n+ **Data Augmentaions**\n\n+ **UNet**\n\n+ **FPN**\n\n+ **UNet ResNeXt50**\n\n+ **Train Models**\n\n+ **Prediction on test data**\n\n+ **Visualization of the prediction on test data**\n\n+ **What's next?**","5f48e381":"### random test sample","8f23dddd":"+ https:\/\/medium.com\/kaggle-blog\/carvana-image-masking-challenge-1st-place-winners-interview-78fcc5c887a8\n\n<p> \n    <img src=\"https:\/\/miro.medium.com\/max\/1400\/0*u6iEAiz9smASIhuk\" width=\"450\" height=\"300\">\n<\/p>","ab567e84":"Global Variable","c040cb7b":"Augmentation Visualization","1b17c58e":"# Train Models","4b296994":"Distribution of positive\/negative diagnosis between each patient","8342e921":"Samples of images with a positive and negative diagnosis","b1309400":"## Train history ","13200962":"## FPN \n+ https:\/\/towardsdatascience.com\/review-fpn-feature-pyramid-network-object-detection-262fc7482610\n<p> \n    <img src=\"https:\/\/miro.medium.com\/max\/2000\/1*XOumTDx4QEZ6qd_z1Rgf5g.png\">\n<\/p>\n\n\n## UNet + FPN\n+ http:\/\/presentations.cocodataset.org\/COCO17-Stuff-FAIR.pdf\n<p> \n    <img src=\"https:\/\/user-images.githubusercontent.com\/527241\/42149076-00cf1d80-7dd5-11e8-94c6-5ac0e79d3b77.png\" width=\"650\" height=\"300\">\n<\/p>\n"}}