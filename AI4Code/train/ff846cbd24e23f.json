{"cell_type":{"faa00787":"code","678315f7":"code","d5cca248":"code","aaaff62f":"code","b8f27e94":"code","749de657":"code","cfbb2c69":"code","29ba4fd9":"code","79c9d7a6":"code","fecc499a":"code","5a286a8e":"code","0ef17f79":"code","70035f8a":"code","079c14a1":"code","3bd0baa5":"code","a5008066":"code","dc5a7907":"markdown","06f3f183":"markdown","67c97750":"markdown","e4cfc3d8":"markdown","61190484":"markdown","d59d5676":"markdown","b807424b":"markdown","460abec9":"markdown","3ae1dd72":"markdown","96b2433c":"markdown","39d56829":"markdown","e2129021":"markdown","818f15ca":"markdown","3e2063a3":"markdown","5bac5e01":"markdown","c052e23b":"markdown"},"source":{"faa00787":"# Example of the Shapiro-Wilk Normality Test\nfrom scipy.stats import shapiro\ndata = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\nstat, p = shapiro(data)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n    print('Probably Gaussian')\nelse:\n    print('Probably not Gaussian')","678315f7":"# Example of the D'Agostino's K^2 Normality Test\nfrom scipy.stats import normaltest\ndata = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\nstat, p = normaltest(data)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n    print('Probably Gaussian')\nelse:\n    print('Probably not Gaussian')","d5cca248":"# Example of the Anderson-Darling Normality Test\nfrom scipy.stats import anderson\ndata = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\nresult = anderson(data)\nprint('stat=%.3f' % (result.statistic))\nfor i in range(len(result.critical_values)):\n\tsl, cv = result.significance_level[i], result.critical_values[i]\n\tif result.statistic < cv:\n\t\tprint('Probably Gaussian at the %.1f%% level' % (sl))\n\telse:\n\t\tprint('Probably not Gaussian at the %.1f%% level' % (sl))","aaaff62f":"# Example of the Pearson's Correlation test\nfrom scipy.stats import pearsonr\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [0.353, 3.517, 0.125, -7.545, -0.555, -1.536, 3.350, -1.578, -3.537, -1.579]\nstat, p = pearsonr(data1, data2)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n\tprint('Probably independent')\nelse:\n\tprint('Probably dependent')","b8f27e94":"# Example of the Spearman's Rank Correlation Test\nfrom scipy.stats import spearmanr\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [0.353, 3.517, 0.125, -7.545, -0.555, -1.536, 3.350, -1.578, -3.537, -1.579]\nstat, p = spearmanr(data1, data2)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n\tprint('Probably independent')\nelse:\n\tprint('Probably dependent')","749de657":"# Example of the Kendall's Rank Correlation Test\nfrom scipy.stats import kendalltau\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [0.353, 3.517, 0.125, -7.545, -0.555, -1.536, 3.350, -1.578, -3.537, -1.579]\nstat, p = kendalltau(data1, data2)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n\tprint('Probably independent')\nelse:\n\tprint('Probably dependent')","cfbb2c69":"# Example of the Chi-Squared Test\nfrom scipy.stats import chi2_contingency\ntable = [[10, 20, 30],[6,  9,  17]]\nstat, p, dof, expected = chi2_contingency(table)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n\tprint('Probably independent')\nelse:\n\tprint('Probably dependent')","29ba4fd9":"# Example of the Augmented Dickey-Fuller unit root test\nfrom statsmodels.tsa.stattools import adfuller\ndata = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nstat, p, lags, obs, crit, t = adfuller(data)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n\tprint('Probably not Stationary')\nelse:\n\tprint('Probably Stationary')","79c9d7a6":"# Example of the Kwiatkowski-Phillips-Schmidt-Shin test\nfrom statsmodels.tsa.stattools import kpss\ndata = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nstat, p, lags, crit = kpss(data)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n\tprint('Probably not Stationary')\nelse:\n\tprint('Probably Stationary')","fecc499a":"# Example of the Student's t-test\nfrom scipy.stats import ttest_ind\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\nstat, p = ttest_ind(data1, data2)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n\tprint('Probably the same distribution')\nelse:\n\tprint('Probably different distributions')","5a286a8e":"# Example of the Paired Student's t-test\nfrom scipy.stats import ttest_rel\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\nstat, p = ttest_rel(data1, data2)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n\tprint('Probably the same distribution')\nelse:\n\tprint('Probably different distributions')","0ef17f79":"# Example of the Analysis of Variance Test\nfrom scipy.stats import f_oneway\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\ndata3 = [-0.208, 0.696, 0.928, -1.148, -0.213, 0.229, 0.137, 0.269, -0.870, -1.204]\nstat, p = f_oneway(data1, data2, data3)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n\tprint('Probably the same distribution')\nelse:\n\tprint('Probably different distributions')","70035f8a":"# Example of the Mann-Whitney U Test\nfrom scipy.stats import mannwhitneyu\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\nstat, p = mannwhitneyu(data1, data2)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n\tprint('Probably the same distribution')\nelse:\n\tprint('Probably different distributions')","079c14a1":"# Example of the Wilcoxon Signed-Rank Test\nfrom scipy.stats import wilcoxon\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\nstat, p = wilcoxon(data1, data2)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n\tprint('Probably the same distribution')\nelse:\n\tprint('Probably different distributions')","3bd0baa5":"# Example of the Kruskal-Wallis H Test\nfrom scipy.stats import kruskal\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\nstat, p = kruskal(data1, data2)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n\tprint('Probably the same distribution')\nelse:\n\tprint('Probably different distributions')","a5008066":"# Example of the Friedman Test\nfrom scipy.stats import friedmanchisquare\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\ndata3 = [-0.208, 0.696, 0.928, -1.148, -0.213, 0.229, 0.137, 0.269, -0.870, -1.204]\nstat, p = friedmanchisquare(data1, data2, data3)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n\tprint('Probably the same distribution')\nelse:\n\tprint('Probably different distributions')","dc5a7907":"<h2>4. Parametric Statistical Hypothesis Tests<\/h2>\nThis section lists statistical tests that you can use to compare data samples.\n<h3>Student\u2019s t-test<\/h3>\nTests whether the means of two independent samples are significantly different.\n\n<b>Assumptions<\/b><br>\nObservations in each sample are independent and identically distributed (iid).<br>\nObservations in each sample are normally distributed.<br>\nObservations in each sample have the same variance.<br>\n<br>\n<b>Interpretation<\/b><br>\nH0: the means of the samples are equal.<br>\nH1: the means of the samples are unequal.","06f3f183":"<h3>Chi-Squared Test<\/h3>\nTests whether two categorical variables are related or independent.\n<br>\n<br>\n<b>Assumptions<\/b>\n<br>\nObservations used in the calculation of the contingency table are independent.<br>\n25 or more examples in each cell of the contingency table.<br>\n<br>\n<b>Interpretation<\/b><br>\nH0: the two samples are independent.<br>\nH1: there is a dependency between the samples.","67c97750":"<h3>Friedman Test<\/h3>\nTests whether the distributions of two or more paired samples are equal or not.<br>\n<br>\n<b>Assumptions<\/b>\n<br>\nObservations in each sample are independent and identically distributed (iid).<br>\nObservations in each sample can be ranked.<br>\nObservations across each sample are paired.<br>\n<br>\n<b>Interpretation<\/b>\n<br>\nH0: the distributions of all samples are equal.<br>\nH1: the distributions of one or more samples are not equal.","e4cfc3d8":"<h3>Kruskal-Wallis H Test<\/h3>\nTests whether the distributions of two or more independent samples are equal or not.<br>\n<br>\n<b>Assumptions<\/b><br>\nObservations in each sample are independent and identically distributed (iid).<br>\nObservations in each sample can be ranked.<br>\n<br>\n<b>Interpretation<\/b><br>\nH0: the distributions of all samples are equal.<br>\nH1: the distributions of one or more samples are not equal.","61190484":"<h3>D\u2019Agostino\u2019s K^2 Test<\/h3>\nTests whether a data sample has a Gaussian distribution.<br>\n<b>Assumptions<\/b><br>\nObservations in each sample are independent and identically distributed (iid).<br>\n<br>\n<b>Interpretation<\/b><br>\nH0: the sample has a Gaussian distribution.<br>\nH1: the sample does not have a Gaussian distribution.","d59d5676":"<h2>2. Correlation Tests<\/h2>\nThis section lists statistical tests that you can use to check if two samples are related.\n\n<h3>Pearson\u2019s Correlation Coefficient<\/h3>\nTests whether two samples have a linear relationship.\n\n<b>Assumptions<\/b>\n\nObservations in each sample are independent and identically distributed (iid).<br>\nObservations in each sample are normally distributed.<br>\nObservations in each sample have the same variance.<br>\n\n<b>Interpretation<\/b>\n\nH0: the two samples are independent.<br>\nH1: there is a dependency between the samples.","b807424b":"<h3>Wilcoxon Signed-Rank Test<\/h3>\nTests whether the distributions of two paired samples are equal or not.\n\n<b>Assumptions<\/b>\n<br>\nObservations in each sample are independent and identically distributed (iid).<br>\nObservations in each sample can be ranked.<br>\nObservations across each sample are paired.<br>\n<br>\n<b>Interpretation<\/b><br>\nH0: the distributions of both samples are equal.<br>\nH1: the distributions of both samples are not equal.","460abec9":"<p>\n<h2>1. Normality Tests<\/h2>\nThis section lists statistical tests that you can use to check if your data has a Gaussian distribution.\n\n<h3>Shapiro-Wilk Test<\/h3>\nTests whether a data sample has a Gaussian distribution.\n\n<b>Assumptions<\/b><br>\nObservations in each sample are independent and identically distributed (iid).<br>\n\n<b>Interpretation<\/b><br>\nH0: the sample has a Gaussian distribution.<br>\nH1: the sample does not have a Gaussian distribution.\n<\/p>","3ae1dd72":"<h3>Analysis of Variance Test (ANOVA)<\/h3>\nTests whether the means of two or more independent samples are significantly different.\n\n<b>Assumptions<\/b><br>\nObservations in each sample are independent and identically distributed (iid).<br>\nObservations in each sample are normally distributed.<br>\nObservations in each sample have the same variance.<br>\n<br>\n<b>Interpretation<\/b><br>\nH0: the means of the samples are equal.<br>\nH1: one or more of the means of the samples are unequal.","96b2433c":"<h2>5. Nonparametric Statistical Hypothesis Tests<\/h2>\n<h3>Mann-Whitney U Test<\/h3>\nTests whether the distributions of two independent samples are equal or not.\n<br>\n<b>Assumptions<\/b><br>\nObservations in each sample are independent and identically distributed (iid).<br>\nObservations in each sample can be ranked.<br>\n<br>\n<b>Interpretation<\/b><br>\nH0: the distributions of both samples are equal.<br>\nH1: the distributions of both samples are not equal.","39d56829":"<h3>Anderson-Darling Test<\/h3>\nTests whether a data sample has a Gaussian distribution.\n\n<b>Assumptions<\/b><br>\nObservations in each sample are independent and identically distributed (iid).<br>\n\n<b>Interpretation<\/b><br>\nH0: the sample has a Gaussian distribution.<br>\nH1: the sample does not have a Gaussian distribution.","e2129021":"<h3>Paired Student\u2019s t-test<\/h3>\nTests whether the means of two paired samples are significantly different.\n<br>\n<b>Assumptions<\/b><br>\nObservations in each sample are independent and identically distributed (iid).<br>\nObservations in each sample are normally distributed.<br>\nObservations in each sample have the same variance.<br>\nObservations across each sample are paired.<br>\n<br>\n<b>Interpretation<\/b><br>\nH0: the means of the samples are equal.<br>\nH1: the means of the samples are unequal.","818f15ca":"<h2>3. Stationary Tests<\/h2>\nThis section lists statistical tests that you can use to check if a time series is stationary or not.\n\n<h3>Augmented Dickey-Fuller Unit Root Test<\/h3>\nTests whether a time series has a unit root, e.g. has a trend or more generally is autoregressive.\n<br>\n<br>\n<b>Assumptions<\/b><br>\nObservations in are temporally ordered.<br><br>\n<b>Interpretation<\/b><br>\nH0: a unit root is present (series is non-stationary).<br>\nH1: a unit root is not present (series is stationary).","3e2063a3":"<h3>Kendall\u2019s Rank Correlation<\/h3>\nTests whether two samples have a monotonic relationship.\n\n<b>Assumptions<\/b>\n\nObservations in each sample are independent and identically distributed (iid).<br>\nObservations in each sample can be ranked.<br>\n<b>Interpretation<\/b>\n\nH0: the two samples are independent.<br>\nH1: there is a dependency between the samples.","5bac5e01":"<h3>Kwiatkowski-Phillips-Schmidt-Shin<\/h3>\nTests whether a time series is trend stationary or not.\n<br>\n<br>\n<b>Assumptions<\/b><br>\nObservations in are temporally ordered.<br><br>\n<b>Interpretation<\/b>\n\nH0: the time series is not trend-stationary.<br>\nH1: the time series is trend-stationary.","c052e23b":"<h3>Spearman\u2019s Rank Correlation<\/h3>\nTests whether two samples have a monotonic relationship.\n\n<b>Assumptions<\/b>\n\nObservations in each sample are independent and identically distributed (iid).<br>\nObservations in each sample can be ranked.<br>\n<b>Interpretation<\/b>\n\nH0: the two samples are independent.<br>\nH1: there is a dependency between the samples.\n"}}