{"cell_type":{"3e9e308b":"code","7666d241":"code","19789a5a":"code","462e3347":"code","e729e314":"code","88ae13fa":"code","c98eacb4":"code","f4e64283":"code","078b7efe":"code","ffb002ed":"code","db049930":"code","b73dc2f5":"code","a6b7ea1c":"code","056c9646":"code","e44f953e":"code","2aafc1c2":"code","1504542f":"code","148c54a2":"code","5f1b6777":"markdown","0a8859bb":"markdown","4fd0f78b":"markdown","db275667":"markdown","86c09dad":"markdown","e93165cc":"markdown","5c106b69":"markdown","671f45d6":"markdown","d21f0e14":"markdown","97d31812":"markdown","cef5d7c1":"markdown","38ae1358":"markdown","8605326e":"markdown","2dd3413c":"markdown","4548b6a7":"markdown","efdbd9d2":"markdown","3ec3ad03":"markdown","eaf6c9a9":"markdown","eea6fc98":"markdown","0ff536eb":"markdown","e0bd9272":"markdown","4f4d6821":"markdown"},"source":{"3e9e308b":"# Set warning messages\nimport warnings\n# Show all warnings in IPython\nwarnings.filterwarnings('always')\n# Ignore specific numpy warnings (as per <https:\/\/github.com\/numpy\/numpy\/issues\/11788#issuecomment-422846396>)\nwarnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\nwarnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")","7666d241":"# Import built-in modules\nimport sys\nimport platform\nimport os\nfrom pathlib import Path\n\n# Import external modules\nfrom IPython import __version__ as IPy_version\nimport IPython.display as ipyd\nimport numpy as np\nimport pandas as pd\nfrom sklearn import __version__ as skl_version\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom bokeh import __version__ as bk_version\nfrom scipy import __version__ as scipy_version\nimport scipy.stats as sps\nfrom statsmodels import __version__ as sm_version\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Check they have loaded and the versions are as expected\nassert platform.python_version_tuple() == ('3', '6', '6')\nprint(f\"Python version:\\t\\t{sys.version}\")\nassert IPy_version == '7.13.0'\nprint(f'IPython version:\\t{IPy_version}')\nassert np.__version__ == '1.18.2'\nprint(f'numpy version:\\t\\t{np.__version__}')\nassert pd.__version__ == '0.25.3'\nprint(f'pandas version:\\t\\t{pd.__version__}')\nassert skl_version == '0.22.2.post1'\nprint(f'sklearn version:\\t{skl_version}')\nassert mpl.__version__ == '3.2.1'\nprint(f'matplotlib version:\\t{mpl.__version__}')\nassert bk_version == '2.0.1'\nprint(f'bokeh version:\\t\\t{bk_version}')\nassert scipy_version == '1.4.1'\nprint(f'scipy version:\\t\\t{scipy_version}')\nassert sm_version == '0.11.0'\nprint(f'statsmodels version:\\t{sm_version}')","19789a5a":"# Load Bokeh for use in a notebook\nfrom bokeh.io import output_notebook\noutput_notebook()","462e3347":"# Output exact environment specification, in case it is needed later\nprint(\"Capturing full package environment spec\")\nprint(\"(But note that not all these packages are required)\")\n!pip freeze > requirements_Kaggle.txt\n!jupyter --version > jupyter_versions.txt","e729e314":"# Simulate data\nsample_size = 200\nBeta_true = np.array([np.log(0.3), 0.2])\nX_des_mx = np.array([\n    [1] * sample_size,\n    sps.norm(loc=10, scale=3).rvs(size=sample_size, random_state=11)\n]).T\ny_vec = sps.poisson(\n    mu=np.exp(np.matmul(X_des_mx, Beta_true))\n).rvs(size=sample_size, random_state=76)\n\ndata_df = pd.concat([\n    pd.DataFrame(\n        X_des_mx,\n        columns=[f\"x_{val}\" for val in range(X_des_mx.shape[1])]\n    ),\n    pd.DataFrame({'y': y_vec})\n], sort=False, axis=1)\ndata_df.describe().style.format('{:.3f}')","88ae13fa":"data_df.plot.scatter('x_1', 'y')\nplt.show()","c98eacb4":"# Get the data\ndata = sm.datasets.fair.load_pandas().data\ndata[\"affairs\"] = np.ceil(data[\"affairs\"])\nselected_cols = 'affairs rate_marriage age yrs_married'.split()\ndata = data[selected_cols]","f4e64283":"# \"Condensed\" data is one row per unique combination of \n# explanatory variables *and* response variable\ndc = data.copy().groupby(selected_cols).agg(\n    freq=('affairs', 'size')).reset_index()\ndc.head()","078b7efe":"# \"Aggregated\" data is one row per unique combination of \n# explanatory variables *only*, with the response variable\n# either summed or averaged.\ndf_a = data.copy().groupby(selected_cols[1:]).agg(**{\n    'affairs_' + func_str: ('affairs', func_str) for \n    func_str in ['mean', 'sum','count']\n}).reset_index()\ndf_a.head()","ffb002ed":"# Initialise DataFrame for holding model info\nmod_fields = ['Data', 'Target', 'Weight', 'Exposure']\nmods_df = pd.DataFrame(np.empty(0, dtype=np.dtype(\n    [(field_name, np.dtype('O')) for field_name in mod_fields] +\n    [('GLMResults', np.dtype('O')),]\n)))","db049930":"explanatory_vars = 'rate_marriage age yrs_married'.split(' ')\n\n# Original data\nmods_df.loc[0, mod_fields] = ['Original', 'count', 'None', 'None']\nmods_df.loc[0, 'GLMResults'] = smf.glm(\n    'affairs ~ rate_marriage + age + yrs_married',\n    data=data, family=sm.families.Poisson()\n).fit()\n\n# Condensed data with freq_weights\nmods_df.loc[1, mod_fields] = ['Condensed', 'count', 'freq', 'None']\nmods_df.loc[1, 'GLMResults'] = smf.glm(\n    'affairs ~ rate_marriage + age + yrs_married',\n    data=dc, family=sm.families.Poisson(), \n    freq_weights=dc['freq']\n).fit()\n\n# Condensed with var_weights\nmods_df.loc[2, mod_fields] = ['Condensed', 'count', 'var', 'None']\nmods_df.loc[2, 'GLMResults'] = smf.glm(\n    'affairs ~ rate_marriage + age + yrs_married',\n    data=dc, family=sm.families.Poisson(),\n    var_weights=dc['freq']\n).fit()\n\n# Aggregated sum with exposure\nmods_df.loc[3, mod_fields] = ['Aggregated', 'count_sum', 'None', 'exposure']\nmods_df.loc[3, 'GLMResults'] = smf.glm(\n    'affairs_sum ~ rate_marriage + age + yrs_married',\n    data=df_a, family=sm.families.Poisson(),\n    exposure=df_a['affairs_count']\n).fit()\n# Same, but with offset = log(exposure)\nmods_df.loc[4, mod_fields] = ['Aggregated', 'count_sum', 'None', 'offset']\nmods_df.loc[4, 'GLMResults'] = smf.glm(\n    'affairs_sum ~ rate_marriage + age + yrs_married',\n    data=df_a, family=sm.families.Poisson(),\n    offset=np.log(df_a['affairs_count'])\n).fit()\n\n# Aggregated mean with var_weights\nmods_df.loc[5, mod_fields] = ['Aggregated', 'count_mean', 'var', 'None']\nmods_df.loc[5, 'GLMResults'] = smf.glm(\n    'affairs_mean ~ rate_marriage + age + yrs_married',\n    data=df_a, family=sm.families.Poisson(),\n    var_weights=np.asarray(df_a['affairs_count'])\n).fit()\n# Same but using freq_weights\nmods_df.loc[6, mod_fields] = ['Aggregated', 'count_mean', 'freq', 'None']\nmods_df.loc[6, 'GLMResults'] = smf.glm(\n    'affairs_mean ~ rate_marriage + age + yrs_married',\n    data=df_a, family=sm.families.Poisson(),\n    freq_weights=np.asarray(df_a['affairs_count'])\n).fit()\n\n# Aggregated sum with *no* exposure or offset\nmods_df.loc[7, mod_fields] = ['Aggregated', 'count_sum', 'None', 'None']\nmods_df.loc[7, 'GLMResults'] = smf.glm(\n    'affairs_mean ~ rate_marriage + age + yrs_married',\n    data=df_a, family=sm.families.Poisson(),\n).fit()","b73dc2f5":"summary_df = mods_df.GLMResults.apply({\n    'coef': lambda x: x.params,\n    'se': lambda x: x.bse,\n    'pvalue': lambda x: x.pvalues,\n    'stats': lambda r: pd.Series({\n        'loglik': r.llf,\n        'deviance': r.deviance,\n        'chi2': r.pearson_chi2\n    }),\n    'model': lambda r: pd.Series({\n        'df_model': r.model.df_model,\n        'df_resid': r.model.df_resid,\n        'chi2': r.pearson_chi2\n    }),\n}).set_index(pd.MultiIndex.from_frame(mods_df[mod_fields])).T\nsummary_df.style.format('{:,.4f}').apply(\n    lambda row_sers: [\n        'background-color: yellow' if not_equal else '' \n        for not_equal in np.append(False, np.abs(\n            row_sers.values[1:] - row_sers.values[:-1]) > 1e-8)\n    ], axis=1\n)","a6b7ea1c":"assert np.abs(mods_df.GLMResults[3].model.exposure - np.log(df_a['affairs_count'])).max() < 1e-15\nprint(\"Note: The resulting `exposure` attribute is the log of the exposure data field\")","056c9646":"# When *predicting* using the results of the models, exposure is *not*\n# permitted as an input. It is assumed to be 1 (i.e. log of it is zero).\n# In that way, you are getting out a rate over a period of 1 = a count.\nassert np.max(np.abs(\n    mods_df.GLMResults[4].predict(dc[explanatory_vars]) - \n    mods_df.GLMResults[5].predict(dc[explanatory_vars])\n)) < 1e-10\nassert np.max(np.abs(\n    mods_df.GLMResults[3].predict(dc[explanatory_vars]) - \n    mods_df.GLMResults[5].predict(dc[explanatory_vars])\n)) < 1e-10\n\n# However, the fittedvalues are targetting the response\n# e.g. 'Aggregated sum with exposure', fitted values *include* the exposure\nassert np.max(np.abs(\n    mods_df.GLMResults[3].fittedvalues \/ df_a.affairs_count - \n    mods_df.GLMResults[5].fittedvalues\n)) < 1e-10\nassert np.max(np.abs(\n    mods_df.GLMResults[4].fittedvalues \/ df_a.affairs_count - \n    mods_df.GLMResults[5].fittedvalues\n)) < 1e-10\nprint(\"Correct: Assertions have passed\")","e44f953e":"# For target of count_sum, the fittedvalues are already adjusted\n# but the predicted values assume a period of 1, so have to be adjusted\nassert np.abs(mods_df.GLMResults[3].fittedvalues.sum() - data.affairs.sum()) < 1e-10\nassert np.abs(\n    (mods_df.GLMResults[3].predict(df_a[explanatory_vars]) * df_a.affairs_count).sum() -\n    data.affairs.sum()\n) < 1e-10\nassert np.abs(mods_df.GLMResults[4].fittedvalues.sum() - data.affairs.sum()) < 1e-10\nassert np.abs(\n    (mods_df.GLMResults[4].predict(df_a[explanatory_vars]) * df_a.affairs_count).sum() -\n    data.affairs.sum()\n) < 1e-10\n\n# For target of count_mean, both need to be adjusted\nassert np.abs(\n    (mods_df.GLMResults[5].fittedvalues * df_a.affairs_count).sum() - \n    data.affairs.sum()\n) < 1e-10\nassert np.abs(\n    (mods_df.GLMResults[5].predict(df_a[explanatory_vars]) * df_a.affairs_count).sum() -\n    data.affairs.sum()\n) < 1e-10\nprint(\"Correct: Assertions have passed\")","2aafc1c2":"# Reminder of what each model is\nmods_df","1504542f":"print(\"==== Break deviance down into individual deviances ====\")\n# Original model\nD_indiv_0 = 2 * (\n    data.affairs * np.log(\n        data.affairs \/ mods_df.GLMResults[0].fittedvalues,\n        # Extra code to cope with: 0 * ln(0) = 0\n        out=np.zeros(len(data.affairs)),\n        where=data.affairs != 0\n    ) - data.affairs + mods_df.GLMResults[0].fittedvalues\n)\nassert np.abs(D_indiv_0.sum() - mods_df.GLMResults[0].deviance) < 1e-10\n\n# Model with exposure\nD_indiv_3 = 2 * (\n    df_a.affairs_sum * np.log(\n        df_a.affairs_sum \/ mods_df.GLMResults[3].fittedvalues,\n        out=np.zeros(len(df_a.affairs_sum)),\n        where=df_a.affairs_sum != 0\n    ) - df_a.affairs_sum + mods_df.GLMResults[3].fittedvalues\n)\nassert np.abs(D_indiv_3.sum() - mods_df.GLMResults[3].deviance) < 1e-10\n\n# Model with var_weights\nD_indiv_5 = 2 * df_a.affairs_count * (\n    df_a.affairs_mean * np.log(\n        df_a.affairs_mean \/ mods_df.GLMResults[5].fittedvalues,\n        out=np.zeros(len(df_a.affairs_mean)),\n        where=df_a.affairs_mean != 0\n    ) - df_a.affairs_mean + mods_df.GLMResults[5].fittedvalues\n)\nassert np.abs(D_indiv_5.sum() - mods_df.GLMResults[5].deviance) < 1e-10\nprint(\"Correct: All assertions passed\")","148c54a2":"print(\"==== Recreate residuals to understand them ====\")\nprint(\"Tests done on each of 'exposure' and 'var_weights' models\\n\")\nassert np.max(np.abs(\n    mods_df.GLMResults[3].resid_response -\n    (df_a.affairs_sum - mods_df.GLMResults[3].fittedvalues)\n)) < 1e-15\nassert np.max(np.abs(\n    mods_df.GLMResults[5].resid_response -\n    (df_a.affairs_mean - mods_df.GLMResults[5].fittedvalues)\n)) < 1e-15\nprint(\"resid_response: y_true - fittedvalues (= endog - fittedvalues)\\n\")\n\nassert np.max(np.abs(\n    mods_df.GLMResults[3].resid_pearson -\n    (df_a.affairs_sum - mods_df.GLMResults[3].fittedvalues) \/ \n    np.sqrt(mods_df.GLMResults[3].fittedvalues)\n)) < 1e-15\nassert np.max(np.abs(\n    mods_df.GLMResults[5].resid_pearson -\n    (df_a.affairs_mean - mods_df.GLMResults[5].fittedvalues) \/\n    np.sqrt(mods_df.GLMResults[5].fittedvalues \/ df_a.affairs_count)\n)) < 1e-14\nprint(\n    \"resid_pearson: \"\n    \"(y_true - fittedvalues) \/ sqrt(var(fittedvalues) \/ var_weights)\\n\"\n    \"\\t= (y_true - fittedvalues) \/ sqrt(fittedvalues \/ var_weights)\\n\"\n    \"\\t(since var(x) = x for Poisson regression)\\n\"\n    \"\\t= resid_response adjusted for variance at different values of response\\n\"\n)\n\nassert np.max(np.abs(\n    mods_df.GLMResults[3].get_influence().resid_studentized - (\n        mods_df.GLMResults[3].resid_pearson \/ \n        np.sqrt(1 - mods_df.GLMResults[3].get_influence().hat_matrix_diag)\n    )\n)) < 1e-15\nassert np.max(np.abs(\n    mods_df.GLMResults[5].get_influence().resid_studentized - (\n        mods_df.GLMResults[5].resid_pearson \/ \n        np.sqrt(1 - mods_df.GLMResults[3].get_influence().hat_matrix_diag)\n    )\n)) < 1e-15\nprint(\n    \"resid_studentized: resid_pearson \/ sqrt(1 - h_i)\\n\"\n    \"\\t= (y_true - fittedvalues) \/ sqrt(var(fittedvalues) * (1 - h_i) \/ var_weights)\\n\"\n    \"\\t= resid_response adjusted for variance at different values of response *and* leverage of point\\n\"\n    \"\\t(leverage = is the observation unusual given the distn of X?)\\n\"\n    \"*Note*: These are sometimes called 'standardized peason residuals'\\n\"\n)\n\nassert np.max(np.abs(\n    np.sign(\n        df_a.affairs_sum - mods_df.GLMResults[3].fittedvalues\n    ) * np.sqrt(D_indiv_3) - mods_df.GLMResults[3].resid_deviance\n)) < 1e-12\nassert np.max(np.abs(\n    np.sign(\n        df_a.affairs_mean - mods_df.GLMResults[5].fittedvalues\n    ) * np.sqrt(D_indiv_5) - mods_df.GLMResults[5].resid_deviance\n)) < 1e-12\nprint(\n    \"resid_deviance: sign(y_true - fittedvalues) * sqrt(D_i)\\n\"\n    \"\\t= sign(y_true - fittedvalues) * sqrt(2 * var_weight_i * (ll(y_true_i, y_true_i) - ll(y_true_i, y_pred_i)))\\n\"\n    \"\\t= sign(y_true - fittedvalues) * sqrt(2 * var_weight_i * d(y_true_i, y_pred_i))\"\n)","5f1b6777":"<!-- This table of contents is updated *manually* -->\n# Contents\n1. [Setup](#Setup)\n1. [Visualise the loss function](#Visualise-the-loss-function)\n1. [Weight and exposure](#Weight-and-exposure): Load and process the data, Build the models, Analyse the results\n1. [Residuals](#Residuals)\n1. [Proportion deviance explained](#Proportion-deviance-explained)","0a8859bb":"<div align=\"right\" style=\"text-align: right\"><a href=\"#Contents\">Back to Contents<\/a><\/div>\n\n# Proportion deviance explained","4fd0f78b":"### Question: Does exposure work for the logistic link function or logistic regression?\n\nBe careful not to confuse this with binomial regression, i.e. each response is a count of the number of successes $Y_i$ of $n_i$ independent trials. The `n_trials` input is available in that case, or you can specify a 2D response consisting of `(Y_i, n_i - Y_i)` = (number of successes, number of failures).\n\nSuppose you have a data set consisting of insurance policy periods (during which all the explanatory factors are constant) and the response is the binary answer to \"was there (at least) one claim during the period?\". In this case, could we use duration of the policy period as exposure, so the answer would be chance of making a claim per unit time? Actually, this doesn't really make sense because once a claim has been made, there is no chance of the remaining time on the policy period changing the response. A more appropriate model in this case is a survival model, i.e. try to predict the length of time until the first claim (an exponential distribution might be suitable), with policy periods that do not return a claim being treated as \"censured\" observations. \n\nSimilarly, if you have a data set consisting of driving trips and the response is the binary answer to \"was there a crash on the trip?\". In this case, a survival model is also the way to go, to estimate the rate per mile, or per unit time, say.\n\nFinally, consider the question purely from an algebraic point of view. The model is:\n$$\nE(Y_i | x_i) = \\mu_i = g^{-1}(\\eta)\n$$\nwhere $g = \\textrm{logit}$, so $g^{-1}(x) = \\textrm{expit}(x) = (1+\\exp(-x))^{-1}$. Now suppose we want to change the response to $Y_i \/ e_i$. Then:\n$$\n\\frac{\\mu_i}{e_i} = (1+\\exp(-x_i\\beta))^{-1} \\\\ \n\\Rightarrow \\mu_i = (e_i^{-1}(1+\\exp(-x_i\\beta)))^{-1} = \n\\left(\\frac{1}{e_i} + \\exp\\left(-(\\ln e_i + x_i\\beta)\\right)\\right)^{-1} \n\\neq \\left(1 + \\exp\\left(-(\\ln e_i + x_i\\beta)\\right)\\right)^{-1} \n$$\nThat is, division of the response by $e_i$ does *not* equate to offset of the linear predictor by $ln e_i$, i.e. the concept of exposure does not work algebraically.\n\n\nSo, we conclude that **no**, exposure is not a concept for logistic regression, or the logit link function. But bear in mind that there are many cases where:\n- The response is the average of a number of *independent* trials. This is binomial regression.\n- We a valid binary regression problem with some observations being more credible than others. This can be specified using `var_weights` (i.e. $w_i$), as above.","db275667":"## Background\nWhen using the MSE (mean squared error) loss function (e.g. in a linear model), we can calculate *proportion of variance explained* and use it to compare how well models fit the data. This calculation is as follows.\n\nConsider observations $i = 1,...,n$ with responses $\\mathbf{y} = (y_i)_{i=1}^n$ and suppose we have built a model that predicts $\\hat{y}_i$ for the observations. \n\nThe most basic model we can propose is the mean model, i.e. for every observation, we predict $\\bar{y}$, the mean of the data. Since this is the most basic model possible, we'll call it the **null model**. Technically, this is the model with *one* parameter, and we find that the value of the parameter that minimises the MSE loss function is indeed the mean \\[derivation not shown here\\]. \n\nIn this case, what is the average discrepancy between an actual response and a predicted response? Since we are using the MSE loss function, then the \"discrepancy\" is measured by the square of the difference, and so the average discrepancy is (proportional to) the sum of squares:\n$$\nV_0 := \\sum_i (y_i - \\bar{y})^2\n$$\nWe see that the above is (proportional to) the (sample) variance of the $y_i$.\n\nWe hope that our proposed model is better than the null model, i.e. we hope that the discrepancy between the actual and predicted responses is, on average, less for our model, that the predicted model. Using the same measure of \"discrepancy\" as above, the average discrepancy for our model is (proportional to):\n$$\nV_1 := \\sum_i (y_i - \\hat{y}_i)^2\n$$\nWe see that this is (proportional to) the variance of the residuals of our model.\n\nSo, if our model is indeed an improvement on the mean model, then one measure of the improvement is how much closer the predictions are to the actuals on average, i.e.:\n$$\nV_0 - V_1 > 0 \\textrm{ (hopefully) } \n$$\nAs an absolute value, the above is difficult to put into context, so let's ensure it is under $1$ by calculating:\n$$\n\\frac{V_0 - V_1}{V_0}\n$$\nTo summarise, we've created a measure of how good our model is, that usually between $0$ and $1$, and the closer to $1$, the better. From the previous descriptions, we can call this the proportion of variance of the responses that is *explained* by the model.\n\nAs you can see, to calculate the proportion of variance explained, we only needed $V_0$ and $V_1$ up to a constant of proportionality (because it cancels out in the final division), which is why I've been careful to state \"proportional to\" in various places above. In practice, we usually set that constant equal to $-2$ - there are other theoretical reasons why this is chosen - and define the function:\n$$\nD(\\mathbf{y}, \\hat{z}) = -2 \\sum_i (y_i - z_i)^2\n$$\nso that\n$$\nV_0 = D(\\mathbf{y}, (\\bar{y})_{i=1}^n) =: D_0 \\\\\nV_1 = D(\\mathbf{y}, (\\hat{y})_{i=1}^n) =: D_1\n$$\nand\n$$\n\\frac{V_0 - V_1}{V_0} = \\frac{D_0 - D_1}{D_0}\n$$\nThis will allow us to align the notation with the generalisation below.","86c09dad":"<div align=\"right\" style=\"text-align: right\"><a href=\"#Contents\">Back to Contents<\/a><\/div>\n\n# Residuals\nContinuing from the above example, the following recreates calculate of the residuals, to confirm the formulae are correct.","e93165cc":"## Build the models","5c106b69":"### Why is there a $2$ in $D$?\n\nThe $2$ is because of [Wilks' theorem](https:\/\/en.wikipedia.org\/wiki\/Wilks%27_theorem) for the asymptotic distribution of the log of the likelihood ratio. Specifically, it says that\n$$\n2 (ll(\\mathbf{y}, \\mathbf{y}) - ll(\\mathbf{y}, \\mathbf{\\mu})) \\; \\dot\\sim \\; \\chi^2_{n-p}\n$$\nwhere ${df}_{saturated} = n$ and ${df}_{model} = p$.\n\nA hypothesis test which uses this as a test statistic is called the likelihood ratio test. Various tests can be posed as a likelihood ratio test. \n\nIn the above, the alternative model (i.e. saturated model, in this case) with more parameters will have greater likelihood than the null model (i.e. parameterised), but test seeks to answer \"is the increase in likelihood significant?\". In reality, you won't want to do this using the saturated model as the alternative, but a model with more factors.\n\n**Caution**: Actually, in the general case, this means that the *scaled* deviance $D* = D \/ \\phi$ is $\\chi^2_{n-p}$, but we're only considering Poisson regression here, where $\\phi$ is known to be $1$. Also, Wilks' theorem only holds for *fixed* degrees of freedom as the number of observations goes to infinity, but in the above, it is $n$ that is going to infinity. Therefore, we need other results to show the above, and it doesn't hold in all cases, notable for logistic regression. \n\nHowever, at this point, we're just trying to motivate the $2$ on deviance, which the above does.","671f45d6":"**NOT COMPLETE**","d21f0e14":"## Incorporating exposure\nSuppose we've used Poisson loss and incorporated exposure into our model, i.e. we've used the $\\log$ *link* function and set an *offset* of $\\log(e_i)$ for each observation with exposure $e_i$. That means that the model predictions are of the form $\\hat{y}_i = \\exp(\\log(e_i) + \\hat{\\eta}_i) = e_i \\exp(\\hat{\\eta}_i)$. How does this change our measure \"proportion of deviance explained\"?\n\nWell, in this case, we consider that the exposures $e_i$ are *known*. This is important because now the most basic model makes predictions $e_i\\exp(\\hat{\\beta})$ for each observation $i$. Note that these predictions *do* now vary, but this is the null model because it uses one *unknown* parameter $\\beta$.\n\nWhat is the value of $\\hat{\\beta}$? We find it by minimising the loss function. Specifically, the loss function of the data is as follows (it is convenient to define $Y_i := y_i \/ e_i$, i.e. $y_i$ is the count of responses and $Y_i$ is the frequency of responses):\n$$\nD\\left(\\mathbf{y}, (e_i\\exp(\\beta))_{i=1}^n\\right) = -2\\sum_i \\left(y_i\\log\\left(\\frac{y_i}{e_i\\exp(\\beta)}\\right) - y_i + e_i\\exp(\\beta)\\right) \\\\\n= -2 \\sum_i e_i \\left(Y_i\\log\\left(\\frac{Y_i}{\\exp(\\beta)}\\right) - Y_i + \\exp(\\beta)\\right)\n= -2 \\sum_i e_i \\left(Y_i\\log(Y_i) - Y_i\\beta - Y_i + \\exp(\\beta)\\right)\n$$\nTo find the turing point (a minimum), we differentiate with respect to $\\beta$ and set equal to zero, to get\n$$\n\\left.\\frac{\\partial}{\\partial\\beta} D\\left(\\mathbf{y}, (e_i\\exp(\\beta))_{i=1}^n\\right) \\right|_{\\beta=\\hat{\\beta}} = 0\n= \\sum_i e_i (Y_i - \\exp(\\hat{\\beta})) = \\sum_i e_i Y_i - \\exp(\\hat{\\beta}) \\sum_i e_i  \\\\\n\\Rightarrow \\exp(\\hat{\\beta}) = \\frac{\\sum e_i Y_i}{\\sum e_i} = \\frac{\\sum y_i}{\\sum e_i}\n$$\n\nThat is, $\\exp(\\hat{\\beta})$ is the *weighted* average of the $Y_i$, and the weights are the exposures.\n\nThen the null deviance is\n$$\nD_0 := D(\\mathbf{y}, (e_i\\exp(\\hat{\\beta}))_{i=1}^n) = \\sum_i d(y_i, e_i\\exp(\\hat{\\beta})) = \\sum_i d\\left(y_i, e_i\\frac{\\sum_j y_j}{\\sum_j e_j}\\right)\n$$\n\nThe model deviance $D_1$ remains as above, so we can calculate the proportion of deviance explained as usual.\n","97d31812":"## Load and process the data","cef5d7c1":"<div align=\"right\" style=\"text-align: right\"><a href=\"#Contents\">Back to Contents<\/a><\/div>","38ae1358":"<div align=\"right\" style=\"text-align: right\"><a href=\"#Contents\">Back to Contents<\/a><\/div>\n\n## Setup","8605326e":"## Analyse the results","2dd3413c":"# Explaining GLMs\nNotes about my investigations of Generalised Linear Models (GLMs).","4548b6a7":"Let\n- $i = 1, ..., n$ = Rows of the input data for a model\n- $f_i$ = Frequency weights\n- $w_i$ = Variance weights\n- $e_i$ = Exposure values\n- Scale, $\\phi$, is always 1, so is omitted\n- Possible response values:\n    - $Y_i$ = Original counts, or summed counts\n    - $y_i = Y_i \/ e_i$ = Rate\n\nIn general:\n$$\nll_i(y_i, \\mu_i) = w_i \\left( y_i\\ln\\mu_i - \\mu_i + \\textrm{const in } y_i \\right) \\\\\nll(\\mathbf{y}, \\mathbf{\\mu}) = \\sum_i f_i ll_i(y_i, \\mu_i) = \n\\sum_i f_i w_i \\left( y_i\\ln\\mu_i - \\mu_i + \\textrm{const in } y_i \\right) \\\\\nD = 2 (ll(\\mathbf{y}, \\mathbf{y}) - ll(\\mathbf{y}, \\mathbf{\\mu})) = \n2 \\sum_i f_i (ll_i(y_i, y_i) - ll_i(y_i, \\mu_i)) = \n2 \\sum_i f_i w_i \\left( y_i \\ln\\left(\\frac{y_i}{\\mu_i}\\right) - y_i + \\mu_i \\right)  = \\sum_i D_i \\\\\n\\Rightarrow D_i = 2 f_i w_i \\left( y_i \\ln\\left(\\frac{y_i}{\\mu_i}\\right) - y_i + \\mu_i \\right) \\\\\n$$\n\nAlso:\n$$\n{resid\\_dev}_i = {sign}(y_i - \\mu_i) \\sqrt{D_i} \\Rightarrow D = \\sum_i {resid\\_dev}^2 \\\\\n{resid\\_response}_i = y_i - \\mu_i \\textrm{ (not weighted) } \\\\\n{resid\\_pearson}_i = \\frac{y_i - \\mu_i}{\\sqrt{V(\\mu_i) \/ w_i}} = \\frac{\\sqrt{w_i}(y_i - \\mu_i)}{\\sqrt{\\mu_i}}\n$$","efdbd9d2":"### 'Aggregated count_sum with exposure' (or offset) vs 'Aggregated count_mean with var_weights'\nThese are the same, apart from log likelihood. We can see why as follows.\n\nLet $Y_i$ = count_sum, $e_i$ = exposure, and $y_i = Y_i \/ e_i$ = rate. The model for 'count_sum with exposure' is \n$$\ne_i\\exp(\\mathbf{x}_i \\beta) = \\exp(\\ln(e_i) + \\mathbf{x}_i \\beta) = \\mu_i = Y_i \\\\\n\\Rightarrow D_i = Y_i\\ln\\left(\\frac{Y_i}{\\mu_i}\\right) - Y_i + \\mu_i = e_i\\left(y_i \\ln\\left(\\frac{y_i}{\\exp(\\mathbf{x}_i \\beta)}\\right) - y_i + \\exp(\\mathbf{x}_i \\beta) \\right)\n$$\nwhich we see is exactly the same as if we had $w_i = e_i$ and response $y_i$.\n\nThe only difference is in log likelihood. We see from the above formulae, the difference must be due to the '$\\textrm{const in }y$' part of the log likelihood.\n\n**So**: These are both modelling the same case, where:\n- One observation (for the purpose of calculating degrees of freedom) is the *aggregated observation* and the response is the frequency, and...\n- ...the observations are weighted by exposure.\nNote 'Aggregated count_mean with var_weights' is more generalisable, because `exposure` is only allowed for `log` link (although you can use `offset` for the same thing), whereas observation weights (i.e. `var_weight`) can be used irrespective of link function.","3ec3ad03":"## Generalisation\nIn general, we want to create a similar metric, but for loss functions other than MSE. That is, instead of the measure of \"discrepancy\" between actual and predicted value being the difference sqaured, we can use the *deviance* function.\n\nLet's look at the case of Poisson loss function. In this case, the (individual) deviance of an observation that has actual response $y_i$ and predicted response $p_i$ is given by:\n$$\nd_i = d(y_i, p_i) = -2\\left(y_i\\log\\left(\\frac{y_i}{p_i}\\right) - y_i + p_i\\right)\n$$\nAnd the total deviance of all the responses is given by\n$$\nD(\\mathbf{y}, \\mathbf{p}) = \\sum_i d(y_i, p_i)\n$$\n\nWe can follow the same steps as above to calculate the \"*proportion of deviance explained*\" as a measure of predictivity of the model. Again, it is usually between $0$ and $1$, with values closer to $1$ suggesting a more predictive model.\n\nSpecifically, under the Poisson loss function, the most basic model (i.e. null model) is still the model that predicts the mean* of the data for every observation. So the null deviance is\n$$\nD_0 := D(\\mathbf{y}, (\\bar{y})_{i=1}^n) = \\sum_i d(y_i, \\bar{y})\n$$\nand the model's deviance is\n$$\nD_1 := D(\\mathbf{y}, (\\hat{y}_i)_{i=1}^n) = \\sum_i d(y_i, \\hat{y}_i)\n$$\nWhich gives the proportion of deviance explained as\n$$\n\\frac{D_0 - D_1}{D_0}\n$$\n\n\\* Again, we have not shown here that the one parameter that minimises the value of the Poisson loss function is the mean.","eaf6c9a9":"### 'Condensed with freq_weights' vs 'Condensed with var_weights'\nThese highlight the differnce between `freq_weights` and `var_weights`, i.e.: \n- `var_weights` tell the model the relative *credibility* of each observation, for the purpose of fitting. That is, the model is aiming for the *weighted* average of the responses.\n- `freq_weights` are the same as `var_weights` but *also* sum to the *effective number of observations* (for the calculation of `df_resid`).\n\nIn other words:\n- `var_weights` say that each row is one observation but some are more credible than others.\n- `freq_weights` say that each row is actually the sum of $f_i$ number of *independent* trials, and that each of these is a *degree of freedom*. \n\nTherefore, for the given *condensed* data, `freq_weights` is the appropriate choice. \n\nFor the *aggregated* data, `freq_weights` is also the correct choice to replicate the original model. If, on the other hand, we supposed that each row in the aggregated data is actually one observation rather than multiple observations, then `var_weights` is the correct choice (and we've shown it matches using `exposure`).","eea6fc98":"Suppose we want to assess the performance of the model considering how close, on average, a prediction is to an actual. For each point, we need to calculate the prediction of `count_sum` *during the period* of that observation. This is the \ncase even for those models that predict *frequency* from the model - in this case, we must multiply the predicted frequency by `exposure` to get a prediction of `count_sum`.\n\nFor example, to calculate **lift**, you'll need take this into account (as it is comparing actual vs predicted on sets of data). \n\nAs we used the canonical link ($\\log$) for all models, the (weighted) sum of the raw residuals should be zero. We can use this as a check of the above.","0ff536eb":"<div align=\"right\" style=\"text-align: right\"><a href=\"#Contents\">Back to Contents<\/a><\/div>\n\n## Visualise the loss function","e0bd9272":"<div align=\"right\" style=\"text-align: right\"><a href=\"#Contents\">Back to Contents<\/a><\/div>\n\n# Weight and exposure\nAdapted from the examples from here: <https:\/\/www.statsmodels.org\/devel\/examples\/notebooks\/generated\/glm_weights.html>","4f4d6821":"### Question: Does exposure work for the Gamma regression?\n\nIn the Gamma case,\n$$\nD_i = \\frac{y_i}{\\mu_i} - 1 - \\ln\\left(\\frac{y_i}{\\mu_i}\\right)\n$$\n\nThen, with the **log link** function (which is *not* the canonical link for Gamma), we proceed as before: Let $Y_i$ = sum, $e_i$ = exposure, and $y_i = Y_i \/ e_i$ = rate (and denote $\\eta_i = \\mathbf{x}_i \\beta$). The model for 'sum with exposure' is \n$$\n\\textrm{log link } \\Rightarrow e_i\\exp(\\eta_i) = \\exp(\\ln(e_i) + \\eta_i) = \\mu_i = Y_i \\\\\n\\Rightarrow D_i = \\frac{Y_i}{\\mu_i} - 1 - \\ln\\left(\\frac{Y_i}{\\mu_i}\\right) =\n\\frac{y_i e_i}{e_i \\exp(\\eta_i)} - 1 - \\ln\\left(\\frac{y_i e_i}{e_i \\exp(\\eta_i)}\\right) =\n\\frac{y_i}{\\exp(\\eta_i)} - 1 - \\ln\\left(\\frac{y_i}{\\exp(\\eta_i)}\\right)\n$$\nwhich we see is exactly the same as if we had response $y_i$ and ***no*** weights.\n\nSo this is different to the Poisson case, but still valid. In fact, the Pearson residuals will be also be the same (and consequently the chi-squared statistic, which is the sum of squares of the Pearson residuals):\n$$\n{resid\\_pearson}_i = \\frac{Y_i - \\mu_i}{\\sqrt{V(\\mu_i) \/ w_i}} = \\frac{Y_i - \\mu_i}{\\mu_i}\n\\textrm{ (since weights are 1 and } V(\\mu) = \\mu^2 \\textrm{ for Gamma) } \\\\\n= \\frac{y_i e_i - e_i \\exp(\\eta_i)}{e_i \\exp(\\eta_i)} = \n\\frac{y_i - \\exp(\\eta_i)}{\\sqrt{\\exp(\\eta_i)^2}}\n$$\nwhich is exactly the same as if we had response $y_i$ and no weights.\n\nWhat if, instead, we used the canonical link function for Gamma. The same approach will *not* work (the following shows the case where $e_i$ is the *offset*, rather than logging it to get the exposure):\n$$\n\\textrm{inverse link } \\Rightarrow Y_i = \\mu_i = \\frac{1}{e_i + \\eta_i} \\\\\n\\Rightarrow D_i = \\frac{Y_i}{\\mu_i} - 1 - \\ln\\left(\\frac{Y_i}{\\mu_i}\\right) =\ny_i e_i(e_i + \\eta_i) - 1 - \\ln\\left(y_i e_i(e_i + \\eta_i)\\right)\n$$\nBut we want\n$$\ne_i\\left(\\frac{y_i}{1\/\\eta_i} - 1 - \\ln\\left(\\frac{y_i}{1\/\\eta_i}\\right)\\right) = e_i(y_i\\eta_i - 1 - \\ln(y_i\\eta_i))\n$$\nand we see that these are *not* equal (to see this, try taking one away from the other)."}}