{"cell_type":{"d1309748":"code","d0c44bbe":"code","c18ef097":"code","6a851192":"code","9a7e14c7":"code","642640e6":"code","01335c66":"code","a6dd9040":"code","e7b87c3e":"code","548b882e":"code","ceeb6118":"code","9e8fa31e":"code","cb8bd120":"code","131ac884":"code","139bb204":"code","6ac27fce":"code","f908fcaf":"code","9e482726":"code","eb269b30":"code","6aa8c9fc":"code","1169f9a1":"code","e490013d":"markdown","ac538f85":"markdown","7419b018":"markdown","cb464fc5":"markdown","092c0781":"markdown","4852ae34":"markdown","fbca559f":"markdown"},"source":{"d1309748":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","d0c44bbe":"df = pd.read_csv('..\/input\/titanic\/titanic_prepared.csv')","c18ef097":"df.head()","6a851192":"df.info()","9a7e14c7":"X = df.drop(['Survived'], axis = 1)\ny = df['Survived']","642640e6":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)","01335c66":"from sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier(max_depth=5, max_leaf_nodes=10, criterion='entropy')\n\nmodel.fit(X_train,y_train)","a6dd9040":"y_pred = model.predict(X_test)","e7b87c3e":"from sklearn.tree import plot_tree\nfrom sklearn.metrics import confusion_matrix,classification_report\n\ndef report_model(model):\n    model_preds = model.predict(X_test)\n    print(classification_report(y_test,model_preds))\n    print('\\n')\n    plt.figure(figsize=(12,8),dpi=150)\n    plot_tree(model,filled=True,feature_names=X.columns);","548b882e":"pd.DataFrame(index=X.columns,data=model.feature_importances_,columns=['Feature Importance'])","ceeb6118":"report_model(model)","9e8fa31e":"from sklearn.pipeline import Pipeline","cb8bd120":"model_grid_search = DecisionTreeClassifier()\nmodel_grid_search.get_params().keys()\n\noperations= [('dec_tree', model_grid_search)]\n\npipe= Pipeline(operations)","131ac884":"from sklearn.model_selection import GridSearchCV","139bb204":"max_leaf_node = range(1, 25)\n\ncriterion = ['gini', 'entropy']\n\nmax_depth = range(1, 20)","6ac27fce":"param_grid = {'dec_tree__criterion':criterion, 'dec_tree__max_depth':max_depth, 'dec_tree__max_leaf_nodes':max_leaf_node}","f908fcaf":"full_cv_classifier= GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy')","9e482726":"full_cv_classifier.fit(X_train, y_train)","eb269b30":"full_cv_classifier.best_estimator_.get_params()","6aa8c9fc":"new_model = DecisionTreeClassifier(max_depth=4, max_leaf_nodes=9, criterion='gini')\nnew_model.fit(X_train, y_train)","1169f9a1":"report_model(new_model)","e490013d":"# Finding Hyperparameters with GridSearch:\n#### to find hyperparameters with another way, we can use __GridSearchCV__ from  sklearn . in this method we don't need to spend time to manually find hyperparameters.","ac538f85":"# Data Overview:","7419b018":"# Decision Trees:\n>Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. Decision trees can handle both categorical and numerical data.\n\n#### in this notebook i have used decision tree for classification on titanic dataset.for better understanding decision tree you can read this [article](https:\/\/medium.com\/swlh\/decision-tree-classification-de64fc4d5aac)","cb464fc5":"# Evaluating the Model:","092c0781":"# Train the Model:","4852ae34":"# Split the Dataset to Train & Test set:","fbca559f":"#### as you can see in below with max_depth equals to 5 , max_leaf_nodes equals to 10 and 'entropy' as criterion , our model accuracy is 85."}}