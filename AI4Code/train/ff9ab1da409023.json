{"cell_type":{"1137577c":"code","10d7c6cf":"code","1ce38b38":"code","1536efc8":"code","56d2f308":"code","e7653a8d":"code","7bd320a7":"code","b826a76c":"code","65b70848":"code","e8a4dbd4":"code","f0f66be6":"code","c8112e00":"code","8bd6eb04":"code","4042b9e2":"code","32b9ec27":"markdown","a25e2b28":"markdown","41abd419":"markdown","212b808b":"markdown","5dfffcf2":"markdown","620fc745":"markdown","195f4e11":"markdown","d58222af":"markdown","01343d61":"markdown","17ab3b9e":"markdown","130ef84b":"markdown","12ee66dc":"markdown","6f3dc59b":"markdown","e8ff6749":"markdown","3229fe3d":"markdown","088abfc4":"markdown","f5620449":"markdown","2235b69e":"markdown","62695e5c":"markdown","b2e0da05":"markdown","267f00ad":"markdown","4bff38c5":"markdown","efd99563":"markdown"},"source":{"1137577c":"import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('train.csv')\ndf.head(6)","10d7c6cf":"def class_survival(Pclass):\n    class_surv = 0\n    total = 0\n    classes = df[['Survived','Pclass']]\n    for i, row in classes.iterrows():\n        if classes['Pclass'][i] == Pclass:\n            class_surv += classes['Survived'][i]\n            total = total + 1\n    surv_rate = class_surv\/total\n    print(f'Class {Pclass} survival rate: {surv_rate}')\n    return surv_rate\nclass1_surival = class_survival(1)\nclass2_surival = class_survival(2)\nclass3_surival = class_survival(3)","1ce38b38":"def agefill(Pclass):\n    agefill = df[['Age', 'Pclass']].copy()\n    for i, row in agefill['Pclass'].iteritems():\n        if agefill['Pclass'][i] != Pclass:\n            agefill = agefill.drop(i)\n    Pclass_mean = np.nanmean(agefill.iloc[:,0].copy())\n    return Pclass_mean\nones_mean = agefill(1)\ntwos_mean = agefill(2)\nthrees_mean = agefill(3)\nprint(f'Class 1 avg age: {ones_mean}\\nClass 2 avg age: {twos_mean}\\nClass 3 avg age: {threes_mean}')","1536efc8":"malesurv, males, females, femalesurv = [0 for _ in range(4)]\n\ngend = df[['Survived','Sex']]\nfor i1, row1 in gend.iterrows():\n    if gend['Sex'][i1] == str('male'):\n        malesurv += gend['Survived'][i1]\n        males = males + 1\n    else:\n        femalesurv += gend['Survived'][i1]\n        females = females + 1\nprint(f'Female survival rate: {femalesurv\/females}\\nMale survival rate: {malesurv\/males}')","56d2f308":"last_names = df['Name'].str.split(\",\", expand=True)[0]\nis_balkan = list()\nfor name in range(len(last_names)):\n    if last_names[name][-2:] != 'ic' and last_names[name][-2:] != 'ff' and last_names[name][-2:] != 'ulos':\n        is_balkan.append(0) \n    else:\n       is_balkan.append(1)\nprint(f'There were (at least) {np.sum(is_balkan)} Southeastern Europeans in the training set.')","e7653a8d":"titles = df['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\ntitles_investigate = pd.DataFrame(data=(df['Survived'],titles)).transpose()\ntypes_titles = pd.DataFrame(data=set( val for dic in titles for val in titles.values)) #17 types of titles\n\ndef title_survive(name_title):\n    name_title_survive = 0\n    name_title_total = 0\n    survival_rate = 0\n    for title, rowz in titles_investigate.iterrows():\n        if titles_investigate.iloc[title,1] == name_title:\n            name_title_total += 1\n            name_title_survive += titles_investigate.iloc[title,0]\n    survival_rate = name_title_survive \/ name_title_total\n    return survival_rate\n\ntitles2 = []\nfor type_title in range(len(types_titles)):\n    titles2.append(title_survive(types_titles[0][type_title]))\n    print(f'{types_titles[0][type_title]} had survivability chance: {titles2[type_title]}')","7bd320a7":"embark = df[['Survived','Embarked']]\ndef embarkchance(port):\n    port_embark = 0\n    port_survive = 0\n    for ie, rowe in embark.iterrows():\n        if embark.iloc[ie,1] == port:\n            port_embark += 1\n            if embark.iloc[ie,0] == 1:\n                port_survive += 1\n    survival_rate = port_survive \/ port_embark\n    return survival_rate\n    \nS_embark = embarkchance('S')\nC_embark = embarkchance('C')\nQ_embark = embarkchance('Q')\nprint(f'Embarking at Port S had survivability chance: {S_embark}\\nEmbarking at Port C had survivability chance: {C_embark}\\nEmbarking at Port Q had survivability chance: {Q_embark}')","b826a76c":"# Age feature\nagefill = df[['Age', 'Pclass']].copy()\nfor i, row in agefill.iterrows():\n    if np.isnan(agefill['Age'][i]):\n        if agefill['Pclass'].iloc[i] == 3:\n            agefill['Age'].iloc[i] = threes_mean\n        elif agefill['Pclass'].iloc[i] == 2:\n            agefill['Age'].iloc[i] = twos_mean\n        else:\n            agefill['Age'].iloc[i] = ones_mean\n    \nagecol = agefill.iloc[:,0].copy()\n\n# Gender feature\ngendercol = df['Sex'].copy()\nfor i2, row2 in gendercol.iteritems():\n    if gendercol[i2] == str('male'):\n        gendercol[i2] = 1\n    else:\n        gendercol[i2] = 2\n\n# Is_balkan feature \n# is_balkan\n\n# Type of title associated\ntypes_titles['Survival_Chance'] = titles2\n\ntitles3 = []\nfor title, row4 in titles.iteritems():\n    for title_type, row5 in types_titles.iterrows():\n        if titles[title] == types_titles.iloc[title_type,0]:\n            titles3.append(types_titles.iloc[title_type,1])\n            \n# Port where passenger embarked\nports = []\nfor embark_port, rowport in embark.iterrows():\n        if embark.iloc[embark_port,1] == 'S':\n            ports.append(S_embark)\n        elif embark.iloc[embark_port,1] == 'C':\n            ports.append(C_embark)\n        else:\n            ports.append(Q_embark)","65b70848":"X = np.array(np.column_stack((df[['Pclass', 'Fare', 'Parch']],agecol, gendercol, is_balkan, titles3, ports))).copy()\ny = np.array(df[['Survived']].copy()).ravel()","e8a4dbd4":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)","f0f66be6":"from sklearn.model_selection import KFold\nkf = KFold(n_splits = 5)\nfor train_index, test_index in kf.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]","c8112e00":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\nmodel = SVC(gamma=1, C=2)\nmodel.fit(X_train, y_train)\ny_predict = model.predict(X_test)\n\ncm = confusion_matrix(y_test, y_predict)\nacc = accuracy_score(y_predict, y_test)\nprint('CM:\\n {}'.format( cm))\nprint('Accuracy: {}'.format(acc))","8bd6eb04":"# For SVM, an example optimization (for demonstration purposes) would be\n\nfrom sklearn.model_selection import GridSearchCV\nparameters = {\n    'gamma': [1, 2],\n    'C': [1, 2],\n    }\ngs = GridSearchCV(model, parameters, cv=3)\ngs.fit(X, y)\nprint(gs.best_params_)","4042b9e2":"sub_format = np.column_stack((df_real['PassengerId'].astype(int),y_predict_test.astype(int)))\ndf_submit = pd.DataFrame(data=sub_format)\ndf_submit.columns = ['PassengerId','Survived']\ndf_submit.to_csv(r'~\\titanic\\submit.csv', index=False)","32b9ec27":"The <a href=\"https:\/\/www.kaggle.com\/c\/titanic\/overview\">task involves a training set with known survival outcomes and a test set with unknown survival outcomes <\/a>.","a25e2b28":"<h4>Hyperparameter Tuning <\/h4>\nTuning the hyperparameters of the model would help us achieve a hire training accuracy (though the model may start to overfit, i.e. be too closely fitted to the training data and perform poorly on test data). We can employ a <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\"> GridSearch technique <\/a> to test combinations of a set of hyperparameters of interest. Then, we will replace our model = SVC(gamma=a, C=b) with the desired combination of a and b.","41abd419":"I approached the problem with 3 separate families of classification algorithms :<br><a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LassoCV.html\">LassoCV<\/a> (a cross-validated Lasso regression, where >=0.5 == 1 and <0.5 == 0 for survival).<br> <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html\">XGBoost<\/a> classifier (an extreme gradient boosting ensemble method).<br><a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.GaussianProcessClassifier.html\">Gaussian Process Classifier<\/a> (a probabilistic approach). ","212b808b":"Of course, I ran several other algorithms which belong to these 3 families of classification algorithms (e.g. SVM, AdaBoost, Random Forest) to validate results.","5dfffcf2":"A little biased, but I noticed everyone with a Southeastern European name (**I'm Bulgarian**) did not survive.<br> Specifically, 27\/27 with a name ending in '-ff', 19\/19 with a name ending in '-ic', as well as 5\/5 edning in '-ulos'. <br>Granted most had 3rd class tickets, but I decided to include the ethnic names as a feature to test it out.","620fc745":"I replaced the NaN values in Age with the average age per class. The rationale is that Pclass is easiest to cross reference age with. Higher class tickets have a higher average age (as well as a higher chance of survival).","195f4e11":"To ensure we are leveraging the maximum of our training set, we will employ <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.KFold.html\">K-Fold Cross-Validation to shuffle our training and testing splits<\/a>. ","d58222af":"Which port the passengers boarded the Titanic also seems to have an effect. Let's see survivability stats by port:","01343d61":"<h1>Practical Titanic Using Scikit-Learn<\/h1>\n<h3> First principles of data science approach to the Hello World of ML <br> Kliment Minchev, Jan 2020<\/h3>","17ab3b9e":"<h2>Feature Selection <\/h2>\nRight off the bat, I discarded PassengerId, Ticket, and Cabin since they carry random (indiscernible) information. <br> Let's see survivability stats by class ticket.","130ef84b":"Finally, let's give variable names to the 5 newly created features for survivability: Age, Gender, Is_Balkan, Title, Port ","12ee66dc":"<h4>Performance Summary<\/h4>\n<table align=\"left\" width=\"50%\">\n  <tr>\n    <th>Model<\/th>\n    <th>Train Accuracy [%]<\/th>\n    <th>Test Accuracy [%]<br>(Kaggle score)<\/th>\n  <\/tr>\n    <tr>\n    <td>Gaussian Process Classifier<\/td>\n    <td>87.07<\/td>\n    <td>78.95<\/td>\n  <\/tr>\n  <tr>\n    <td>XGBoost<\/td>\n    <td>80.00<\/td>\n    <td>76.55<\/td>\n  <\/tr>\n    <tr>\n    <td>LassoCV<\/td>\n    <td>74.23<\/td>\n    <td>67.94<\/td>\n  <\/tr>\n    <tr>\n    <td>SVM<\/td>\n    <td>81.46<\/td>\n    <td><\/td>\n  <\/tr>\n    <tr>\n    <td>Random Forest<\/td>\n    <td>83.11<\/td>\n    <td><\/td>\n  <\/tr>\n    <tr>\n    <td>AdaBoost<\/td>\n    <td>86.53<\/td>\n    <td><\/td>\n  <\/tr>\n<\/table>","6f3dc59b":"Below are: our independent variable matrix X (all the features) and our dependent variable vector Y (our label, survival)","e8ff6749":"Import the two best know data pre-processing libraries (pandas and numpy) and load the training data.","3229fe3d":"<h3><i>What the data says:<\/i><\/h3><br> Using SVM, we were able to predict 81.46% of the survival outcomes in our <i>Training set<\/i>.<br><br> The <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.confusion_matrix.html\">Confusion Matrix<\/a> yields another two valuable metrics: <br>Precision (fraction of survivors from the pool we believed would survive) <br> Recall (fraction of the ones we believed would survive from the total pool of survivors). <br> Precision: 101\/(101+14) = 0.8783 <br> Recall: 101\/(101+19) = 0.8417","088abfc4":"<h2>Model Fitting<\/h2>","f5620449":"Many of the names had a title associated, i.e. Mister, Master, Miss, Mrs, etc. Let's see survivability stats by title:","2235b69e":"<h2>Feature Construction <\/h2>\nLet's do some <b>feature engineering<\/b> to give our algorithm more clues and improve accuracy. <br> At a glance, it seems that <u>females had a higher chance of survival than males<\/u>. So, we will <i>add a gender_column as a feature<\/i>.","62695e5c":"Because all our features have different ranges of data, e.g. binary (gender), linear (age) etc, we will employ a <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\">scaler to have a unit range for our data<\/a>.","b2e0da05":"<h2>Model Testing and Kaggle Test Accuracy<\/h2>","267f00ad":"**At this stage, we are ready to fit any model of our choice and evaluate it using common metrics.** <br> <h4> As an example, I demonstrate SVM with accuracy_score and a confusion_matrix (to calculate Precision and Recall). <br>Any scikit-learn model of your choice can be fitted.<\/h4> <h5> My Kaggle submission was done using a Gaussian Process Classifier, as it yielded the highest test accuracy.","4bff38c5":"The final step is to use the approach we developed above for the test set (separate file, named test.csv without a known Survival label). <br><br>Same exact functions developed above are used for constructing the features for our new Test set: Gender, Southeast European name, Title, and Port the passenger embarked the Titanic. <br>Finally, we generate an output csv file of our predictions in the desired Kaggle competition format shown below:","efd99563":"<h4>TL;DR<\/h4>An overview of my personal approach to developing Data Science insights. Below is a top quartile, easily portable solution to the <a href=\"https:\/\/www.kaggle.com\/c\/titanic\/overview\">Kaggle Titanic problem.<\/a> I used a probabilistic algorithm (Gaussian Process) to achieve 87% training accuracy and 78.95% Kaggle testing accuracy."}}