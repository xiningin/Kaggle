{"cell_type":{"8d670b9a":"code","8044a534":"code","6d837fd0":"code","3c09913a":"code","0d2d1c2b":"code","a38dd6ec":"code","bf325b0b":"code","6d81ec64":"code","dbd53fd5":"code","a0dcf55f":"code","b2acef17":"code","583778cb":"code","4073eebc":"code","60e52d39":"code","81034b65":"code","e99d5e72":"code","1a409d5b":"code","6e696c2c":"code","480c9cbc":"code","f893b342":"code","61035ef6":"code","1b8ce914":"code","eb7b5752":"code","01d10cba":"code","3e9e74d7":"code","18e81264":"code","cfc94cfa":"markdown","bc28bda1":"markdown","b5ae81fa":"markdown","801ef398":"markdown","870cefc4":"markdown","9dd68733":"markdown","e167dc1e":"markdown","3e414b5e":"markdown"},"source":{"8d670b9a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (6,6)\n\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints\n\nfrom keras.layers import Dense, Input, LSTM, Bidirectional, Activation, Conv1D, GRU, TimeDistributed\nfrom keras.layers import Dropout, Embedding, GlobalMaxPooling1D, MaxPooling1D, Add, Flatten, SpatialDropout1D\nfrom keras.layers import GlobalAveragePooling1D, BatchNormalization, concatenate\nfrom keras.layers import Reshape, merge, Concatenate, Lambda, Average\nfrom keras.models import Sequential, Model, load_model\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.initializers import Constant\nfrom keras.layers.merge import add\n\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer, text_to_word_sequence\nfrom keras.utils import np_utils\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","8044a534":"# load data\n\ndf = pd.read_json('..\/input\/news-category-dataset\/News_Category_Dataset.json', lines=True)\ndf.head()","6d837fd0":"cates = df.groupby('category')\nprint(\"total categories:\", cates.ngroups)\nprint(cates.size())","3c09913a":"# as shown above, THE WORLDPOST and WORLDPOST should be the same category, so merge them.\n\ndf.category = df.category.map(lambda x: \"WORLDPOST\" if x == \"THE WORLDPOST\" else x)","0d2d1c2b":"# using headlines and short_description as input X\n\ndf['text'] = df.headline + \" \" + df.short_description\n\n# tokenizing\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df.text)\nX = tokenizer.texts_to_sequences(df.text)\ndf['words'] = X\n\n# delete some empty and short data\n\ndf['word_length'] = df.words.apply(lambda i: len(i))\ndf = df[df.word_length >= 5]\n\ndf.head()","a38dd6ec":"df.word_length.describe()","bf325b0b":"# using 50 for padding length\n\nmaxlen = 50\nX = list(sequence.pad_sequences(df.words, maxlen=maxlen))","6d81ec64":"# category to id\n\ncategories = df.groupby('category').size().index.tolist()\ncategory_int = {}\nint_category = {}\nfor i, k in enumerate(categories):\n    category_int.update({k:i})\n    int_category.update({i:k})\n\ndf['c2id'] = df['category'].apply(lambda x: category_int[x])","dbd53fd5":"word_index = tokenizer.word_index\n\nEMBEDDING_DIM = 100\n\nembeddings_index = {}\nf = open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s unique tokens.' % len(word_index))\nprint('Total %s word vectors.' % len(embeddings_index))","a0dcf55f":"embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\nembedding_layer = Embedding(len(word_index)+1,\n                            EMBEDDING_DIM,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=maxlen,\n                            trainable=False)","b2acef17":"# prepared data \n\nX = np.array(X)\nY = np_utils.to_categorical(list(df.c2id))\n\n# and split to training set and validation set\n\nseed = 29\nx_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=seed)","583778cb":"inp = Input(shape=(maxlen,), dtype='int32')\nembedding = embedding_layer(inp)\nstacks = []\nfor kernel_size in [2, 3, 4]:\n    conv = Conv1D(64, kernel_size, padding='same', activation='relu', strides=1)(embedding)\n    pool = MaxPooling1D(pool_size=3)(conv)\n    drop = Dropout(0.5)(pool)\n    stacks.append(drop)\n\nmerged = Concatenate()(stacks)\nflatten = Flatten()(merged)\ndrop = Dropout(0.5)(flatten)\noutp = Dense(len(int_category), activation='softmax')(drop)\n\nTextCNN = Model(inputs=inp, outputs=outp)\nTextCNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nTextCNN.summary()","4073eebc":"textcnn_history = TextCNN.fit(x_train, \n                              y_train, \n                              batch_size=128, \n                              epochs=20, \n                              validation_data=(x_val, y_val))","60e52d39":"acc = textcnn_history.history['acc']\nval_acc = textcnn_history.history['val_acc']\nloss = textcnn_history.history['loss']\nval_loss = textcnn_history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\nplt.legend()\n\nplt.show()","81034b65":"# Bidrectional LSTM with convolution\n# from https:\/\/www.kaggle.com\/eashish\/bidirectional-gru-with-convolution\n\ninp = Input(shape=(maxlen,), dtype='int32')\nx = embedding_layer(inp)\nx = SpatialDropout1D(0.2)(x)\nx = Bidirectional(GRU(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\nx = Conv1D(64, kernel_size=3)(x)\navg_pool = GlobalAveragePooling1D()(x)\nmax_pool = GlobalMaxPooling1D()(x)\nx = concatenate([avg_pool, max_pool])\noutp = Dense(len(int_category), activation=\"softmax\")(x)\n\nBiGRU = Model(inp, outp)\nBiGRU.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n\nBiGRU.summary()","e99d5e72":"# training\n\nbigru_history = BiGRU.fit(x_train, \n                          y_train, \n                          batch_size=128, \n                          epochs=20, \n                          validation_data=(x_val, y_val))","1a409d5b":"plt.rcParams['figure.figsize'] = (6,6)\n\nacc = bigru_history.history['acc']\nval_acc = bigru_history.history['val_acc']\nloss = bigru_history.history['loss']\nval_loss = bigru_history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\nplt.legend()\n\nplt.show()","6e696c2c":"# from https:\/\/www.kaggle.com\/qqgeogor\/keras-lstm-attention-glove840b-lb-0-043\/code\n\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n        if self.bias:\n            eij += self.b\n        eij = K.tanh(eij)\n        a = K.exp(eij)\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim\n    \n\nlstm_layer = LSTM(300, dropout=0.25, recurrent_dropout=0.25, return_sequences=True)\n\ninp = Input(shape=(maxlen,), dtype='int32')\nembedding= embedding_layer(inp)\nx = lstm_layer(embedding)\nx = Dropout(0.25)(x)\nmerged = Attention(maxlen)(x)\nmerged = Dense(256, activation='relu')(merged)\nmerged = Dropout(0.25)(merged)\nmerged = BatchNormalization()(merged)\noutp = Dense(len(int_category), activation='softmax')(merged)\n\nAttentionLSTM = Model(inputs=inp, outputs=outp)\nAttentionLSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n\nAttentionLSTM.summary()","480c9cbc":"attlstm_history = AttentionLSTM.fit(x_train, \n                                    y_train, \n                                    batch_size=128, \n                                    epochs=20, \n                                    validation_data=(x_val, y_val))","f893b342":"acc = attlstm_history.history['acc']\nval_acc = attlstm_history.history['val_acc']\nloss = attlstm_history.history['loss']\nval_loss = attlstm_history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\nplt.legend()\n\nplt.show()","61035ef6":"# confusion matrix\n\npredicted = AttentionLSTM.predict(x_val)\ncm = pd.DataFrame(confusion_matrix(y_val.argmax(axis=1), predicted.argmax(axis=1)))","1b8ce914":"from IPython.display import display\npd.options.display.max_columns = None\ndisplay(cm)","eb7b5752":"def evaluate_accuracy(model):\n    predicted = model.predict(x_val)\n    diff = y_val.argmax(axis=-1) - predicted.argmax(axis=-1)\n    corrects = np.where(diff == 0)[0].shape[0]\n    total = y_val.shape[0]\n    return float(corrects\/total)","01d10cba":"print(\"model TextCNN accuracy:          %.6f\" % evaluate_accuracy(TextCNN))\nprint(\"model Bidirectional GRU + Conv:  %.6f\" % evaluate_accuracy(BiGRU))\nprint(\"model LSTM with Attention:       %.6f\" % evaluate_accuracy(AttentionLSTM))","3e9e74d7":"def evaluate_accuracy_ensemble(models):\n    res = np.zeros(shape=y_val.shape)\n    for model in models:\n        predicted = model.predict(x_val)\n        res += predicted\n    res \/= len(models)\n    diff = y_val.argmax(axis=-1) - res.argmax(axis=-1)\n    corrects = np.where(diff == 0)[0].shape[0]\n    total = y_val.shape[0]\n    return float(corrects\/total)","18e81264":"print(evaluate_accuracy_ensemble([TextCNN, BiGRU, AttentionLSTM]))","cfc94cfa":"# TextCNN","bc28bda1":"# Bidirectional GRU + Conv","b5ae81fa":"# evaluate accuracy","801ef398":"# prepare data","870cefc4":"# LSTM with Attention","9dd68733":"# Ensemble","e167dc1e":"# glove embedding","3e414b5e":"# split dataset"}}