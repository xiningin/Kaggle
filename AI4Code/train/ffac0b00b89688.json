{"cell_type":{"7695fd34":"code","628f0257":"code","95f4047a":"code","a0c75f9d":"code","11492aec":"code","fd41726c":"code","6cb1d4ae":"code","9549f796":"code","c2b1dcc9":"code","d47fc7e5":"code","c458104d":"code","992c29fe":"code","e315fe1e":"code","61dfdb79":"code","ab6bfbd5":"code","3309d363":"code","7d115952":"markdown","5ab5f4e2":"markdown","1d94766f":"markdown","5f809978":"markdown","b7232c07":"markdown","ce5fea22":"markdown","9dbbecd7":"markdown","b5f3985c":"markdown","e8122b96":"markdown","6782441b":"markdown","b48e1e5f":"markdown","8d6023a6":"markdown","40eb4159":"markdown","96ade4d5":"markdown","016c3583":"markdown","df4651c2":"markdown","fb2d051e":"markdown","551da8bc":"markdown","8b39f5aa":"markdown"},"source":{"7695fd34":"# Example of the Shapiro-Wilk Normality Test\nfrom scipy.stats import shapiro\ndata = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\nstat, p = shapiro(data)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n    print('Probably has Gaussian distribution')\nelse:\n    print('Probably do not have Gaussian distribution')","628f0257":"# Example of the D'Agostino's K^2 Normality Test\nfrom scipy.stats import normaltest\ndata = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\nstat, p = normaltest(data)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n    print('Probably has Gaussian distribution')\nelse:\n    print('Probably not Gaussian')","95f4047a":"# Example of the Anderson-Darling Normality Test\nfrom scipy.stats import anderson\ndata = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\nresult = anderson(data)\nprint('stat=%.3f' % (result.statistic))\nfor i in range(len(result.critical_values)):\n    sl, cv = result.significance_level[i], result.critical_values[i]\n    if result.statistic < cv:\n        print('Probably Gaussian at the %.1f%% level' % (sl))\n    else:\n        print('Probably not Gaussian at the %.1f%% level' % (sl))","a0c75f9d":"# Example of the Pearson's Correlation test\nfrom scipy.stats import pearsonr\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [0.353, 3.517, 0.125, -7.545, -0.555, -1.536, 3.350, -1.578, -3.537, -1.579]\nstat, p = pearsonr(data1, data2)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n    print('The two samples are independent.')\nelse:\n    print('There is a dependency between the samples')","11492aec":"# Example of the Spearman's Rank Correlation Test\nfrom scipy.stats import spearmanr\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [0.353, 3.517, 0.125, -7.545, -0.555, -1.536, 3.350, -1.578, -3.537, -1.579]\nstat, p = spearmanr(data1, data2)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n    print('Probably independent: the two samples are independent.')\nelse:\n    print('Probably dependent: there is a dependency between the samples. ')","fd41726c":"# Example of the Kendall's Rank Correlation Test\nfrom scipy.stats import kendalltau\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [0.353, 3.517, 0.125, -7.545, -0.555, -1.536, 3.350, -1.578, -3.537, -1.579]\nstat, p = kendalltau(data1, data2)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n    print('Probably independent: the two samples are independent.')\nelse:\n    print('Probably dependent: there is a dependency between the samples.')","6cb1d4ae":"# Example of the Chi-Squared Test\nfrom scipy.stats import chi2_contingency\ntable = [[10, 20, 30],[6,  9,  17]]\nstat, p, dof, expected = chi2_contingency(table)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n    print('Probably independent: the two samples are independent.')\nelse:\n    print('Probably dependent: there is a dependency between the samples.')","9549f796":"# Example of the Augmented Dickey-Fuller unit root test\nfrom statsmodels.tsa.stattools import adfuller\ndata = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nstat, p, lags, obs, crit, t = adfuller(data)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n    print('Probably not Stationary: Null Hypothesis is true H0: a unit root is present (series is non-stationary).')\nelse:\n    print('Probably Stationary: H1: a unit root is not present (series is stationary).')","c2b1dcc9":"# Example of the Kwiatkowski-Phillips-Schmidt-Shin test\nfrom statsmodels.tsa.stattools import kpss\ndata = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nstat, p, lags, crit = kpss(data)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n    print('Probably is not trend-Stationary')\nelse:\n    print('Probably is trend-Stationary')","d47fc7e5":"# Example of the Student's t-test\nfrom scipy.stats import ttest_ind\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\nstat, p = ttest_ind(data1, data2)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n    print('Probably has same distribution: H0: the means of the samples are equal. ')\nelse:\n    print('Probably has different distributions: H1: the means of the samples are unequal.')","c458104d":"# Example of the Paired Student's t-test\nfrom scipy.stats import ttest_rel\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\nstat, p = ttest_rel(data1, data2)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n    print('Probably has same distribution i.e H0: the means of the samples are equal.')\nelse:\n    print('Probably has different distributions i.e H1: the means of the samples are unequal.')","992c29fe":"# Example of the Analysis of Variance Test\nfrom scipy.stats import f_oneway\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\ndata3 = [-0.208, 0.696, 0.928, -1.148, -0.213, 0.229, 0.137, 0.269, -0.870, -1.204]\nstat, p = f_oneway(data1, data2, data3)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n    print('Probably has same distribution i.e the means of the samples are equal.')\nelse:\n    print('Probably has different distributions i.e one or more of the means of the samples are unequal.')","e315fe1e":"# Example of the Mann-Whitney U Test\nfrom scipy.stats import mannwhitneyu\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\nstat, p = mannwhitneyu(data1, data2)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n    print('Probably has same distribution i.e the distributions of both samples are equal.')\nelse:\n    print('Probably has different distributions i.e the distributions of both samples are not equal.')","61dfdb79":"# Example of the Wilcoxon Signed-Rank Test\nfrom scipy.stats import wilcoxon\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\nstat, p = wilcoxon(data1, data2)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n    print('Probably has same distribution i.e the distributions of both samples are equal.')\nelse:\n    print('Probably has different distributions i.e the distributions of both samples are not equal.')","ab6bfbd5":"# Example of the Kruskal-Wallis H Test\nfrom scipy.stats import kruskal\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\nstat, p = kruskal(data1, data2)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n    print('Probably has same distribution i.e the distributions of all samples are equal.')\nelse:\n    print('Probably has different distributions i.e the distributions of one or more samples are not equal.')","3309d363":"# Example of the Friedman Test\nfrom scipy.stats import friedmanchisquare\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\ndata3 = [-0.208, 0.696, 0.928, -1.148, -0.213, 0.229, 0.137, 0.269, -0.870, -1.204]\nstat, p = friedmanchisquare(data1, data2, data3)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n    print('Probably has same distribution: i.e the distributions of all samples are equal.')\nelse:\n    print('Probably has different distributions i.e the distributions of one or more samples are not equal.')","7d115952":"# Paired Student\u2019s t-test\nTests whether the means of two paired samples are significantly different.\n\n**Assumptions**\n- Observations in each sample are independent and identically distributed (iid).\n- Observations in each sample are normally distributed.\n- Observations in each sample have the same variance.\n- Observations across each sample are paired.\n\n**Interpretation**\n- H0: the means of the samples are equal.\n- H1: the means of the samples are unequal.\n\n**Python Code**","5ab5f4e2":"# Wilcoxon Signed-Rank Test\nTests whether the distributions of two paired samples are equal or not.\n\n**Assumptions**\n- Observations in each sample are independent and identically distributed (iid).\n- Observations in each sample can be ranked.\n- Observations across each sample are paired.\n\n**Interpretation**\n- H0: the distributions of both samples are equal.\n- H1: the distributions of both samples are not equal.\n\n**Python Code**","1d94766f":"# Spearman\u2019s Rank Correlation\nTests whether two samples have a monotonic relationship.\n\n**Assumptions**\n- Observations in each sample are independent and identically distributed (iid).\n- Observations in each sample can be ranked.\n\n**Interpretation**\n- H0: the two samples are independent.\n- H1: there is a dependency between the samples.\n\n**Python Code**","5f809978":"# Chi-Squared Test\nTests whether two **categorical variables** are related or independent.\n\n**Assumptions**\n- Observations used in the calculation of the contingency table are independent.\n- 25 or more examples in each cell of the contingency table.\n\n**Interpretation**\n- H0: the two samples are independent.\n- H1: there is a dependency between the samples.\n\n**Python Code**","b7232c07":"# 3. Stationary Tests\nThis section lists statistical tests that you can use to check if a time series is stationary or not.\n\n# Augmented Dickey-Fuller Unit Root Test\nTests whether a time series has a unit root, e.g. has a trend or more generally is autoregressive.\n\n**Assumptions**\n- Observations in are temporally ordered.\n\n**Interpretation**\n- H0: a unit root is present (series is non-stationary).\n- H1: a unit root is not present (series is stationary).\n\n**Python Code**","ce5fea22":"# Summary\nIn this post, you discovered the key statistical hypothesis tests that you may need to use in a machine learning project.\n\nSpecifically, you learned:\n\n- The types of tests to use in different circumstances, such as normality checking, relationships between variables, and differences between samples.\n- The key assumptions for each test and how to interpret the test result.\n- How to implement the test using the Python API.","9dbbecd7":"# 5. Nonparametric Statistical Hypothesis Tests\n# Mann-Whitney U Test\nTests whether the distributions of two independent samples are equal or not.\n\n**Assumptions**\n- Observations in each sample are independent and identically distributed (iid).\n- Observations in each sample can be ranked.\n\n**Interpretation**\n- H0: the distributions of both samples are equal.\n- H1: the distributions of both samples are not equal.\n\n**Python Code**","b5f3985c":" # **1. Normality Tests**\n- This section lists statistical tests that you can use to check if your data has a Gaussian distribution.\n\n# **Shapiro-Wilk Test**\n- Tests whether a data sample has a Gaussian distribution.\n\n**Assumptions**\n\n- Observations in each sample are independent and identically distributed (iid).\n\n**Interpretation**\n\n- H0: the sample has a Gaussian distribution.\n- H1: the sample does not have a Gaussian distribution.","e8122b96":"This post is divided into 5 parts; they are:\n\n1. Normality Tests\n    - Shapiro-Wilk Test\n    - D\u2019Agostino\u2019s K^2 Test\n    - Anderson-Darling Test\n2. Correlation Tests\n    - Pearson\u2019s Correlation Coefficient\n    - Spearman\u2019s Rank Correlation\n    - Kendall\u2019s Rank Correlation\n    - Chi-Squared Test\n3. Stationary Tests\n    - Augmented Dickey-Fuller\n    - Kwiatkowski-Phillips-Schmidt-Shin\n4. Parametric Statistical Hypothesis Tests\n    - Student\u2019s t-test\n    - Paired Student\u2019s t-test\n    - Analysis of Variance Test (ANOVA)\n5. Nonparametric Statistical Hypothesis Tests\n    - Mann-Whitney U Test\n    - Wilcoxon Signed-Rank Test\n    - Kruskal-Wallis H Test\n    - Friedman Test","6782441b":"# D\u2019Agostino\u2019s K^2 Test\nTests whether a data sample has a Gaussian distribution.\n\n**Assumptions**\n\n**Observations** \n- in each sample are independent and identically distributed (iid).\n\n**Interpretation**\n\n- H0: the sample has a Gaussian distribution.\n- H1: the sample does not have a Gaussian distribution.\n\n**Python Code**","b48e1e5f":"# Kendall\u2019s Rank Correlation\nTests whether two samples have a monotonic relationship.\n\n**Assumptions**\n- Observations in each sample are independent and identically distributed (iid).\n- Observations in each sample can be ranked.\n\n**Interpretation**\n- H0: the two samples are independent.\n- H1: there is a dependency between the samples.\n\n**Python Code**","8d6023a6":"# Kwiatkowski-Phillips-Schmidt-Shin\nTests whether a time series is trend stationary or not.\n\n**Assumptions**\n- Observations in are temporally ordered.\n\n**Interpretation**\n- H0: the time series is not trend-stationary.\n- H1: the time series is trend-stationary.\n\n**Python Code**","40eb4159":"# Kruskal-Wallis H Test\nTests whether the distributions of two or more independent samples are equal or not.\n\n**Assumptions**\n- Observations in each sample are independent and identically distributed (iid).\n- Observations in each sample can be ranked.\n\n**Interpretation**\n- H0: the distributions of all samples are equal.\n- H1: the distributions of one or more samples are not equal.\n\n**Python Code**","96ade4d5":"# Analysis of Variance Test (ANOVA)\nTests whether the means of two or more independent samples are significantly different.\n\n**Assumptions**\n- Observations in each sample are independent and identically distributed (iid).\n- Observations in each sample are normally distributed.\n- Observations in each sample have the same variance.\n\n**Interpretation**\n- H0: the means of the samples are equal.\n- H1: one or more of the means of the samples are unequal.\n\n**Python Code**","016c3583":"# Anderson-Darling Test\nTests whether a data sample has a Gaussian distribution.\n\n**Assumptions**\n\n**Observations** \n- In each sample are independent and identically distributed (iid).\n\n**Interpretation**\n- H0: the sample has a Gaussian distribution.\n- H1: the sample does not have a Gaussian distribution.\n\n**Python Code**","df4651c2":"# Statistical Hypothesis Tests in Python\nQuick-reference guide to the commonly statistical hypothesis tests that you need in\napplied machine learning, with sample code in Python.\nAlthough there are hundreds of statistical hypothesis tests that you could use, there is only a small subset that you may need to use in a machine learning project.\n\nIn this post, you will discover a cheat sheet for the most popular statistical hypothesis tests for a machine learning project with examples using the Python API.\n\nEach statistical test is presented in a consistent way, including:\n\n- The name of the test.\n- What the test is checking.\n- The key assumptions of the test.\n- How the test result is interpreted.\n- Python API for using the test.\n\nNote, when it comes to assumptions such as the expected distribution of data or sample size, the results of a given test are likely to degrade gracefully rather than become immediately unusable if an assumption is violated.\n\nGenerally, data samples need to be representative of the domain and large enough to expose their distribution to analysis.\n\nIn some cases, the data can be corrected to meet the assumptions, such as correcting a nearly normal distribution to be normal by removing outliers, or using a correction to the degrees of freedom in a statistical test when samples have differing variance, to name two examples.\n\nFinally, there may be multiple tests for a given concern, e.g. normality. We cannot get crisp answers to questions with statistics; instead, we get probabilistic answers. As such, we can arrive at different answers to the same question by considering the question in different ways. Hence the need for multiple different tests for some questions we may have about data.","fb2d051e":"# 2. Correlation Tests\nThis section lists statistical tests that you can use to check if two samples are related.\n\n# Pearson\u2019s Correlation Coefficient\nTests whether two samples have a linear relationship.\n\n**Assumptions**\n\n- Observations in each sample are independent and identically distributed (iid).\n- Observations in each sample are normally distributed.\n- Observations in each sample have the same variance.\n\n**Interpretation**\n\n- H0: the two samples are independent.\n- H1: there is a dependency between the samples.\n\n**Python Code**","551da8bc":"# Friedman Test\nTests whether the distributions of two or more paired samples are equal or not.\n\n**Assumptions**\n- Observations in each sample are independent and identically distributed (iid).\n- Observations in each sample can be ranked.\n- Observations across each sample are paired.\n\n**Interpretation**\n- H0: the distributions of all samples are equal.\n- H1: the distributions of one or more samples are not equal.\n\n**Python Code**","8b39f5aa":"# 4. Parametric Statistical Hypothesis Tests\nThis section lists statistical tests that you can use to compare data samples.\n\n# Student\u2019s t-test\nTests whether the means of two independent samples are significantly different.\n\n**Assumptions**\n- Observations in each sample are independent and identically distributed (iid).\n- Observations in each sample are normally distributed.\n- Observations in each sample have the same variance.\n\n**Interpretation**\n- H0: the means of the samples are equal.\n- H1: the means of the samples are unequal.\n\n**Python Code**"}}