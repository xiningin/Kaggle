{"cell_type":{"70ad48f0":"code","f221fd98":"code","5fe7fc4f":"code","eb3d8531":"code","95525417":"code","101cd607":"code","2a5a37a4":"code","5792b76c":"code","3841c6f3":"code","18f70340":"code","13c2f7c8":"code","6f191dac":"code","caffb2de":"code","27fda1c7":"code","f1e894fd":"code","b7bca43f":"code","9d5155b8":"code","fe827d30":"code","0f46f975":"code","0f0d54ca":"code","e276bb8e":"code","c7120d67":"code","bbac7557":"code","6685bd64":"code","af0d7742":"code","48555142":"code","4a61d2aa":"code","5f646da7":"code","7a1aa42f":"code","d8398c47":"code","77142a2b":"code","a298fb67":"code","b3c43320":"code","515ba1a4":"code","15d15802":"code","7b72c19c":"code","7fe47a46":"code","0f47e97e":"code","4f816744":"code","35b3d693":"code","bd24f912":"code","19105f1b":"code","57bda8ca":"code","dfde25a0":"code","84c5752d":"code","b3a8238f":"code","22afe232":"code","aa3a352e":"code","f110c24d":"code","aef1eeda":"code","45b9c065":"code","dae6b833":"code","152a0ae7":"code","bf52faaf":"code","5a13f6d5":"code","7cdb1f30":"code","63618b9b":"code","fe2eff66":"code","ac9c64c5":"code","57bde189":"code","848e07d4":"code","aef1b3f8":"code","99668aa1":"code","2b2e90e6":"code","d503a513":"code","bb15b805":"code","6b6f7c80":"code","ef7ec82c":"code","fca46e57":"code","884ea77a":"code","2d48f38a":"code","c45a834a":"code","26031f3b":"code","0da79ecc":"code","14968908":"code","88f6e706":"code","39fb9427":"code","3a20c54d":"code","1eede599":"code","0b8ea3a1":"code","5d93148d":"code","02e11020":"code","c029c4c5":"code","facbee2e":"code","383eb97d":"code","a2650a7b":"code","b366c8b9":"code","d4f115ca":"code","0bb5e14b":"code","fb7cd999":"code","e94b5b01":"code","678351c9":"code","68d589f1":"code","f29fda61":"code","87cea6d8":"code","40c50b0f":"code","7d58b817":"code","3015f5b8":"code","f8743221":"code","04f1fbbb":"code","7ec1c319":"code","9dc0a207":"code","762d0438":"code","5fedadf8":"code","b52a5a5e":"code","f10296c6":"code","2d76421a":"code","be212afb":"code","b5725764":"code","c0adaecd":"code","ea25bb30":"code","09bf5021":"code","32d0d71c":"code","cf064e1a":"code","711daf98":"code","c3049e2b":"code","a8289935":"code","e8ec5426":"code","56be1e90":"code","599c7103":"code","32b027f5":"code","bd6d5dc1":"code","d819d89e":"code","ea937ebc":"code","e7dcd1b5":"code","3bf7b1b6":"code","e21f2431":"code","5c27834d":"code","9aa8db20":"code","99d4d9d6":"code","ba294d5b":"code","dd8facb1":"code","53f9fc0f":"code","0fb46bc4":"code","0b56c797":"code","3cd7bfda":"code","802944c3":"code","8939a45a":"code","36f3d701":"code","f18d0c7e":"code","b7071de4":"code","e877eff2":"code","7250cff3":"code","66b61f42":"code","d2f2beab":"code","068b1345":"code","09099f2a":"code","f6a5db33":"code","3c982993":"code","4e154bd8":"code","578e7402":"code","0eddecba":"code","dad84825":"code","05ee4216":"code","cceff330":"code","30e8dea6":"code","bc189354":"code","d1eb5103":"code","fdf86577":"code","ab7d2199":"code","d9b54709":"code","3fad27ac":"code","3a0abe43":"code","845058af":"code","7627fbc5":"code","89e24c50":"code","ce003f8d":"code","ee4da8a1":"code","1d9fead6":"code","ee7f5a0d":"markdown","8fcf7fc6":"markdown","21023cc4":"markdown","9d5710c0":"markdown","f597421a":"markdown","64a824e9":"markdown","0d3976ce":"markdown","40900c8b":"markdown","c2dae095":"markdown","b988756f":"markdown","cff23f15":"markdown","2b42842d":"markdown","df946751":"markdown","5694fe73":"markdown","25fd71c5":"markdown","7d3a2b79":"markdown","cf5b558c":"markdown","de755e0f":"markdown","7761ac16":"markdown","41cd6af4":"markdown","e530d608":"markdown","03b9a7d9":"markdown","8b39dab8":"markdown","c4dbbdb3":"markdown","66da5654":"markdown","f50f91fa":"markdown","71ae42ba":"markdown","039f7724":"markdown","279235e7":"markdown","0e0f35c4":"markdown","f0d14280":"markdown","a7b05ca6":"markdown","41cbfc59":"markdown","851e9b29":"markdown","118748f4":"markdown","2a0cce18":"markdown","dab77e9a":"markdown","2ef0ae83":"markdown","d11e238a":"markdown","7bfd7c28":"markdown","004baf08":"markdown","20259ad9":"markdown","e5d470c0":"markdown","77af4eb1":"markdown","ada79f50":"markdown","4a5ac6d1":"markdown","0a01964b":"markdown","485746d2":"markdown","2047775b":"markdown","a83fed1a":"markdown","54cf4240":"markdown","f5f17d92":"markdown","b0dd8653":"markdown","8ec4212c":"markdown","3453d790":"markdown","b3bf0e35":"markdown","28f1c2e8":"markdown","fea5fcbe":"markdown","106cb28b":"markdown","c70dbf89":"markdown","16b0b7c3":"markdown","62e08900":"markdown","adc166d0":"markdown","62f17668":"markdown","adf22c16":"markdown","22ddbd3e":"markdown","9134f90c":"markdown","ea201f53":"markdown","f0bfb941":"markdown","a06d0b9d":"markdown","1f51ef4e":"markdown","c93de016":"markdown","8a0c5279":"markdown","c8dadb06":"markdown","4aec4647":"markdown","02a29bed":"markdown","c5bccfe0":"markdown","e30128a3":"markdown","3db90283":"markdown","f8e85db8":"markdown","4646a544":"markdown","53f99a4a":"markdown","b5833f77":"markdown","03ad2199":"markdown","57ccd4b0":"markdown","9674ef21":"markdown","af6a919f":"markdown","eafdadd2":"markdown","a6056c7d":"markdown","09696aa8":"markdown","5f781e7d":"markdown","d47a30d2":"markdown","93afe9d3":"markdown","29468d19":"markdown"},"source":{"70ad48f0":"%%html\n<iframe style=\"border:none\" width=\"1200\" height=\"450\" src=\"https:\/\/whimsical.com\/embed\/KSg3TdL3ydNPUr8cmS9Y5D\"><\/iframe>","f221fd98":"!pip install dabl missingno ppscore","5fe7fc4f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nimport operator","eb3d8531":"train_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\").copy(deep = True)\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\").copy(deep = True)\ncombined_df = pd.concat(objs=[train_df, test_df], axis=0).reset_index(drop=True)","95525417":"# to be used for submission\nPassengerId = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\").copy(deep = True)[['PassengerId']]","101cd607":"train_df.head()","2a5a37a4":"combined_df.tail()","5792b76c":"# import pandas_profiling\n# profile = train_df.profile_report()\n# profile.to_file(\"train_data_pandas_profiling.html\")","3841c6f3":"import dabl # remember to install it first --> !pip install dabl \ndabl_titanic_df = dabl.clean(combined_df, verbose=1)","18f70340":"dabl.detect_types(dabl_titanic_df)","13c2f7c8":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","6f191dac":"# https:\/\/dzone.com\/articles\/pandas-find-rows-where-columnfield-is-null\ndef get_columns_containing_null(df):\n    return df.columns[df.isnull().any()]\n\ncolumns_with_nulls = get_columns_containing_null(train_df)\ntrain_df[columns_with_nulls].isnull().sum()","caffb2de":"columns_with_nulls = get_columns_containing_null(test_df)\ntest_df[columns_with_nulls].isnull().sum()","27fda1c7":"# importing the library --> don't forget to install: !pip install missingno\nimport missingno as msno","f1e894fd":"msno.matrix(train_df)","b7bca43f":"msno.matrix(test_df)","9d5155b8":"msno.matrix(combined_df)","fe827d30":"#freq_port = train_df.Embarked.dropna().mode()[0] #S\n#print (freq_port)","0f46f975":"train_df[train_df.Embarked.isnull()]","0f0d54ca":"# train_df[train_df.Cabin.str.contains(\"B\", na=False) & train_df.Pclass == 1 & train_df.Sex == \"female\"] --> boolean indexing error\n\n# # Can be REDACTED - stays for study purposes only.\n# def apply_to_train_test_combined_dataframes(function_to_apply, list_of_dataframes=[train_df, test_df, combined_df]):\n#     for df in list_of_dataframes:\n#         df = df.apply(function_to_apply)\n#     return list_of_dataframes\n# train_df, test_df, combined_df = apply_to_train_test_combined_dataframes(mask_boolean_indexing_of_sex)\n\n\ndef mask_boolean_indexing_of_sex(df):\n    # https:\/\/stackoverflow.com\/questions\/54052471\/mapping-values-in-place-for-example-with-gender-from-string-to-int-in-pandas-d\n    df = df.copy()\n    df['Sex_is_Female'] = (df['Sex'].values == 'female').astype(int)\n    return df\n\ndef apply_to_multiple_dataframes(function_to_apply):    \n    # why use global and not mutable default arguments?\n    # https:\/\/docs.python-guide.org\/writing\/gotchas\/#mutable-default-arguments\n    # https:\/\/stackoverflow.com\/questions\/42718870\/defining-a-default-argument-as-a-global-variable\n    global train_df\n    global test_df\n    global combined_df\n    # https:\/\/stackoverflow.com\/questions\/10212445\/python-map-list-item-to-function-with-arguments\n    return [function_to_apply(df) for df in [train_df, test_df, combined_df]]\n\ntrain_df, test_df, combined_df = apply_to_multiple_dataframes(mask_boolean_indexing_of_sex)","e276bb8e":"## conditions\n# condition for each independant variable\ncondition_sex = (train_df.Sex_is_Female == 1)\ncondition_pclass = (train_df.Pclass == 1)\ncondition_age = ((train_df.Age < 62) & (train_df.Age > 38))\ncondition_fare = ((train_df.Fare < 100) & (train_df.Fare > 60)) # originally 80, I used 60-100 range for variance.\ncondition_cabin = (train_df.Cabin.str.contains(\"B\", na=False))\n# condition_alone = (train_df.SibSp+temp_df.Parch == 0) --> 790 values, it's too frequent. Hence, not considered.","c7120d67":"passengers_representing_those_with_missing_embarked = train_df[\n    (\n        # 3-5 independent variables are correlated.\n        sum([condition_sex, condition_pclass, condition_age, condition_fare, condition_cabin]) >= 3\n    )] # 128 passengers, compared to two passengers at the beginning.\n\nmissing_embarked_value = passengers_representing_those_with_missing_embarked.Embarked.dropna().mode()[0] # C\npassengers_representing_those_with_missing_embarked","bbac7557":"### Other implementations that couldn't scale when I decided to correlate more than two independant variables\n\n## version 1\n# passengers_representing_those_with_missing_embarked1 = temp_df[\n#     (temp_df.Sex_is_Female == 1) & \n#     (\n#         (temp_df.Cabin.str.contains(\"B\", na=False)) | \n#         (temp_df.Pclass == 1) |\n\n#         (\n#             (\n#                 (temp_df.Age < 62) & (temp_df.Age > 38)\n#             ) | \n#             (temp_df.Fare == 80)\n#         )\n#     )]\n# passengers_representing_those_with_missing_embarked1\n\n## version 2\n# passengers_representing_those_with_missing_embarked2 = temp_df[(\n#     (\n#         (condition_female) & \n#         (\n#             (condition_pclass1 | condition_age_38_to_62 | condition_fare)\n#         )\n#     ) |\n#     (\n#         (condition_pclass1) & \n#         (\n#             (condition_female | condition_age_38_to_62 | condition_fare)\n#         )\n#     ) |\n#     (\n#         (condition_age_38_to_62) & \n#         (\n#             (condition_female | condition_pclass1 | condition_fare)\n#         )\n#     ) |\n#     (\n#         (condition_fare) & \n#         (\n#             (condition_female | condition_pclass1 | condition_age_38_to_62)\n#         )\n#     )\n# )]","6685bd64":"train_df[\"Embarked\"] = train_df[\"Embarked\"].fillna(missing_embarked_value)\n# fillna inplace doesn't work - https:\/\/stackoverflow.com\/questions\/21998354\/pandas-wont-fillna-inplace\n# train_df.Embarked.fillna(missing_embarked_value, inplace=True)\n# train_df[train_df[\"Embarked\"].isnull()] = missing_embarked_value\n\n# update combined_df with missing embarked value\ncombined_df = pd.concat(objs=[train_df, test_df], axis=0).reset_index(drop=True)\nprint(missing_embarked_value)","af0d7742":"# checks that changes were saved correctly\ntrain_df[train_df.Embarked.isnull()]","48555142":"combined_df[combined_df.Embarked.isnull()]","4a61d2aa":"index_of_passenger_missing_fare = int(test_df[test_df[\"Fare\"].isnull()].index.values)\ntest_df[test_df[\"Fare\"].isnull()]","5f646da7":"## conditions\n# condition for each independant variable\ncondition_sex = (test_df.Sex_is_Female == 0) # male\ncondition_pclass = (test_df.Pclass == 3)\ncondition_age = ((test_df.Age < 65) & (test_df.Age > 55)) # reasonable range for age 60.5\ncondition_embarked = (test_df.Embarked == \"S\")\ncondition_unknown_cabin = (test_df.Cabin.isnull())","7a1aa42f":"passengers_representing_those_with_missing_fare = test_df[\n    (\n        # 4-5 independent variables are correlated.\n        sum([condition_sex, condition_pclass, condition_age, condition_embarked, condition_unknown_cabin]) >= 4\n    )] # 371 Passengers share these commonalities\nmissing_fare_value = passengers_representing_those_with_missing_fare[\"Fare\"].median() \nprint(\"independant variables method --> median of missing Fare for 371 passengers: \" + str(missing_fare_value))\nprint(\"test_df median of 'Fare' for 1309 passengers: \" + str(test_df.Fare.median()))","d8398c47":"print(\"number of passengers that represent those with missing fare: {}\".format(passengers_representing_those_with_missing_fare.shape[0]))","77142a2b":"test_df['Fare'] = test_df['Fare'].fillna(missing_fare_value)\ncombined_df = pd.concat(objs=[train_df, test_df], axis=0).reset_index(drop=True)\n\ntest_df.iloc[index_of_passenger_missing_fare]","a298fb67":"# checks that changes were saved correctly\ntest_df[test_df.Fare.isnull()]","b3c43320":"combined_df[combined_df.Fare.isnull()]","515ba1a4":"def create_cabin_prefix_column(df):\n    \"\"\"iterate passengers who have a Cabin, and assign the prefix accordingly\"\"\"\n    df = df.copy()\n    df[\"Cabin_Prefix\"] = np.nan\n    for i, dataset in enumerate(df[\"Cabin\"]):\n        # method 1\n        if (not pd.isna(dataset)):\n            df[\"Cabin_Prefix\"][i] = df[\"Cabin\"][i][0]\n        # method 2\n        # df[\"Cabin_Prefix\"].loc(not pd.isna(dataset)) = df[\"Cabin\"][i][0] --> SyntaxError: can't assign to function call  \n        # method 3\n        # https:\/\/www.kaggle.com\/masumrumi\/a-statistical-analysis-ml-workflow-of-titanic\/comments\n        # all_data.Cabin = [i[0] for i in all_data.Cabin]\n    return df\n\ntrain_df, test_df, combined_df = apply_to_multiple_dataframes(create_cabin_prefix_column)","15d15802":"test_df.Cabin_Prefix.describe()","7b72c19c":"combined_df.groupby(\"Cabin_Prefix\")['Fare'].mean().sort_values()","7fe47a46":"# https:\/\/stackoverflow.com\/questions\/25140998\/pandas-compute-mean-or-std-standard-deviation-over-entire-dataframe\ntrain_df.groupby(\"Cabin_Prefix\")['Fare'].std(ddof=1).sort_values(ascending=True)","0f47e97e":"train_df.groupby(\"Cabin_Prefix\")['Fare'].mean().sort_values()","4f816744":"# train_df.groupby(\"Cabin_Prefix\")['Fare'].sort_values()\n\n# https:\/\/stackoverflow.com\/questions\/17679089\/pandas-dataframe-groupby-two-columns-and-get-counts\/17679517#17679517\ntrain_df.groupby([\"Fare\", \"Cabin_Prefix\"]).size().head(20).groupby(\"Cabin_Prefix\", level=1).size()","35b3d693":"# https:\/\/stackoverflow.com\/questions\/17995024\/how-to-assign-a-name-to-the-a-size-column\ntemp_df = pd.DataFrame(train_df.groupby([\"Fare\", \"Cabin_Prefix\",\"Pclass\"]).size().reset_index(name='Size'))\ntemp_df","bd24f912":"import seaborn as sns\nax = sns.barplot(x=\"Cabin_Prefix\", y=\"Fare\",\n                     hue=\"Pclass\", data=temp_df)\n# https:\/\/seaborn.pydata.org\/examples\/scatterplot_matrix.html","19105f1b":"sns.pairplot(temp_df, hue=\"Pclass\")","57bda8ca":"# Passenger in the T deck is changed to A\nidx = train_df[train_df['Cabin_Prefix'] == 'T'].index\ntrain_df.loc[idx, 'Cabin_Prefix'] = 'A'","dfde25a0":"# https:\/\/stackoverflow.com\/questions\/45003806\/python-pandas-use-slice-with-describe-versions-greater-than-0-20\n# train_df[train_df[\"Cabin_Prefix\"].notnull()].groupby(\"Cabin_Prefix\").describe(include='all')\ntrain_df.Pclass = train_df.Pclass.map({1: 'First', 2: 'Second', 3: 'Third'}).astype('object') # for mode()\npassengers_without_cabin = train_df[train_df[\"Cabin_Prefix\"].isnull()].reset_index()\npassengers_with_cabin = train_df[train_df[\"Cabin_Prefix\"].notnull()].reset_index()\n\n# Create two dataframes for passengers with cabin - each dataframe for numerical\/categorical set of features\ndescription_of_passengers_with_cabin_numeric_features = passengers_with_cabin.groupby(\"Cabin_Prefix\").describe(include=np.number)\ndescription_of_passengers_with_cabin_categorical_features = passengers_with_cabin.groupby(\"Cabin_Prefix\").describe(exclude=np.number)\n# https:\/\/stackoverflow.com\/questions\/10665889\/how-to-take-column-slices-of-dataframe-in-pandas\/44736467\n\n# filter frequency of features based on mean+-std for numerical features, mode & freq for categorical features\nfiltered_df_of_passengers_with_cabin_mean_std_numeric = description_of_passengers_with_cabin_numeric_features.loc[:,(slice(None),['mean','std', 'top'])].loc[:, ['Fare','Age','Parch','SibSp']]\ndescription_of_passengers_with_cabin_categorical_features = description_of_passengers_with_cabin_categorical_features.loc[:,(slice(None),['top', 'freq'])].loc[:, ['Pclass','Embarked', \"Sex\"]]\nfinal_df_cabin_prefix_features_description = pd.merge(filtered_df_of_passengers_with_cabin_mean_std_numeric, description_of_passengers_with_cabin_categorical_features, on='Cabin_Prefix', how='inner')\nfinal_df_cabin_prefix_features_description\n# passengers_with_cabin\n# final_df_cabin_prefix_features_description = pd.merge(filtered_df_of_passengers_with_cabin_mean_std_numeric, description_of_passengers_with_cabin_categorical_features, on='Cabin_Prefix', how='inner').join(count_df_of_passengers_with_cabin_per_prefix)\n# final_df_cabin_prefix_features_description","84c5752d":"# pd.MultiIndex.from_frame(final_df_cabin_prefix_features_description)","b3a8238f":"# for row_index, row_series in passengers_without_cabin.iterrows():\n# #     array of true false = function(row_series, final_df_cabin_prefix_features_description)\n# #     estimated letter = function(sum of true false for each prefix in array)\n#     pass\n\n# def get_estimated_letter_bundle():\n#     pass\n\n# def get_mode_of_categorical_variable(df=final_df_cabin_prefix_features_description, cabin_prefix=cabin_prefix, df_column=passengers_without_cabin_column):\n#     \"\"\" \n#     slice a pd.DataFrame.describe() by Cabin_Prefix and by feature to get the mode of the categorical feature for a specific Cabin_Prefix\n#     \"\"\"\n#     sliced_description_df_by_cabin_prefix = df.loc(axis=0)[(slice(cabin_prefix, cabin_prefix))]\n#     sliced_description_df_by_cabin_prefix_and_feature = sliced_description_df_by_cabin_prefix.loc(axis=1)[df_column]\n#     return sliced_description_df_by_cabin_prefix_and_feature['top'][0] # top equalls mode() for pd.DataFrame.describe()\n\n# def get_mean_and_std_of_numerical_variable(df=final_df_cabin_prefix_features_description, cabin_prefix=cabin_prefix, df_column=passengers_without_cabin_column):\n#     \"\"\"\n#     slice a pd.DataFrame.describe() by Cabin_Prefix and by feature to get the mean and std of the numerical feature for a specific Cabin_Prefix\n#     \"\"\"\n#    # assign mean and std to variables\n#     sliced_description_df_by_cabin_prefix = df.loc(axis=0)[(slice(cabin_prefix, cabin_prefix))]\n#     sliced_description_df_by_cabin_prefix_and_feature = sliced_description_df_by_cabin_prefix.loc(axis=1)[df_column]\n#     mean_of_numerical_variable = sliced_description_df_by_cabin_prefix_and_feature['mean'][0]\n#     std_of_numerical_variable = sliced_description_df_by_cabin_prefix_and_feature['std'][0]\n#     return mean_of_numerical_variable, std_of_numerical_variable\n\n# def get_mean_plus_minus_std(mean=mean_of_numerical_variable, std=std_of_numerical_variable):\n#     return round(mean + std), round(mean - std) # round for Parch and SibSp\n\n# def get_boolean_list_for_categorical_correlation_of_passenger_value_and_mode(one_passenger_without_cabin=row_series, cabin_prefix_dictionary=boolean_dictionary_per_cabin_prefix, final_df_cabin_prefix_features_description=final_df_cabin_prefix_features_description):\n#     print (\"get_boolean_list_for_categorical_correlation_of_passenger_value_and_mode\"+\"\\n\")\n#     print (\"*\"*40)\n#     temp_boolean_list = []\n#     categorical_columns = ['Sex', 'Embarked', 'Pclass']\n    \n#     # iterate ABCDEFGT cabin prefixes\n#     for cabin_prefix in cabin_prefix_dictionary.keys():\n#         print(\"\\nCabin Prefix: \"+  cabin_prefix)\n        \n#         # iterate categorical columns\n#         for passengers_without_cabin_column, passengers_without_cabin_value in one_passenger_without_cabin[categorical_columns].items():\n#             # https:\/\/stackoverflow.com\/questions\/13389203\/pandas-slice-a-multiindex-by-range-of-secondary-index\n#             # https:\/\/stackoverflow.com\/questions\/24922867\/key-error-and-multiindex-lexsort-depth\n            \n#             # correlate mode of categorical feature with the passenger's value\n#             mode_of_categorical_variable = get_mode_of_categorical_variable(df=final_df_cabin_prefix_features_description, cabin_prefix=cabin_prefix, df_column=passengers_without_cabin_column)\n#             is_correlated_feature = (mode_of_categorical_variable == passengers_without_cabin_value)\n#             temp_boolean_list.append(is_correlated_feature)\n#             print (\"Mode: \"+ mode_of_categorical_variable)\n#             print (\"Passenger's value: \" + passengers_without_cabin_value)\n#             print (is_correlated_feature)\n#     return temp_boolean_list\n            \n# # # https:\/\/stackoverflow.com\/questions\/11011756\/is-there-any-pythonic-way-to-combine-two-dicts-adding-values-for-keys-that-appe\n# # >>> from collections import Counter\n# # >>> A = Counter({'a':1, 'b':2, 'c':3})\n# # >>> B = Counter({'b':3, 'c':4, 'd':5})\n# # >>> A + B\n# # Counter({'c': 7, 'b': 5, 'd': 5, 'a': 1})\n\n# def get_boolean_list_for_numerical_correlation_of_passenger_value_mean_and_std(one_passenger_without_cabin=row_series, cabin_prefix_dictionary=boolean_dictionary_per_cabin_prefix, final_df_cabin_prefix_features_description=final_df_cabin_prefix_features_description):\n#     print (\"get_boolean_list_for_numerical_correlation_of_passenger_value_mean_and_std\"+\"\\n\")\n#     print (\"*\"*40)\n#     temp_boolean_list = []\n#     numerical_columns = ['Age', 'SibSp', 'Parch', 'Fare']\n#     # iterate ABCDEFGT cabin prefixes\n#     for cabin_prefix in cabin_prefix_dictionary.keys():\n#         print(\"\\nCabin Prefix: \"+  cabin_prefix)\n#         for passengers_without_cabin_column, passengers_without_cabin_value in one_passenger_without_cabin[numerical_columns].items():\n#             # iterate numerical columns\n#             print (passengers_without_cabin_column, passengers_without_cabin_value)\n\n#             # assign mean and std to variables + calculations\n#             mean_of_numerical_variable, std_of_numerical_variable = get_mean_and_std_of_numerical_variable(df=final_df_cabin_prefix_features_description, cabin_prefix=cabin_prefix, df_column=passengers_without_cabin_column)\n#             mean_plus_std, mean_minus_std = get_mean_plus_minus_std(mean=mean_of_numerical_variable, std=std_of_numerical_variable)\n#             is_correlated_feature = ((mean_plus_std >= passengers_without_cabin_value) & \\\n#                                      (mean_minus_std <= passengers_without_cabin_value))\n#             temp_boolean_list.append(is_correlated_feature)\n#             print (\"mean +- range: \"+ str(mean_minus_std) + \" - \" + str(mean_plus_std))\n#             print (is_correlated_feature)\n#             print (\"\\n\")\n        \n#         # assign the number of independant variables of the passenger without cabin that are correlated with the variable of each cabin_prefix\n#         boolean_dictionary_per_cabin_prefix[cabin_prefix] += sum(temp_boolean_list)\n#         print(boolean_dictionary_per_cabin_prefix)\n#         print(\"\\n\" + \"-\"*40)\n\n#         # empty temp_boolean_list for each cabin_prefix - before the next iteration\n#         temp_boolean_list = []\n#     return boolean_dictionary_per_cabin_prefix\n\n# def get_array_of_boolean_correlations_for_each_feature_per_cabin_prefix(one_passenger_without_cabin=row_series, final_df_cabin_prefix_features_description=final_df_cabin_prefix_features_description):\n#     ##### maybe no arguments needed\n    \n#     # assign a variable to store the boolean values\n#     temp_boolean_list = []\n#     boolean_dictionary_per_cabin_prefix = {\"A\": 0, \"B\": 0, \"C\": 0, \"D\": 0, \"E\": 0, \"F\": 0, \"G\": 0, \"T\": 0}\n#     categorical_columns = ['Sex', 'Embarked', 'Pclass']\n#     numerical_columns = ['Age', 'SibSp', 'Parch', 'Fare']\n    \n#     boolean_dictionary_per_cabin_prefix += get_boolean_list_for_categorical_correlation_of_passenger_value_and_mode() ##### maybe arguments needed\n#     boolean_dictionary_per_cabin_prefix += get_boolean_list_for_numerical_correlation_of_passenger_value_mean_and_std() ##### maybe arguments needed\n#     print (temp_boolean_list)\n    \n# #     # iterate ABCDEFGT cabin prefixes\n# #     for cabin_prefix in boolean_dictionary_per_cabin_prefix.keys():\n# #         print(\"\\nCabin Prefix: \"+  cabin_prefix)\n        \n# #         # iterate categorical columns\n# #         for passengers_without_cabin_column, passengers_without_cabin_value in example_row[categorical_columns].items():\n# #             # https:\/\/stackoverflow.com\/questions\/13389203\/pandas-slice-a-multiindex-by-range-of-secondary-index\n# #             # https:\/\/stackoverflow.com\/questions\/24922867\/key-error-and-multiindex-lexsort-depth\n            \n# #             # correlate mode of categorical feature with the passenger's value\n# #             mode_of_categorical_variable = get_mode_of_categorical_variable(df=final_df_cabin_prefix_features_description, cabin_prefix=cabin_prefix, df_column=passengers_without_cabin_column)\n# #             is_correlated_feature = (mode_of_categorical_variable == passengers_without_cabin_value)\n# #             temp_boolean_list.append(is_correlated_feature)\n# #             print (\"Mode: \"+ mode_of_categorical_variable)\n# #             print (\"Passenger's value: \" + passengers_without_cabin_value)\n# #             print (is_correlated_feature)\n            \n# #         for passengers_without_cabin_column, passengers_without_cabin_value in example_row[numerical_columns].items():\n# #             # iterate numerical columns\n# #             print (passengers_without_cabin_column, passengers_without_cabin_value)\n\n# #             # assign mean and std to variables + calculations\n# #             mean_of_numerical_variable, std_of_numerical_variable = get_mean_and_std_of_numerical_variable(df=final_df_cabin_prefix_features_description, cabin_prefix=cabin_prefix, df_column=passengers_without_cabin_column)\n# #             mean_plus_std, mean_minus_std = get_mean_plus_minus_std(mean=mean_of_numerical_variable, std=std_of_numerical_variable)\n# #             is_correlated_feature = ((mean_plus_std >= passengers_without_cabin_value) & \\\n# #                                      (mean_minus_std <= passengers_without_cabin_value))\n# #             temp_boolean_list.append(is_correlated_feature)\n# #             print (\"mean +- range: \"+ str(mean_minus_std) + \" - \" + str(mean_plus_std))\n# #             print (is_correlated_feature)\n# #             print (\"\\n\")\n# #         print (temp_boolean_list)\n        \n#         # assign the number of independant variables of the passenger without cabin that are correlated with the variable of each cabin_prefix\n#         boolean_dictionary_per_cabin_prefix[cabin_prefix] = sum(temp_boolean_list)\n#         print(boolean_dictionary_per_cabin_prefix)\n#         print(\"\\n\" + \"-\"*40)\n\n#         # empty temp_boolean_list for each cabin_prefix - before the next iteration\n#         temp_boolean_list = []\n        \n#     # https:\/\/stackoverflow.com\/questions\/268272\/getting-key-with-maximum-value-in-dictionary\n#     import operator\n#     estimated_cabin_prefix = max(boolean_dictionary_per_cabin_prefix.items(), key=operator.itemgetter(1))[0]\n#     print(boolean_dictionary_per_cabin_prefix)\n#     print(estimated_cabin_prefix)\n    \n# def get_estimated_letter_based_on_boolean_array(boolean_dictionary_per_cabin_prefix=boolean_dictionary_per_cabin_prefix, temp_boolean_list=temp_boolean_list):\n#     # iterate ABCDEFGT cabin prefixes\n#     for cabin_prefix in boolean_dictionary_per_cabin_prefix.keys():\n#         print(\"\\nCabin Prefix: \"+  cabin_prefix)\n        \n#         boolean_dictionary_per_cabin_prefix[cabin_prefix] = sum(temp_boolean_list)\n#         print(boolean_dictionary_per_cabin_prefix)\n#         print(\"\\n\" + \"-\"*40)\n\n#         # empty temp_boolean_list for each cabin_prefix - before the next iteration\n#         temp_boolean_list = []","22afe232":"# example_row = passengers_without_cabin.iloc[0]\n# categorical_columns = ['Sex', 'Embarked', 'Pclass'] # ['Sex', 'Embarked', 'Pclass'] for some reason Pclass is not in df, hence cant iterate it!!!\n# example_row['Pclass']","aa3a352e":"# https:\/\/thispointer.com\/python-how-to-get-all-keys-with-maximum-value-in-a-dictionary\/\nimport random\ndef get_list_of_keys_with_max_values(sampleDict):\n    # Find item with Max Value in Dictionary\n    itemMaxValue = max(sampleDict.items(), key=lambda x: x[1])\n#     print('Maximum Value in Dictionary : ', itemMaxValue[1])\n    listOfKeys = list()\n    # Iterate over all the items in dictionary to find keys with max value\n    for key, value in sampleDict.items():\n        if value == itemMaxValue[1]:\n            listOfKeys.append(key)\n#     print('Keys with maximum Value in Dictionary : ', listOfKeys)\n    return listOfKeys\n\ndef choose_random_key_from_keys_with_max_values(listOfKeys):\n    return random.choice(listOfKeys)\n\ndef get_key_with_max_values(sampleDict):\n    listOfKeys = get_list_of_keys_with_max_values(sampleDict)\n    return choose_random_key_from_keys_with_max_values(listOfKeys)","f110c24d":"for row_index, example_row in passengers_without_cabin.reset_index().iterrows():\n    print (\"Passenger index: \" + str(row_index))\n    print (\"~\"*40)\n    temp_boolean_list = []\n    boolean_dictionary_per_cabin_prefix = {\"A\": [], \"B\": [], \"C\": [], \"D\": [], \"E\": [], \"F\": [], \"G\": []} # T represents only one passenger, hence redacted\n\n    categorical_columns = ['Sex', 'Embarked', 'Pclass'] # ['Sex', 'Embarked', 'Pclass'] for some reason Pclass is not in df, hence cant iterate it!!!\n    numerical_columns = ['Age', 'SibSp', 'Parch', 'Fare']\n\n    for cabin_prefix in boolean_dictionary_per_cabin_prefix.keys():\n          # iterate ABCDEFGT cabin prefixes\n        print(\"\\nCabin Prefix: \"+  cabin_prefix)\n        for passengers_without_cabin_column, passengers_without_cabin_value in example_row[categorical_columns].items():\n        # https:\/\/stackoverflow.com\/questions\/13389203\/pandas-slice-a-multiindex-by-range-of-secondary-index\n        # https:\/\/stackoverflow.com\/questions\/24922867\/key-error-and-multiindex-lexsort-depth\n            sliced_description_df_by_cabin_prefix = final_df_cabin_prefix_features_description.loc(axis=0)[(slice(cabin_prefix, cabin_prefix))]\n            sliced_description_df_by_cabin_prefix_and_feature = sliced_description_df_by_cabin_prefix.loc(axis=1)[passengers_without_cabin_column]\n            mode_of_categorical_variable = sliced_description_df_by_cabin_prefix_and_feature['top'][0]\n            is_correlated_feature = mode_of_categorical_variable == passengers_without_cabin_value\n            temp_boolean_list.append(is_correlated_feature)\n            print (\"Mode: \"+ mode_of_categorical_variable)\n            print (\"Passenger's value: \" + passengers_without_cabin_value)\n            print (is_correlated_feature)\n        for passengers_without_cabin_column, passengers_without_cabin_value in example_row[numerical_columns].items():\n            print (passengers_without_cabin_column, passengers_without_cabin_value)\n\n            # assign mean and std to variables\n            sliced_description_df_by_cabin_prefix = final_df_cabin_prefix_features_description.loc(axis=0)[(slice(cabin_prefix, cabin_prefix))]\n            sliced_description_df_by_cabin_prefix_and_feature = sliced_description_df_by_cabin_prefix.loc(axis=1)[passengers_without_cabin_column]\n            mean_of_numerical_variable = sliced_description_df_by_cabin_prefix_and_feature['mean'][0]\n            std_of_numerical_variable = sliced_description_df_by_cabin_prefix_and_feature['std'][0]\n\n            # calculate mean +- standard deviation (round for Parch and SibSp)\n            mean_plus_std = round(mean_of_numerical_variable + std_of_numerical_variable)\n            mean_minus_std = round(mean_of_numerical_variable - std_of_numerical_variable)\n            is_correlated_feature = ((mean_plus_std >= passengers_without_cabin_value) & \\\n                                     (mean_minus_std <= passengers_without_cabin_value))\n            temp_boolean_list.append(is_correlated_feature)\n            print (\"mean +- range: \"+ str(mean_minus_std) + \" - \" + str(mean_plus_std))\n            print (is_correlated_feature)\n            print (\"\\n\")\n        print (temp_boolean_list)\n\n        # assign the number of independant variables of the passenger without cabin that are correlated with the variable of each cabin_prefix\n        boolean_dictionary_per_cabin_prefix[cabin_prefix] = sum(temp_boolean_list)\n        print(boolean_dictionary_per_cabin_prefix)\n        print(\"\\n\" + \"-\"*40)\n\n        # empty temp_boolean_list for each cabin_prefix - before the next iteration\n        temp_boolean_list = []\n    # end of for loops, choose estimated cabin_prefix based on max values of correlated variables\n    # method 1 - using random.choise() of multiple maximum cabin_prefixes\n    estimated_cabin_prefix = get_key_with_max_values(boolean_dictionary_per_cabin_prefix)\n    passengers_without_cabin['Cabin_Prefix'].iloc[row_index] = estimated_cabin_prefix\n    print(boolean_dictionary_per_cabin_prefix)\n    print(\"estimated_cabin_prefix: \" + estimated_cabin_prefix)\n    \n    # method 2 - prone to FP due to the nature of max() implementation which chooses the first maximum value alphabetically\n#     # https:\/\/stackoverflow.com\/questions\/268272\/getting-key-with-maximum-value-in-dictionary\n#     estimated_cabin_prefix = max(boolean_dictionary_per_cabin_prefix.items(), key=operator.itemgetter(1))[0]\n#     print(boolean_dictionary_per_cabin_prefix)\n#     print(estimated_cabin_prefix)\n#     passengers_without_cabin['Cabin_Prefix'].iloc[row_index] = estimated_cabin_prefix\npassengers_without_cabin.reset_index().head()","aef1eeda":"# assign complete cabin_prefix changes to train_df\ntrain_df = pd.concat(objs=[passengers_without_cabin, passengers_with_cabin], axis=0).reset_index(drop=True)\n# train_df.groupby(\"Cabin_Prefix\").size()","45b9c065":"# checks that changes were saved correctly\ntrain_df[train_df[\"Cabin_Prefix\"].isnull()]","dae6b833":"passenger_with_cabin_grouped = pd.DataFrame(passengers_with_cabin.groupby(\"Cabin_Prefix\").size()).reset_index()\ntrain_df_cabin_grouped = pd.DataFrame(train_df.groupby(\"Cabin_Prefix\").size()).reset_index()\npassenger_with_cabin_grouped.merge(train_df_cabin_grouped, on=\"Cabin_Prefix\", how='left', suffixes=('_before', '_after'))\\\n.rename(columns = {'0_before': 'Before', '0_after': 'After'})","152a0ae7":"# https:\/\/stackoverflow.com\/questions\/45003806\/python-pandas-use-slice-with-describe-versions-greater-than-0-20\ntest_df.Pclass = test_df.Pclass.map({1: 'First', 2: 'Second', 3: 'Third'}).astype('object') # for mode()\npassengers_without_cabin = test_df[test_df[\"Cabin_Prefix\"].isnull()].reset_index()\npassengers_with_cabin = test_df[test_df[\"Cabin_Prefix\"].notnull()].reset_index()\n\n# Create two dataframes for passengers with cabin - each dataframe for numerical\/categorical set of features\ndescription_of_passengers_with_cabin_numeric_features = passengers_with_cabin.groupby(\"Cabin_Prefix\").describe(include=np.number)\ndescription_of_passengers_with_cabin_categorical_features = passengers_with_cabin.groupby(\"Cabin_Prefix\").describe(exclude=np.number)\n# https:\/\/stackoverflow.com\/questions\/10665889\/how-to-take-column-slices-of-dataframe-in-pandas\/44736467\n\n# filter frequency of features based on mean+-std for numerical features, mode & freq for categorical features\nfiltered_df_of_passengers_with_cabin_mean_std_numeric = description_of_passengers_with_cabin_numeric_features.loc[:,(slice(None),['mean','std', 'top'])].loc[:, ['Fare','Age','Parch','SibSp']]\ndescription_of_passengers_with_cabin_categorical_features = description_of_passengers_with_cabin_categorical_features.loc[:,(slice(None),['top', 'freq'])].loc[:, ['Pclass','Embarked', \"Sex\"]]\nfinal_df_cabin_prefix_features_description = pd.merge(filtered_df_of_passengers_with_cabin_mean_std_numeric, description_of_passengers_with_cabin_categorical_features, on='Cabin_Prefix', how='inner')\nfinal_df_cabin_prefix_features_description","bf52faaf":"for row_index, example_row in passengers_without_cabin.reset_index().iterrows():\n    print (\"Passenger index: \" + str(row_index))\n    print (\"~\"*40)\n    temp_boolean_list = []\n    boolean_dictionary_per_cabin_prefix = {\"A\": [], \"B\": [], \"C\": [], \"D\": [], \"E\": [], \"F\": [], \"G\": []}\n\n    categorical_columns = ['Sex', 'Embarked', 'Pclass'] # ['Sex', 'Embarked', 'Pclass'] for some reason Pclass is not in df, hence cant iterate it!!!\n    numerical_columns = ['Age', 'SibSp', 'Parch', 'Fare']\n\n    for cabin_prefix in boolean_dictionary_per_cabin_prefix.keys():\n          # iterate ABCDEFGT cabin prefixes\n        print(\"\\nCabin Prefix: \"+  cabin_prefix)\n        for passengers_without_cabin_column, passengers_without_cabin_value in example_row[categorical_columns].items():\n        # https:\/\/stackoverflow.com\/questions\/13389203\/pandas-slice-a-multiindex-by-range-of-secondary-index\n        # https:\/\/stackoverflow.com\/questions\/24922867\/key-error-and-multiindex-lexsort-depth\n            sliced_description_df_by_cabin_prefix = final_df_cabin_prefix_features_description.loc(axis=0)[(slice(cabin_prefix, cabin_prefix))]\n            sliced_description_df_by_cabin_prefix_and_feature = sliced_description_df_by_cabin_prefix.loc(axis=1)[passengers_without_cabin_column]\n            mode_of_categorical_variable = sliced_description_df_by_cabin_prefix_and_feature['top'][0]\n            is_correlated_feature = mode_of_categorical_variable == passengers_without_cabin_value\n            temp_boolean_list.append(is_correlated_feature)\n            print (\"Mode: \"+ mode_of_categorical_variable)\n            print (\"Passenger's value: \" + passengers_without_cabin_value)\n            print (is_correlated_feature)\n        for passengers_without_cabin_column, passengers_without_cabin_value in example_row[numerical_columns].items():\n            print (passengers_without_cabin_column, passengers_without_cabin_value)\n\n            # assign mean and std to variables\n            sliced_description_df_by_cabin_prefix = final_df_cabin_prefix_features_description.loc(axis=0)[(slice(cabin_prefix, cabin_prefix))]\n            sliced_description_df_by_cabin_prefix_and_feature = sliced_description_df_by_cabin_prefix.loc(axis=1)[passengers_without_cabin_column]\n            mean_of_numerical_variable = sliced_description_df_by_cabin_prefix_and_feature['mean'][0]\n            std_of_numerical_variable = sliced_description_df_by_cabin_prefix_and_feature['std'][0]\n\n            # calculate mean +- standard deviation (round for Parch and SibSp)\n            mean_plus_std = round(mean_of_numerical_variable + std_of_numerical_variable)\n            mean_minus_std = round(mean_of_numerical_variable - std_of_numerical_variable)\n            is_correlated_feature = ((mean_plus_std >= passengers_without_cabin_value) & \\\n                                     (mean_minus_std <= passengers_without_cabin_value))\n            temp_boolean_list.append(is_correlated_feature)\n            print (\"mean +- range: \"+ str(mean_minus_std) + \" - \" + str(mean_plus_std))\n            print (is_correlated_feature)\n            print (\"\\n\")\n        print (temp_boolean_list)\n\n        # assign the number of independant variables of the passenger without cabin that are correlated with the variable of each cabin_prefix\n        boolean_dictionary_per_cabin_prefix[cabin_prefix] = sum(temp_boolean_list)\n        print(boolean_dictionary_per_cabin_prefix)\n        print(\"\\n\" + \"-\"*40)\n\n        # empty temp_boolean_list for each cabin_prefix - before the next iteration\n        temp_boolean_list = []\n    \n    ### method 1 -\n    estimated_cabin_prefix = get_key_with_max_values(boolean_dictionary_per_cabin_prefix)\n    passengers_without_cabin['Cabin_Prefix'].iloc[row_index] = estimated_cabin_prefix\n    print(boolean_dictionary_per_cabin_prefix)\n    print(\"estimated_cabin_prefix: \" + estimated_cabin_prefix)\n        \n    ### method 2 - get estimated_cabin_prefix by max() of counter. however, what happens if there are several cabin_prefixes with the same amount of correlated variables? hence, i decided to use random.choise()\n    ## https:\/\/stackoverflow.com\/questions\/268272\/getting-key-with-maximum-value-in-dictionary\n    # estimated_cabin_prefix = max(boolean_dictionary_per_cabin_prefix.items(), key=operator.itemgetter(1))[0]\n    # print(boolean_dictionary_per_cabin_prefix)\n    # print(\"estimated_cabin_prefix: \" + estimated_cabin_prefix)\n    # passengers_without_cabin['Cabin_Prefix'].iloc[row_index] = estimated_cabin_prefix\npassengers_without_cabin.reset_index().iloc[0:2]","5a13f6d5":"passengers_with_cabin.reset_index().iloc[0:2] ### doesn't have Cabin_Prefix?","7cdb1f30":"# assign complete cabin_prefix changes to train_df\ntest_df = pd.concat(objs=[passengers_without_cabin, passengers_with_cabin], axis=0).reset_index(drop=True)\n# test_df.groupby(\"Cabin_Prefix\").size()\ncombined_df = pd.concat(objs=[train_df, test_df], axis=0).reset_index(drop=True)","63618b9b":"# checks that changes were saved correctly\ntest_df[\"Cabin_Prefix\"].isnull().sum()","fe2eff66":"combined_df[\"Cabin_Prefix\"].isnull().sum()","ac9c64c5":"passenger_with_cabin_grouped = pd.DataFrame(passengers_with_cabin.groupby(\"Cabin_Prefix\").size()).reset_index()\ntest_df_cabin_grouped = pd.DataFrame(test_df.groupby(\"Cabin_Prefix\").size()).reset_index()\npassenger_with_cabin_grouped.merge(test_df_cabin_grouped, on=\"Cabin_Prefix\", how='left', suffixes=('_before', '_after'))\\\n.rename(columns = {'0_before': 'Before', '0_after': 'After'})","57bde189":"# https:\/\/www.kaggle.com\/omarelgabry\/a-journey-through-titanic   \n\n# get average, std, and number of NaN values in titanic_df\naverage_age_train   = train_df[\"Age\"].mean()\nstd_age_train       = train_df[\"Age\"].std()\ncount_nan_age_train = train_df[\"Age\"].isnull().sum()\n\n# get average, std, and number of NaN values in test_df\naverage_age_test   = test_df[\"Age\"].mean()\nstd_age_test       = test_df[\"Age\"].std()\ncount_nan_age_test = test_df[\"Age\"].isnull().sum()\n\n# generate random numbers between (mean - std) & (mean + std)\nrand_1 = np.random.randint(average_age_train - std_age_train, average_age_train + std_age_train, size = count_nan_age_train)\nrand_2 = np.random.randint(average_age_test - std_age_test, average_age_test + std_age_test, size = count_nan_age_test)\n\n# fill NaN values in Age column with random values generated\ntrain_df[\"Age\"][np.isnan(train_df[\"Age\"])] = rand_1\ntest_df[\"Age\"][np.isnan(test_df[\"Age\"])] = rand_2\n\n# convert from float to int\ntrain_df['Age'] = train_df['Age'].astype(int)\ntest_df['Age'] = test_df['Age'].astype(int)\ncombined_df = pd.concat(objs=[train_df, test_df], axis=0).reset_index(drop=True)","848e07d4":"# checks that changes were saved correctly\nprint(\"number of passengers with missing Age - train: {}\".format(train_df[\"Age\"].isnull().sum()))\nprint(\"number of passengers with missing Age - test: {}\".format(train_df[\"Age\"].isnull().sum()))\nprint(\"number of passengers with missing Age - combined: {}\".format(train_df[\"Age\"].isnull().sum()))","aef1b3f8":"def create_family_size_feature(df):\n    df['Family_Size'] = df['SibSp'] + df['Parch'] + 1\n    return df\ntrain_df, test_df, combined_df = apply_to_multiple_dataframes(create_family_size_feature)","99668aa1":"combined_df.groupby(\"Family_Size\").size()","2b2e90e6":"def create_calculated_fare_feature(df):\n    df['Calculated_Fare'] = df.Fare \/ df.Family_Size\n    return df\ntrain_df, test_df, combined_df = apply_to_multiple_dataframes(create_calculated_fare_feature)","d503a513":"# https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial#2.2-Frequency-Encoding\n# https:\/\/pbpython.com\/pandas_transform.html\n\ncombined_df['Ticket_Frequency'] = combined_df.groupby('Ticket')['Ticket'].transform('count')","bb15b805":"combined_df.groupby(\"Ticket_Frequency\").size()","6b6f7c80":"import string\ndef extract_families_and_titles_from_name(data):    \n    families = []\n    titles = []\n    for i in range(len(data)):        \n        name = data.iloc[i]\n        if '(' in name:\n            name_no_bracket = name.split('(')[0] \n        else:\n            name_no_bracket = name\n        family = name_no_bracket.split(',')[0]\n        title = name_no_bracket.split(',')[1].strip().split(' ')[0].replace('.','')\n        titles.append(title)\n        for c in string.punctuation:\n            family = family.replace(c, '').strip()\n        families.append(family)\n    return families, titles\ncombined_df['Family_Name'], combined_df['Title'] = extract_families_and_titles_from_name(combined_df['Name'])\ntrain_df = combined_df.loc[:890]\ntest_df = combined_df.loc[891:]","ef7ec82c":"# Creating a list of families and tickets that are occuring in both training and test set\nnon_unique_families = [x for x in train_df['Family_Name'].unique() if x in test_df['Family_Name'].unique()]\nnon_unique_tickets = [x for x in train_df['Ticket'].unique() if x in test_df['Ticket'].unique()]\n\ndf_family_survival_rate = train_df.groupby('Family_Name')['Survived', 'Family_Name','Family_Size'].median()\ndf_ticket_survival_rate = train_df.groupby('Ticket')['Survived', 'Ticket','Ticket_Frequency'].median()\n\nfamily_rates = {}\nticket_rates = {}\n\nfor i in range(len(df_family_survival_rate)):\n    # Checking a family exists in both training and test set, and has members more than 1\n    if df_family_survival_rate.index[i] in non_unique_families and df_family_survival_rate.iloc[i, 1] > 1:\n        family_rates[df_family_survival_rate.index[i]] = df_family_survival_rate.iloc[i, 0]\n\nfor i in range(len(df_ticket_survival_rate)):\n    # Checking a ticket exists in both training and test set, and has members more than 1\n    if df_ticket_survival_rate.index[i] in non_unique_tickets and df_ticket_survival_rate.iloc[i, 1] > 1:\n        ticket_rates[df_ticket_survival_rate.index[i]] = df_ticket_survival_rate.iloc[i, 0]","fca46e57":"mean_survival_rate = np.mean(train_df['Survived'])\n\ntrain_family_survival_rate = []\ntrain_family_survival_rate_NA = []\ntest_family_survival_rate = []\ntest_family_survival_rate_NA = []\n\nfor i in range(len(train_df)):\n    if train_df['Family_Name'][i] in family_rates:\n        train_family_survival_rate.append(family_rates[train_df['Family_Name'][i]])\n        train_family_survival_rate_NA.append(1)\n    else:\n        train_family_survival_rate.append(mean_survival_rate)\n        train_family_survival_rate_NA.append(0)\n        \nfor i in range(len(test_df)):\n    if test_df['Family_Name'].iloc[i] in family_rates:\n        test_family_survival_rate.append(family_rates[test_df['Family_Name'].iloc[i]])\n        test_family_survival_rate_NA.append(1)\n    else:\n        test_family_survival_rate.append(mean_survival_rate)\n        test_family_survival_rate_NA.append(0)\n        \ntrain_df['Family_Survival_Rate'] = train_family_survival_rate\ntrain_df['Family_Survival_Rate_NA'] = train_family_survival_rate_NA\ntest_df['Family_Survival_Rate'] = test_family_survival_rate\ntest_df['Family_Survival_Rate_NA'] = test_family_survival_rate_NA\n\ntrain_ticket_survival_rate = []\ntrain_ticket_survival_rate_NA = []\ntest_ticket_survival_rate = []\ntest_ticket_survival_rate_NA = []\n\nfor i in range(len(train_df)):\n    if train_df['Ticket'][i] in ticket_rates:\n        train_ticket_survival_rate.append(ticket_rates[train_df['Ticket'][i]])\n        train_ticket_survival_rate_NA.append(1)\n    else:\n        train_ticket_survival_rate.append(mean_survival_rate)\n        train_ticket_survival_rate_NA.append(0)\n        \nfor i in range(len(test_df)):\n    if test_df['Ticket'].iloc[i] in ticket_rates:\n        test_ticket_survival_rate.append(ticket_rates[test_df['Ticket'].iloc[i]])\n        test_ticket_survival_rate_NA.append(1)\n    else:\n        test_ticket_survival_rate.append(mean_survival_rate)\n        test_ticket_survival_rate_NA.append(0)\n        \ntrain_df['Ticket_Survival_Rate'] = train_ticket_survival_rate\ntrain_df['Ticket_Survival_Rate_NA'] = train_ticket_survival_rate_NA\ntest_df['Ticket_Survival_Rate'] = test_ticket_survival_rate\ntest_df['Ticket_Survival_Rate_NA'] = test_ticket_survival_rate_NA","884ea77a":"for df in [train_df, test_df]:\n    df['Survival_Rate'] = (df['Ticket_Survival_Rate'] + df['Family_Survival_Rate']) \/ 2\n    df['Survival_Rate_NA'] = (df['Ticket_Survival_Rate_NA'] + df['Family_Survival_Rate_NA']) \/ 2    \ncombined_df = pd.concat(objs=[train_df, test_df], axis=0).reset_index(drop=True)","2d48f38a":"test_df[['Survival_Rate', 'Survival_Rate_NA']].describe()","c45a834a":"msno.matrix(combined_df)","26031f3b":"def correct_title_names(df):\n    df['Title'] = df['Title'].replace('Mlle', 'Miss')\n    df['Title'] = df['Title'].replace('Ms', 'Miss')\n    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n    return df\ntrain_df = correct_title_names(train_df)\ntest_df = correct_title_names(test_df)\ncombined_df = pd.concat(objs=[train_df, test_df], axis=0).reset_index(drop=True)\n# train_df, test_df, combined_df = apply_to_multiple_dataframes(correct_title_names) # 'Title' Keyerror","0da79ecc":"df_title_count_before_title_grouping = pd.DataFrame(combined_df.groupby('Title').size())\ndf_title_count_before_title_grouping","14968908":"def clean_rare_title_names(df): \n    stat_min = 10 #common minimum in statistics: http:\/\/nicholasjjackson.com\/2012\/03\/08\/sample-size-is-10-a-magic-number\/\n    title_names = (df['Title'].value_counts() < stat_min)\n    df['Title'] = df['Title'].apply(lambda x: 'Rare' if title_names.loc[x] == True else x)\n    return df \n\ntrain_df = clean_rare_title_names(train_df)\ntest_df = clean_rare_title_names(test_df)\ncombined_df = pd.concat(objs=[train_df, test_df], axis=0).reset_index(drop=True)\n# train_df, test_df, combined_df = apply_to_multiple_dataframes(clean_rare_title_names) # 'Title' keyerror\n","88f6e706":"combined_df[combined_df[\"Title\"] == \"Rare\"].groupby(\"Title\").size()","39fb9427":"# not needed\npd.DataFrame(combined_df.groupby('Title').size()).join(df_title_count_before_title_grouping, on=\"Title\", lsuffix='_after_grouping', rsuffix='_before_grouping')","3a20c54d":"def create_is_married(df):\n    df['Is_Married'] = 0\n    df['Is_Married'].loc[df['Title'] == 'Mrs'] = 1\n    return df\n# train_df, test_df, combined_df = apply_to_multiple_dataframes(create_is_married) # 'Title' keyerror\ntrain_df = create_is_married(train_df)\ntest_df = create_is_married(test_df)\ncombined_df = pd.concat(objs=[train_df, test_df], axis=0).reset_index(drop=True)\ntrain_is_married_df = pd.DataFrame(train_df.groupby('Is_Married').size())\ntest_is_married_df = pd.DataFrame(test_df.groupby('Is_Married').size())\ntrain_is_married_df.merge(test_is_married_df, on=['Is_Married'], how='left').rename(columns={'0_x': 'train count', '0_y': 'test count'})\n#          pd.DataFrame(test_df.groupby('Title').size())","1eede599":"msno.matrix(combined_df)","0b8ea3a1":"# # https:\/\/medium.com\/@ODSC\/transforming-skewed-data-for-machine-learning-90e6cc364b0\n# num_features=train_df.dtypes[train_df.dtypes!='object'].index\n# skew_features = train_df[num_features].skew().sort_values(ascending=False)\n# skewness = pd.DataFrame({'Skew': skew_features})\n# skewness\n\nprint (\"Calculated_Fare level of skewness: \" + str(train_df.Calculated_Fare.skew()))\nprint (\"Age level of skewness: \" + str(train_df.Age.skew()))","5d93148d":"from scipy.stats import shapiro\nprint(shapiro(train_df.Calculated_Fare)[1]) # less than 0.05\nprint(shapiro(train_df.Age)[1])","02e11020":"# Explore Fare distribution \ng = sns.distplot(train_df[\"Calculated_Fare\"], color=\"m\", label=\"Calculated_Fare Skewness : %.2f\"%(train_df[\"Calculated_Fare\"].skew()))\ng = sns.distplot(train_df[\"Age\"], color=\"b\", label=\"Age Skewness : %.2f\"%(train_df[\"Age\"].skew()))\ng = g.legend(loc=\"best\")","c029c4c5":"# # Apply log to Fare to reduce skewness distribution\n# train_df[\"Fare\"] = dataset[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)","facbee2e":"from pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom matplotlib import pyplot\n\n# retrieve just the numeric input values\npower_transform_box_cox_df = train_df[['Age', 'Calculated_Fare']]\n# perform a box-cox transform of the dataset\nscaler = MinMaxScaler(feature_range=(1, 2)) # unknown why feature_range=(1, 2) instead of the default feature_range=(0, 1)\npower = PowerTransformer(method='box-cox')\npipeline = Pipeline(steps=[('s', scaler),('p', power)])\npower_transform_box_cox_df = pipeline.fit_transform(power_transform_box_cox_df)\n# convert the array back to a dataframe\npower_transform_box_cox_df = pd.DataFrame(power_transform_box_cox_df)\n# histograms of the variables\npower_transform_box_cox_df.hist()\npyplot.show()","383eb97d":"g = sns.distplot(power_transform_box_cox_df[0], color=\"b\", label=\"Age Skewness : %.2f\"%(power_transform_box_cox_df[0].skew()))\ng = sns.distplot(power_transform_box_cox_df[1], color=\"m\", label=\"Calculated_Fare Skewness : %.2f\"%(power_transform_box_cox_df[1].skew()))\ng = g.legend(loc=\"best\")","a2650a7b":"print(shapiro(power_transform_box_cox_df[0])[1])\nprint(shapiro(power_transform_box_cox_df[1])[1])","b366c8b9":"# retrieve just the numeric input values\npower_transform_yeo_johnson_df = train_df[['Age', 'Calculated_Fare']]\n# perform a yeo-johnson transform of the dataset\n# define the pipeline\nscaler = StandardScaler()\npower = PowerTransformer(method='yeo-johnson')\npipeline = Pipeline(steps=[('s', scaler), ('p', power)])\npower_transform_yeo_johnson_df = pipeline.fit_transform(power_transform_yeo_johnson_df)\n# convert the array back to a dataframe\npower_transform_yeo_johnson_df = pd.DataFrame(power_transform_yeo_johnson_df)\n# histograms of the variables\npower_transform_yeo_johnson_df.hist()\npyplot.show()","d4f115ca":"g = sns.distplot(power_transform_yeo_johnson_df[0], color=\"b\", label=\"Age yeo_joh Skewness : %.2f\"%(power_transform_yeo_johnson_df[0].skew()))\ng = sns.distplot(power_transform_yeo_johnson_df[1], color=\"m\", label=\"Calculated_Fare yeo_joh Skewness : %.2f\"%(power_transform_yeo_johnson_df[1].skew()))\ng = g.legend(loc=\"best\")","0bb5e14b":"print(\"train_df Age Shapiro's p-value: {}\".format(shapiro(power_transform_yeo_johnson_df[0])[1]))\nprint(\"train_df Calculated_Fare Shapiro's p-value: {}\".format(shapiro(power_transform_yeo_johnson_df[1])[1]))","fb7cd999":"train_df['Age'] = power_transform_yeo_johnson_df[0]\ntrain_df['Calculated_Fare'] = power_transform_yeo_johnson_df[1]","e94b5b01":"shapiro(train_df.Calculated_Fare)[1]","678351c9":"shapiro(train_df.Age)[1]","68d589f1":"# retrieve just the numeric input values\npower_transform_test_df = test_df[['Age', 'Calculated_Fare']].reset_index(drop=True)\n# perform a yeo-johnson transform of the dataset\n# define the pipeline\nscaler = StandardScaler()\npower = PowerTransformer(method='yeo-johnson')\npipeline = Pipeline(steps=[('s', scaler), ('p', power)])\npower_transform_test_df = pipeline.fit_transform(power_transform_test_df)\n# convert the array back to a dataframe\npower_transform_test_df = pd.DataFrame(power_transform_test_df)\n# histograms of the variables\npower_transform_test_df.hist()\npyplot.show()","f29fda61":"g = sns.distplot(power_transform_test_df[0], color=\"b\", label=\"Age yeo_joh Skewness : %.2f\"%(power_transform_test_df[0].skew()))\ng = sns.distplot(power_transform_test_df[1], color=\"m\", label=\"Calculated_Fare yeo_joh Skewness : %.2f\"%(power_transform_test_df[1].skew()))\ng = g.legend(loc=\"best\")","87cea6d8":"test_df = test_df.reset_index(drop=True)\ntest_df['Age'] = power_transform_test_df[0]\ntest_df['Calculated_Fare'] = power_transform_test_df[1]","40c50b0f":"shapiro(test_df.Age)[1] ##### is it too low??? still skewed?","7d58b817":"shapiro(test_df.Fare)[1]","3015f5b8":"combined_df = pd.concat(objs=[train_df, test_df], axis=0).reset_index(drop=True)","f8743221":"msno.matrix(combined_df)","04f1fbbb":"combined_df.info()","7ec1c319":"combined_df[\"Calculated_Fare\"].head()","9dc0a207":"%%html\n<iframe style=\"border:none\" width=\"1200\" height=\"450\" src=\"https:\/\/whimsical.com\/embed\/KSg3TdL3ydNPUr8cmS9Y5D\"><\/iframe>","762d0438":"from astropy.stats import bayesian_blocks\n#scotts binned histogram\nH1 = plt.hist(\n    train_df[\"Calculated_Fare\"], \n    bins = 'scott', \n    histtype = 'stepfilled', \n    alpha = .2, \n    density = False)\n#blocks binned histogram\nH2 = plt.hist(\n    train_df[\"Calculated_Fare\"], \n#     bins = bayesian_blocks(train_df[\"Calculated_Fare\"]), \n    bins = bayesian_blocks(train_df[\"Calculated_Fare\"],\n                           fitness='events',\n                           p0=0.01),   # https:\/\/www.kaggle.com\/vikasmalhotra08\/feature-binning-using-bayesian-blocks#Use-of-Bayesian-Block-in-Santander-Customer-Transaction-Prediction-Challenge:\n    color = 'black', \n    histtype = 'step', \n    density = False)","5fedadf8":"# using Scott's rule:\n\n# Calculated_Fare_bins = int(round(\n#         (train_df[\"Calculated_Fare\"].max() - train_df[\"Calculated_Fare\"].min())*\n#         (train_df.shape[0]**(1\/3))\/\n#         (3.49 * train_df[\"Calculated_Fare\"].std()\n#         )))\n# Calculated_Fare_bins\n\n# # https:\/\/stackoverflow.com\/questions\/37906210\/applying-pandas-qcut-bins-to-new-data\n# # https:\/\/stackoverflow.com\/questions\/11633874\/numpy-use-bins-with-infinite-range\n\n# _, bins_temp = pd.cut(train_df[\"Calculated_Fare\"], bins=Calculated_Fare_bins, retbins=True,  labels=False)\n# bins_temp\n\n# train_df[\"Calculated_Fare\"], bins_temp_with_inf = pd.cut(train_df[\"Calculated_Fare\"], bins=np.r_[-np.inf, bins_temp, np.inf], retbins=True, labels=False)\n# test_df[\"Calculated_Fare\"] = pd.cut(test_df[\"Calculated_Fare\"], bins=np.r_[-np.inf, bins_temp, np.inf], labels=False)\n# bins_temp_with_inf","b52a5a5e":"# using Bayesian_blocks\nbins_temp = bayesian_blocks(train_df[\"Calculated_Fare\"],\n                           fitness='events',\n                           p0=0.01)","f10296c6":"train_df[\"Calculated_Fare\"], bins_temp_with_inf = pd.cut(train_df[\"Calculated_Fare\"], bins=np.r_[-np.inf, bins_temp, np.inf], retbins=True, labels=False)\ntest_df[\"Calculated_Fare\"] = pd.cut(test_df[\"Calculated_Fare\"], bins=np.r_[-np.inf, bins_temp, np.inf], labels=False)\nbins_temp_with_inf\n\n# train_df[\"Calculated_Fare\"], train_calculated_fare_bins_array = pd.cut(train_df[\"Calculated_Fare\"], bins=Calculated_Fare_bins, retbins=True, labels=False)\n# train_calculated_fare_bins_array","2d76421a":"train_df[[\"Calculated_Fare\"]].groupby('Calculated_Fare').size()","be212afb":"test_df[[\"Calculated_Fare\"]].groupby('Calculated_Fare').size()","b5725764":"from astropy.stats import bayesian_blocks\n\n#scotts binned histogram\nH1 = plt.hist(\n    train_df[\"Age\"], \n    bins = 'scott', \n    histtype = 'stepfilled', \n    alpha = .2, \n    density = False)\n#blocks binned histogram\nH2 = plt.hist(\n    train_df[\"Age\"], \n#     bins = bayesian_blocks(train_df[\"Age\"]), \n    bins = bayesian_blocks(train_df[\"Age\"],\n                           fitness='events',\n                           p0=0.01),   # https:\/\/www.kaggle.com\/vikasmalhotra08\/feature-binning-using-bayesian-blocks#Use-of-Bayesian-Block-in-Santander-Customer-Transaction-Prediction-Challenge:\n    color = 'black', \n    histtype = 'step', \n\n    density = False)","c0adaecd":"# scott's rule\n# Calculated_Age_bins = int(round(\n#         (train_df[\"Age\"].max() - train_df[\"Age\"].min())*\n#         (train_df.shape[0]**(1\/3))\/\n#         (3.49 * train_df[\"Age\"].std()\n#         )))\n# Calculated_Age_bins\n\n# _, bins_temp = pd.cut(train_df[\"Age\"], bins=Calculated_Age_bins, retbins=True,  labels=False)\n# train_df[\"Age\"], bins_temp_with_inf = pd.cut(train_df[\"Age\"], bins=np.r_[-np.inf, bins_temp, np.inf], retbins=True, labels=False)\n# test_df[\"Age\"] = pd.cut(test_df[\"Age\"], bins=np.r_[-np.inf, bins_temp, np.inf], labels=False)\n# bins_temp_with_inf","ea25bb30":"bins_temp = bayesian_blocks(train_df[\"Age\"],\n                           fitness='events',\n                           p0=0.01)","09bf5021":"train_df[\"Age\"], bins_temp_with_inf = pd.cut(train_df[\"Age\"], bins=np.r_[-np.inf, bins_temp, np.inf], retbins=True, labels=False)\ntest_df[\"Age\"] = pd.cut(test_df[\"Age\"], bins=np.r_[-np.inf, bins_temp, np.inf], labels=False)\nbins_temp_with_inf","32d0d71c":"train_df[[\"Age\"]].groupby('Age').size()","cf064e1a":"test_df[[\"Age\"]].groupby('Age').size()","711daf98":"#scotts binned histogram\nH1 = plt.hist(\n    train_df[\"Family_Size\"], \n    bins = 'scott', \n    histtype = 'stepfilled', \n    alpha = .2, \n    density = False)\n#blocks binned histogram\nH2 = plt.hist(\n    train_df[\"Family_Size\"], \n    bins = bayesian_blocks(train_df[\"Family_Size\"],\n                           fitness='events',\n                           p0=0.01),   # https:\/\/www.kaggle.com\/vikasmalhotra08\/feature-binning-using-bayesian-blocks#Use-of-Bayesian-Block-in-Santander-Customer-Transaction-Prediction-Challenge:    color = 'black', \n    histtype = 'step', \n    density = False)","c3049e2b":"bins_temp = bayesian_blocks(train_df[\"Family_Size\"],\n                           fitness='events',\n                           p0=0.01)","a8289935":"train_df[\"Family_Size\"], bins_temp_with_inf = pd.cut(train_df[\"Family_Size\"], bins=np.r_[-np.inf, bins_temp, np.inf], retbins=True, labels=False)\ntest_df[\"Family_Size\"] = pd.cut(test_df[\"Family_Size\"], bins=np.r_[-np.inf, bins_temp, np.inf], labels=False)\nbins_temp_with_inf\n# _, bins_temp = pd.cut(train_df[\"Family_Size\"], bins=Family_Size_bins, retbins=True,  labels=False)\n# train_df[\"Family_Size\"], bins_temp_with_inf = pd.cut(train_df[\"Family_Size\"], bins=np.r_[-np.inf, bins_temp, np.inf], retbins=True, labels=False)\n# test_df[\"Family_Size\"] = pd.cut(test_df[\"Family_Size\"], bins=np.r_[-np.inf, bins_temp, np.inf], labels=False)\n# bins_temp_with_inf","e8ec5426":"train_df[[\"Family_Size\"]].groupby('Family_Size').size()","56be1e90":"test_df[[\"Family_Size\"]].groupby('Family_Size').size()","599c7103":"combined_df = pd.concat(objs=[train_df, test_df], axis=0).reset_index(drop=True)","32b027f5":"combined_df.info()","bd6d5dc1":"drop_features = []\n# remove Text Features\ntext_features = ['Ticket', 'Name', 'Cabin']\ndrop_features += text_features\n# remove Family\/Ticket Survival Rate features that were usued to create Survival_Rate and Survival_Rate_NA\ndrop_features += [\"Family_Survival_Rate\", \"Family_Survival_Rate_NA\", \"Ticket_Survival_Rate\", \"Ticket_Survival_Rate_NA\"]\n# remove the features which were used to build other features\ndrop_features += ['index', 'PassengerId', \"Sex_is_Female\", 'Family_Name', 'Fare'] #we have Calculated_Fare instead\ndrop_features","d819d89e":"def drop_features_bulk_dataframes(drop_features):\n    for df in [train_df, test_df, combined_df]:\n        for column in drop_features:\n            try:\n                df.drop(columns=column, inplace=True)\n            except KeyError:\n                continue\ndrop_features_bulk_dataframes(drop_features)","ea937ebc":"# https:\/\/stackoverflow.com\/questions\/24458645\/label-encoding-across-multiple-columns-in-scikit-learn\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\ntrain_df = train_df.apply(LabelEncoder().fit_transform)\ntest_df = test_df.apply(LabelEncoder().fit_transform)\ncombined_df = combined_df.apply(LabelEncoder().fit_transform)","e7dcd1b5":"# https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial#2.5.2-One-Hot-Encoding-the-Categorical-Features\ndef one_hot_encode_dataframe(df, nominal_categorical_features = ['Pclass']):\n    for feature in nominal_categorical_features:   \n        encoded_feat = OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n        n = df[feature].nunique() # number of categories for a categorical feature\n        # rename columns\n        cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n        encoded_df.index = df.index\n        # assign changes to input dataframe and delete the original non-one-hot-encoded feature\n        df = pd.concat([df, encoded_df], axis=1)\n        df.drop(columns=feature, inplace=True) # remove cols[0] to avoid multicollinarity\n        # df.drop(columns=[feature, cols[0]], inplace=True) # use this to avoid multicollinarity without checking VIF as I am about to do.\n    return df","3bf7b1b6":"nominal_categorical_columns = ['Pclass', 'Embarked', 'Cabin_Prefix', 'Title']\n\n# apply changes on all dataframes\ntrain_df = one_hot_encode_dataframe(df = train_df, nominal_categorical_features = nominal_categorical_columns)\ntest_df = one_hot_encode_dataframe(df = test_df, nominal_categorical_features = nominal_categorical_columns)\ncombined_df = pd.concat(objs=[train_df, test_df], axis=0).reset_index(drop=True)","e21f2431":"import statsmodels.api as sm\n\n# Function to calculate VIF\ndef calculate_vif(data):\n    # same as from statsmodels.stats.outliers_influence import variance_inflation_factor\n    vif_df = pd.DataFrame(columns = ['Feature', 'VIF'])\n    x_var_names = data.columns\n    for i in range(0, x_var_names.shape[0]):\n        y = data[x_var_names[i]]\n        x = data[x_var_names.drop([x_var_names[i]])]\n        r_squared = sm.OLS(y,x).fit().rsquared\n        vif = round(1\/(1-r_squared),2)\n        vif_df.loc[i] = [x_var_names[i], vif]\n    return vif_df.sort_values(by = ['VIF', 'Feature'], axis = 0, ascending=False, inplace=False)\nvif_df = calculate_vif(train_df)\nvif_df","5c27834d":"def correlate_onehotencoded_feature_to_drop(df, feature):\n    vif_df = calculate_vif(df)\n    before_vif_df = vif_df[vif_df['Feature'].str.contains(feature)]\n    number_of_features = len(before_vif_df)\n    merged_df = before_vif_df.copy(deep = True)\n    for i in range(1, number_of_features + 1):\n        next_feature_name = feature + \"_\" + str(i)\n        after_vif_df = calculate_vif(df.drop(next_feature_name, axis = 1))\n        after_vif_df = after_vif_df.append(pd.DataFrame({'Feature': [next_feature_name], 'VIF': ['X']}))\n        after_vif_df = after_vif_df[after_vif_df['Feature'].str.contains(feature[:-1])]\n        after_vif_df.rename(columns={\"VIF\": \"VIF_\" + str(i)}, inplace = True)\n        merged_df = pd.merge(merged_df, after_vif_df, on = 'Feature', how='inner')\n    return merged_df\n\ndef find_onehotencoded_feature_to_drop(corr_df):\n    # https:\/\/stackoverflow.com\/questions\/17114904\/python-pandas-replacing-strings-in-dataframe-with-numbers\n    # https:\/\/stackoverflow.com\/questions\/40111161\/pandas-sort-column-by-maximum-values\n    # sort correlation dataframe by maximum, access the last column (the one with the \"lowest maximum\" vif) and choose the feature to drop to achieve this vif scores\n    mymap = {'X': 0}\n    corr_df = corr_df.applymap(lambda s: mymap.get(s) if s in mymap else s)\n    sorted_df = corr_df[list(corr_df.columns[2:])].loc[:, corr_df[list(corr_df.columns[2:])].max().sort_values(ascending=False).index]\n    corr_df = pd.concat([corr_df[list(corr_df.columns[:2])], sorted_df], axis = 1)\n    feature_to_drop = corr_df.loc[corr_df.iloc[:, -1] == 0, ['Feature']]         \n    feature_to_drop = feature_to_drop['Feature'].iloc[0]\n    print(feature_to_drop)\n    return corr_df, feature_to_drop\n    \nmulticollinarity_features = []","9aa8db20":"train_df.columns","99d4d9d6":"corr_df = correlate_onehotencoded_feature_to_drop(train_df, 'Pclass')\ncorr_df","ba294d5b":"corr_df, feature_to_drop = find_onehotencoded_feature_to_drop(corr_df)\ncorr_df","dd8facb1":"def find_best_features_to_drop_for_best_multicollinarity(df, feature_list):\n    multicollinarity_features = []\n    for feature in feature_list:\n        corr_df = correlate_onehotencoded_feature_to_drop(df, feature)\n        corr_df, feature_to_drop = find_onehotencoded_feature_to_drop(corr_df)\n        multicollinarity_features.append(feature_to_drop)\n    return multicollinarity_features\nmulticollinarity_features = [\"Pclass\", \"Title\", \"Embarked\", \"Cabin_Prefix\"]\nmulticollinarity_exact_features = find_best_features_to_drop_for_best_multicollinarity(train_df, multicollinarity_features)\ndrop_features_bulk_dataframes(multicollinarity_exact_features)","53f9fc0f":"before_vif_df = vif_df\nbefore_vif_df.rename(columns={\"VIF\": \"VIF_before\"}, inplace = True)\nafter_vif_df = calculate_vif(train_df)\nfor feature in multicollinarity_exact_features:\n    after_vif_df.rename(columns={\"VIF\": \"VIF_after\"}, inplace = True)\n    after_vif_df = after_vif_df.append(pd.DataFrame({'Feature': [feature], 'VIF_after': ['X']}))\nmerged_vif_df = pd.merge(before_vif_df, after_vif_df, on = 'Feature', how='inner')\n","0fb46bc4":"mymap = {'X': 0}\nmerged_vif_df = merged_vif_df.applymap(lambda s: mymap.get(s) if s in mymap else s)\nmerged_vif_df.sort_values(by = ['Feature', 'VIF_after'], axis = 0, ascending=False).replace(0, 'X')","0b56c797":"after_vif_df_2 = calculate_vif(train_df.copy(deep = True).drop(columns = 'Is_Married'))\nmulticollinarity_exact_features.extend(['Is_Married'])\nfor feature in multicollinarity_exact_features:\n    after_vif_df_2.rename(columns={\"VIF\": \"VIF_after_Is_Married_Removal\"}, inplace = True)\n    after_vif_df_2 = after_vif_df_2.append(pd.DataFrame({'Feature': [feature], 'VIF_after_Is_Married_Removal': ['X']}))\nmerged_vif_df_2 = pd.merge(merged_vif_df, after_vif_df_2, on = 'Feature', how='inner')\nmerged_vif_df_2","3cd7bfda":"drop_features_bulk_dataframes(['Is_Married'])","802944c3":"# !pip install ppscore","8939a45a":"import ppscore as pps\n\ndef heatmap(df):\n    ax = sns.heatmap(df, vmin=0, vmax=1, cmap=\"Blues\", linewidths=0.5, annot=True)\n    ax.set_title('PPS matrix')\n    ax.set_xlabel('feature')\n    ax.set_ylabel('target')\n    return ax\n\n\ndef corr_heatmap(df):\n    ax = sns.heatmap(df, vmin=-1, vmax=1, cmap=\"BrBG\", linewidths=0.5, annot=True)\n    ax.set_title('Correlation matrix')\n    return ax","36f3d701":"pps.score(train_df, \"Sex\", \"Survived\")","f18d0c7e":"\n# # Correlation Matrix\n# f = plt.figure(figsize=(16,8))\n# f.add_subplot(1,2, 1)\n# corr_heatmap(train_df.corr())\n\n# f.add_subplot(1,2, 2)\n# matrix_df = pps.matrix(train_df).pivot(columns='x', index='y',  values='ppscore')\n# sns.set(rc={'figure.figsize':(13,9.1)})\n# sns.heatmap(matrix_df, cmap=\"BrBG\", annot=True)","b7071de4":"# from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\n# onehot_encoder = OneHotEncoder(sparse=False)\n\n# temp_train_df = train_df_copy.copy(deep=True)\n\n# # integer_encoded = label_encoder.fit_transform(temp_train_df['Pclass'].values)\n# a = onehot_encoder.fit_transform(temp_train_df['Pclass'].values.reshape(-1, 1))\n# n = temp_train_df['Pclass'].nunique()\n# #         cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n# #         encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n# #         print(encoded_df.shape[0])\n# print(n)\n\n# # LabelEncoder().inverse_transform([np.argmax(onehot_encoded[0, :])])\n\n\n\n\n\n\n# # # binary encode\n# # onehot_encoder = OneHotEncoder(sparse=False)\n# # integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n# # onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n# # print(onehot_encoded)\n# # # invert first example\n# # inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n# # print(inverted)","e877eff2":"# from sklearn.preprocessing import OneHotEncoder\n\n# low_dimensional_nominal_categorical_columns = ['Pclass', 'Embarked']\n# high_dimensional_nominal_categorical_columns = ['Cabin_Prefix', 'Title']\n# nominal_categorical_columns = low_dimensional_nominal_categorical_columns + high_dimensional_nominal_categorical_columns\n\n# onehot_encoder = OneHotEncoder(sparse=False)\n\n# for df in [train_df_copy]:\n#     for column in nominal_categorical_columns:\n# #         print(df[column].head())\n# #         break\n#         reshaped_column = df[column].reshape(len(integer_encoded), 1)\n# #         print(reshaped_column)\n# #         break\n#         onehot_encoded = onehot_encoder.fit_transform(reshaped_column)\n# #         print(onehot_encoded)\n# #         break\n# # # invert first example\n# # inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n# # print(inverted)\n\n\n# # integer encode\n# label_encoder = LabelEncoder()\n# integer_encoded = label_encoder.fit_transform(values)\n# print(integer_encoded)\n\n# # # binary encode\n# # onehot_encoder = OneHotEncoder(sparse=False)\n# # integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n# # onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n# # print(onehot_encoded)\n# # # invert first example\n# # inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n# # print(inverted)","7250cff3":"### ##  One-Hot-Encoding Nominal Categorical Features\n# dummies = pd.get_dummies(train_df_copy, columns=nominal_categorical_features, drop_first=True)\n# dummies\n\n# from category_encoders import TargetEncoder\n# enc = TargetEncoder(cols=['Name_of_col','Another_name'])\n# training_set = enc.fit_transform(X_train, y_train)\n\n# from category_encoders import LeaveOneOutEncoder\n# enc = LeaveOneOutEncoder(cols=['Name_of_col','Another_name'])\n# training_set = enc.fit_transform(X_train, y_train)\n\n# from category_encoders import WOEEncoder\n# enc = WOEEncoder(cols=['Name_of_col','Another_name'])\n# training_set = enc.fit_transform(X_train, y_train)","66b61f42":"# # https:\/\/datascience.stackexchange.com\/questions\/71804\/how-to-perform-one-hot-encoding-on-multiple-categorical-columns\n# from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n# onehotencoder = OneHotEncoder()\n\n# nominal_categorical_features = ['Cabin_Prefix', 'Pclass', 'Embarked', 'Sex']\n\n# train_df_copy = train_df.copy(deep = True)\n# test_df_copy = test_df.copy(deep = True)\n# combined_df_copy = test_df.copy(deep = True)\n\n# # instantiate labelencoder object\n# le = LabelEncoder()\n\n# # apply le on categorical feature columns\n\n# def onehotencode_dataframe(df, nominal_categorical_features=nominal_categorical_features, le=le):\n#     # apply le on categorical feature columns\n#     df[nominal_categorical_features] = df[nominal_categorical_features].apply(lambda col: le.fit_transform(col))    \n#     concatenated_df = df\n#     transformed_data = onehotencoder.fit_transform(df[nominal_categorical_features]).toarray()\n#     # the above transformed_data is an array so convert it to dataframe\n#     encoded_data = pd.DataFrame(transformed_data, index=df.index)\n#     #Extract only the columns that didnt need to be encoded\n#     data_other_cols = df.drop(columns=nominal_categorical_features)\n#     # now concatenate the original data and the encoded data using pandas\n#     concatenated_df = pd.concat([data_other_cols, encoded_data], axis=1)\n# #     df = concatenated_data\n#     return concatenated_df\n\n# # https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial#2.5.2-One-Hot-Encoding-the-Categorical-Features\n# train_df_copy = onehotencode_dataframe(train_df_copy, nominal_categorical_features)\n    \n# train_df_copy.head()","d2f2beab":"# from sklearn.preprocessing import OneHotEncoder\n# nominal_categorical_features = ['Cabin_Prefix', 'Pclass', 'Embarked', 'Sex']\n\n# train_df_copy = train_df.copy(deep = True)\n# test_df_copy = test_df.copy(deep = True)\n# combined_df_copy = test_df.copy(deep = True)\n\n# # https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial#2.5.2-One-Hot-Encoding-the-Categorical-Features\n# for df in [test_df_copy, train_df_copy, combined_df_copy]:\n#     for feature in nominal_categorical_features:        \n#         encoded_feat = OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n#         n = df[feature].nunique()\n#         cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n#         encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n#         print(encoded_df.shape[0])\n#         encoded_df.index = df.index\n#         encoded_features.append(encoded_df)\n# # train_encoded_features = encoded_features\n# # test_encoded_features = encoded_features.copy()\n\n# # for df in test_encoded_features:\n# #     print(len(test_df))\n# #     print(len(df))\n# #     print(df.drop(df.index[[len(test_df), len(df)-1]]))\n# # #     indexes_to_keep = set(range(df.shape[0])) - set(indexes_to_drop)\n# #     indexes_to_keep = len(test_df)\n# #     df = df.take(list(indexes_to_keep))\n# # #     df.drop(df.index[[len(test_df), len(df)-1]], inplace=True)\n# #     break\n# # test_encoded_features[0]\n\n\n\n# # train_df_copy = pd.concat([train_df_copy, *encoded_features[:len(nominal_categorical_features)]], axis=1)\n# test_df_copy = pd.concat([test_df_copy, *encoded_df[:len(nominal_categorical_features)]], axis=1)\n# # test_df_copy = pd.concat([test_df_copy, *encoded_features[len(nominal_categorical_features):]], axis=1)\n# # combined_df_copy = pd.concat(objs=[train_df_copy, test_df_copy], axis=0).reset_index(drop=True)","068b1345":"# # remove duplicate columns\n# # https:\/\/stackoverflow.com\/a\/45789165\n# import re\n# def concat_duplicate_columns(df):\n#     dupli = {}\n#     # populate dictionary with column names and count for duplicates \n#     for column in df.columns:\n#         dupli[column] = dupli[column] + 1 if column in dupli.keys() else 1\n#     # rename duplicated keys with \u00b0\u00b0\u00b0 number suffix\n#     for key, val in dict(dupli).items():\n#         del dupli[key]\n#         if val > 1:\n#             for i in range(val):\n#                 dupli[key+'\u00b0\u00b0\u00b0'+str(i)] = val\n#         else: dupli[key] = 1\n#     # rename columns so that we can now access abmigous column names\n#     # sorting in dict is the same as in original table\n#     df.columns = dupli.keys()\n#     # for each duplicated column name\n#     for i in set(re.sub('\u00b0\u00b0\u00b0(.*)','',j) for j in dupli.keys() if '\u00b0\u00b0\u00b0' in j):\n#         i = str(i)\n#         # for each duplicate of a column name\n#         for k in range(dupli[i+'\u00b0\u00b0\u00b00']-1):\n#             # concatenate values in duplicated columns\n#             df[i+'\u00b0\u00b0\u00b00'] = df[i+'\u00b0\u00b0\u00b00'].astype(str) + df[i+'\u00b0\u00b0\u00b0'+str(k+1)].astype(str)\n#             # Drop duplicated columns from which we have aquired data\n#             df = df.drop(i+'\u00b0\u00b0\u00b0'+str(k+1), 1)\n#     # resort column names for proper mapping\n#     df = df.reindex(sorted(df.columns), axis = 1) #### change reindex_axis to reindex for kaggle compatibility\n#     # rename columns\n#     df.columns = sorted(set(re.sub('\u00b0\u00b0\u00b0(.*)','',i) for i in dupli.keys()))\n#     return df\n# train_df_copy = concat_duplicate_columns(train_df_copy)\n# test_df_copy = concat_duplicate_columns(test_df_copy)\n# combined_df_copy = pd.concat(objs=[train_df_copy, test_df_copy], axis=0).reset_index(drop=True)","09099f2a":"# # https:\/\/stackoverflow.com\/questions\/48647534\/python-pandas-find-difference-between-two-data-frames\n# set(train_df_copy.columns).symmetric_difference(train_df.columns)","f6a5db33":"# use ppscore (Predictive Power Score (PPS) ################ TO DO https:\/\/www.kaggle.com\/parulpandey\/useful-python-libraries-for-data-science\n# use chi-squared statistic and the mutual information statistic https:\/\/machinelearningmastery.com\/feature-selection-with-categorical-data\/","3c982993":"print('Train columns with null values: \\n', train_df.isnull().sum())\nprint('Test\/Validation columns with null values: \\n', test_df.isnull().sum())","4e154bd8":"\n#define y variable aka target\/outcome\nTarget = ['Survived']\n\n#define x variables for original features aka feature selection\ntrain_df_x_calc = list(train_df.columns)\ntrain_df_x_calc.remove('Survived')\ntrain_df_x = train_df_x_calc\n# data1_x = [\"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Family_Size\", \"Calculated_Fare\", \"Ticket_Frequency\", \"Survival_Rate\", \"Survival_Rate_NA\", \"Pclass\", \"Embarked\", \"Cabin_Prefix\", \"Title\"] #pretty name\/values for charts\n# data1_x_calc = list() #coded for algorithm calculation\ntrain_df_xy =  Target + train_df_x\nprint('Original X Y: ', train_df_xy, '\\n')","578e7402":"from sklearn import model_selection\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(train_df[train_df_x_calc], train_df[Target], random_state = 0)\n#random_state -> seed or control random number generator: https:\/\/www.quora.com\/What-is-seed-in-random-number-generation\n\nprint(\"train_df Shape: {}\".format(train_df.shape))\nprint(\"Train1 Shape: {}\".format(train1_x.shape))\nprint(\"Test1 Shape: {}\".format(test1_x.shape))","0eddecba":"# !pip install dabl\nimport dabl\nec = dabl.SimpleClassifier(random_state=0).fit(train_df, target_col=\"Survived\") ","dad84825":"from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    XGBClassifier()    \n    ]","05ee4216":"#split dataset in cross-validation with this splitter class: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n#note: this is an alternative to train_test_split\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60\/30 split intentionally leaving out 10%\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD', 'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\nMLA_predict = train_df[Target]\n\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n    cv_results = model_selection.cross_validate(alg, train_df[train_df_x_calc], train_df[Target], cv  = cv_split, return_train_score = True)\n\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean() * 100\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean() * 100   \n    #if this is a non-bias random sample, then +\/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std() * 100 * 3   #let's know the worst that can happen!\n    \n\n    #save MLA predictions - see section 6 for usage\n    alg.fit(train_df[train_df_x_calc], train_df[Target])\n    MLA_predict[MLA_name] = alg.predict(train_df[train_df_x_calc])\n    \n    row_index+=1\n\n    \n#print and sort table: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.sort_values.html\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare\n#MLA_predict","cceff330":"#barplot using https:\/\/seaborn.pydata.org\/generated\/seaborn.barplot.html\nfig_dims = (12, 8)\nfig, ax = plt.subplots(figsize=fig_dims)\ng = sns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', ax = ax, data = MLA_compare, palette=\"Blues_d\")\n\n\n# https:\/\/stackoverflow.com\/a\/56780852\ndef show_values_on_bars(axs, h_v=\"v\", space=0.4):\n    def _show_on_single_plot(ax):\n        if h_v == \"v\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() \/ 2\n                _y = p.get_y() + p.get_height()\n                value = \"{:.2f}\".format(p.get_width())\n                ax.text(_x, _y, value, ha=\"center\") \n        elif h_v == \"h\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width()\n                _y = p.get_y() + p.get_height()\n                value = \"{:.2f}\".format(p.get_width())\n                ax.text(_x, _y, value, ha=\"left\")\n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)\n       \nshow_values_on_bars(g, \"h\", 0.3)\n\n#prettify using pyplot: https:\/\/matplotlib.org\/api\/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","30e8dea6":"import decimal\n\ndef float_range(start, stop, step):\n  while start < stop:\n    yield float(start)\n    start += decimal.Decimal(step)\n\n    \nparameters_dict = {\n\"AdaBoostClassifier\": {'n_estimators': range(50,100,10), 'learning_rate': list(float_range(0, 1.01, '0.1')), 'algorithm': ['SAMME', 'SAMME.R'], 'random_state': [None, 42]},\n\"BaggingClassifier\": {'n_estimators': range(10,20), 'bootstrap': [True, False], 'bootstrap_features': [True, False], 'oob_score': [True, False], 'warm_start': [True, False], 'random_state': [None, 42]},\n# \"ExtraTreesClassifier\": {'n_estimators': [50, 70, 100, 120], 'criterion': ['gini', 'entropy'], 'max_depth': [2,4,6,8,10,None], 'min_samples_split': [2,5,10,.03,.05], 'min_samples_leaf': [1,5,10,.03,.05], 'max_features': ['auto', 'sqrt', 'log2'], 'bootstrap': [True, False], 'oob_score': [True, False], 'warm_start': [True, False]},\n\"ExtraTreesClassifier\": {'n_estimators': [50, 70, 100, 120], 'criterion': ['gini', 'entropy'], 'max_depth': [2,4,None], 'min_samples_split': [2,5], 'min_samples_leaf': [1,2], 'max_features': ['auto', 'sqrt', 'log2']},\n# \"GradientBoostingClassifier\": {'loss': ['deviance', 'exponential'], 'n_estimators': range(10,150, 30), 'criterion': ['friedman_mse', 'mse', 'mae'], 'min_samples_split': [2,5,10,.03,.05], 'max_depth': [2,3,4,6,8,10], 'min_samples_split': [2,5,10,.03,.05], 'min_samples_leaf': [1,5,10,.03,.05], 'max_features': ['auto', 'sqrt', 'log2']},\n\"GradientBoostingClassifier\": {'loss': ['deviance', 'exponential'], 'n_estimators': [70, 90 ,100, 110], 'criterion': ['friedman_mse', 'mse', 'mae'], 'max_depth': [2,4,6], 'min_samples_split': [4,5], 'min_samples_leaf': [3,2], 'max_features': ['auto', 'sqrt', 'log2']},\n# \"RandomForestClassifier\": {'n_estimators': [50, 70, 100, 120], 'criterion': ['gini', 'entropy'], 'max_depth': [2,3,4,6,8,10, None], 'min_samples_split': [2,5,10,.03,.05], 'min_samples_leaf': [1,5,10,.03,.05], 'max_features': ['auto', 'sqrt', 'log2']},\n\"RandomForestClassifier\": {'n_estimators': [50, 70, 100, 120], 'criterion': ['gini', 'entropy'], 'max_depth': [2,4,None], 'min_samples_split': [2,5], 'min_samples_leaf': [1,2], 'max_features': ['auto', 'sqrt', 'log2']},\n\"GaussianProcessClassifier\": {'multi_class': ['one_vs_rest', 'one_vs_one'], 'warm_start': [True, False]},\n\"LogisticRegressionCV\": {'fit_intercept': [True, False], 'cv': [3,5,7,10], 'dual': [True, False], 'penalty': ['l1', 'l2', 'elasticnet'], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], 'multi_class': ['auto', 'ovr', 'multinomial'], 'class_weight': ['balanced'], 'max_iter': [1000]},\n\"PassiveAggressiveClassifier\": {'fit_intercept': [True, False], 'warm_start': [True, False]},\n\"RidgeClassifierCV\": {},\n\"SGDClassifier\": {'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'], 'penalty': ['l2', 'l1', 'elasticnet'], 'average': [True, False]},\n\"Perceptron\": {'penalty': ['l2', 'l1', 'elasticnet'], 'warm_start': [True, False]},\n\"BernoulliNB\": {},\n\"GaussianNB\": {},\n\"KNeighborsClassifier\": {'weights': ['uniform', 'distance'], 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'], 'leaf_size': range(20, 40, 5)},\n\"SVC\": {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'degree': range(1,5), 'gamma': ['scale', 'auto'], 'shrinking': [True, False], 'probability': [True, False]},\n\"NuSVC\": {'nu': [0.4, 0.7, 0.9], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid']},\n# \"NuSVC\": {'nu': [0.1, 0.4, 0.7, 1], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'degree': range(1,5), 'shrinking': [True, False], 'probability': [True, False], 'gamma': ['scale', 'auto'], 'decision_function_shape': ['ovo', 'ovr'], 'break_ties': [True, False]},\n\"LinearSVC\": {'penalty': ['l1', 'l2'], 'loss': ['hinge', 'squared_hinge'], 'dual': [True, False]},\n# \"DecisionTreeClassifier\": {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random'], 'min_samples_split': [2,5,10,.03,.05], 'max_depth': [2,3,4,6,8,10,None], 'min_samples_split': [2,5,10,.03,.05], 'min_samples_leaf': [1,5,10,.03,.05], 'max_features': ['auto', 'sqrt', 'log2']},\n\"DecisionTreeClassifier\": {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random'], 'max_depth': [2,4,None], 'min_samples_split': [2,5], 'min_samples_leaf': [1,2], 'max_features': ['auto', 'sqrt', 'log2'], 'class_weight': ['balanced'], 'min_impurity_decrease': [0.01]},\n# \"ExtraTreeClassifier\": {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random'], 'min_samples_split': [2,5,10,.03,.05], 'max_depth': [2,3,4,6,8,10,None], 'min_samples_split': [2,5,10,.03,.05], 'min_samples_leaf': [1,5,10,.03,.05], 'max_features': ['auto', 'sqrt', 'log2']},\n\"ExtraTreeClassifier\": {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random'], 'max_depth': [2,4,None], 'min_samples_split': [2,5], 'min_samples_leaf': [1,2], 'max_features': ['auto', 'sqrt', 'log2']},\n\"LinearDiscriminantAnalysis\": {'solver': ['svd', 'lsqr', 'eigen'], 'shrinkage': [None, 'auto', 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1], 'n_components' : range(len(train_df.columns)), 'store_covariance': [True, False]},\n\"QuadraticDiscriminantAnalysis\": {'store_covariance': [True, False]},\n\"XGBClassifier\": {'booster': ['gbtree', 'gblinear', 'dart']}\n}","bc189354":"# parameters_dict = {\n# \"AdaBoostClassifier\": {'n_estimators': range(50,100,10), 'learning_rate': list(float_range(0, 1.01, '0.1')), 'algorithm': ['SAMME', 'SAMME.R'], 'random_state': [None, 42]},\n# \"BaggingClassifier\": {'n_estimators': range(10,20), 'bootstrap': [True, False], 'bootstrap_features': [True, False], 'oob_score': [True, False], 'warm_start': [True, False], 'random_state': [None, 42]},\n# # \"ExtraTreesClassifier\": {'n_estimators': [50, 70, 100, 120], 'criterion': ['gini', 'entropy'], 'max_depth': [2,4,6,8,10,None], 'min_samples_split': [2,5,10,.03,.05], 'min_samples_leaf': [1,5,10,.03,.05], 'max_features': ['auto', 'sqrt', 'log2'], 'bootstrap': [True, False], 'oob_score': [True, False], 'warm_start': [True, False]},\n# \"ExtraTreesClassifier\": {'n_estimators': [50, 70, 100, 120], 'criterion': ['gini', 'entropy'], 'max_depth': [2,4,None], 'min_samples_split': [2,5], 'min_samples_leaf': [1,2], 'max_features': ['auto', 'sqrt', 'log2']},\n# # \"GradientBoostingClassifier\": {'loss': ['deviance', 'exponential'], 'n_estimators': range(10,150, 30), 'criterion': ['friedman_mse', 'mse', 'mae'], 'min_samples_split': [2,5,10,.03,.05], 'max_depth': [2,3,4,6,8,10], 'min_samples_split': [2,5,10,.03,.05], 'min_samples_leaf': [1,5,10,.03,.05], 'max_features': ['auto', 'sqrt', 'log2']},\n# \"GradientBoostingClassifier\": {'loss': ['deviance', 'exponential'], 'n_estimators': [50, 70, 100, 120], 'criterion': ['friedman_mse', 'mse', 'mae'], 'max_depth': [2,4,None], 'min_samples_split': [2,5], 'min_samples_leaf': [1,2], 'max_features': ['auto', 'sqrt', 'log2']},\n# # \"RandomForestClassifier\": {'n_estimators': [50, 70, 100, 120], 'criterion': ['gini', 'entropy'], 'max_depth': [2,3,4,6,8,10, None], 'min_samples_split': [2,5,10,.03,.05], 'min_samples_leaf': [1,5,10,.03,.05], 'max_features': ['auto', 'sqrt', 'log2']},\n# \"RandomForestClassifier\": {'n_estimators': [50, 70, 100, 120], 'criterion': ['gini', 'entropy'], 'max_depth': [2,4,None], 'min_samples_split': [2,5], 'min_samples_leaf': [1,2], 'max_features': ['auto', 'sqrt', 'log2']},\n# \"GaussianProcessClassifier\": {'multi_class': ['one_vs_rest', 'one_vs_one'], 'warm_start': [True, False]},\n# \"LogisticRegressionCV\": {'fit_intercept': [True, False], 'cv': [3,5,7,10], 'dual': [True, False], 'penalty': ['l1', 'l2', 'elasticnet'], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], 'multi_class': ['auto', 'ovr', 'multinomial']},\n# \"PassiveAggressiveClassifier\": {'fit_intercept': [True, False], 'warm_start': [True, False]},\n# \"RidgeClassifierCV\": {},\n# \"SGDClassifier\": {'penalty': ['l2', 'l1', 'elasticnet'], 'average': [True, False]},\n# \"Perceptron\": {'penalty': ['l2', 'l1', 'elasticnet'], 'warm_start': [True, False]},\n# \"BernoulliNB\": {},\n# \"GaussianNB\": {},\n# \"KNeighborsClassifier\": {'weights': ['uniform', 'distance'], 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'], 'leaf_size': range(20, 40, 5)},\n# \"SVC\": {'kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'], 'degree': range(1,5), 'gamma': ['scale', 'auto'], 'shrinking': [True, False], 'probability': [True, False]},\n# \"NuSVC\": {'nu': list(float_range(0, 1.01, '0.1')), 'kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'], 'degree': range(1,5), 'shrinking': [True, False], 'probability': [True, False], 'gamma': ['scale', 'auto'], 'decision_function_shape': ['ovo', 'ovr'], 'break_ties': [True, False]},\n# \"LinearSVC\": {'penalty': ['l1', 'l2'], 'loss': ['hinge', 'squared_hinge'], 'dual': [True, False]},\n# # \"DecisionTreeClassifier\": {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random'], 'min_samples_split': [2,5,10,.03,.05], 'max_depth': [2,3,4,6,8,10,None], 'min_samples_split': [2,5,10,.03,.05], 'min_samples_leaf': [1,5,10,.03,.05], 'max_features': ['auto', 'sqrt', 'log2']},\n# \"DecisionTreeClassifier\": {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random'], 'max_depth': [2,4,None], 'min_samples_split': [2,5], 'min_samples_leaf': [1,2], 'max_features': ['auto', 'sqrt', 'log2']},\n# # \"ExtraTreeClassifier\": {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random'], 'min_samples_split': [2,5,10,.03,.05], 'max_depth': [2,3,4,6,8,10,None], 'min_samples_split': [2,5,10,.03,.05], 'min_samples_leaf': [1,5,10,.03,.05], 'max_features': ['auto', 'sqrt', 'log2']},\n# \"ExtraTreeClassifier\": {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random'], 'max_depth': [2,4,None], 'min_samples_split': [2,5], 'min_samples_leaf': [1,2], 'max_features': ['auto', 'sqrt', 'log2']},\n# \"LinearDiscriminantAnalysis\": {'solver': ['svd', 'lsqr', 'eigen'], 'shrinkage': [None, 'auto', 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1], 'n_components' : range(len(train_df.columns)), 'store_covariance': [True, False]},\n# \"QuadraticDiscriminantAnalysis\": {'store_covariance': [True, False]},\n# \"XGBClassifier\": {'booster': ['gbtree', 'gblinear', 'dart']}\n# }","d1eb5103":"MLA_dict = {   \n    #Ensemble Methods\n    'AdaBoostClassifier': ensemble.AdaBoostClassifier(),\n    'BaggingClassifier': ensemble.BaggingClassifier(),\n    'ExtraTreesClassifier': ensemble.ExtraTreesClassifier(),\n    'GradientBoostingClassifier': ensemble.GradientBoostingClassifier(),\n    'RandomForestClassifier': ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    'GaussianProcessClassifier': gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    'LogisticRegressionCV': linear_model.LogisticRegressionCV(),\n    'PassiveAggressiveClassifier': linear_model.PassiveAggressiveClassifier(),\n    'RidgeClassifierCV': linear_model.RidgeClassifierCV(),\n    'SGDClassifier': linear_model.SGDClassifier(),\n    'Perceptron': linear_model.Perceptron(),\n    \n    #Navies Bayes\n    'BernoulliNB': naive_bayes.BernoulliNB(),\n    'GaussianNB': naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    'KNeighborsClassifier': neighbors.KNeighborsClassifier(),\n\n    #SVM\n    'SVC': svm.SVC(probability=True),\n    'NuSVC': svm.NuSVC(probability=True),\n    'LinearSVC': svm.LinearSVC(),\n    \n    #Trees    \n    'DecisionTreeClassifier': tree.DecisionTreeClassifier(),\n    'ExtraTreeClassifier': tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    'LinearDiscriminantAnalysis': discriminant_analysis.LinearDiscriminantAnalysis(),\n    'QuadraticDiscriminantAnalysis': discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    'XGBClassifier': XGBClassifier()\n}","fdf86577":"before_mean_train_score, before_mean_test_score, before_mean_test_score_std = [], [], []\nafter_mean_train_score, after_mean_test_score, after_mean_test_score_std = [], [], []\nbest_param = []\nfrom tqdm import tqdm\nfor model_name, model in tqdm(MLA_dict.items()):\n    base_results = model_selection.cross_validate(model, train_df[train_df_x_calc], train_df[Target], cv  = cv_split, return_train_score = True)\n    model.fit(train_df[train_df_x_calc], train_df[Target])\n    before_mean_train_score.append(base_results['train_score'].mean()*100)\n    before_mean_test_score.append(base_results['test_score'].mean()*100)\n    before_mean_test_score_std.append(base_results['test_score'].std()*100*3)\n    \n    param_grid = parameters_dict[model_name]\n    print(\"\")\n    print(model_name)\n    print(param_grid)\n    tune_model = model_selection.GridSearchCV(model, param_grid=param_grid, scoring = 'roc_auc', cv = cv_split, return_train_score = True)\n    tune_model.fit(train_df[train_df_x_calc], train_df[Target])\n    try:\n        print(\"Best parameters for model: \" + model_name)\n    except Exception as e:\n        pass\n    print(tune_model.best_params_)\n    \n    after_mean_train_score.append(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)\n    after_mean_test_score.append(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100)\n    after_mean_test_score_std.append(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3)\n    best_param.append(tune_model.best_params_)\nprint(\"DONE\")","ab7d2199":"column_names = ['MLA_name', 'Best Hyper Parameters', 'Before Train Accuracy Mean', 'After Train Accuracy Mean', 'Before Test Accuracy Mean', 'After Test Accuracy Mean', 'Before Test Accuracy 3*STD', 'After Test Accuracy 3*STD']\ncontent = list(zip(MLA_dict.keys(), best_param, before_mean_train_score, after_mean_train_score, before_mean_test_score, after_mean_test_score, before_mean_test_score_std, after_mean_test_score_std))\nMLA_compare_hyper_param = pd.DataFrame(content, columns = column_names)\nMLA_compare_hyper_param.sort_values(by = ['After Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare_hyper_param","d9b54709":"before_plot_df = MLA_compare[['MLA Name', 'MLA Test Accuracy Mean']].rename(columns={'MLA Test Accuracy Mean': 'Test Accuracy Mean'})\nbefore_plot_df['Hyper Parameters'] = \"Before\"\nafter_plot_df = MLA_compare_hyper_param[['MLA_name', 'After Test Accuracy Mean']].rename(columns={'MLA_name': 'MLA Name', 'After Test Accuracy Mean': 'Test Accuracy Mean'})\nafter_plot_df['Hyper Parameters'] = \"After\"\njoined_plot_df = pd.concat([before_plot_df, after_plot_df], axis=0, ignore_index=False).sort_values(by = ['Test Accuracy Mean'], axis = 0, ascending=False)\n\nfig_dims = (12, 8)\nfig, ax = plt.subplots(figsize=fig_dims)\ng = sns.barplot(x = \"Test Accuracy Mean\", y = \"MLA Name\", hue = \"Hyper Parameters\",ax=ax, data = joined_plot_df, palette=\"Blues_d\", dodge=False)\n\n# https:\/\/stackoverflow.com\/a\/56780852\ndef show_values_on_bars(axs, h_v=\"v\", space=0.4):\n    def _show_on_single_plot(ax):\n        if h_v == \"v\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() \/ 2\n                _y = p.get_y() + p.get_height()\n                value = \"{:.2f}\".format(p.get_width())\n                ax.text(_x, _y, value, ha=\"center\") \n        elif h_v == \"h\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() + float(space)\n                _y = p.get_y() + p.get_height()\n                value = \"{:.2f}\".format(p.get_width())\n                ax.text(_x, _y, value, ha=\"left\")\n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)\n       \nshow_values_on_bars(g, \"h\", 0.3)\n\nplt.legend(fontsize='x-large', title = 'Hyper Parameters', title_fontsize='14', loc='upper left')\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')\n","3fad27ac":"MLA_dict_with_best_hyper_parameters = {   \n    #Ensemble Methods\n    'AdaBoostClassifier': ensemble.AdaBoostClassifier(algorithm = 'SAMME', learning_rate = 0.3, n_estimators = 90, random_state = None),\n    'BaggingClassifier': ensemble.BaggingClassifier(bootstrap = True, bootstrap_features = True, n_estimators = 14, oob_score = True, random_state = 42, warm_start = False),\n    'ExtraTreesClassifier': ensemble.ExtraTreesClassifier(criterion = 'entropy', max_depth = None, max_features = 'log2', min_samples_leaf =  2, min_samples_split = 5, n_estimators = 100),\n    'GradientBoostingClassifier': ensemble.GradientBoostingClassifier(criterion = 'mae', loss = 'deviance', max_depth = 4, max_features = 'sqrt', min_samples_leaf =  2, min_samples_split = 5, n_estimators = 100),\n    'RandomForestClassifier': ensemble.RandomForestClassifier(criterion = 'entropy', max_depth = None, max_features = 'sqrt', min_samples_leaf =  2, min_samples_split = 5, n_estimators = 50),\n\n    #Gaussian Processes\n    'GaussianProcessClassifier': gaussian_process.GaussianProcessClassifier(multi_class = 'one_vs_rest', warm_start = True),\n    \n    #GLM\n    'LogisticRegressionCV': linear_model.LogisticRegressionCV(class_weight = 'balanced', cv = 3, dual = True, fit_intercept = False, max_iter =  1000, multi_class = 'auto', penalty = 'l2', solver = 'liblinear'),\n    'PassiveAggressiveClassifier': linear_model.PassiveAggressiveClassifier(fit_intercept = True, warm_start = False),\n    'RidgeClassifierCV': linear_model.RidgeClassifierCV(),\n    'SGDClassifier': linear_model.SGDClassifier(average = True, loss = 'modified_huber', penalty = 'elasticnet'),\n    'Perceptron': linear_model.Perceptron(penalty = 'l1', warm_start = True),\n    \n    #Navies Bayes\n    'BernoulliNB': naive_bayes.BernoulliNB(),\n    'GaussianNB': naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    'KNeighborsClassifier': neighbors.KNeighborsClassifier(algorithm = 'brute', leaf_size = 20, weights = 'uniform'),\n\n    #SVM\n    'SVC': svm.SVC(degree = 1, gamma = 'auto', kernel = 'poly', probability = True, shrinking = True),\n    'NuSVC': svm.NuSVC(kernel = 'linear', nu = 0.4),\n    'LinearSVC': svm.LinearSVC(dual = False, loss = 'squared_hinge', penalty = 'l1'),\n    \n    #Trees    \n    'DecisionTreeClassifier': tree.DecisionTreeClassifier(class_weight = 'balanced', criterion = 'entropy', max_depth = None, max_features = 'auto', min_impurity_decrease = 0.01, min_samples_leaf =  2, min_samples_split = 2, splitter = 'best'),\n    'ExtraTreeClassifier': tree.ExtraTreeClassifier(criterion = 'gini', max_depth = 4, max_features = 'sqrt', min_samples_leaf =  1, min_samples_split = 5, splitter = 'best'),\n    \n    #Discriminant Analysis\n    'LinearDiscriminantAnalysis': discriminant_analysis.LinearDiscriminantAnalysis(n_components = 0, shrinkage = 0.1, solver = 'lsqr', store_covariance = True),\n    'QuadraticDiscriminantAnalysis': discriminant_analysis.QuadraticDiscriminantAnalysis(store_covariance = True),\n\n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    'XGBClassifier': XGBClassifier(booster = 'gbtree')\n}","3a0abe43":"#why choose one model, when you can pick them all with voting classifier\n#http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.VotingClassifier.html\n#removed models w\/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60\/30 split intentionally leaving out 10%\n\nvote_est = [   \n    #Ensemble Methods\n    ('AdaBoostClassifier', ensemble.AdaBoostClassifier(algorithm = 'SAMME', learning_rate = 0.3, n_estimators = 90, random_state = None)),\n    ('BaggingClassifier', ensemble.BaggingClassifier(bootstrap = True, bootstrap_features = True, n_estimators = 14, oob_score = True, random_state = 42, warm_start = False)),\n    ('ExtraTreesClassifier', ensemble.ExtraTreesClassifier(criterion = 'entropy', max_depth = None, max_features = 'log2', min_samples_leaf =  2, min_samples_split = 5, n_estimators = 100)),\n    ('GradientBoostingClassifier', ensemble.GradientBoostingClassifier(criterion = 'mae', loss = 'deviance', max_depth = 4, max_features = 'sqrt', min_samples_leaf =  2, min_samples_split = 5, n_estimators = 100)),\n    ('RandomForestClassifier', ensemble.RandomForestClassifier(criterion = 'entropy', max_depth = None, max_features = 'sqrt', min_samples_leaf =  2, min_samples_split = 5, n_estimators = 50)),\n\n    #Gaussian Processes\n    ('GaussianProcessClassifier', gaussian_process.GaussianProcessClassifier(multi_class = 'one_vs_rest', warm_start = True)),\n    \n    #GLM\n    ('LogisticRegressionCV', linear_model.LogisticRegressionCV(class_weight = 'balanced', cv = 3, dual = True, fit_intercept = False, max_iter =  1000, multi_class = 'auto', penalty = 'l2', solver = 'liblinear')),\n    \n    #Navies Bayes\n    ('BernoulliNB', naive_bayes.BernoulliNB()),\n    ('GaussianNB', naive_bayes.GaussianNB()),\n    \n    #Nearest Neighbor\n    ('KNeighborsClassifier', neighbors.KNeighborsClassifier(algorithm = 'brute', leaf_size = 20, weights = 'uniform')),\n\n    #SVM\n    ('SVC', svm.SVC(degree = 1, gamma = 'auto', kernel = 'poly', probability = True, shrinking = True)),\n\n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    ('XGBClassifier', XGBClassifier(booster = 'gbtree'))\n]\n\n\n#Hard Vote or majority rules\nvote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\nvote_hard_cv = model_selection.cross_validate(vote_hard, train_df[train_df_x_calc], train_df[Target], cv  = cv_split, return_train_score = True)\nvote_hard.fit(train_df[train_df_x_calc], train_df[Target])\n\nprint(\"Hard Voting Training w\/bin score mean: {:.2f}\". format(vote_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting Test w\/bin score mean: {:.2f}\". format(vote_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting Test w\/bin score 3*std: +\/- {:.2f}\". format(vote_hard_cv['test_score'].std()*100*3))\nprint('-'*10)\n\n\n#Soft Vote or weighted probabilities\nvote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\nvote_soft_cv = model_selection.cross_validate(vote_soft, train_df[train_df_x_calc], train_df[Target], cv  = cv_split, return_train_score = True)\nvote_soft.fit(train_df[train_df_x_calc], train_df[Target])\n\nprint(\"Soft Voting Training w\/bin score mean: {:.2f}\". format(vote_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting Test w\/bin score mean: {:.2f}\". format(vote_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting Test w\/bin score 3*std: +\/- {:.2f}\". format(vote_soft_cv['test_score'].std()*100*3))\nprint('-'*10)","845058af":"try:\n    test_df.drop(columns = 'Survived', inplace = True)\nexcept KeyError:\n    pass","7627fbc5":"# best_model_for_submission = ensemble.GradientBoostingClassifier(criterion = 'mae', loss = 'deviance', max_depth = 4, max_features = 'sqrt', min_samples_leaf =  2, min_samples_split = 5, n_estimators = 100)\n# best_model_for_submission.fit(train_df[train_df_x_calc], train_df[Target])\n# result = best_model_for_submission.predict(test_df).astype(int)\n# result = pd.DataFrame(data=result, columns=['Survived'])  # 1st row as the column names\n# submit = pd.concat([PassengerId, result], axis=1)\n# submit","89e24c50":"# base_results = model_selection.cross_validate(best_model_for_submission, train_df[train_df_x_calc], train_df[Target], cv  = cv_split, return_train_score = True)\n# best_model_for_submission.fit(train_df[train_df_x_calc], train_df[Target])\n# print(base_results['train_score'].mean()*100)\n# print(base_results['test_score'].mean()*100)\n# print(base_results['test_score'].std()*100*3)","ce003f8d":"# best_model_for_submission = vote_hard\n# # test_df['Survived'] = best_model_for_submission.predict(test_df).astype(int)\n\n# test_survived = best_model_for_submission.predict(test_df).astype(int)\n# result = pd.DataFrame(data=test_survived, columns=['Survived'])  # 1st row as the column names\n# submit = pd.concat([PassengerId, result], axis=1)\n# #submit file\n# # submit = pd.concat([PassengerId, test_df[['Survived']]], axis=1)\n# # submit = pd.concat([PassengerId, test_survived], axis=1)\n# # submit = test_df[['PassengerId','Survived']]\n# submit\n# # # Generate Submission File \n# # StackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n# #                             'Survived': predictions })\n# # StackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)","ee4da8a1":"best_model_for_submission = vote_hard\ntest_survived = best_model_for_submission.predict(test_df).astype(int)\nresult = pd.DataFrame(data=test_survived, columns=['Survived'])  # 1st row as the column names\nsubmit = pd.concat([PassengerId, result], axis=1)\nsubmit","1d9fead6":"submit.to_csv(\"..\/working\/submission.csv\", index=False)","ee7f5a0d":"## 3.2. Features breakdown","8fcf7fc6":"# Install Libraries","21023cc4":"## 9.3. Hyperparameters Optimization - Grid Search\n* We'll first fit each basic algorithm, then iterate through various parameters for each algorithm and compare the results. Then, we could identify how much the hyperparameters helped to increase the model score.\n* Note that Grid Search is a slow method. There are other methods to explore, such as Bayesian Optimization, Particle-based methods, Convex optimizers, and Simulated annealing","9d5710c0":"# Part 11: Credits and Resources\n* [A Data Science Framework: To Achieve 99% Accuracy](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy) - Huge inspiration for wrangling data, completing missing features, and modeling\n* [A Statistical Analysis & ML workflow of Titanic](https:\/\/www.kaggle.com\/masumrumi\/a-statistical-analysis-ml-workflow-of-titanic#Embarked-feature) - Completing missing features and creating `calculated_fare`\n* [A Journey through Titanic](https:\/\/www.kaggle.com\/omarelgabry\/a-journey-through-titanic) - Completing `Age`\n* [Titanic - Advanced Feature Engineering Tutorial](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial#2.4-Target-Encoding) - Creating `survival_rate` and `Survival_Rate_NA`\n* [Titanic Top 4% with ensemble modeling](https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling#Fare) - Reduce `Fare` and `Age` skewness - using Shapiro's law and power transforms\n* [Multicollinearity & Variance Infalction Factor (VIF)](https:\/\/www.analyticsvidhya.com\/blog\/2020\/03\/one-hot-encoding-vs-label-encoding-using-scikit-learn\/) - Feature Selection\n\nhttps:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n\nhttps:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python\n\nhttps:\/\/www.kaggle.com\/helgejo\/an-interactive-data-science-tutorial\n\nhttps:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial\n\nhttps:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n\nhttps:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python","f597421a":"## Import Libraries","64a824e9":"[[1]](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial#2.5-Feature-Transformation)\n\n2.5.1 Label Encoding Non-Numerical Features\n\nEmbarked, Sex, Deck , Title and Family_Size_Grouped are object type, and Age and Fare features are category type. They are converted to numerical type with LabelEncoder. LabelEncoder basically labels the classes from 0 to n. This process is necessary for models to learn from those features.","0d3976ce":"# Cleaning things up (pre-feature selection)","40900c8b":"## 5.3.1. Cabin Prefix - Train","c2dae095":"## Part 8: Feature Selection","b988756f":"## 9.1. Define X and y features\n* x: independent\/features\/explanatory\/predictor\/etc.\n* y: dependent\/target\/outcome\/response\/etc\n\n[Source](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy#3.23-Convert-Formats)","cff23f15":"### 9.3.1. Models without Hyperparameters","2b42842d":"## 6.7. Features Correcting - Power Transforms\n\n* Fare and Age features has outliers\/are skewed.\n\n### Correct outliers and skewness\nIn several notebooks I saw some that detect and remove outliers, whereas others reduce skewness. To understand what's the best or more percise way to deal with outliers\/skewness, I got inspiration from Jochen Wilhelm [here](https:\/\/www.researchgate.net\/post\/Outlier_and_skewness_effect_on_Normality_and_homogeneity_of_variance_testing)\n\nThe bottom line\n> \"Removing outliers is only sensible if these values are \"bad values\", that is, when they are extremely **implausible**\"\n\nSince the outliers of Age and Fare values are plausible, and since we want to provide certain models with features represented by normal distribution, **I decided to opt for skewness reduction**. \n\nHowever, for those who still want to detect and remove outliers, [here](https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling#2.2-Outlier-detection) is an excellent code snippet for that based on Tukey method (1977) using IQR.","df946751":"## Feature Binning - Bayesian Blocks","5694fe73":"Unfortunately, I discovered that I created a complex model overfitting the dataset. Although I got a high accuracy score when it comes to training the model (0.9), the real prediction score turned to be 0.55. I'd love to fix this. Please comment this notebook and submit your suggestions!","25fd71c5":"### Cabin Outlier Correcting\n\nAccording to [Titanic - Advanced Feature Engineering Tutorial](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial#1.2.4-Cabin)\nthere is one person on the boat deck in T cabin and he is a 1st class passenger. T cabin passenger has the closest resemblance to A Cabin_Prefix passengers so he is grouped with A Cabin.","7d3a2b79":"## 9.4. Voting Classifier","cf5b558c":"## 6.4. Family & Title\nAll text and code derived from this excellent [Resource](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial#2.4-Target-Encoding).\n\n`extract_surname` function is used for extracting surnames of passengers from the `Name` feature. `Family` feature is created with the extracted surname. This is necessary for grouping passengers in the same family. \n\n","de755e0f":"## 3.3. Insights and Observations","7761ac16":"### Describe datasets","41cd6af4":"# Part 2: Question \/ Problem Definition\n**Find out who survived the titanic incident.**\n","e530d608":"## Dabl & Pandas_profiling for quick insights","03b9a7d9":"![ChooseModel](https:\/\/scikit-learn.org\/stable\/_static\/ml_map.png)","8b39dab8":"[Resource1](https:\/\/www.kaggle.com\/masumrumi\/a-statistical-analysis-ml-workflow-of-titanic#Embarked-feature), [Resource2](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy#3.22-Clean-Data).\n\nWe may be able to solve these two missing values by looking at other independent variables of the two rows.\nThis rational is derived from [here](https:\/\/www.kaggle.com\/masumrumi\/a-statistical-analysis-ml-workflow-of-titanic#Embarked-feature):\n\nThe passengers with missing \"Embarked\" values have the following independent variables:\n1. Sex = Female\n2. 38 < Age < 62\n3. Pclass = 1\n4. Fare = 80\n5. Cabin = B28\n6. Same ticket\n7. Alone - SibSp+Parch = 0\n\nLet's check who has multiple independent variables like the above-mentioned ones. To avoid over-filtering, I will correlate 3+ independent variables (Sex, Pclass, Age, Fare, Cabin_prefix).\n\nThen, the dataset of similar passengers will better represent the passengers with missing \"Embarked\" values. From dataset of two passengers, the dataset grew to 128 passengers with correlated independent variables.\n\nThe mode() operation will be used to impute the missing \"embarked\" values based on a larger dataset.\n\nSpoiler - \"C\" it is!","c4dbbdb3":"[Resource](https:\/\/towardsdatascience.com\/data-types-from-a-machine-learning-perspective-with-examples-111ac679e8bc)  \n**Most data can be categorized into 4 basic types from a Machine Learning perspective:  \nnumerical data, categorical data, time series data, and text.**\n![](https:\/\/miro.medium.com\/max\/351\/1*D8B5_HlEfwSQURgQkymoBA.jpeg)\n\n### 3.2.1. Text\n* **Name**  \n    * The names also contain titles. some titles can indicate a certain age group.\n    * Some persons might share the same surname; indicating family relations.\n  \n* **Cabin** (string\/object)\n    * Meaning - an alphanumeric representation of the cabin number of each passenger.  \n* **Ticket** (string\/object)\n    * Meaning - an alphanumeric representation of the ticket number of each passenger.  \n    * Passenger who travel together may have purchased tickets with the same prefix.\n\n### 3.2.2. Numerical \nDefinition - *Numerical data is any data where data points are exact numbers. This data has meaning as a measurement \nNumerical data can be characterized into continuous or discrete data. Continuous data can assume any value within a range whereas discrete data has distinct values.*\n![](https:\/\/miro.medium.com\/max\/501\/1*lheLiN7y4sSD2JKvow-clw.jpeg)\n\n#### Discrete \/ Ordinal \nDefinition - *Ordinal feature means its values may be arranged in some order that makes logical sense.*\n* **SibSp** (Integer)\n    * Meaning - the number of siblings or spouses travelling with each passenger.\n    * Test dataset range - 0-10\n* **Parch** (Integer)\n    * Meaning - the number of parents or children travelling with each passenger.\n    * Test dataset range - 0-6\n    \n#### Continous \n* **Fare** (Float)\n    * Meaning - how much each passenger paid for their journey.\n    * Test dataset range - 0-512.3292(USD)\n* **Age** (Float)\n    * Test dataset range - 0.42-80(y\/o)\n\n### 3.2.3. Categorical\nEmbarked and Sex will be encoded using OneHotEncoding\n#### Nominal\n* **Embarked** (character).\n    * Meaning - Port of Embarkation\n    * Options - \"S\", \"Q\", \"C\" (Southampton, Queenstown, Cherbourg).\n\n* **Pclass** (Integer)\n    * Meaning - represents social-economic-status (SES).\n    * 3 Ticket-classes - 1 (First),2 (Second),3 (Third).\n    * This is a categorical nominal feature that was already mapped to numerical values.\n\n#### Binary\n* **Sex** (string)\n    * Meaning - an indicator whether the passenger was female or male.\n    * According to the titanic tale, women and children were evacuated first. Hence, I'll convert this binary feature to a nominal feature, adding \"child\" as an option. For those who have seen the Titanic movie (1997), I am sure, we all remember this sentence during the evacuation : \"Women and children first\".\n* **Survived** (integer)\n    * Target\n    * Found in train dataset only. Needs to be predicted in test dataset.\n   \n\n### 3.2.4. Time Series Data\nnone.\n\n### 3.2.5. Useless\n* **PassengerID** (Integer)\n    * A running index for the dataset, no true numerical or categorical meaning.\n\n### 3.2.6. Summary\nIn summary, we have 11\/12 (test\/train) features - 4 numerical features (Fare, Age, SibSp, Parch), 3 text features (Name, Cabin, Ticket), 4 catrgorical features (Embarked, Pclass, Sex, Survived) and 1 useless feature (PassengerID):\n* 2 Numerical-Continous features as float datatype (Fare, Age), \n* 2 Numerical-Discrete\/Ordinal features as integer datatype (SibSp, Parch)\n* 3 Text features as string\/object dataype (Name, Cabin, Ticket)\n* 1 Categorical-Nominal features as character datatype (Embarked)\n* 1 Categorical-ordinal features as integer datatype (Pclass). Already mapped from categories to numerical values.\n* 1 Catrgorical-Binary feature as string\/object datatype (Sex) --> To be converted to 3 options nominal feature (male, female, child)\n* 1 running index with no meaning (PassengerID)\n* 1 Catrgorical-Binary feature as integer datatype (**Survived**)","66da5654":"## 5.3. Complete Cabin - Cabin Prefix\n\nAccording to [A Statistical Analysis & ML workflow of Titanic](https:\/\/www.kaggle.com\/masumrumi\/a-statistical-analysis-ml-workflow-of-titanic#Cabin-Feature), one way to complete Cabin feature is by calculating the mean Fare of each Cabin_Prefix. For each missing Cabin, the value will be imputed by a cabin_estimator function based on a Fare range of each Cabin_Prefix. \n\nHowever, when I tried to do so, I discovered a big standard deviation of Fare for each Cabin_Prefix (same for calculating median instead of mean to overcome skewness). Hence, a cabin_estimator function will mislead us and will most likely impute false values to complete Cabin feature.\n\nTo tackle this, I decided to plot the Fare range for each Cabin_prefix, but Fare is too Skewed and no insights could be concluded.\n\nAt this stage I decided to re-evaluate whether mean(SD)\/SEM is the right way to complete the Cabin feature. Apparantly, Standard Deviation (SD) and Standard Error of Mean (SE\/SEM) are used interchangeably by mistake in various medical journals [1](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC2959222\/),[2](http:\/\/www.medicine.mcgill.ca\/epidemiology\/hanley\/IntMedResidents\/SD-SE.pdf). Hoping that I haven't fallen to this pit, this is the summary:\n\n* The Standard Error of Mean (SEM) is a measure of precision for an estimated population mean. \n* Standard Deviation (SD) is a measure of data variability around mean of a sample of population.\n\ndescriptive statistics ([resource](https:\/\/towardsdatascience.com\/10-essential-numerical-summaries-in-statistics-for-data-science-theory-python-and-r-f3ee5e0eca32)):\n* normal distribution --> mean +- standard deviation (SD) \n* no normal distribution --> median and range. \n\n[Resource](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial#1.2.4-Cabin)\n![Titanic_Deck](https:\/\/vignette.wikia.nocookie.net\/titanic\/images\/f\/f9\/Titanic_side_plan.png\/revision\/latest?cb=20180322183733)","f50f91fa":"## 7.1. Feature Binning\nHandling Continious Features - Grouping \/ Binning \/ Banding","71ae42ba":"#### Considering that passengers from the same family and\/or the same ticket are found in train and test datasets, we can calculate the family survival rate and ticket survival rate.\n\n`Family_Survival_Rate` is calculated from families in training set since there is no `Survived` feature in test set. A list of family names that are occuring in both training and test set (non_unique_families), is created. The survival rate is calculated for families with more than 1 members in that list, and stored in `Family_Survival_Rate` feature.\n\nAn extra binary feature `Family_Survival_Rate_NA` is created for families that are unique to the test set. This feature is also necessary because there is no way to calculate those families' survival rate. This feature implies that family survival rate is not applicable to those passengers because there is no way to retrieve their survival rate.\n\n`Ticket_Survival_Rate` and `Ticket_Survival_Rate_NA` features are also created with the same method. `Ticket_Survival_Rate` and `Family_Survival_Rate` are averaged and become `Survival_Rate`, and `Ticket_Survival_Rate_NA` and `Family_Survival_Rate_NA` are also averaged and become `Survival_Rate_NA`.","039f7724":"\n### 3.3.1. Observations in a nutshell for all features","279235e7":"## Recap\n","0e0f35c4":"* Names are unique across the dataset (count=unique=891)\n* Sex variable as two possible values with 65% male (top=male, freq=577\/count=891).\n* Embarked takes three possible values. S port used by most passengers (top=S)\n* **Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin.**\n* **Ticket feature has high ratio (22%) of duplicate values (unique=681). Alternatively, passengers shared a \"group\" purchase.**\n    **Decision:**\n    * find out which passengers share the same cabin.\n    * find out which passengers share the same group puchase by ticket.","f0d14280":"## 5.1. Completing Train Embarked","a7b05ca6":"#### Title Grouping - Rare","41cbfc59":"## 7.3. OneHotEncoding","851e9b29":"## Feature Completing\n[Resource](https:\/\/www.kaggle.com\/headsortails\/pytanic)\n### Train Dataset - missing values (X\/891 Passengers)\n* Cabin, Age, Embarked\n* 2 Embarked values are missing. <1%\n* 177 Age values are missing. ~20%\n* 687 Cabin values are missing - large majority of Cabin numbers are missing. ~77%\n\n### Test Dataset - missing values (X\/418 Passengers)\n* Cabin, Age, Fare\n* 1 Fare missing. <1%\n* 86 Age values are missing. ~21%\n* 327 Cabin values are missing, only 91 Cabin numbers were preserved. 78%\n\n### Steps:\n1. Complete missing values in Train Embarked.\n2. Complete missing value in Test Fare.\n3. Complete missing values in Train+Test Cabin.\n4. Complete missing values in Train+Test Age.\n\n1. Complete Cabin - by mean + estimators (grouping\/binning\/banding?)\n\n### Completing techniques: (WIP)\n1.  \n2.  \n3.  \n\nTechniques for completing of <1% values vs. completing ~20% values vs. completing 80% values.\n\n## Feature Correcting\n\n### Detect outliers\n* Fare\n\n### Reduce skewness\n* Box-Cox \/ Yeo-Johnson Power Transforms","118748f4":"## 5.4. Complete Age - Train & Test DataFrames\n* 20% missing values\n* use np.random.randint with mean+-std range for each DataFrame to impute missing Age values.\n[A Journey through Titanic](https:\/\/www.kaggle.com\/omarelgabry\/a-journey-through-titanic)","2a0cce18":"Mainly taken from [here](https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic#Observations-in-a-Nutshell-for-all-features).\n\n**Sex**: \nThe chance of survival for women is high as compared to men.\n\n**Fare**\nHigher fare paying passengers had better survival chances (Money Matters). \n\n**Pclass**:\nThe upper-class passengers (Pclass=1) were more likely to have survived, compared to second-third-classes (Pclass=2 OR Pclass = 3). This trend is conserved when we look at both male and female passengers. For women, the chance of survival from Pclass1 is almost 1 and is high too for those from Pclass2. Money Wins!!!.\n\n**Age**:\nChildren less than 5-10 years do have a high chance of survival, In particular, infants (Age <=4) had high survival rate. Most Passengers are between age group 15 to 35, and died a lot. Oldest passengers (Age = 80) survived.\n\n**Embarked**: \nThe chances of survival at C looks to be better than even though the majority of Pclass1 passengers got up at S. Passengers at Q were all from Pclass3.\n\n**Parch+SibSp**: \nHaving 1-2 siblings,spouse on board or 1-3 Parents shows a greater chance of probablity rather than being alone or having a large family travelling with you.","dab77e9a":"> Most observations were taken from [here](https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic), and [here](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions), some observations were rephrased.","2ef0ae83":"for each passenger with no cabin, check if his independant variables match multiple values correlated with those passengers with cabin per cabin_prefix\n\nfor passenger_no_cabin in dataframe_of_passengers_with_no_cabin:\n    function\n    \nfunction(input - series, output - Cabin_Prefix estimation):\n    check if the passenger has independant variables that fit each cabin prefix:\n    for each independant variable execute -\n        function - (input - pd.Series of one passenger with no cabin), df2 (final_df_cabin_prefix_features_description), output-dicionary key for each cabin prebix with value of list of True\/False in range for each feature based on SD\/MODE\n        \n","d11e238a":"# Part 1: Introduction\n\n## 1.1) Background\nAspired by a brilliant home-assignment given by a data-science startup, and not merely any background, I decided to get a hands-on understanding of Machine Learning. Aside from ML being a buzzword, I discovered a whole world, thanks to Kaggle. This notebook contains a mixture of lessons I've learned from various notebooks, as well as on my own. \n\n## 1.2) Goal\nExplore, Learn, and apply best practices in various machine learning fields.\nIn this notebook I will include techniques for Feature Engineering, ML twicking, Hyperparamaters, Ensambling, etc.\n\n## 1.3) Strategy\n* No exploratory data analysis. \n* Learn from others, implement others' code to form a whole that is greater that the sum of its parts. \n* Focus on sophisticated feature engineering to create the most optimal dataset.\n\n## 1.4) What to expect\n* Data Wrangling - **Feature completing**, **Feature correcting** (detect outliers, handle skewness and kurtosis using power transforms and Shapiro-Wilks test), \n* Feature Engineering \n* Feature Transformation - **Feature binning** (Scott's rule, Bayesian Blocks), Label encoding, OneHotEncoding\n* Feautre Selection - Handle multicollinearity using Variance Infalction Factor (VIF)\n* Modeling - Hyperparameters Optimization via Grid Search, VotingClassifier","7bfd7c28":"## 3.1. Pre-Processing Facts","004baf08":"## 8.1. Multicollinearity & Variance Infalction Factor (VIF)\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/03\/one-hot-encoding-vs-label-encoding-using-scikit-learn\/\nDummy Variable Trap is a scenario in which variables are highly correlated to each other.\nThe Dummy Variable Trap leads to the problem known as multicollinearity. Multicollinearity occurs where there is a dependency between the independent features.\nOne of the common ways to check for multicollinearity is the Variance Inflation Factor (VIF):\n* VIF=1, Very Less Multicollinearity\n* VIF<5, Moderate Multicollinearity\n* VIF>5, Extreme Multicollinearity (This is what we have to avoid)","20259ad9":"Yeo Johnson Transform handled skewness better, hence we'll use it.","e5d470c0":"# Table of Contents\n1. [Introduction](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#Part-1:-Introduction)\n2. [Question \/ Problem Definition](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#Part-2:-Question-\/-Problem-Definition)\n3. [Data Analysis Insights](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#Part-3:-About-This-Dataset)  \n    3.1. [Pre-Processing Facts](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#3.1.-Pre-Processing-Facts)  \n    3.2. [Features Breakdown](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#3.2.-Features-breakdown)  \n    3.3. [Insights and Observations](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#3.3.-Insights-and-Observations)  \n    3.4. [Observations in a nutshell](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#3.3.1.-Observations-in-a-nutshell-for-all-features)  \n4. [Acquire training and testing data](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#Part-4:-Acquire-training-and-testing-data)  \n5. [Data Wrangling](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#Part-5:-Data-Wrangling)  \n    5.1. [Completing Embarked](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#5.1.-Completing-Train-Embarked)  \n    5.2. [Completing Fare](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#5.2.-Complete-Test-Fare)  \n    5.3. [Completing Cabin + Cabin Prefix](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#5.3.-Complete-Cabin---Cabin-Prefix)  \n    5.4. [Completing Age](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#5.4.-Complete-Age---Train-&-Test-DataFrames)  \n6. [Feature Engineering](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#Part-6:-Creating---Feature-Engineering)  \n    6.1. [Family_Size](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#6.1.-Family_Size---Ordinal-Numeric-Feature)  \n    6.2. [Calculated_Fare](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#6.2.-Calculated_Fare-based-on-Family_Size)  \n    6.3. [Ticket_Frequency](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#6.3.-Ticket_Frequency---Ordinal-Numeric-Feature)  \n    6.4. [Family_Name & Title](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#6.4.-Family-&-Title), [Title Correcting & Grouping](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#Title-Correcting-and-Grouping----Rare)  \n    6.5. [Family_Survival_Rate & Ticket_Survival_Rate](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#6.5.-Survival_Rate-&-Survival_Rate_NA)  \n    6.6. [Is_Married](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#6.6.-Is_Married-Feature)  \n    6.7. [Feature Correcting - Power Transforms](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#6.7.-Features-Correcting---Power-Transforms), [Shapiro-Wilks test](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#Shapiro-Wilks-test), [Box-Cox Transform](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#Box-Cox-Transform), [Yeo-Johnson Transform](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#Yeo-Johnson-Transform)  \n7. [Feature Transformation](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#Part-7:-Feature-Transformation)  \n    7.1. [Feature Binning](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#7.1.-Feature-Binning), [Bayesian Blocks](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#Feature-Binning---Bayesian-Blocks)  \n    7.2. [Label Encoding](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#7.2.-Label-Encoding)  \n    7.3. [OneHotEncoding](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#7.3.-OneHotEncoding)  \n8. [Feature Selection](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#Part-8:-Feature-Selection)  \n    8.1. [Multicollinearity & Variance Infalction Factor (VIF)](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#8.1.-Multicollinearity-&-Variance-Infalction-Factor-(VIF))  \n    8.2. [Predictive Power Score - WIP](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#8.2.-Predictive-Power-Score-(WIP))  \n9. [Modelling](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#Part-9:-Modelling)  \n    9.1. [Define X and y features](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#9.1.-Define-X-and-y-features)  \n    9.2. [Split Training data in two datasets (anti-overfitting)](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#9.2.-Split-Training-data-in-two-datasets-(anti-overfitting))  \n    9.3. [Hyperparameters Optimization - Grid Search](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#9.3.-Hyperparameters-Optimization---Grid-Search), [Hyperparameters Tuning Impact](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#Hyperparameters-Tuning-Impact)  \n    9.4. [Voting Classifier](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#9.4.-Voting-Classifier)\n10. [Submission](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#Part-10:-Submission)\n11. [Credits and Resources](https:\/\/www.kaggle.com\/orblat\/titanic-ml-or-blatt\/#Part-11:-Credits-and-Resources)","77af4eb1":"## 6.1. Family_Size - Ordinal Numeric Feature\n[Resource1](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial#2.2-Frequency-Encoding), [Resource2](https:\/\/triangleinequality.wordpress.com\/2013\/09\/08\/basic-feature-engineering-with-the-titanic-data\/)\n\nFamily_Size is created by adding SibSp, Parch and 1. SibSp is the count of siblings and spouse, and Parch is the count of parents and children. Those columns are added in order to find the total size of families. Adding 1 at the end, is the current passenger. ","ada79f50":"### 3.3.2. What is the distribution of numerical feature values across the samples?","4a5ac6d1":"As far as I understand, this means that the bins are:\n1. Alone\n2. Couple\n3. Small Family - 3 people\n4. Big Family - 4-7 people\n5. Huge Family - 8-12 people","0a01964b":"### Shapiro-Wilks test\nThe null hypothesis for Shapiro-Wilks test is that the data is a sample from a normal distribution, so a p-value less than 0.05 indicates significant skewness.","485746d2":"### Summary of Multicollinary Changes based on VIF decrease and feature removal","2047775b":"## 6.2. Calculated_Fare based on Family_Size\n[Resource](https:\/\/www.kaggle.com\/masumrumi\/a-statistical-analysis-ml-workflow-of-titanic#calculated_fare)\n> Some people have travelled in groups like family or friends. It seems like Fare column kept a record of the total fare rather than the fare of individual passenger, therefore calculated fare will be much handy in this situation.","a83fed1a":"### 5.3.2. Complete Cabin Prefix - Test","54cf4240":"\nhttps:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy#3.25-Split-Training-and-Testing-Data  \n use sklearn function to split the training data in two datasets; 75\/25 split. This is important, so we don't overfit our model.\nIt's important our algorithm has not seen the subset we will use to test, so it doesn't \"cheat\" by memorizing the\n![](https:\/\/miro.medium.com\/max\/1007\/1*pIptNvUJHFiJ_lizQsxyOw.png) [Source](https:\/\/towardsdatascience.com\/cross-validation-430d9a5fee22)","f5f17d92":"### 9.3.2. Tune Model with Hyper-Parameters\n[Source](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy#5.12-Tune-Model-with-Hyper-Parameters)","b0dd8653":"# Part 9: Modelling ","8ec4212c":"----","3453d790":"Box-Cox Transform managed to deal with Age, but not with Fare due to zero values. \nHence, we should use Yeo-Johnson Transform\n\n### Yeo-Johnson Transform\nUnlike Box-Cox, it supports zero values and negative values. \nSometimes a lift in performance can be achieved by first standardizing the raw dataset prior to performing a Yeo-Johnson transform.\nWe can explore this by adding a StandardScaler as a first step in the pipeline.","b3bf0e35":"#### Power Transforms\nA power transform[[1]](https:\/\/en.wikipedia.org\/wiki\/Power_transform)[[2]](https:\/\/machinelearningmastery.com\/power-transforms-with-scikit-learn\/) will make the probability distribution of a variable more Gaussian. This is often described as removing a skew in the distribution.\n\nThere are two popular approaches for automatic power transforms; they are:\n* Box-Cox Transform\n* Yeo-Johnson Transform","28f1c2e8":"##### pd.cut Age","fea5fcbe":"### Reduce skewness in Test Data","106cb28b":"## 6.3. Ticket_Frequency - Ordinal Numeric Feature\n\n[Resource1](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial#2.2-Frequency-Encoding), [Resource 2](https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling#5.4-Ticket), [Resource 3](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial#2.4-Target-Encoding)  \nThere are too many unique Ticket values to analyze, so grouping them up by their frequencies makes things easier.\n\nHow is this feature different than Family_Size? \nMany passengers travelled along with groups. Those groups consist of friends, nannies, maids and etc. They weren't counted as family, but they used the same ticket.","c70dbf89":"\n**Sex**: \n* Sex<-->Survival\n    *\tThe chance of survival for women is high as compared to men. Sex<-->Pclass<-->Survival\n    *\tSex = Female, Pclass = 1|2 --> Survival = ~1\n* Sex<-->Age\n    *\tAge distribution seems to be the same in Male and Female subpopulations, so Sex is not informative to predict Age.\n* Sex<-->Age<-->Survival\n    *\tFor males, the survival chances decreases with an increase in age. \n\n**Fare**\n*\tHigher fare paying passengers had better survival chances (Money Matters).\n\n**Pclass**:\n* Pclass <--> Survival\n    *\tPclass1 --> highest chances of survival, most survivals.\n    *\tPclass3 --> lowest chances of survival, most passaengers.\n* Pclass <--> Sex <--> Survival\n    *\tSex=Female, Pclass=1 --> Survival = ~1 \n    *\tSex=Female, Pclass=2 --> high chances of survival. Money Wins!!! \n* Pclass<-->Age<--> Survival\n    *\tInfant\/Toddlers (age<=4) passengers in Pclass=2 and Pclass=3 mostly survived.\n* Pclass <--> Age <--> Sex <--> Survival\n    *\tSurvival chances for Passenegers aged 20-50 from Pclass1 is high and is even better for Women. \n* Pclass<-->Parch\n    *\tThe number of children increases with Pclass.\n* Pclass<-->Age\n    * 1st class passengers are older than 2nd class passengers who are also older than 3rd class passengers. \n \n **Age**:\n* Age<-->Survival\n    *\tInfant\/Toddlers (age<=4) were saved in large numbers irrespective of the class (The Women and Child First Policy). \n    *\tChildren (age<=10) --> high chance of survival. \n    *\t15 < Age < 35 --> died a lot. \n    *\tMaximum number of deaths were in the age group of 30-40. \n    *\tThe oldest Passengers were saved(80 years). \n* Age<-->(Parch+SibSp)\n    *\tMoreover, the more a passenger has parents\/children the older he is and the more a passenger has siblings\/spouses the younger he is. \n    *\tMore Parch --> Older Age\n    *\tMore SibSp --> Younger Age\n\n**Embarked**: \n*\tThe chances for survival for Port C is highest around 0.55 while it is lowest for S. \n*\tEmbarked = C --> highest survival chances\n*\tEmbarked = S --> lowest survival chances\n* Embarked<-->Pclass:\n    *\tEmbarked = C, Pclass = 1 --> better survival chances, even though the majority of Pclass1 passengers got up at S.\n    *\t95% of the Passengers at Q were from Pclass3. \n    *\tMaximum passengers boarded from S. \n    *\tEmbarked = S -->  majority of the rich people boarded and majority of Pclass3 passengers boarded. Still, the chances for survival is low here, that is because many passengers from Pclass3 around 81% didn't survive.\n* Embarked<-->Sex\n    *\tPort Q looks looks to be unlukiest for Men, as almost all were from Pclass 3, which is known for low chances of survival for as compared to other Pclasses. \n\n**Parch+SibSp**: \n* (Parch+SibSp)<-->Survival:\n    *\t(SibSp=1-2 U Parch=1-3) > (alone U large family). To be more precise, The chances of survival is good for somebody who has 1-3 parents on the ship. Being alone also proves to be fatal and the chances for survival decreases when somebody has >4 parents on the ship.\n    *\tthe survival for families with 5-8 members is 0% --> all the large families in Pclass3(>3) died.","16b0b7c3":"## 8.2. Predictive Power Score (WIP)","62e08900":"# Interesting Resources","adc166d0":"## 6.5. Survival_Rate & Survival_Rate_NA","62f17668":"[How Many Survived?](https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic#How-many-Survived??) - Out of 891 passengers in training set, only around 350 survived  \ni.e only 38.4% of the total training set survived the crash. \n\n[The Survived variable is our outcome or dependent variable](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy#3.2-Meet-and-Greet-Data). It is a binary nominal datatype of 1 for survived and 0 for did not survive. All other variables are potential predictor or independent variables.  \n**It's important to note, more predictor variables do not make a better model, but the right variables.**","adf22c16":"# Part 3: About This Dataset","22ddbd3e":"Although `Sex` Feature still has high VIF score, I'd preserve it. Feel free to suggest how I can fix this.","9134f90c":"We can see that Title_4 and Is_Married create multicullinarity. Hence, we'll remove Is_Married feature","ea201f53":"## 7.2. Label Encoding","f0bfb941":"![](https:\/\/miro.medium.com\/max\/1050\/1*MAr4rWj6zw0Rdo01ecZu1A.png)\n[Resource](https:\/\/towardsdatascience.com\/feature-engineering-deep-dive-into-encoding-and-binning-techniques-5618d55a6b38)\n\nContinious features need to be binned, and then encoded, in order to be used by ML algorithms.","a06d0b9d":"## 9.2. Split Training data in two datasets (anti-overfitting)","1f51ef4e":"### 3.3.3. What is the distribution of categorical features?","c93de016":"### Visualize distribution of missing values using missingno\nFound in an amazing [notebook](https:\/\/www.kaggle.com\/parulpandey\/useful-python-libraries-for-data-science) - [Github](https:\/\/github.com\/ResidentMario\/missingno)","8a0c5279":"### 3.3.4. All Observations","c8dadb06":"### Title Correcting and Grouping -  Rare\n1. Correct typos in ('Mlle', 'Miss'), ('Ms', 'Miss'), ('Mme', 'Mrs')\n2. Group rare title names. [Resource](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy#3.22-Clean-Data)","4aec4647":"## 5.2. Complete Test Fare \n\nInstead of imputing the missing value from the fare median for all passengers in train+test datasets, I decided to narrow down the list via correlation of independant variables of the sole passenger who has NaN Fare. (1309 --> 371 passengers).\n\nThese passengers better represent the passenger with missing Fare because they share same characteristics, which are depicted by the correlated independant variables.","02a29bed":"Predictive Power Score (PPS). The PPS is an asymmetric, data-type-agnostic score that can detect linear or non-linear relationships between two columns. The score ranges from 0 (no predictive power) to 1 (perfect predictive power). It can be used as an alternative to the correlation (matrix). [Source](https:\/\/www.kaggle.com\/parulpandey\/useful-python-libraries-for-data-science#Single-Predictive-Power-Score)","c5bccfe0":"##### pd.cut Family_Size","e30128a3":"# Part 4: Acquire training and testing data","3db90283":"## 6.6. Is_Married Feature\nCalculate Is_Married boolean value based on Title of Mrs. [Resource](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial#2.3-Title-&-Is-Married)","f8e85db8":"### Feature Engineering sneak peek","4646a544":"# Change Log\n\n21\/02\/21 Kernel is open to feedback. Thanks!","53f99a4a":"### Deal with \"Fare\" Skewness by log\nMany machine learning algorithms perform better when the distribution of variables is Gaussian distribution (aka normal distribution).\nTo achieve Gaussian distribution, we'll use and compare two power transforms - Yeo-Johnson Transform vs. Cox-Box Transform. [resource](https:\/\/machinelearningmastery.com\/power-transforms-with-scikit-learn\/), [resource2](https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling#Fare)\n\nFare distribution is very skewed\nThis can lead to overweigth very high values in the model, even if it is scaled.","b5833f77":"# Part 10: Submission","03ad2199":"This helps us determine, among other early insights, how representative is the training dataset of the actual problem domain.\n\n* Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).\n* Around 38% samples survived representative of the actual survival rate at 32%.\n* Most passengers (> 75%) did not travel with parents or children.\n* Nearly 30% of the passengers had siblings and\/or spouse aboard.\n\n#### **Outliers:**\n* Fares varied significantly with few passengers (<1%) paying as high as 512USDs.\n* Few elderly passengers (<1%) within age range 65-80.","57ccd4b0":"# Part 5: Data Wrangling\n[Great resource for the 4 Cs](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy#3.21-The-4-C's-of-Data-Cleaning:-Correcting,-Completing,-Creating,-and-Converting)","9674ef21":"### 1) Continious Features Binning\nFeature Binning: Conversion of a continuous variable to categorical\n\n**Types of Binning**\n* Unsupervised Binning: Equal width binning, Equal frequency binning (adaptive binning \/ fixed binning)\n* Supervised Binning: Entropy-based binning\n\nLet's use bayesian_blocks to determine bin size, and then bin `age`, `Calculated_Fare` and `Family_Size`.\nSince I used Power Transforms (\"Yeo-Johnsons\") on `age` and `Calculated_Fare`, I assume that there's a big difference between Scott's rule bin size and Bayesian blocks bin size. Since all available information [[1]](http:\/\/users.stat.umn.edu\/~gmeeden\/papers\/hist.pdf) [[2]](https:\/\/arxiv.org\/pdf\/physics\/0605197.pdf) (except [astropy](https:\/\/docs.astropy.org\/en\/stable\/visualization\/histogram.html), [Scott's rule](https:\/\/medium.com\/analytics-vidhya\/data-scientists-stop-randomly-binning-histograms-1069d7380c3a))  I could find is mostly theoretical and not implemented python-wise, I'd appreciate if anyone can enlighten me on this!\n\n![](https:\/\/miro.medium.com\/max\/700\/1*n-QnKwkk_hxNihfAYxfAzg.png)","af6a919f":"# Part 6: Feature Engineering\n\n1. Family_Size (SibSp + Parch + 1)\n2. Ticket_Frequency (Ticket groupby --> transform('count'))\n3. Name_Length","eafdadd2":"https:\/\/www.kaggle.com\/python10pm\/pandas-100-tricks  \nhttps:\/\/www.kaggle.com\/headsortails\/pytanic  \nhttps:\/\/www.kaggle.com\/jeffd23\/scikit-learn-ml-from-start-to-finish\nhttps:\/\/www.kaggle.com\/masumrumi\/a-statistical-analysis-ml-workflow-of-titanic#Part-7:-Modeling-the-Data\nhttps:\/\/www.kaggle.com\/sinakhorami\/titanic-best-working-classifier#Classifier-Comparison\n\nhttps:\/\/www.kaggle.com\/helgejo\/an-interactive-data-science-tutorial#2.2-Setup-helper-Functions\nClassifiers\nEnsambling\nVoting\nConfusion matrix\ncross_validation (avoid overfitting, underfitting) CV - use cross validation for small datasets. https:\/\/stackoverflow.com\/questions\/13610074\/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio\/13623707#13623707\n\nFeature ranking with recursive feature elimination and cross-validated selection of the best number of features.\nhttps:\/\/www.kaggle.com\/helgejo\/an-interactive-data-science-tutorial#5.2.1-Automagic\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFECV.html\n\n\n","a6056c7d":"## Hyperparameters Tuning Impact","09696aa8":"#### Title Correcting - typos ","5f781e7d":"### Dabl \n* Quick various model fitting using Dabl","d47a30d2":"From the output, we see that there's a multicollinearity problem and we can conclude the following:\n1. The dummy variables which are created using one-hot encoding have infinite VIF. We'll find the best one to remove and then check VIF again\n2. `Sex` and `Family_Size` have much higher VIF than 5. \n3. `Parch` and `SibSp` have slightly more than 5 VIF.\n\nNow, let us drop one of the dummy variables to solve the multicollinearity issue:","93afe9d3":"### Box-Cox Transform\nassumes positive values, 0 and negative values are not supported.\nOne way to solve this problem is to use a MixMaxScaler transform first to scale the data to positive values, then apply the transform.\nWe can use a Pipeline object to apply both transforms in sequence","29468d19":"# Part 7: Feature Transformation"}}