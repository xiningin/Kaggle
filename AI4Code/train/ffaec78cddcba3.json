{"cell_type":{"3d329a27":"code","68ae6278":"code","61813ddc":"code","f100b834":"code","34507209":"code","9bf66d7b":"code","788de26e":"code","032426d3":"code","0cb7425d":"code","5a66e0ea":"code","729afbe1":"code","b37007e7":"code","daf67b33":"code","e06b344d":"code","07026e69":"code","41f4bd7e":"code","6aecd234":"code","20874873":"code","844a0bd8":"code","1467709a":"code","868609ad":"code","7f887da8":"code","dc826dd1":"code","7889f074":"code","95c424eb":"code","a0b7476f":"code","96a9318e":"code","61a4c439":"code","9290671b":"code","de854e01":"code","7e77b842":"code","2a150b12":"code","1ade01ad":"code","962a25dd":"code","8691c6a1":"code","83e5812b":"code","4eb0e2d1":"code","b363d193":"code","9728256e":"code","505baca0":"code","8862d18a":"markdown","b149877f":"markdown","60c49666":"markdown","79b305ca":"markdown","2ed87d3d":"markdown","7338ef64":"markdown","eb6006fb":"markdown","1a6b2a0d":"markdown","9d3fe5b8":"markdown","7cc59696":"markdown","42a1cbfe":"markdown","a9d92297":"markdown","a0bb33ca":"markdown","8b843993":"markdown","d0aeec2e":"markdown","cfbe16d9":"markdown","e61daef7":"markdown","5505afb3":"markdown","dac11c08":"markdown","9a1f1643":"markdown","e2348a2c":"markdown","7fb94f15":"markdown","da7197d3":"markdown","e4d3482b":"markdown","8f840718":"markdown","dae6c450":"markdown","ebf54ed8":"markdown","a34e3270":"markdown","55b50489":"markdown","d1f276f2":"markdown","2e06027e":"markdown","1261a8ba":"markdown","287cd0bb":"markdown","1996abc7":"markdown","0b4d550a":"markdown","a9de9e8a":"markdown","414557f5":"markdown","0e932d08":"markdown"},"source":{"3d329a27":"import os\nimport time\nimport numpy as np\nimport pandas as pd\nimport datatable as dt\nimport warnings\nimport gc\n\n# Scikit-learn\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import *\n\n# Boosters\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\n\n# Hyperparameter \nimport optuna\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\nwarnings.filterwarnings('ignore')\n\nDEMO=True","68ae6278":"# The id column disrupts the training so delete it\ntrain = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv').drop(columns=['id'])\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv', index_col=0) # another way to drop the column \nsubmission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')","61813ddc":"del train\ndel test\ndel submission\ngc.collect(); # To not output don't forget \";\"","f100b834":"train =dt.fread('\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv', columns=lambda cols:[col.name not in ('id') for col in cols]).to_pandas()\ntest = dt.fread('\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv', columns=lambda cols:[col.name not in ('id') for col in cols]).to_pandas()\nsubmission = dt.fread('\/kaggle\/input\/tabular-playground-series-nov-2021\/sample_submission.csv').to_pandas()","34507209":"train.head().T","9bf66d7b":"train.info(verbose=True, memory_usage=\"deep\")","788de26e":"train.describe().T","032426d3":"train.sample(4).T","0cb7425d":"print(f'There is:\\n{train.isna().sum().sum()} null values in train dataframe\\n{test.isna().sum().sum()} null values in test dataframe')","5a66e0ea":"# from sklearn.preprocessing import StandardScaler\nif not DEMO:\n    z = StandardScaler()\n\n    # Separate (method 1)\n    z.fit(train)\n    train_z = z.transform(train)\n\n    # Together (method 1)\n    train_z = z.fit_transform(train)","729afbe1":"# from sklearn.preprocessing import QuantileTransformer\nif not DEMO:\n    qt = QuantileTransformer(output_distribution = 'normal')\n    \n    # Separate (method 1)\n    qt.fit(train.iloc[:,:-1])\n    train_qt = qt.transform(train.iloc[:,:-1])\n    test_qt = qt.transform(test)\n\n    # Together (method 2)\n    train_qt = qt.fit_transform(train.iloc[:,:-1])\n    test_qt = qt.transform(test)","b37007e7":"# from lightgbm import LGBMClassifier\nparameters = {\n    'objective' : 'binary',\n    'metric' : 'auc',\n    'device' : 'gpu'\n}\nmodel = LGBMClassifier(**parameters)\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_test, y_test)],\n    verbose=False\n    )\ny_predicted = model.predict_proba(X_test)","daf67b33":"# from catboost import CatBoostClassifier\nparameters = {\n    'objective' : 'Logloss',\n    'eval_metric' : 'AUC',\n    'task_type' : 'GPU'\n}\n\nmodel = CatBoostClassifier(**parameters)\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_test, y_test)],\n    verbose=False\n    )\ny_predicted = model.predict_proba(X_test)","e06b344d":"# from xgboost import XGBClassifier\nparameters = {\n    'objective': 'binary:logistic',\n    'eval_metric' : 'auc',\n    'tree_method': 'gpu_hist',\n    'predictor': 'gpu_predictor'\n}\n\nmodel = XGBClassifier(**parameters)\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_test, y_test)],\n    verbose=False\n    )\ny_predicted = model.predict_proba(X_test)","07026e69":"n_splits = 4 # 25% test and 75% train\ny = train['target']\nX = train.drop(columns=['target'])","41f4bd7e":"X_test, X_train = np.split(X, [train.shape[0] \/\/ n_splits])\ny_test, y_train = np.split(y, [train.shape[0] \/\/ n_splits])","6aecd234":"print(f' X_train.shape: {X_train.shape} \\n X_test.shape: {X_test.shape} \\n y_train.shape: {y_train.shape} \\n y_test.shape: {y_test.shape}')","20874873":"del y, X, X_test, X_train, y_test, y_train\ngc.collect();","844a0bd8":"train_sh = train.sample(frac=1).reset_index(drop=True).copy()\n\ny = train_sh['target']\nX = train_sh.drop(columns=['target'])\n\nX_test, X_train = np.split(X, [train_sh.shape[0] \/\/ n_splits])\ny_test, y_train = np.split(y, [train_sh.shape[0] \/\/ n_splits])\nprint(f' X_train.shape: {X_train.shape} \\n X_test.shape: {X_test.shape} \\n y_train.shape: {y_train.shape} \\n y_test.shape: {y_test.shape}\\n')","1467709a":"# from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1 \/ n_splits), random_state=59, shuffle=True)\nprint(f' X_train.shape: {X_train.shape} \\n X_test.shape: {X_test.shape} \\n y_train.shape: {y_train.shape} \\n y_test.shape: {y_test.shape}\\n')","868609ad":"del X_test, X_train, y_test, y_train\ngc.collect();","7f887da8":"# from sklearn.model_selection import KFold\n\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=59)\n\nfor fold, (train_index, test_index) in enumerate(kf.split(X=X)):\n    X_train = X.iloc[train_index]\n    X_test = X.iloc[test_index]\n    y_train = y.iloc[train_index]\n    y_test = y.iloc[test_index]\n\n    print(f'\\n===== fold {fold} ====\\n X_train.shape: {X_train.shape} \\n X_test.shape: {X_test.shape} \\n y_train.shape: {y_train.shape} \\n y_test.shape: {y_test.shape}')","dc826dd1":"del X_test, X_train, y_test, y_train\ngc.collect();","7889f074":"# from sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=59) \n\nfor train_index, test_index in skf.split(X=X, y=y):\n    X_train = X.iloc[train_index]\n    X_test = X.iloc[test_index]\n    y_train = y.iloc[train_index]\n    y_test = y.iloc[test_index]\n\n    print(f'\\n{\"=\"*20}\\n X_train.shape: {X_train.shape} \\n X_test.shape: {X_test.shape} \\n y_train.shape: {y_train.shape} \\n y_test.shape: {y_test.shape}')","95c424eb":"del X_test, X_train, y_test, y_train\ngc.collect();","a0b7476f":"# from sklearn.model_selection import ShuffleSplit\n\nshs = ShuffleSplit(n_splits=n_splits, random_state=59)\n\nfor fold, (train_index, test_index) in enumerate(shs.split(X=X)):\n    X_train = X.iloc[train_index]\n    X_test = X.iloc[test_index]\n    y_train = y.iloc[train_index]\n    y_test = y.iloc[test_index]\n    \n    print(f'\\n===== fold {fold} ====\\n X_train.shape: {X_train.shape} \\n X_test.shape: {X_test.shape} \\n y_train.shape: {y_train.shape} \\n y_test.shape: {y_test.shape}')","96a9318e":"# y_predicted_oof = np.zeros((train.shape[0],))\nparameters = {\n    'objective': 'binary:logistic',\n    'eval_metric' : 'auc',\n    'tree_method': 'gpu_hist',\n    'predictor': 'gpu_predictor'\n}\nmodel = XGBClassifier(**parameters)\n\nshs = ShuffleSplit(n_splits=n_splits, random_state=59)\n\nfor fold, (train_index, test_index) in enumerate(shs.split(X=X)):\n    X_train = X.iloc[train_index]\n    X_test  = X.iloc[test_index]\n    y_train = y.iloc[train_index]\n    y_test  = y.iloc[test_index]\n    \n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_test, y_test)],\n        verbose=False\n    )\n    y_predicted = model.predict_proba(X_test)\n    \n    # temp_oof = model.predict(X_valid)\n    # y_predicted_oof[test_index] = temp_oof\n    # print(f'Fold {fold} RMSE: ', get_accuracy_2(y_test, temp_oof))\n    # print(f'fold: {fold} |  Score: {get_score_1(y_test, y_predicted)} \\n')\n    \n    \n# print(f'OOF Accuracy: ', get_accuracy_2(train['claim'], y_predicted_oof) )","61a4c439":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1 \/ n_splits), random_state=59, shuffle=True)","9290671b":"def lgbm_objective(trial):\n    parameters = {\n        \"device\" : \"gpu\",\n        \"objective\": \"binary\",\n        \"metric\": \"binary_logloss\",\n        \"verbosity\": -1, # < 0: Fatal, = 0: Error (Warning), = 1: Info, > 1: Debug\n        \"boosting_type\": \"gbdt\", # gbdt, rf, dart, goss\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),# 0.1<-<1.0\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),# 0.1<-<1.0\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n    }\n    \n    model = LGBMClassifier(**parameters)\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_test, y_test)]\n        )\n    y_predicted = model.predict_proba(X_test)\n    \n    return roc_auc_score(y_test, y_predicted)","de854e01":"# import optuna\nlgbm_study = optuna.create_study(direction=\"maximize\", study_name=\"LGBM Classifier\")\nlgbm_study.optimize(lgbm_objective, n_trials=100)\n\nprint(\"Number of finished trials: {}\".format(len(lgbm_study.trials)))\n\nprint(\"Best trial for LGBM:\")\ntrial = lgbm_study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","7e77b842":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1 \/ n_splits), random_state=59, shuffle=True)","2a150b12":"def catboost_objective(trial):\n    parameters = {\n            \"eval_metric\" : \"AUC\",\n            \"task_type\" : \"GPU\",\n            \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n            \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n            \"depth\": trial.suggest_int(\"depth\", 1, 12),\n            \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n            \"bootstrap_type\": trial.suggest_categorical(\n                \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n            )\n        }\n\n    if parameters[\"bootstrap_type\"] == \"Bayesian\":\n        parameters[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif parameters[\"bootstrap_type\"] == \"Bernoulli\":\n        parameters[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n        \n    model = CatBoostClassifier(**parameters)\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_test, y_test)],\n        verbose=False,\n        early_stopping_rounds=100\n        )\n    y_predicted = model.predict_proba(X_test)\n    \n    return roc_auc_score(y_test, y_predicted)","1ade01ad":"# import optuna\ncatboost_study = optuna.create_study(direction=\"maximize\", study_name=\"CatBoost Classifier\")\ncatboost_study.optimize(catboost_objective, n_trials=100)\n\nprint(\"Number of finished trials: {}\".format(len(catboost_study.trials)))\n\nprint(\"Best trial for CatBoost:\")\ntrial = catboost_study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","962a25dd":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1 \/ n_splits), random_state=59, shuffle=True)","8691c6a1":"def xgboost_objective(trial):\n    parameters = {\n        \"verbosity\": 0,\n        \"objective\": \"binary:logistic\",\n        \"predictor\": \"gpu_predictor\",\n        \"eval_metric\" : \"auc\",\n        # use exact for small dataset.\n        \"tree_method\": \"gpu_hist\",\n        # defines booster, gblinear for linear functions.\n        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\"]),\n        # L2 regularization weight.\n        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n        # L1 regularization weight.\n        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n        # sampling ratio for training data.\n        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n        # sampling according to each tree.\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n    }\n\n    # maximum depth of the tree, signifies complexity of the tree.\n    parameters[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n    # minimum child weight, larger the term more conservative the tree.\n    parameters[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n    parameters[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n    # defines how selective algorithm is.\n    parameters[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n    parameters[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n\n    model = XGBClassifier(**parameters)\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_test, y_test)],\n        verbose=False,\n        )\n    y_predicted = model.predict_proba(X_test)\n    \n    return roc_auc_score(y_test, y_predicted)","83e5812b":"# import optuna\nxgboost_study = optuna.create_study(direction=\"maximize\", study_name=\"XGBoost Classifier\")\nxgboost_study.optimize(xgboost_objective, n_trials=100)\n\nprint(\"Number of finished trials: {}\".format(len(xgboost_study.trials)))\n\nprint(\"Best trial for XGBoost:\")\ntrial = xgboost_study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","4eb0e2d1":"# from sklearn.metrics import *\ndef get_score_1(y_test, y_predicted):\n    fpr, tpr, _ = roc_curve(y_test, y_predicted[:, 1])\n    return auc(fpr, tpr)","b363d193":"# from sklearn.metrics import *\ndef get_score_2(y_test, y_predicted):\n    return roc_auc_score(y_test, y_predicted[:, 1])","9728256e":"# from sklearn.metrics import *\ndef get_accuracy_1(y_test, y_predicted):\n    return mean_squared_error(y_test, y_predicted, squared=False)","505baca0":"submission['target'] = model.predict_proba(test)[:, 1]\nsubmission.to_csv('tps-1121-submit.csv', index = False)","8862d18a":" ## 7.2 Optuna for CatBoost<a id=\"7.2\"><\/a>\n - [https:\/\/github.com\/optuna\/optuna-examples\/blob\/main\/catboost\/catboost_simple.py](https:\/\/github.com\/optuna\/optuna-examples\/blob\/main\/catboost\/catboost_simple.py)","b149877f":" It semble [transpose](https:\/\/pandas.pydata.org\/pandas-docs\/version\/0.25.0\/reference\/api\/pandas.DataFrame.T.html) is good idea to show any other methods of DataFrame.","60c49666":" ### 6.2.3 StratifiedKFold<a id=\"6.2.3\"><\/a>","79b305ca":" ## 5.2 CatBoost<a id=\"5.2\"><\/a>\n\nOfficial documentation for parameters:\n - [https:\/\/catboost.ai\/docs\/concepts\/python-reference_parameters-list.html#python-reference_parameters-list](https:\/\/catboost.ai\/docs\/concepts\/python-reference_parameters-list.html#python-reference_parameters-list)","2ed87d3d":" Display head of DataFrame is not normally legible. so we [transpose](https:\/\/pandas.pydata.org\/pandas-docs\/version\/0.25.0\/reference\/api\/pandas.DataFrame.T.html) it.","7338ef64":" ### 6.2.4 ShuffleSplit<a id=\"6.2.4\"><\/a>","eb6006fb":"![image.png](attachment:71e02876-3a01-4709-9bf4-e5f7b0cad5bd.png)","1a6b2a0d":" ### 6.2.2 K-Fold<a id=\"6.2.2\"><\/a>\n\n In the following three methods, the splitting operation is repeated `n_splits` times.","9d3fe5b8":" ## 4.2 Standardization<a id=\"4.2\"><\/a>\n This section will not be useful if you want to use classification algorithms","7cc59696":" ## 6.1 Numpy<a id=\"6.1\"><\/a>","42a1cbfe":" ## 7.3 Optuna for XGBoost<a id=\"7.3\"><\/a>\n - [https:\/\/github.com\/optuna\/optuna-examples\/blob\/main\/xgboost\/xgboost_simple.py](https:\/\/github.com\/optuna\/optuna-examples\/blob\/main\/xgboost\/xgboost_simple.py)","a9d92297":" # 7 Hyperparameter <a id=\"7\"><\/a>\n As I said, parameters are very important in LightGBM, CatBoost and XGBoost methods. Optuna is one of the automatic hyperparameter optimization software framework.\n - [https:\/\/optuna.org](https:\/\/optuna.org)\n - [https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.trial.Trial.html](https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.trial.Trial.html)\n\n ## 7.1 Optuna for LightGBM<a id=\"7.1\"><\/a>\n - [https:\/\/github.com\/optuna\/optuna-examples\/blob\/main\/lightgbm\/lightgbm_simple.py](https:\/\/github.com\/optuna\/optuna-examples\/blob\/main\/lightgbm\/lightgbm_simple.py)","a0bb33ca":" The `QuantileTransformer` method seems to be better. \n \n  - \ud83d\ude4f <https:\/\/www.kaggle.com\/melanie7744\/tps9-how-to-transform-your-data>","8b843993":" # 9 Submission<a id=\"9\"><\/a>","d0aeec2e":" ## 4.1 Missing values\n\n There is no Missing values in [Tabular Playground Series - Oct 2021 data](https:\/\/www.kaggle.com\/c\/tabular-playground-series-oct-2021\/data)","cfbe16d9":" <div class=\"alert alert-danger\">\n <svg xmlns=\"http:\/\/www.w3.org\/2000\/svg\" width=\"32\" height=\"32\" viewBox=\"0 0 16 16\" fill=\"currentColor\">\n   <path d=\"M8.982 1.566a1.13 1.13 0 0 0-1.96 0L.165 13.233c-.457.778.091 1.767.98 1.767h13.713c.889 0 1.438-.99.98-1.767L8.982 1.566zM8 5c.535 0 .954.462.9.995l-.35 3.507a.552.552 0 0 1-1.1 0L7.1 5.995A.905.905 0 0 1 8 5zm.002 6a1 1 0 1 1 0 2 1 1 0 0 1 0-2z\"\/>\n <\/svg>\n <b style=\"font-size: x-large;\">ATTENTION<\/b><br>\n     Standardization with <code>sklearn.preprocessing.*<\/code> may overflow memory.\n <\/div>","e61daef7":"<div class=\"alert alert-danger\">\n<svg xmlns=\"http:\/\/www.w3.org\/2000\/svg\" width=\"32\" height=\"32\" fill=\"currentColor\" viewBox=\"0 0 16 16\">\n  <path d=\"M8.982 1.566a1.13 1.13 0 0 0-1.96 0L.165 13.233c-.457.778.091 1.767.98 1.767h13.713c.889 0 1.438-.99.98-1.767L8.982 1.566zM8 5c.535 0 .954.462.9.995l-.35 3.507a.552.552 0 0 1-1.1 0L7.1 5.995A.905.905 0 0 1 8 5zm.002 6a1 1 0 1 1 0 2 1 1 0 0 1 0-2z\"\/>\n<\/svg>\n<b style=\"font-size: x-large;\">ATTENTION<\/b><br>\nCatboost is not compatible with Boolean type.\n<\/div>","5505afb3":" # 3 Load Data<a id=\"3\"><\/a>\n\n It seems that Pandas load data slowly. One alternative is to use `datatable` and convert theme to `Pandas`. [Link](https:\/\/www.kaggle.com\/bextuychiev\/how-to-work-w-million-row-datasets-like-a-pro#Read-in-the-massive-dataset)","dac11c08":" For better results, we can `shuffle` the data","9a1f1643":" ## 3.2 Datatable<a id=\"3.2\"><\/a>\n\n To learn more about Datatable [see this kaggle code](https:\/\/www.kaggle.com\/sudalairajkumar\/getting-started-with-python-datatable). Datatable `fread` documentation [is here](https:\/\/datatable.readthedocs.io\/en\/latest\/api\/dt\/fread.html)","e2348a2c":" The difference between the three methods: [(see here for more info)](https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_cv_indices.html)\n\n ![kfold-StratifiedKFold](https:\/\/raw.githubusercontent.com\/akmeghdad\/data-science-note\/master\/src\/images\/model-selection-3-models-kfold.jpg)","7fb94f15":" # 8 Quality of predictions<a id=\"8\"><\/a>","da7197d3":" # 2 Modules<a id=\"2\"><\/a>","e4d3482b":" <div class=\"alert alert-info\">\n   <svg xmlns=\"http:\/\/www.w3.org\/2000\/svg\" width=\"32\" height=\"32\" fill=\"currentColor\" class=\"bi bi-info-circle-fill\" viewBox=\"0 0 16 16\">\n   <path d=\"M8 16A8 8 0 1 0 8 0a8 8 0 0 0 0 16zm.93-9.412-1 4.705c-.07.34.029.533.304.533.194 0 .487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703 0-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381 2.29-.287zM8 5.5a1 1 0 1 1 0-2 1 1 0 0 1 0 2z\"\/>\n <\/svg>\n <\/svg>\n <b style=\"font-size: x-large;\">MORE INFO<\/b><br>\n    ___The difference between <code>Pandas<\/code> and <code>Datatable<\/code> and the best way to load data is fully <code><a href=\"https:\/\/www.kaggle.com\/akmeghdad\/tps-1021-how-to-make-better-use-of-kaggle-memory\" target=\"_blank\">explained here<\/a><\/code>\n <\/div>","8f840718":" ## 5.3 XGBoost<a id=\"5.3\"><\/a>\n Official documentation for parameters:\n - [https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html)\n - [https:\/\/github.com\/dmlc\/xgboost\/blob\/master\/doc\/parameter.rst](https:\/\/github.com\/dmlc\/xgboost\/blob\/master\/doc\/parameter.rst)","dae6c450":" The `StandardScaler` method uses the following formula\n\n ![StandardScaler](https:\/\/raw.githubusercontent.com\/akmeghdad\/data-science-note\/master\/src\/images\/StandardScaler-formula.jpg)","ebf54ed8":" ## 3.1 Pandas<a id=\"3.1\"><\/a>","a34e3270":" ## 6.3 Boosters with cross validation<a id=\"6.3\"><\/a>","55b50489":" # Contents<a id=\"0\"><\/a>\n\n- [1 Abstract](#1)\n- [2 Modules](#2)\n- [3 Load Data](#3)\n  - [3.1 Pandas](#3.1)\n  - [3.2 Datatable](#3.2)\n  - [3.3 Statistics](#3.3)\n  - [3.4 Resource optimization (In progress)](#3.4)\n- [4 Preprocessing](#4)\n  - [4.1 Missing values](#4.1)\n  - [4.2 Standardization](#4.2)\n- [5 Modeling](#5)\n  - [5.1 LightGBM](#5.1)\n  - [5.2 CatBoost](#5.1)\n  - [5.3 XGBoost](#5.3)\n- [6 Cross Validation (K-Fold)](#6)\n  - [6.1 Numpy](#6.1)\n  - [6.2 Scikit Learn](#6.2)\n    - [6.2.1 train_test_split](#6.2.1)\n    - [6.2.2 K-Fold](#6.2.2)\n    - [6.2.3 StratifiedKFold](#6.2.3)\n    - [6.2.4 ShuffleSplit](#6.2.4)\n  - [6.3 Boosters with cross validation](#6.3)\n- [7 Hyperparameter](#7)\n  - [7.1 Optuna for LightGBM](#7.1)\n  - [7.2 Optuna for CatBoost](#7.2)\n  - [7.3 Optuna for XGBoost](#7.3)\n- [8 Quality of predictions](#8)\n- [9 Submission](#9)","d1f276f2":" # 1 Abstract<a id=\"1\"><\/a>\n\n These are my personal notes. I would like to share them with you and I hope it can be useful for you, even if it is a small help. I will try to update these notes over time.","2e06027e":" We can't view all the columns and datatypes when we want print a concise summary of a DataFrame, we use `verbose` as argument to solve this problem","1261a8ba":" ## 3.3 Statistics<a id=\"3.3\"><\/a>","287cd0bb":" ## 6.2 Scikit Learn<a id=\"6.2\"><\/a>\n\n The two above operations are performed in different ways\n\n ### 6.2.1 train_test_split<a id=\"6.2.1\"><\/a>","1996abc7":" # 4 Preprocessing","0b4d550a":" # 5 Modeling<a id=\"5\"><\/a>\n\n LightGBM, CatBoost and XGBoost are known as new ways to build models. Parameter settings are very important in these three methods. Only the basic parameters are listed here.\n\n\n ## 5.1 LightGBM<a id='5.1'><\/a>\n Official documentations for parameters:\n - [https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.LGBMClassifier.html](https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.LGBMClassifier.html)\n - [https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst](https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst)","a9de9e8a":" # 6 Cross Validation (K-Fold)<a id=\"6\"><\/a>\n\n There are several ways to split data. In the following, we consider the number of parts equal to 4. We put 25% of the data for testing and 75% for training.","414557f5":" ## 5.4 Ensemble (in proses)<a id=\"5.4\"><\/a>\n - <https:\/\/towardsdatascience.com\/boosting-showdown-scikit-learn-vs-xgboost-vs-lightgbm-vs-catboost-in-sentiment-classification-f7c7f46fd956>","0e932d08":" ## 3.4 Resource optimization (In progress) <a id=\"3.4\"><\/a>"}}