{"cell_type":{"9c733a94":"code","459224e2":"code","4772ee9c":"code","48fdbfdd":"code","8dbfffb6":"code","ecb47ca0":"code","b8fabac6":"code","f26f213c":"code","dabb632e":"code","914c9f95":"code","69526036":"code","97ec1fe0":"code","0bf57c33":"code","8814c87b":"code","e6f1e1bb":"code","537e87fa":"code","36e315ce":"code","a84bfbf6":"code","80399e17":"code","a6ed72fb":"code","5fce39f1":"code","6d8a9057":"code","30067e27":"code","6c89cf01":"code","d2b18eaa":"code","0b25663c":"code","1cc212c9":"code","329cd6c2":"code","9785df12":"code","56d2ff53":"code","2c2d83a5":"code","562137fc":"code","d6a98a46":"code","2d1cb735":"code","fd2cc7fa":"code","b89d590b":"code","7245a8b1":"code","01fc56d3":"code","feeebd53":"code","d64c4a0e":"code","09b226ad":"code","90235259":"code","8437cf0a":"markdown","d8be69f2":"markdown","52254616":"markdown","9cb856cf":"markdown","1e47b95e":"markdown"},"source":{"9c733a94":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","459224e2":"\n!pip install -q tf-nightly\n!pip install -q tf-models-nightly","4772ee9c":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\n\nimport tensorflow_hub as hub\nimport tensorflow_datasets as tfds\ntfds.disable_progress_bar()\n\nfrom official.modeling import tf_utils\nfrom official import nlp\nfrom official.nlp import bert\n\n# Load the required submodules\nimport official.nlp.optimization\nimport official.nlp.bert.bert_models\nimport official.nlp.bert.configs\nimport official.nlp.bert.run_classifier\nimport official.nlp.bert.tokenization\nimport official.nlp.data.classifier_data_lib\nimport official.nlp.modeling.losses\nimport official.nlp.modeling.models\nimport official.nlp.modeling.networks","48fdbfdd":"train = \"..\/input\/nlp-getting-started\/train.csv\"\ntest = \"..\/input\/nlp-getting-started\/test.csv\"","8dbfffb6":"data = (pd.read_csv(train)).iloc[:,3:]\ndata.shape","ecb47ca0":"gs_folder_bert = \"gs:\/\/cloud-tpu-checkpoints\/bert\/keras_bert\/uncased_L-12_H-768_A-12\"\ntf.io.gfile.listdir(gs_folder_bert)","b8fabac6":"tokenizer = bert.tokenization.FullTokenizer(\n    vocab_file=os.path.join(gs_folder_bert, \"vocab.txt\"),\n     do_lower_case=True)","f26f213c":"print(\"Vocab size:\", len(tokenizer.vocab))\ntokenizer.convert_tokens_to_ids(['[CLS]', '[SEP]'])","dabb632e":"def encode_sentence(s):\n    tokens = list(tokenizer.tokenize(s))\n    tokens.append('[SEP]')\n    return tokenizer.convert_tokens_to_ids(tokens)\n\nsentence = tf.ragged.constant([\n    encode_sentence(s) for s in  data['text']])","914c9f95":"print(\"Sentence shape:\", sentence.shape.as_list())","69526036":"cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence.shape[0]\ninput_word_ids = tf.concat([cls, sentence], axis=-1)\n_ = plt.pcolormesh(input_word_ids.to_tensor())\n\n","97ec1fe0":"input_mask = tf.ones_like(input_word_ids).to_tensor()\n\nplt.pcolormesh(input_mask)","0bf57c33":"def encode_sentence(s, tokenizer):\n    tokens = list(tokenizer.tokenize(s))\n    tokens.append('[SEP]')\n    return tokenizer.convert_tokens_to_ids(tokens)\n\ndef bert_encode(data, tokenizer):\n    num_examples = len(data[\"text\"])\n  \n    sentence = tf.ragged.constant([\n       encode_sentence(s, tokenizer)\n       for s in np.array(data[\"text\"])])\n\n    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence.shape[0]\n    input_word_ids = tf.concat([cls, sentence], axis=-1)\n\n    input_mask = tf.ones_like(input_word_ids).to_tensor()\n\n    type_cls = tf.zeros_like(cls)\n    type_s = tf.zeros_like(sentence)\n    input_type_ids = tf.concat(\n      [type_cls, type_s], axis=-1).to_tensor()\n\n    inputs = {\n      'input_word_ids': input_word_ids.to_tensor(),\n      'input_mask': input_mask,\n      'input_type_ids': input_type_ids}\n\n    return inputs","8814c87b":"d_train,d_val,d_test = np.split(data.sample(frac=1), [int(.6*len(data)), int(.8*len(data))])","e6f1e1bb":"print(\"train \",d_train.shape)\nprint(\"val \",d_val.shape)\nprint(\"test \",d_test.shape)","537e87fa":"data_train = bert_encode(d_train, tokenizer)\ndata_train_labels = d_train['target']\n\ndata_validation = bert_encode(d_val, tokenizer)\ndata_validation_labels = d_val['target']\n\ndata_test = bert_encode(d_test, tokenizer)\ndata_test_labels  = d_test['target']","36e315ce":"for key, value in data_train.items():\n  print(f'{key:15s} shape: {value.shape}')\n\nprint(f'glue_train_labels shape: {data_train_labels.shape}')","a84bfbf6":"\nconfig_dict = {'attention_probs_dropout_prob': 0.1,\n 'hidden_act': 'gelu',\n 'hidden_dropout_prob': 0.1,\n 'hidden_size': 768,\n 'initializer_range': 0.02,\n 'intermediate_size': 3072,\n 'max_position_embeddings': 512,\n 'num_attention_heads': 12,\n 'num_hidden_layers': 12,\n 'type_vocab_size': 2,\n 'vocab_size': 30522}\nbert_config = bert.configs.BertConfig.from_dict(config_dict)\n\nconfig_dict","80399e17":"bert_classifier, bert_encoder = bert.bert_models.classifier_model(\n    bert_config, num_labels=1)","a6ed72fb":"data_batch = {key: val[:10] for key, val in data_train.items()}\n\nbert_classifier(\n    data_batch, training=True\n).numpy()","5fce39f1":"checkpoint = tf.train.Checkpoint(model=bert_encoder)\ncheckpoint.restore(\n    os.path.join(gs_folder_bert, 'bert_model.ckpt')).assert_consumed()","6d8a9057":"# Set up epochs and steps\nepochs = 3\nbatch_size = 32\neval_batch_size = 32\n\ntrain_data_size = len(data_train_labels)\nsteps_per_epoch = int(train_data_size \/ batch_size)\nnum_train_steps = steps_per_epoch * epochs\nwarmup_steps = int(epochs * train_data_size * 0.1 \/ batch_size)\n\n# creates an optimizer with learning rate schedule\noptimizer = nlp.optimization.create_optimizer(\n    2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)","30067e27":"metrics = [tf.keras.metrics.BinaryAccuracy('binary_accuracy', dtype=tf.float32)]\nloss = tf.keras.losses.BinaryCrossentropy(from_logits=True,name='binary_crossentropy')\nbert_classifier.compile(\n    optimizer=optimizer,\n    loss=loss,\n    metrics=metrics)\n","6c89cf01":"bert_classifier.fit(\n      data_train, data_train_labels,\n      validation_data=(data_validation, data_validation_labels),\n      batch_size=32,\n      epochs=epochs)  ","d2b18eaa":"y_pred = bert_classifier.predict(data_test)","0b25663c":"y_pred","1cc212c9":"result = bert_classifier(data_test, training=False)\n","329cd6c2":" result","9785df12":"check = [1 if i>0 else 0 for i in result]","56d2ff53":"from sklearn.metrics import classification_report\nprint(classification_report(check, data_test_labels,))","2c2d83a5":"export_dir='.\/saved_model'\ntf.saved_model.save(bert_classifier, export_dir=export_dir)","562137fc":"export_dir='.\/saved_model'\nclf = tf.saved_model.load(export_dir)","d6a98a46":"!ls saved_model\/assets\/","2d1cb735":"sub = pd.read_csv(test)","fd2cc7fa":"sub.head()","b89d590b":"data_sub = bert_encode(sub, tokenizer)","7245a8b1":"for key, value in data_sub.items():\n  print(f'{key:15s} shape: {value.shape}')\n","01fc56d3":"sub_val = clf([data_sub['input_word_ids'],\n              data_sub['input_mask'],\n              data_sub['input_type_ids']], training=False)\n","feeebd53":"sub_val","d64c4a0e":"target = [1 if i>0 else 0 for i in sub_val]","09b226ad":"target\n","90235259":"my_submission = pd.DataFrame({'id': sub.id, 'Target': target})\nmy_submission.to_csv('submission.csv', index=False)","8437cf0a":"# submit","d8be69f2":"# Evaluation and Accuracy","52254616":"# Bert-Tokenizer","9cb856cf":"# Model","1e47b95e":"# Encoder and classifier"}}