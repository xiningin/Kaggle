{"cell_type":{"7745b707":"code","797d709e":"code","c289db65":"code","dbc48479":"code","6eca423d":"code","3ced6bdf":"code","666d2f51":"code","eed57191":"code","4fe08904":"code","ca63366b":"code","7c6c3382":"code","a7a6fdea":"code","c45d15af":"code","e21b9709":"code","aecb296f":"code","99aab7fa":"code","6aee3643":"code","1f132a17":"code","d0106773":"code","4967b98d":"code","9aef9e84":"code","3b1d8c9c":"code","fe500339":"code","83971f81":"code","2d9798f4":"code","74d5faf9":"code","9d113238":"code","c8d92673":"code","faa1f4ff":"code","4dae7218":"code","ef547ae0":"code","213f5897":"code","228b8810":"code","7b830d55":"code","c0cf4e8a":"code","82509ae0":"code","aac34611":"code","90b64100":"code","43808fc0":"code","fa961a69":"code","36e8cd31":"code","f0a7e115":"code","99838ed3":"code","912d40ee":"code","776126a8":"code","50f70550":"code","32ab4dbd":"code","eff27e00":"code","bd47bc8e":"code","7017831e":"code","2cdcf2a6":"code","d810506d":"code","6d4702c7":"code","8f21ffb7":"code","3a0b2211":"code","4d3fdac2":"code","6416bfbd":"code","70095324":"code","eae37f6e":"code","a1f6d10b":"code","2fb03d79":"code","39bdd720":"code","80a0967c":"code","0740be5e":"code","94ef69a6":"code","be1ba5a2":"code","233c507d":"code","a1677a4f":"code","f0e1df1b":"code","06ecbe04":"code","3a41e85e":"code","92fe3695":"code","f7864003":"code","e757a21f":"code","263f23b8":"code","c60e5533":"code","004a8df9":"code","37eaeee0":"code","1c3e314c":"code","fb04bc79":"code","970ce5ff":"code","80c8582c":"code","322fbb6c":"code","740b784e":"code","1743b5bb":"code","d77c82ba":"code","0c204d1a":"code","6a002009":"code","03d1afb9":"code","c7cf34b0":"code","e49d58c7":"code","2efc46b0":"code","fcd0028a":"code","fbf09618":"code","54b9c734":"code","1c046f57":"code","cc2f3159":"code","4088d94b":"code","0b186e5b":"code","fed06775":"markdown","88e0f9f3":"markdown","224d8ab4":"markdown","34425b98":"markdown","a28c6357":"markdown","35c7a96e":"markdown","47180b0f":"markdown","ce7e9aaa":"markdown","3b5ea7c6":"markdown","9697ba9d":"markdown","79e09a3f":"markdown","bb8a7625":"markdown","ecf3b5f5":"markdown","26ffb9ce":"markdown","9e4b4bf3":"markdown","349f58f9":"markdown","047f38a3":"markdown","612f71b9":"markdown","0dc9e3ab":"markdown","6d3fd1c7":"markdown","96c30dac":"markdown","b034679e":"markdown","f615b506":"markdown","f2013ab8":"markdown","93318b1d":"markdown","25bf9836":"markdown","ae91d9b8":"markdown","13f3ac8f":"markdown","340a030e":"markdown","0ea6cd93":"markdown","a477f566":"markdown","7fb324f7":"markdown","c4760759":"markdown","9aa235cb":"markdown","2cee86b4":"markdown","5224991b":"markdown","165d5230":"markdown","48c14924":"markdown","3941570a":"markdown"},"source":{"7745b707":"# Importing basic libraries\nimport numpy as np\nimport pandas as pd\n\n# Importing libraries for data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Importing libraries for model preparing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Importing library for model building\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn import metrics\nfrom sklearn.metrics import recall_score\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import precision_recall_curve","797d709e":"# Importing libraries for removing warnings\nimport warnings as warnings\nwarnings.filterwarnings(\"ignore\")","c289db65":"# Importing the file\nleads_df = pd.read_csv(\"..\/input\/leads-dataset\/Leads.csv\")\nleads_df.head()","dbc48479":"# Checking Shape\nleads_df.shape","6eca423d":"# Checking other values\nleads_df.describe()","3ced6bdf":"# Checking columns details\nleads_df.info()","666d2f51":"# Calculating the shape after removing duplicates\nleads_modified_df = leads_df.drop_duplicates(keep = 'first')\nleads_modified_df.shape","eed57191":"# Removing columns of no significance\nleads_modified_df.drop(['Prospect ID', 'Lead Number'], axis=1, inplace=True)\nleads_modified_df.shape","4fe08904":"# Change the mapping of a column\nleads_modified_df['A free copy of Mastering The Interview'] = leads_modified_df['A free copy of Mastering The Interview'].map({'Yes':1, 'No':0})\nleads_modified_df['A free copy of Mastering The Interview'].value_counts()","ca63366b":"# Replacing the 'Select' with NaN\nleads_modified_df.replace('Select', np.NAN, inplace=True)\nleads_modified_df.head()","7c6c3382":"# Calculating % of missing values\nround(leads_modified_df.isnull().sum() * 100 \/ len(leads_modified_df), 2)","a7a6fdea":"# Function to remove the columns having more than threshold values\ndef rmissingvaluecol(dff, threshold):\n    col = []\n    col = list(dff.drop(dff.loc[:,list((100*(dff.isnull().sum()\/len(dff.index)) >= threshold))].columns, 1).columns.values)\n    print(\"Columns having more than %s percent missing values: \"%threshold, (dff.shape[1] - len(col)))\n    print(\"Columns to be dropped                             : \", list(set(list((dff.columns.values))) - set(col)))\n    return col\n\n# Removing columns having 40% missing values\ncol = rmissingvaluecol(leads_modified_df, 40)\nleads_modified_df = leads_modified_df[col]\nleads_modified_df.head()","c45d15af":"# Deleting rows containing either 70% or more than 70% NaN Values\nperc = 70.0 # Here N is 70\nmin_count =  int(((100-perc)\/100)*leads_modified_df.shape[1] + 1)\nleads_modified_df = leads_modified_df.dropna(axis=0, thresh=min_count)\nleads_modified_df.shape","e21b9709":"# Checking the unique categories\ncolumns_not_to_be_considered = ['Converted', 'TotalVisits', 'Total Time Spent on Website', 'Page Views Per Visit'] #Continous Values\ncolumn_names = leads_modified_df.columns\ncolumn_names = column_names.drop(columns_not_to_be_considered)\n\nfor column_name in column_names:\n    print(\"Column Name        :\", column_name)\n    print(\"------------------------------------------\")\n    print(leads_modified_df[column_name].value_counts(normalize=True, dropna=False)*100)\n    print('\\n')","aecb296f":"# Removing columns of highly skewed data\nskewed_columns_to_be_dropped = ['Do Not Email', 'Do Not Call', 'Search', 'Magazine', 'Newspaper Article', 'X Education Forums',\n                                'Newspaper', 'Digital Advertisement', 'Through Recommendations', \n                                'Receive More Updates About Our Courses', 'Update me on Supply Chain Content',\n                                'Get updates on DM Content', 'I agree to pay the amount through cheque' ]\n\nleads_modified_df.drop(skewed_columns_to_be_dropped, axis=1, inplace=True)\nleads_modified_df.shape","99aab7fa":"# Listing down categorical columns with missing values\ncategorical_column_names = ['Lead Source', 'Last Activity', 'Country',  'City',\n                'What is your current occupation', 'What matters most to you in choosing a course']\n\nfor column_name in categorical_column_names:\n    print(\"Column Name        :\", column_name)\n    print(\"------------------------------\")\n    print(\"Unique Values      : \", leads_modified_df[column_name].unique())\n    \n    values_to_be_imputed = leads_modified_df[column_name].isnull().sum()\n    print(\"Any Null (Before)  :\", values_to_be_imputed)\n    \n    leads_modified_df[column_name].fillna(leads_modified_df[column_name].mode()[0], inplace=True)\n    print(values_to_be_imputed, \" values imputed with mode values of the column.\")\n    \n    print(\"Null Values (After):\", leads_modified_df[column_name].isnull().sum())\n    print('\\n')","6aee3643":"# Imputing the 'NAN' of 'Specialization' with 'Unspecified' \n\nprint(\"Values to be imputed : \", leads_modified_df.Specialization.isnull().sum())\n\nleads_modified_df.Specialization.fillna(\"Unspecified\", inplace=True)","1f132a17":"# Checking the unique categories\ncolumns_not_to_be_considered = ['Converted', 'TotalVisits', 'Total Time Spent on Website', 'Page Views Per Visit']\ncolumn_names = leads_modified_df.columns\ncolumn_names = column_names.drop(columns_not_to_be_considered)\n\nfor column_name in column_names:\n    print(\"Column Name        :\", column_name)\n    print(\"-----------------------------------------------------\")\n    print(leads_modified_df[column_name].value_counts(normalize=True, dropna=False)*100)\n    print('\\n')","d0106773":"# Removing columns of highly skewed data\nskewed_columns_to_be_dropped = ['Country', 'What is your current occupation', 'What matters most to you in choosing a course']\n\nleads_modified_df.drop(skewed_columns_to_be_dropped, axis=1, inplace=True)\nleads_modified_df.shape","4967b98d":"# Changing categories with lesser percentage to 'others'\n\ndef change_to_others(x, value_counts_df):\n    for key, val in value_counts_df.to_dict().items():\n        if key == x and val < 10:\n            return 'others'\n    return x\n\ncolumns_to_be_changed = ['Lead Origin', 'Lead Source', 'Last Activity', 'Specialization', 'City',\n                         'A free copy of Mastering The Interview', 'Last Notable Activity']\n\nfor column_name in columns_to_be_changed:\n    print(\"Column Name : \", column_name)\n    print(\"-----------------------------------------\")\n\n    value_counts_df = leads_modified_df[column_name].value_counts(normalize=True) * 100\n    print(\"Before :\")\n    print(value_counts_df)\n    print('\\n')\n\n    leads_modified_df[column_name] = leads_modified_df[column_name].apply(lambda x:change_to_others(x, value_counts_df))\n    value_counts_df = leads_modified_df[column_name].value_counts(normalize=True) * 100\n    print(\"After :\")\n    print(value_counts_df)\n    print('\\n')","9aef9e84":"# Listing down continuos columns with missing values\ncategorical_column_names = ['TotalVisits', 'Page Views Per Visit']\n\nfor column_name in categorical_column_names:\n    print(\"Column Name        :\", column_name)\n    print(\"------------------------------\")\n    values_to_be_imputed = leads_modified_df[column_name].isnull().sum()\n    print(\"Any Null (Before)  :\", values_to_be_imputed)\n    \n    leads_modified_df[column_name].fillna(leads_modified_df[column_name].median(), inplace=True)\n    print(values_to_be_imputed, \" values imputed with mode values of the column.\")\n    \n    print(\"Null Values (After):\", leads_modified_df[column_name].isnull().sum())\n    print('\\n')","3b1d8c9c":"# Changing the datatype of a column\nleads_modified_df['TotalVisits'] = leads_modified_df['TotalVisits'].astype(int)\nleads_modified_df.head()","fe500339":"# Dropping a few more columns\nleads_modified_df.drop(['Last Activity', 'Last Notable Activity', 'Tags'], axis=1, inplace=True)\nleads_modified_df.shape","83971f81":"# Verifying various parameters after cleaning\nprint(\"Before Cleaning Data\")\nprint(\"*********************************\")\nprint(\"Shape: \", leads_df.shape)\nprint(\"Missing:\")\nprint(\"-----------------------\")\nprint(round(leads_df.isnull().sum()*100\/len(leads_df), 2))\n\nprint('\\n')\nprint(\"After Cleaning Data\")\nprint(\"*********************************\")\nprint(\"Shape: \", leads_modified_df.shape)\nprint(\"Missing:\")\nprint(\"-----------------------\")\nprint(round(leads_modified_df.isnull().sum()*100\/len(leads_modified_df), 2))","2d9798f4":"# Visualizing continuous data\nplt.figure(figsize=(20,15))\n\nplt.subplot(2,1,1)\nplt.title(\"Total Visits\", fontsize=25)\ngraph1 = sns.countplot(x='TotalVisits', data=leads_modified_df)\ngraph1.set(xlabel=None)\n\nplt.subplot(2,1,2)\nplt.title(\"Total Time Spent on Website\", fontsize=25)\ngraph2 = sns.distplot(leads_modified_df['Total Time Spent on Website'], bins=100)\ngraph2.set(xlabel=None)\n\nplt.show()","74d5faf9":"# Visualizing catgorical data\nplt.figure(figsize=(15,6))\n\nplt.subplot(1,2,1)\nplt.title(\"Lead Source vs Total Visits\", fontsize=15)\ngrpah1 = sns.boxplot(x=\"Lead Source\", y=\"TotalVisits\", data=leads_modified_df)\ngraph1.set(xlabel=None)\n\nplt.subplot(1,2,2)\nplt.title(\"Lead Source vs Total Time Spent on Website\", fontsize=15)\ngrpah2 = sns.boxplot(x=\"Lead Source\", y=\"Total Time Spent on Website\", data=leads_modified_df)\ngraph2.set(xlabel=None)\n\nplt.show()","9d113238":"# Imputing season as categorical 'season' values\ncolumn_names = ['Lead Source', 'Lead Origin', 'Specialization', 'City']\n\nfor column_name in column_names:\n    dummies = pd.get_dummies(leads_modified_df[column_name])\n    dummies.drop('others', axis=1, inplace=True)\n    leads_modified_df = pd.concat([leads_modified_df, dummies], axis=1)\n    leads_modified_df.drop(column_name, axis=1, inplace=True)\n    print(\"Dummies created for: \", column_name)\n\nleads_modified_df.head()","c8d92673":"# Putting feature variable to X\nX = leads_modified_df.drop('Converted', axis=1)\n\nX.head()","faa1f4ff":"# Putting response variable to y\ny = leads_modified_df.Converted\n\ny.head()","4dae7218":"# Splitting the data into train and test on a ratio of 70-30\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","ef547ae0":"# Initializing the scaler and scaling the data\nscaler = StandardScaler()\n\ncolumns_to_be_scaled = ['TotalVisits','Total Time Spent on Website','Page Views Per Visit']\nX_train[columns_to_be_scaled] = scaler.fit_transform(X_train[columns_to_be_scaled])\n\nX_train.head()","213f5897":"# Checking the Converted Rate\nconverted = round((sum(leads_modified_df['Converted'])\/len(leads_modified_df['Converted'].index))*100, 2)\nconverted","228b8810":"# Let's see the correlation matrix \nplt.figure(figsize = (20,10))        # Size of the figure\nsns.heatmap(leads_modified_df.corr(), annot = True, cmap=\"Blues\")\nplt.show()","7b830d55":"# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","c0cf4e8a":"# Initializing LogisticRegression\nlogreg = LogisticRegression()\n\n# Running RFE with 12 variables as output\nrfe = RFE(logreg, 12)             \nrfe = rfe.fit(X_train, y_train)\n\n# Listing the columns\nlist(zip(X_train.columns, rfe.support_, rfe.ranking_))","82509ae0":"# Listing down the columns important for building a model\ncol = X_train.columns[rfe.support_]\n\n# Listing down the columns not important\nX_train.columns[~rfe.support_]","aac34611":"# Building our second model\nX_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","90b64100":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred[:10]","43808fc0":"# Forming prediction table \ny_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Converted_Prob':y_train_pred})\ny_train_pred_final['LeadId'] = y_train.index\ny_train_pred_final.head()","fa961a69":"y_train_pred_final['Predicted'] = y_train_pred_final.Converted_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","36e8cd31":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted)\nprint(confusion)","f0a7e115":"# Let's check the overall report.\nprint(metrics.classification_report(y_train_pred_final.Converted, y_train_pred_final.Predicted))","99838ed3":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","912d40ee":"# Dropping a column which is least impactful\ncol = col.drop('Direct Traffic', 1)\ncol","776126a8":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","50f70550":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred[:10]","32ab4dbd":"y_train_pred_final['Converted_Prob'] = y_train_pred\n\n# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['Predicted'] = y_train_pred_final.Converted_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","eff27e00":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted)\nprint(confusion)","bd47bc8e":"# Let's check the overall accuracy.\nprint(metrics.classification_report(y_train_pred_final.Converted, y_train_pred_final.Predicted))","7017831e":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","2cdcf2a6":"# Dropping a column which is least significant\ncol = col.drop('Page Views Per Visit', 1)\ncol","d810506d":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","6d4702c7":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred[:10]","8f21ffb7":"y_train_pred_final['Converted_Prob'] = y_train_pred\n\n# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['Predicted'] = y_train_pred_final.Converted_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","3a0b2211":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted)\nprint(confusion)","4d3fdac2":"# Let's check the overall accuracy.\nprint(metrics.classification_report(y_train_pred_final.Converted, y_train_pred_final.Predicted))","6416bfbd":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","70095324":"# Dropping one more column which is least significant\ncol = col.drop('Finance Management', 1)\ncol","eae37f6e":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm5.fit()\nres.summary()","a1f6d10b":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred[:10]","2fb03d79":"y_train_pred_final['Converted_Prob'] = y_train_pred\n\n# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['Predicted'] = y_train_pred_final.Converted_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","39bdd720":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted)\nprint(confusion)","80a0967c":"# Let's check the overall accuracy.\nprint(metrics.classification_report(y_train_pred_final.Converted, y_train_pred_final.Predicted))","0740be5e":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","94ef69a6":"# Dropping one more column which is least significant\ncol = col.drop('Organic Search', 1)\ncol","be1ba5a2":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm6 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm6.fit()\nres.summary()","233c507d":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred[:10]","a1677a4f":"y_train_pred_final['Converted_Prob'] = y_train_pred\n\n# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['Predicted'] = y_train_pred_final.Converted_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","f0e1df1b":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted)\nprint(confusion)","06ecbe04":"# Let's check the overall report.\nprint(metrics.classification_report(y_train_pred_final.Converted, y_train_pred_final.Predicted))","3a41e85e":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","92fe3695":"# Dropping one more column which is least significant\ncol = col.drop('Google', 1)\ncol","f7864003":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm7 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm7.fit()\nres.summary()","e757a21f":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred[:10]","263f23b8":"y_train_pred_final['Converted_Prob'] = y_train_pred\n\n# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['Predicted'] = y_train_pred_final.Converted_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","c60e5533":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted)\nprint(confusion)","004a8df9":"# Let's check the overall report.\nprint(metrics.classification_report(y_train_pred_final.Converted, y_train_pred_final.Predicted))","37eaeee0":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","1c3e314c":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","fb04bc79":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Converted_Prob, drop_intermediate = False )","970ce5ff":"draw_roc(y_train_pred_final.Converted, y_train_pred_final.Converted_Prob)","80c8582c":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Converted_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","322fbb6c":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","740b784e":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","1743b5bb":"y_train_pred_final['final_predicted'] = y_train_pred_final.Converted_Prob.map( lambda x: 1 if x > 0.3 else 0)\n\ny_train_pred_final.head()","d77c82ba":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted)\nprint(confusion)","0c204d1a":"# Let's check the overall accuracy.\nprint(metrics.classification_report(y_train_pred_final.Converted, y_train_pred_final.Predicted))","6a002009":"metrics.recall_score(y_train_pred_final.Converted, y_train_pred_final.Predicted)","03d1afb9":"p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Converted_Prob)","c7cf34b0":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","e49d58c7":"columns_to_be_scaled = ['TotalVisits','Total Time Spent on Website','Page Views Per Visit']\nX_test[columns_to_be_scaled] = scaler.transform(X_test[columns_to_be_scaled])\nX_test = X_test[col]\nX_test.head()","2efc46b0":"#### Predicting the model on test dataset","fcd0028a":"X_test_sm = sm.add_constant(X_test)\ny_test_pred = res.predict(X_test_sm)\ny_test_pred[:10]","fbf09618":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)\n\n# Let's see the head\ny_pred_1.head()","54b9c734":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)\n\n# Putting CustID to index\ny_test_df['LeadId'] = y_test_df.index\n\n# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)\n\n# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)\n\ny_pred_final.head()","1c046f57":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Converted_Prob'})\n\n# Rearranging the columns\ny_pred_final = y_pred_final.reindex(['LeadId','Converted','Converted_Prob'], axis=1)\n\n# Let's see the head of y_pred_final\ny_pred_final.head()","cc2f3159":"y_pred_final['final_predicted'] = y_pred_final.Converted_Prob.map(lambda x: 1 if x > 0.3 else 0)\n\ny_pred_final.head()","4088d94b":"metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.final_predicted)","0b186e5b":"# Let's check the overall report.\nprint(metrics.classification_report(y_pred_final.Converted, y_pred_final.final_predicted))","fed06775":"### Data Cleaning\n- We will try to change 'Select' values of the columns to <i>NAN<\/i>.\n- We will try to drop columns having 35% missing values.\n- We will try to drop rows having 70% missing values\n- We will try to merge unique categories if they are multiples.\n- We will impute the missing values\n    * Categorical with mode value\n    * Continuous with median value\n- We will compare orginal with cleaned data","88e0f9f3":"### Data Sourcing\n\n* We will import all libraries used in the entire assignment.\n* We will import the file in dataframe.\n* We will try to check basic information.","224d8ab4":"####  Check the number of unique categories in each categorical column.","34425b98":"<b>Note:<\/b> Since we have 13 columns remaining and we chose to find RFE on 12, that's why we have one column to be discarded i.e <i>Mumbai<\/i>. ","a28c6357":"#### For the columns with less percentage of missing, use some imputation technique.","35c7a96e":"### Model Building\n- We will try to build our first.\n- We will use RFE to know how much parameters can be considered.\n- We will iteratively remove columns either having high <b>p<\/b> or <b>VIF<\/b>.","47180b0f":"#### Plotting graph for 'accuracy','sensitivity' and 'specificity'","ce7e9aaa":"### Data Preparation\n- We will create dummies for categorical columns.\n- We will split data into train-test set.\n- We will perform scaling.","3b5ea7c6":"### Model Evaluation\n- We will scale test dataset.\n- We will predict 'Converted' on it.\n- We will read the report and check the <b>sensitivity<\/b>.","9697ba9d":"#### Create dummies for all categorical columns.","79e09a3f":"#### Building our third model","bb8a7625":"#### Importing libraries","ecf3b5f5":"<b>Note :-<\/b> No duplicate values found.","26ffb9ce":"<b>Note :-<\/b> No rows deleted.","9e4b4bf3":"### Model Assessment\n- We will draw ROC curve.\n- We will create data with different probabilities.\n- We will plot a graph for 'accuracy','sensitivity' and 'specificity'.","349f58f9":"#### Scaling the test dataset","047f38a3":"#### Scaling the continuous data","612f71b9":"#### Building our sixth model","0dc9e3ab":"#### Drop columns that are having high percentage of missing values.","6d3fd1c7":"<b>Note :-<\/b> 7 columns removed.","96c30dac":"#### Checking information","b034679e":"# Lead Scoring Assignment","f615b506":"#### Checking the overall report","f2013ab8":"#### Creating columns with different probabilities","93318b1d":"<b>Note :-<\/b> A few columns are removed as they may skew the model.","25bf9836":"#### Drawing ROC Curve.","ae91d9b8":"#### Building our fifth model","13f3ac8f":"#### Splitting the data into train-test set","340a030e":"#### Building our fourth model","0ea6cd93":"#### Building our first model ","a477f566":"### Data Analysis\n- Analyzing a few continuous columns\n- Analyzing a few categorical columns","7fb324f7":"#### Building our second model","c4760759":"#### Dropping Duplicates & Basic Cleaning","9aa235cb":"#### <b>It seems that the model can predict with sensitivity 77%.<\/b>","2cee86b4":"#### Running RFE to check parameter significance","5224991b":"#### Handle the \u201cSelect\u201d level that is present in many of the categorical variables.","165d5230":"#### Sourcing File","48c14924":"#### Drop rows that are having high percentage of missing values.","3941570a":"#### Building our final model"}}