{"cell_type":{"fa6e79fc":"code","97e5bbcb":"code","792a0ce8":"code","5364f8b8":"code","e8fcc515":"code","4ddac2d6":"code","6ce5f41b":"code","07c88843":"code","c54205e2":"code","fa428d77":"code","5aef47a4":"code","8ce0bfbc":"code","e25324fa":"code","9b8ccc3c":"code","8569c6fd":"code","a51e8fa0":"code","8d6aff99":"code","a5a85e2a":"code","3cd28999":"markdown","dbeaba67":"markdown","5b98205d":"markdown","46659e17":"markdown","0095b25f":"markdown","c9f93b18":"markdown","f9fb0d0c":"markdown","21ffa52f":"markdown","0ee56329":"markdown","24a9af61":"markdown","77a782c2":"markdown","0ae95fee":"markdown"},"source":{"fa6e79fc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport scipy.signal\nimport scipy.stats\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/\"))\n\n# Any results you write to the current directory are saved as output.","97e5bbcb":"%%time\ntrain = pd.read_csv('..\/input\/LANL-Earthquake-Prediction\/train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})","792a0ce8":"train.head()","5364f8b8":"%matplotlib inline\n\ntrain_acoustic_data_small = train['acoustic_data'].values[::50]\ntrain_time_to_failure_small = train['time_to_failure'].values[::50]\n\nfig, ax1 = plt.subplots(figsize=(16, 8))\nplt.title(\"Trends of acoustic_data and time_to_failure. 2% of data (sampled)\")\nplt.plot(train_acoustic_data_small, color='b')\nax1.set_ylabel('acoustic_data', color='b')\nplt.legend(['acoustic_data'])\nax2 = ax1.twinx()\nplt.plot(train_time_to_failure_small, color='g')\nax2.set_ylabel('time_to_failure', color='g')\nplt.legend(['time_to_failure'], loc=(0.875, 0.9))\nplt.grid(False)\n\ndel train_acoustic_data_small\ndel train_time_to_failure_small","e8fcc515":"%%time\nfeat_mat = np.zeros((15000,23))\nfs = 4000000 # Hz\nwindow_time = 0.0375 # seconds\noffset = 0.01 # seconds\nwindow_size = int(window_time*fs)\n\nfor i in np.arange(0,feat_mat.shape[0]):\n    start = int(i*offset*fs)\n    stop = int(window_size+i*offset*fs)\n    seg = train.iloc[start:stop,0]\n\n    feat_mat[i,0] = np.mean(seg)\n    feat_mat[i,1] = np.var(seg)\n    feat_mat[i,2] = scipy.stats.skew(seg)\n    feat_mat[i,3] = scipy.stats.kurtosis(seg)\n    feat_mat[i,-1] = train.iloc[stop,1]\n    ","4ddac2d6":"lower_perc = np.percentile(train.iloc[:,0],np.arange(1,10))\nupper_perc = np.percentile(train.iloc[:,0],np.arange(91,100))\n\nfor i in np.arange(0,feat_mat.shape[0]):\n    start = int(i*offset*fs)\n    stop = int(window_size+i*offset*fs)\n    seg = train.iloc[start:stop,0]\n    \n    for j in np.arange(0,lower_perc.shape[0]):\n        perc = np.size(np.where(seg>lower_perc[j])[0])\n        feat_mat[i,4+j] = perc\/seg.shape[0] \n        \n    for j in np.arange(0,upper_perc.shape[0]):\n        perc = np.size(np.where(seg>upper_perc[j])[0])\n        feat_mat[i,13+j] = perc\/seg.shape[0] \n\n    ","6ce5f41b":"df = pd.DataFrame(feat_mat,columns=['mean', 'var', 'skew', 'kurt','perc1','perc2','perc3','perc4','perc5','perc6','perc7','perc8','perc9','perc91','perc92','perc93','perc94','perc95','perc96','perc97','perc98','perc99','time_to_failure'],dtype=np.float64)","07c88843":"df.head()","c54205e2":"from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, BaggingRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import r2_score, mean_absolute_error\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.preprocessing import scale\nfrom sklearn.linear_model import LinearRegression, LassoCV, LassoLarsCV\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neural_network import MLPRegressor","fa428d77":"dataset = df.values\n\nX_train, X_test, y_train, y_test = train_test_split(dataset[:,:-1],dataset[:,-1],test_size=0.2)","5aef47a4":"lr = LinearRegression()\nlr.fit(X_train, y_train)\npred_lr = lr.predict(X_test)\n\nprint(\"MAE = \",mean_absolute_error(y_test,pred_lr))","8ce0bfbc":"# Instantiate model with 1000 decision trees\nrf = RandomForestRegressor(n_estimators = 100, random_state = 42)\n\n# Train the model on training data\nrf.fit(X_train, y_train)\n\npred_rf = rf.predict(X_test)\nprint(\"MAE = \",mean_absolute_error(y_test,pred_rf))","e25324fa":"NN = MLPRegressor()\nNN.fit(scale(X_train),y_train)\n\npred_nn = NN.predict(scale(X_test))\n\nprint(\"MAE = \", mean_absolute_error(y_test,pred_nn))","9b8ccc3c":"rf = RandomForestRegressor(n_estimators = 100, random_state = 42)\nlcv = LassoCV()\nllcv = LassoLarsCV()\nlr = LinearRegression()\nNN = MLPRegressor()\n\n# Put as many of the models as you want in this list\nclf_array = [rf,NN,lcv,llcv,lr]","8569c6fd":"for clf in clf_array:\n    clf.fit(scale(X_train),y_train)\n    pred = clf.predict(scale(X_test))\n    vanilla_scores = mean_absolute_error(y_test,pred)\n    \n    bagging_clf = BaggingRegressor(clf, max_samples=0.25, max_features=1.0, random_state=27)\n    bagging_clf.fit(scale(X_train),y_train)\n    pred_bag = bagging_clf.predict(scale(X_test))\n    bag_scores = mean_absolute_error(y_test,pred_bag)\n    \n    print(\"vanilla {}: {}\",clf,vanilla_scores)\n    print(\"bagging {}: {}\",clf,bag_scores)\n\n    ","a51e8fa0":"from tqdm import tqdm_notebook\n\nsubmission = pd.read_csv('..\/input\/LANL-Earthquake-Prediction\/sample_submission.csv', index_col='seg_id', dtype={\"time_to_failure\": np.float32})\n\nX_test = pd.DataFrame(columns=df.columns, dtype=np.float64, index=submission.index)\n\nfor i, seg_id in enumerate(tqdm_notebook(X_test.index)):\n    seg = pd.read_csv('..\/input\/LANL-Earthquake-Prediction\/test\/' + seg_id + '.csv')\n    X_test.loc[seg_id, 'mean'] = np.mean(seg.values)\n    X_test.loc[seg_id, 'var'] = np.var(seg.values)\n    X_test.loc[seg_id, 'skew'] = scipy.stats.skew(seg.values)\n    X_test.loc[seg_id, 'kurt'] = scipy.stats.kurtosis(seg.values)\n    \n    for j in np.arange(0,9):\n        perc = np.size(np.where(seg>lower_perc[j])[0])\n        X_test.loc[seg_id, 'perc{}'.format(j+1)] = perc\/seg.shape[0] \n        \n        perc = np.size(np.where(seg>upper_perc[j])[0])\n        X_test.loc[seg_id, 'perc9{}'.format(j+1)] = perc\/seg.shape[0]\n\n    ","8d6aff99":"X_test.drop(columns=['time_to_failure'],inplace=True)\nX_test.head()","a5a85e2a":"# Pick any model to make predictions and place it where \"rf\" is currently.\n\nrf.fit(scale(dataset[:,:-1]),dataset[:,-1])\n\npredictions = rf.predict(scale(X_test))\n\nsubmission['time_to_failure'] = predictions\nsubmission.to_csv('submission.csv')","3cd28999":"## 4c) Neural Network\nSee https:\/\/machinelearningmastery.com\/regression-tutorial-keras-deep-learning-library-python\/","dbeaba67":"# 4) Apply models","5b98205d":"## 4a) Linear Regression","46659e17":"# 3) Generate the features\nHere I will calculate some features from a moving window similar to https:\/\/doi.org\/10.1002\/2017GL074677\n\nFeatures: Mean, variance, skewness, kurtosis (normalized and not)\n\nBEWARE OF LARGE DATA! You can't load the entire dataset into memory at once (At least I couldn't on my laptop).\n\n<a href=\"http:\/\/tinypic.com?ref=vcut1j\" target=\"_blank\"><img src=\"http:\/\/i66.tinypic.com\/vcut1j.jpg\" border=\"0\"><\/a>","0095b25f":"# Earthquake Prediction\nIn this kernel I will explore the data and try to use some different models to predict the time of the \"earthquake\". This is my first kernel so any feedback is appreciated!","c9f93b18":"# 1) Load the Data","f9fb0d0c":"### 4di) Bagging\nMake a bunch of models and give them to the BaggingRegressor to see if it improves the score.","21ffa52f":"# Load the test data and make predictions","0ee56329":"## 4b) Random Forest\n\nPrimer on Random Forest:\n\nhttps:\/\/towardsdatascience.com\/random-forest-in-python-24d0893d51c0","24a9af61":"# 2) Visualize the Data\nI like to start by looking at the data, which in this case is a pretty simple time series.","77a782c2":"## 4d) Ensemble methods","0ae95fee":"Next, we will calculate percentiles and add the time the signal spends above those \"thresholds\" as a feature. This is intuitive because you can see from the data, as the earthquake comes closer, the signal goes through bouts of higher magnitude. That is, the time the signal spends in the 91st percentile, for example, should go up as time_to_failure goes down. In the paper cited above, they calculate features based on the time spent above a certain threshold (1st to 9th and 91st to 99th)."}}