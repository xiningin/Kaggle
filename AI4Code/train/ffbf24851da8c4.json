{"cell_type":{"044e7b3a":"code","7d07d21e":"code","43130185":"code","8e0bb480":"code","628ca375":"code","c43887af":"code","4fb5bcd9":"code","db372c1e":"code","2ce51153":"code","75efc4f9":"code","57d0a42a":"code","381a11b6":"code","24cbf2d5":"code","c168862b":"code","3a32d3e3":"code","a7e9a415":"code","2aac4ddd":"code","968a9d4f":"code","8936aa51":"code","18a7d6af":"code","78efb137":"code","2c0acc4e":"code","362b7dbf":"code","20a62f60":"code","b5c9bc20":"code","9d67b377":"code","ee97d82e":"code","38514d0d":"code","6a3a7b35":"code","dfb87c6b":"code","c9996d38":"code","d9d0a6be":"code","14cc4ed5":"code","05dd63e6":"code","b3e5dbb3":"code","189030ae":"code","91504abe":"code","bdc3689f":"code","a097fcac":"code","032b6545":"code","b70e613a":"code","5e081299":"code","1a7bd3cb":"code","84ca9e34":"code","c591c747":"code","d7aec916":"code","c320cb88":"code","59b48986":"code","c895d805":"code","779f8598":"code","3f4cf734":"code","697ba7d3":"code","26180bd8":"code","b88cbd36":"code","9a1b28a4":"code","071e526d":"code","50017d22":"code","128b42c6":"markdown","8c4b139d":"markdown","8760f404":"markdown","d932c47c":"markdown","c2066402":"markdown","a44a8b4a":"markdown","c489f269":"markdown","1f549424":"markdown","6d924bf8":"markdown","74038abb":"markdown","4f00cba5":"markdown","7b5274d8":"markdown","c0fac74d":"markdown","e1635060":"markdown","2463b9b7":"markdown","2fadc329":"markdown","a3616ae4":"markdown","47f4bca8":"markdown","d9104f80":"markdown","f6bdac03":"markdown","1816a1db":"markdown","f29549d5":"markdown","47b3614a":"markdown","0b289740":"markdown"},"source":{"044e7b3a":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","7d07d21e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)","43130185":"df = pd.read_csv('..\/input\/hotel-booking-demand\/hotel_bookings.csv')\ndf.head()","8e0bb480":"df.info()","628ca375":"nulls = df.isnull().sum()\nnulls[nulls > 0]\n#currently only 4 columns has missing values","c43887af":"df = df.replace('Undefined', np.NaN)\n# there are few coloumns have Undefined value instead of NaN,\n#replace 'Undefined' to NaN in the dataframe","4fb5bcd9":"#calculate the total missing values across the whole dataset\ndf.isnull().sum().sum()\/(len(df.index)*31)\n","db372c1e":"percentage = df.isnull().sum()\/ len(df)\npercentage.sort_values(ascending=False).head()","2ce51153":"#company has 94.4% missing values, not helpful, drop the column\ndf.drop(['company'], axis=1, inplace=True)","75efc4f9":"# reservation_status_date contain a lot of variety \ndf.drop(['reservation_status_date'], axis=1, inplace=True)  # objects & 926 varieties \n# reservation_status includes 'Canceled' feature \n#By keeping reservation_status in data,\n# it is possible to achieve 100% accuracy rate because that feature is direct way to predict cancellations\n# so, drop the reservation_status coumns\ndf.drop(['reservation_status'], axis=1, inplace=True)\n","57d0a42a":"df = df.dropna()","381a11b6":"#transform column to binary value\ndf['hotel'] = df['hotel'].map({'Resort Hotel':0, 'City Hotel':1}).astype(int)\n\ndf['arrival_date_month'] = df['arrival_date_month'].map({'January':1, 'February': 2, 'March':3, 'April':4, 'May':5, 'June':6, 'July':7,\n                                                            'August':8, 'September':9, 'October':10, 'November':11, 'December':12}).astype(int)","24cbf2d5":"# Since country colomn has high varity data,so need to transfer to numerical data\n# transfer to catergorical data first, then transfer to numeric data\ndf['country'] = df['country'].astype('category')\ndf['country'] = df['country'].cat.codes","c168862b":"#inspect data again\ndf.info()","3a32d3e3":"# create new colomns: 'is_family' , 'deposit_given', 'total_nights'\ndef family_check(df):\n    if ((df['adults'] > 0) & (df['children'] > 0)):\n        val = 1\n    elif ((df['adults'] > 0) & (df['babies'] > 0)):\n        val = 1\n    else:\n        val = 0\n    return val\n\ndef deposit(df):\n    if ((df['deposit_type'] == 'No Deposit') | (df['deposit_type'] == 'Refundable')):\n        return 0\n    else:\n        return 1\n    \ndef previous_cancellations_check(df):\n    if df['previous_cancellations'] == 0:\n        return 0\n    else:\n        return 1\n    \ndef previous_bookings_not_canceled_check(df):\n    if df['previous_bookings_not_canceled'] == 0:\n        return 1\n    else:\n        return 0    \n    \ndef booking_changed_check(df):\n    if df['booking_changes'] == 0:\n        return 0\n    else:\n        return 1    \n    \ndef feature(df):\n    # create new column 'is_family' base on 'adults', 'children', 'babies'\n    df['is_family'] = df.apply(family_check, axis = 1)\n    # create new column 'deposit_given' base on 'deposit_type'\n    df['deposit_given'] = df.apply(deposit, axis=1)\n    df['previous_cancelled'] = df.apply(previous_cancellations_check, axis=1)\n    df['previous_bookings_not_canceled_check'] = df.apply(previous_bookings_not_canceled_check, axis=1)\n    df['booking_changed'] = df.apply(booking_changed_check, axis=1)\n    # create new column 'total_nights' base on 'stays_in_weekend_nights' and 'stays_in_week_nights'\n    df['total_nights'] = df['stays_in_weekend_nights']+ df['stays_in_week_nights']\n    df['booking_times'] = df['previous_cancellations'] + df['previous_bookings_not_canceled']\n    return df\n\ndf = feature(df)","a7e9a415":"# since we create 'deposit_given' column from 'deposit_type', so can drop 'deposit_type'\ndf.drop(['deposit_type'], axis=1, inplace=True)\ndf.drop(['previous_cancellations'], axis=1, inplace=True)\ndf.drop(['previous_bookings_not_canceled'], axis=1, inplace=True)\ndf.drop(['booking_changes'], axis=1, inplace=True)","2aac4ddd":"# check the correlation in the dataset with \"is_canceled\"\ncorr_matrix = df.corr()\ncorr_matrix['is_canceled'].sort_values(ascending=False)\n# can see 'arrival_date_month','arrival_date_week_number','arrival_date_year',\n#          'children','stays_in_week_nights','arrival_date_day_of_month','total_nights'\n# have very low correlation(< 0.01%) with 'is_cancelled', whcih is not significant, \n# so it a good idea to drop these columns for further prediction","968a9d4f":"# drop the colomns with low correlation(< 0.01%) \n\ndf.drop(['arrival_date_month','arrival_date_week_number',\n          'stays_in_week_nights','children','arrival_date_year','total_nights',\n        'arrival_date_day_of_month'], axis=1, inplace=True)","8936aa51":"df.info()","18a7d6af":"df = pd.get_dummies(data = df, columns = ['meal', 'market_segment', 'distribution_channel',\n                                            'reserved_room_type', 'assigned_room_type', 'customer_type'])","78efb137":"#inspect data\ndf.shape\ndf.describe()","2c0acc4e":"# feature set and targer set\nX = df.drop('is_canceled', axis = 1)\ny = df['is_canceled']","362b7dbf":"# split into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)","20a62f60":"#data scalering - StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","b5c9bc20":"#GridsearchCV and cross validation searching for KNN hypterparameter\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'n_neighbors': np.arange(1, 11)}\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, param_grid, cv=5,return_train_score = False,n_jobs = -1)# return_train_score = True,\nknn_cv.fit(X_train, y_train)\nknn_cv.best_params_","9d67b377":"import pandas as pd\nknn_cv_result = pd.DataFrame(knn_cv.cv_results_)\nknn_cv_result.head()","ee97d82e":"#k=6 is the best hyperparameter, applied this value in the model\nclf = KNeighborsClassifier(n_neighbors=6,n_jobs = -1)\nclf.fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(clf.score(X_train, y_train)))\nprint(\"Test set accuracy: {:.3f}\".format(clf.score(X_test, y_test)))\nprint(\"Best parameters: {}\".format(knn_cv.best_params_))\nprint(\"Best cross-validation score: {:.4f}\".format(knn_cv.best_score_))","38514d0d":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\ngrid={\"C\":[0.001, 0.01, 0.1, 1, 10, 100, 1000], \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\nlogreg=LogisticRegression(solver='liblinear')\nlogreg_cv=GridSearchCV(logreg,grid,cv=5,return_train_score = False ,n_jobs = -1)\nlogreg_cv.fit(X_train,y_train)\nlogreg_cv.best_params_","6a3a7b35":"import pandas as pd\nlogreg_cv_result = pd.DataFrame(logreg_cv.cv_results_)\nlogreg_cv_result.head()","dfb87c6b":"#use best parameter C value \nlogreg1 = LogisticRegression(C=10, penalty = 'l1',solver='liblinear',random_state=0).fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(logreg1.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(logreg1.score(X_test, y_test)))\nprint(\"Best parameters: {}\".format(logreg_cv.best_params_))\nprint(\"Best cross-validation score: {:.4f}\".format(logreg_cv.best_score_))","c9996d38":"#select best hyperparameter #slow\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import LinearSVC\nCs = [0.1, 1, 10, 100]\nparam_grid = {'C': Cs}\nlinearSVC = GridSearchCV(LinearSVC(max_iter=500000), param_grid, cv=5,return_train_score = False,n_jobs = -1)\nlinearSVC.fit(X_train, y_train)\nlinearSVC.best_params_","d9d0a6be":"import pandas as pd\nlinearSVC_result = pd.DataFrame(linearSVC.cv_results_)\nlinearSVC_result.head()","14cc4ed5":"#use best parameter C value \nsvm = LinearSVC(C=10).fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(svm.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(svm.score(X_test, y_test)))\nprint(\"Best parameters: {}\".format(linearSVC.best_params_))\nprint(\"Best cross-validation score: {:.4f}\".format(linearSVC.best_score_))","05dd63e6":"#linear hyperparameter selection \nfrom sklearn.svm import SVC\nCs = [0.01, 0.1, 1, 10, 100]\nparam_grid = {'C': Cs}\nkerenl_lin = GridSearchCV(SVC(kernel='linear'), param_grid, cv=5,return_train_score = False,n_jobs = -1)\nkerenl_lin.fit(X_train, y_train)\nprint(\"The best classifier is: \", kerenl_lin.best_params_)","b3e5dbb3":"# GridSearchCV(cv=5, estimator=SVC(kernel='linear'), n_jobs=-1,\n#              param_grid={'C': [0.01, 0.1, 1, 10, 100]})\n# The best classifier is:  {'C': 100}","189030ae":"import pandas as pd\nkerenl_lin_result = pd.DataFrame(kerenl_lin.cv_results_)\nkerenl_lin_result.head()","91504abe":"#use best parameter C value \nsvc = SVC(kernel='linear', C=100,gamma='auto').fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(svc.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(svc.score(X_test, y_test)))\nprint(\"Best parameters: {}\".format(kerenl_lin.best_params_))\nprint(\"Best cross-validation score: {:.4f}\".format(kerenl_lin.best_score_))","bdc3689f":"# Training set score: 0.797\n# Test set score: 0.799\n# Best parameters: {'C': 100}\n# Best cross-validation score: 0.7962","a097fcac":"#rbf hypterparameter selection \nC_range = [0.001, 0.01, 0.1, 1, 10, 100]\nparam_grid = {'C': C_range}\nkernel_rbf = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5,return_train_score = False, n_jobs = -1)\nkernel_rbf.fit(X_train, y_train)\nprint(\"The best classifier is: \", kernel_rbf.best_estimator_)","032b6545":"# GridSearchCV(cv=5, estimator=SVC(), n_jobs=-1,\n#              param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100]})\n# The best classifier is:  SVC(C=100)","b70e613a":"import pandas as pd\nkernel_rbf_result = pd.DataFrame(kernel_rbf.cv_results_)\nkernel_rbf_result.head()","5e081299":"#use best parameter C value \nsvc = SVC(kernel='rbf', C=100,gamma='auto').fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(svc.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(svc.score(X_test, y_test)))\nprint(\"Best parameters: {}\".format(kernel_rbf.best_params_))\nprint(\"Best cross-validation score: {:.4f}\".format(kernel_rbf.best_score_))","1a7bd3cb":"# Training set score: 0.822\n# Test set score: 0.825\n# Best parameters: {'C': 100}\n# Best cross-validation score: 0.8232","84ca9e34":"#poly hyperparameter selection \nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\nCs = [0.1, 1, 10, 100]\nparam_grid = {'C': Cs}\nkernel_poly = GridSearchCV(SVC(kernel='poly'), param_grid, cv=3,return_train_score = False,n_jobs = -1)\nkernel_poly.fit(X_train, y_train)\nprint(\"The best classifier is: \", kernel_poly.best_params_)","c591c747":"# GridSearchCV(cv=3, estimator=SVC(kernel='poly'), n_jobs=-1,\n#              param_grid={'C': [0.1, 1, 10, 100]})\n# The best classifier is:  {'C': 100}","d7aec916":"import pandas as pd\nkernel_poly_result = pd.DataFrame(kernel_poly.cv_results_)\nkernel_poly_result.head()","c320cb88":"#use best parameter C value \nsvc = SVC(kernel = 'poly',C=100,gamma='auto').fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(svc.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(svc.score(X_test, y_test)))\nprint(\"Best parameters: {}\".format(kernel_poly.best_params_))\nprint(\"Best cross-validation score: {:.4f}\".format(kernel_poly.best_score_))","59b48986":"# Training set score: 0.815\n# Test set score: 0.817\n# Best parameters: {'C': 100}\n# Best cross-validation score: 0.8228","c895d805":"#decision tree hyperparameter\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nparameters={'min_samples_split' : range(10,500,20),'max_depth': range(1,20,2)}\nclf_tree=DecisionTreeClassifier()\ngrid_search=GridSearchCV(clf_tree,parameters, cv=10,return_train_score = False,n_jobs = -1)\ngrid_search.fit(X_train, y_train)\nprint(\"The best classifier is: \", grid_search.best_params_)","779f8598":"# GridSearchCV(cv=10, estimator=DecisionTreeClassifier(), n_jobs=-1,\n#              param_grid={'max_depth': range(1, 20, 2),\n#                          'min_samples_split': range(10, 500, 20)})\n# The best classifier is:  {'max_depth': 19, 'min_samples_split': 10}","3f4cf734":"import pandas as pd\ngrid_search_result = pd.DataFrame(grid_search.cv_results_)\ngrid_search_result.head()","697ba7d3":"#use best parameters values \nclf_tree=DecisionTreeClassifier(max_depth=19,min_samples_split=10).fit(X_train, y_train)\nprint(\"Training clf_treeset score: {:.3f}\".format(clf_tree.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(clf_tree.score(X_test, y_test)))\nprint(\"Best parameters: {}\".format(grid_search.best_params_))\nprint(\"Best cross-validation score: {:.4f}\".format(grid_search.best_score_))","26180bd8":"# Training clf_treeset score: 0.888\n# Test set score: 0.849\n# Best parameters: {'max_depth': 19, 'min_samples_split': 10}\n# Best cross-validation score: 0.8478","b88cbd36":"# random forest hyperparameter\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nparameters={'min_samples_split' : range(10,500,20),'max_depth': range(1,20,2)}\nclf_treeR=RandomForestClassifier()\ngrid_searchR=GridSearchCV(clf_treeR,parameters, cv=10,return_train_score = False,n_jobs = -1)\ngrid_searchR.fit(X_train, y_train)\nprint(\"The best classifier is: \", grid_searchR.best_params_)","9a1b28a4":"import pandas as pd\ngrid_searchR_result = pd.DataFrame(grid_searchR.cv_results_)\ngrid_searchR_result.head()","071e526d":"#use best parameters values \nclf_treeR=RandomForestClassifier(max_depth=19,min_samples_split=10).fit(X_train, y_train)\nprint(\"Training clf_treeset score: {:.3f}\".format(clf_treeR.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(clf_treeR.score(X_test, y_test)))\nprint(\"Best parameters: {}\".format(grid_searchR.best_params_))\nprint(\"Best cross-validation score: {:.4f}\".format(grid_searchR.best_score_))","50017d22":"from sklearn.metrics import accuracy_score, roc_auc_score\n\ny_pred = grid_searchR.predict(X_test)\nprint('accuracy_score: ', accuracy_score(y_test, y_pred))\nprint('roc_auc_score: ', roc_auc_score(y_test, grid_searchR.predict_proba(X_test)[:,1]))","128b42c6":"# 3. Classification","8c4b139d":"##### 2. Logistic Regression:  train  0.794,  test 0.796, Best cross-validation score 0.7935","8760f404":"# 1. Data Inspectation","d932c47c":"#### (1) Drop unhelpful columns & rows","c2066402":"##### 4.Kerenilzed Support Vector Machine (rbf, poly, and linear):   \n##### (1) Linear train 0.797, test 0.799, Best cross-validation score: 0.7962 \n##### (2) Rbf train 0.822, test 0.825, Best cross-validation score: 0.8232\n##### (3) Poly train 0.815 test 0.817, Best cross-validation score: 0.8228","a44a8b4a":"# 4. Find the best model","c489f269":"### (3) Linear Support Vector Machine","1f549424":"#### 5. decision tree: train 0.888  test 0.849, Best cross-validation score: 0.8478","6d924bf8":"##### Choosing scaling\n##### use MixMax scaling in order to normalize the data set. The data in our data set are spread across a wide range of values, which might result in various features affecting the final result more than the other feature. MixMax scaling reduces this effect by re-scaling the data to a specificed range of values, in this case 0-1","74038abb":"# 2. Data Preprocessing for missing values","4f00cba5":"### (2) Logistic Regression","7b5274d8":"#### (4) Data scaling","c0fac74d":"#### 6. random forest: train 0.881  test 0.864, Best cross-validation score: 0.8617","e1635060":"### Random forest model has the highest training and test score. In addition, it also has the highest Best cross-validation score-- 0.8617. Therefore, Random forest model is the best option","2463b9b7":"####  Evaluation strategy: The training set score and test set scores are close, the highest test set score among all models is better","2fadc329":"### (5) Decision Tree","a3616ae4":"#### (3). Data Preprocessing for ML","47f4bca8":"### (6) Random Forest","d9104f80":"###  (1) KNN Classification","f6bdac03":"### (4) Kerenilzed Support Vector Machine (rbf, poly, and linear)","1816a1db":"Acknowledgements\n\nThe data is originally from the article Hotel Booking Demand Datasets, written by Nuno Antonio, Ana Almeida, and Luis Nunes for Data in Brief, Volume 22, February 2019.https:\/\/www.sciencedirect.com\/science\/article\/pii\/S2352340918315191\n\nThe data was downloaded and cleaned by Thomas Mock and Antoine Bichat for #TidyTuesday during the week of February 11th, 2020.https:\/\/github.com\/rfordatascience\/tidytuesday\/blob\/master\/data\/2020\/2020-02-11\/readme.md\n\nraw data: https:\/\/raw.githubusercontent.com\/rfordatascience\/tidytuesday\/master\/data\/2020\/2020-02-11\/hotels.csv\n\nTask: cluster cancel and non- cancel  \n\nReference:\nhttps:\/\/www.kaggle.com\/jessemostipak\/hotel-booking-demand\/notebooks\n","f29549d5":"##### 3. Linear Support Vector Machine : train 0.794, test 0.795, Best cross-validation score: 0.7932","47b3614a":"##### 1. Knn: train 0.873, test 0.835, Best cross-validation score 0.8286","0b289740":"#### (2) inspect all the columns unqiue values for replacing the missing values"}}