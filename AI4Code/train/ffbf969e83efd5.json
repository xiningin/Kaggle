{"cell_type":{"6aed480e":"code","fc07ddf5":"code","46c5f974":"code","5ab77272":"code","e0775546":"code","2cf4f177":"code","220fe030":"code","e620208b":"code","6aa8ee3a":"code","e950fc95":"code","4f261b06":"code","7847e977":"code","4ff852bf":"code","75d3f4eb":"code","a5f33829":"code","7c4ca2a2":"code","54ceda82":"code","55f5e408":"code","0c42b131":"code","86d6b61d":"code","63ccda6c":"code","4a079f11":"code","f1dcb80a":"code","9cb2d8be":"code","dc3a0ce0":"code","601d63c2":"code","ad78c068":"code","46ca46fe":"code","ecb58cef":"code","6a35e69b":"code","726ce632":"code","ececc1ec":"code","4a6c1edc":"code","6056fb29":"code","bede504b":"code","fbaab2a3":"code","d8833239":"code","f7257d66":"code","0661abcd":"code","0188aabe":"code","a298c3b1":"code","977dba6a":"code","45cbb2e6":"code","98654690":"code","d1bd5004":"code","7a02524a":"code","a77267d3":"code","666b44ed":"code","19d0dff4":"code","8fd5c1a7":"markdown","2abad9f7":"markdown","6aa0c9af":"markdown","d817399c":"markdown","a9d4a7e9":"markdown","81c5c5fe":"markdown","94510fcf":"markdown","3ba08d84":"markdown","6b4880c9":"markdown","e15cb794":"markdown","7f10afa6":"markdown","1f6955af":"markdown","ac546ba8":"markdown","3a2ffbfe":"markdown","72d7fb52":"markdown","e76eb22d":"markdown","37017702":"markdown","5bd60281":"markdown","f2937939":"markdown","ef2b2cc5":"markdown","80f50a05":"markdown","054cb9f4":"markdown","86e39244":"markdown","1ef894ed":"markdown","8aae1977":"markdown","6822ecc2":"markdown","b37c5079":"markdown","ef857d89":"markdown","8f30779a":"markdown","73bf6ec5":"markdown"},"source":{"6aed480e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport string\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fc07ddf5":"import warnings\nfrom math import pi\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom IPython.display import HTML,display\n\nsns.set(style=\"whitegrid\", font_scale=1.75)\n\n\n# prettify plots\nplt.rcParams['figure.figsize'] = [20.0, 5.0]\n    \n%matplotlib inline\n\nwarnings.filterwarnings(\"ignore\")\n\npercentiles = [value\/100 for value in range(10, 100, 10)] + [0.05, 0.25, 0.75, 0.95]","46c5f974":"EXTENDED_DATA_FOLDER='\/kaggle\/input\/tweet-sentiment-extraction-extended'","5ab77272":"def reduce_memory_footprint(dataset: pd.DataFrame, columns_to_convert: list = [], drop_na: bool=False) -> pd.DataFrame:\n    columns_converted = []\n    columns_failed = []\n    \n    if not columns_to_convert:\n        columns_to_convert = dataset.columns\n\n    if drop_na:\n        dataset = dataset.dropna()\n\n    for each_col in columns_to_convert:\n        if hasattr(dataset[each_col], 'dtype'):\n            if dataset[each_col].dtype is np.dtype('O'):\n                dataset[each_col] = dataset[each_col].astype('category')\n                columns_converted.append(each_col)\n        else:\n            columns_failed.append(each_col)\n        \n    print(f'Columns converted to type category: {columns_converted}')\n    print(f'Columns that do NOT have the attribute \"dtype\": {columns_failed}')\n    return dataset","e0775546":"training_dataset = reduce_memory_footprint(pd.read_csv(f'{EXTENDED_DATA_FOLDER}\/train.csv'), drop_na=True)\ntest_dataset = reduce_memory_footprint(pd.read_csv(f'{EXTENDED_DATA_FOLDER}\/test.csv'), drop_na=True)","2cf4f177":"def jaccard_score(str1: str, str2: str) -> float: \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","220fe030":"def is_selected_text_in_text(dataset: pd.DataFrame) -> list:\n    results = []\n    for text_str, selected_text_str in zip(dataset['text'].values, dataset['selected_text'].values):\n        text_str, selected_text_str = str(text_str), str(selected_text_str)\n        if selected_text_str in text_str:\n            results.append('Yes') \n        else:\n            results.append('No')\n            \n    return results\n\ntraining_dataset['is_selected_text_in_text'] = is_selected_text_in_text(training_dataset)","e620208b":"def plot_chart(dataframe, width=20.0, height=7.0, title='<No title assigned>', \n               ylabel_title='<No xlabel title assigned>', stacked=False):\n    plt.rcParams['figure.figsize'] = [width, height]\n    font = {'size': 16}\n    matplotlib.rc('font', **font)\n    ax = dataframe.plot(kind='barh', stacked=stacked, title=title)\n    ax.set_ylabel(ylabel_title)","6aa8ee3a":"def plot_sns_chart(dataframe, y_axis, class_separator=None, width=20.0, height=9.0, \n                   font_scale=2, title = 'Title has not been set',\n                   xlabel_title=\"Xlabel title not been set\", ylabel_title=\"Ylabel title not been set\"):\n    plt.rcParams['figure.figsize'] = [width, height]\n    sns.set(font_scale=font_scale)\n    g = sns.countplot(y=y_axis, hue=class_separator, data=dataframe)\n    g.set(xlabel=xlabel_title, ylabel=ylabel_title)\n    g.set_title(title)\n    total = float(len(dataframe))\n    margin = 0.0025\n    for patch in g.patches:\n        percentage = ' {:.1f}%'.format(100 * patch.get_width() \/ total)\n        x = patch.get_width() + (patch.get_width() * margin)\n        y = patch.get_y() + (patch.get_height() \/ 2) \n        g.annotate(percentage, (x, y), va='center') ","e950fc95":"plot_sns_chart(training_dataset, 'is_selected_text_in_text', width=35.0, height=2.0, \n               title=f'Is the text in selected_text present in the text column.',\n               ylabel_title='Is selected_text present in text', \n               xlabel_title=f'Number of rows. Total rows {training_dataset.shape[0]}')","4f261b06":"def apply_jaccard_similarity(selected_text_col, text_col) -> float:\n    selected_texts = selected_text_col.values\n    texts = text_col.values\n    scores = []\n    for selected_text, text in zip(selected_texts, texts):\n        scores.append(jaccard_score(selected_text, text))\n    return scores    ","7847e977":"training_dataset['selected_text_and_text_jaccard_similarity'] = apply_jaccard_similarity(training_dataset['selected_text'], training_dataset['text'])","4ff852bf":"plt.rcParams['figure.figsize'] = [20, 10]\ntraining_dataset['selected_text_and_text_jaccard_similarity'].hist()\nplt.xlabel('Jaccard similarity score') \nplt.title('Jaccard similarity between selected_text and text (overall)')","75d3f4eb":"_, plots = plt.subplots(3,1)\nplt.rcParams['figure.figsize'] = [20, 30]\n\nsentiment = training_dataset['sentiment'] == 'positive'\ntraining_dataset['selected_text_and_text_jaccard_similarity'][sentiment].hist(ax=plots[0])\nplots[0].set_title('positive')\n\nsentiment = training_dataset['sentiment'] == 'neutral'\ntraining_dataset['selected_text_and_text_jaccard_similarity'][sentiment].hist(ax=plots[1])\nplots[1].set_title('neutral')\n\nsentiment = training_dataset['sentiment'] == 'negative'\ntraining_dataset['selected_text_and_text_jaccard_similarity'][sentiment].hist(ax=plots[2])\nplots[2].set_title('negative')\n\nplt.xlabel('Jaccard similarity between selected_text and text (by sentiments)')\nplt.ylabel('Jaccard similarity score') \nplt.tight_layout()","a5f33829":"jaccard_similarity_by_sentiment_pivot_table = training_dataset.pivot_table('selected_text_and_text_jaccard_similarity', 'sentiment')\njaccard_similarity_by_sentiment_pivot_table = jaccard_similarity_by_sentiment_pivot_table.T.reset_index(drop=True)","7c4ca2a2":"plt.rcParams['figure.figsize'] = [20, 5]\ng = jaccard_similarity_by_sentiment_pivot_table.plot(kind='barh')\nsns.set(font_scale=2)\nplt.xlabel('Jaccard similarity') \nplt.ylabel('Sentiments') \n\nplt.title('Jaccard similarity between selected_text and text (by sentiments)')\ntotal = 3.0\nmargin = 0.00001\nfor patch, column in zip(g.patches, jaccard_similarity_by_sentiment_pivot_table.columns):\n    value = jaccard_similarity_by_sentiment_pivot_table[0:1][column][0]\n    percentage = ' {:.1f}%'.format(value * 100)\n    x = patch.get_width() + (patch.get_width() * margin)\n    y = patch.get_y() + (patch.get_height() \/ 2) \n    g.annotate(percentage, (x, y), va='center') \nplt.legend(loc='center')","54ceda82":"selected_repeat_columns = [\n    'repeated_letters_count', 'repeated_digits_count', 'repeated_spaces_count', \n    'repeated_whitespaces_count', 'repeated_punctuations_count', 'english_characters_count', \n    'non_english_characters_count'\n]","55f5e408":"profiled_train_text_sentiment = pd.read_csv(f'{EXTENDED_DATA_FOLDER}\/profiled_train_text_sentiment.csv')\nprofiled_train_text_ease_of_reading = pd.read_csv(f'{EXTENDED_DATA_FOLDER}\/profiled_train_text_ease_of_reading.csv')\nprofiled_train_text_grammar = pd.read_csv(f'{EXTENDED_DATA_FOLDER}\/profiled_train_text_grammar_check.csv')\nprofiled_train_text_spelling = pd.read_csv(f'{EXTENDED_DATA_FOLDER}\/profiled_train_text_spelling_check.csv')\nprofiled_train_text_granular = pd.read_csv(f'{EXTENDED_DATA_FOLDER}\/profiled_train_text_granular_features.csv')","0c42b131":"profiled_train_text = pd.concat([training_dataset[['text', 'sentiment']], profiled_train_text_sentiment, profiled_train_text_ease_of_reading, \n                                 profiled_train_text_grammar, profiled_train_text_spelling, profiled_train_text_granular], axis=1)\nprofiled_train_text = reduce_memory_footprint(profiled_train_text)\nprofiled_train_text['sentiment_polarity_summarised'] = profiled_train_text['sentiment_polarity_summarised'].apply(lambda x: x.lower())","86d6b61d":"profiled_train_text","63ccda6c":"def check_if_sentiment_of_text_matches(dataset: pd.DataFrame) -> list:\n    result = []\n    for sentiment_str, sentiment_polarity_summarised_str in zip(dataset.sentiment.values, dataset.sentiment_polarity_summarised.values):\n        sentiment_str, sentiment_polarity_summarised_str = str(sentiment_str).strip(), str(sentiment_polarity_summarised_str).strip()\n        if sentiment_str == sentiment_polarity_summarised_str:\n            result.append('Yes')\n        else:\n            result.append('No')\n    return result\n","4a079f11":"profiled_train_text['do_sentiments_match'] = check_if_sentiment_of_text_matches(profiled_train_text)\nprofiled_train_text['do_sentiments_match'] = profiled_train_text['do_sentiments_match'].astype('category')","f1dcb80a":"plot_sns_chart(profiled_train_text, 'do_sentiments_match', width=35.0, height=3.0, \n               title=f'Do the sentiments for \"text\" provided match-up.',\n               ylabel_title='Sentiments match', \n               xlabel_title=f'Number of rows. Total rows {profiled_train_text.shape[0]}')","9cb2d8be":"plot_sns_chart(profiled_train_text, 'sentiment', class_separator='do_sentiments_match', width=35.0, height=8.0, \n               title=f'Do the sentiments for \"text\" provided match-up (grouped by Did sentiments match).',\n               ylabel_title='Sentiments match', \n               xlabel_title=f'Number of rows. Total rows {profiled_train_text.shape[0]}')","dc3a0ce0":"plot_sns_chart(profiled_train_text, 'spelling_quality_summarised', width=35.0, height=4.0, \n               title=f'Are the spellings correct in \"text\".',\n               ylabel_title='Spelling quality', \n               xlabel_title=f'Number of rows. Total rows {profiled_train_text.shape[0]}')","601d63c2":"plot_sns_chart(profiled_train_text, 'sentiment', class_separator='spelling_quality_summarised', width=35.0, height=8.0, \n               title=f'Are the spellings correct in \"text\" (grouped by Spelling Quality).',\n               ylabel_title='Sentiments match', \n               xlabel_title=f'Number of rows. Total rows {profiled_train_text.shape[0]}')","ad78c068":"plot_sns_chart(profiled_train_text, 'ease_of_reading_summarised', width=35.0, height=6.0, \n               title=f'Is the text in \"text\" column easy to read',\n               ylabel_title='Ease of reading', \n               xlabel_title=f'Number of rows. Total rows {profiled_train_text.shape[0]}')","46ca46fe":"plot_sns_chart(profiled_train_text, 'sentiment', class_separator='ease_of_reading_summarised', width=35.0, height=12.0, \n               title=f'Is the text in \"text\" column easy to read (grouped by Ease of Reading Quality).',\n               ylabel_title='Ease of reading', \n               xlabel_title=f'Number of rows. Total rows {profiled_train_text.shape[0]}')","ecb58cef":"def plot_repeated_entities_groups(dataset):\n    new_dataset = dataset.copy()\n    shortened_column_names = ['letters', 'digits', 'spaces', 'whitespaces', \n                              'punctuations', 'english', 'non-english']\n    _, plots = plt.subplots(len(selected_repeat_columns), 1)\n    plt.rcParams['figure.figsize'] = [20.0, 12.5]\n    filter_greater_than_zero = new_dataset[selected_repeat_columns] > 0\n    filtered_dataset = new_dataset[filter_greater_than_zero]\n    for index, each_column in enumerate(selected_repeat_columns):\n        g = filtered_dataset[each_column].hist(ax=plots[index], bins=5)\n        g.set_ylabel(shortened_column_names[index])\n\n    plt.xlabel('Number of groups of repeated entities')\n    plt.tight_layout()\n    \n    del new_dataset","6a35e69b":"filter_columns = profiled_train_text[selected_repeat_columns] > 0\nprofiled_train_text[filter_columns][selected_repeat_columns].describe(percentiles=percentiles)","726ce632":"plot_repeated_entities_groups(profiled_train_text)","ececc1ec":"profiled_train_selected_text_sentiment = pd.read_csv(f'{EXTENDED_DATA_FOLDER}\/profiled_train_selected_text_sentiment.csv')\nprofiled_train_selected_text_ease_of_reading = pd.read_csv(f'{EXTENDED_DATA_FOLDER}\/profiled_train_selected_text_ease_of_reading.csv')\nprofiled_train_selected_text_grammar = pd.read_csv(f'{EXTENDED_DATA_FOLDER}\/profiled_train_selected_text_grammar_check.csv')\nprofiled_train_selected_text_spelling = pd.read_csv(f'{EXTENDED_DATA_FOLDER}\/profiled_train_selected_text_spelling_check.csv')\nprofiled_train_selected_text_granular = pd.read_csv(f'{EXTENDED_DATA_FOLDER}\/profiled_train_selected_text_granular_features.csv')","4a6c1edc":"profiled_train_selected_text = pd.concat([training_dataset['selected_text'], profiled_train_selected_text_sentiment,\n                                          profiled_train_selected_text_ease_of_reading, profiled_train_selected_text_grammar, \n                                          profiled_train_selected_text_spelling, profiled_train_selected_text_granular], axis=1)\nprofiled_train_selected_text = reduce_memory_footprint(profiled_train_selected_text)","6056fb29":"profiled_train_selected_text","bede504b":"filter_columns = profiled_train_selected_text[selected_repeat_columns] > 0\nprofiled_train_selected_text[selected_repeat_columns][filter_columns].describe(percentiles=percentiles)","fbaab2a3":"plot_repeated_entities_groups(profiled_train_selected_text)","d8833239":"profiled_test_text_sentiment = pd.read_csv(f'{EXTENDED_DATA_FOLDER}\/profiled_test_text_sentiment.csv')\nprofiled_test_text_ease_of_reading = pd.read_csv(f'{EXTENDED_DATA_FOLDER}\/profiled_test_text_ease_of_reading.csv')\nprofiled_test_text_grammar = pd.read_csv(f'{EXTENDED_DATA_FOLDER}\/profiled_test_text_grammar_check.csv')\nprofiled_test_text_spelling = pd.read_csv(f'{EXTENDED_DATA_FOLDER}\/profiled_test_text_spelling_check.csv')\nprofiled_test_text_granular = pd.read_csv(f'{EXTENDED_DATA_FOLDER}\/profiled_test_text_granular_features.csv')","f7257d66":"profiled_test_text = pd.concat([test_dataset[['text', 'sentiment']], profiled_test_text_sentiment,  \n                                profiled_test_text_ease_of_reading, profiled_test_text_grammar, \n                                profiled_test_text_spelling, profiled_test_text_granular], axis=1)\nprofiled_test_text = reduce_memory_footprint(profiled_test_text)\nprofiled_test_text['sentiment_polarity_summarised'] = profiled_test_text['sentiment_polarity_summarised'].apply(lambda x: x.lower())","0661abcd":"profiled_test_text","0188aabe":"profiled_test_text['do_sentiments_match'] = check_if_sentiment_of_text_matches(profiled_test_text)\nprofiled_test_text['do_sentiments_match'] = profiled_test_text['do_sentiments_match'].astype('category')","a298c3b1":"plot_sns_chart(profiled_train_text, 'do_sentiments_match', width=35.0, height=5.0, \n               title=f'Do the sentiments for \"text\" provided match-up.',\n               ylabel_title='Sentiments match', \n               xlabel_title=f'Number of rows. Total rows {profiled_train_text.shape[0]}')","977dba6a":"plot_sns_chart(profiled_test_text, 'sentiment', class_separator='do_sentiments_match', width=35.0, height=8.0, \n               title=f'Do the sentiments for \"text\" provided match-up (grouped by Sentiment).',\n               ylabel_title='Spellings are correct', \n               xlabel_title=f'Number of rows. Total rows {profiled_test_text.shape[0]}')","45cbb2e6":"plot_sns_chart(profiled_test_text, 'spelling_quality_summarised', width=35.0, height=5.0, \n               title=f'Spelling check.',\n               ylabel_title='Spellings are correct', \n               xlabel_title=f'Number of rows. Total rows {profiled_test_text.shape[0]}')","98654690":"plot_sns_chart(profiled_test_text, 'sentiment', class_separator='spelling_quality_summarised', width=35.0, height=8.0, \n               title=f'Are the spellings correct in \"text\" (grouped by Sentiment).',\n               ylabel_title='Spellings are correct', \n               xlabel_title=f'Number of rows. Total rows {profiled_test_text.shape[0]}')","d1bd5004":"profiled_test_text","7a02524a":"plot_sns_chart(profiled_test_text, 'ease_of_reading_summarised', width=35.0, height=6.0, \n               title=f'Is the text in \"text\" column easy to read.',\n               ylabel_title='Ease of reading', \n               xlabel_title=f'Number of rows. Total rows {profiled_test_text.shape[0]}')","a77267d3":"plot_sns_chart(profiled_test_text, 'sentiment', class_separator='ease_of_reading_summarised', width=35.0, height=12.0, \n               title=f'Is the text in \"text\" column easy to read (grouped by Ease of Reading Quality).',\n               ylabel_title='Ease of reading', \n               xlabel_title=f'Number of rows. Total rows {profiled_test_text.shape[0]}')","666b44ed":"filter_columns = profiled_test_text[selected_repeat_columns] > 0\nprofiled_test_text[selected_repeat_columns][filter_columns].describe(percentiles=percentiles)","19d0dff4":"plot_repeated_entities_groups(profiled_test_text)","8fd5c1a7":"### Hypothesis\/question: can we find 'selected_text' in every 'text' column of the training dataset?\n\nIt's possible there is a percentage of rows where they do not match exactly (bad data or another reason)","2abad9f7":"## Test dataset: text column","6aa0c9af":"<i><p style=\"font-size:20px; background-color: #FFF1D7; border: 2px solid black; margin: 20px; padding: 20px;\">Conclusion: It appears that `76.6%` (`65.1%` + `11.5%`) of the `text` in the test dataset are readable (human\/machine understandable). Similarly, it would be good to assess the rest of the `23.4%` across the different sentiments and tally with other findings above. This can help us determine that during the ML training\/inference, if these entries are going to impact our machine learning or model building in any form, it is to be seen.","d817399c":"### Hypothesis\/question: is the sentiment of the 'text' column correct?\n\nIt's possible there is a percentage of rows where the sentiment may be incorrect or different from what we would expect (incorrect data or another reason)","a9d4a7e9":"### Hypothesis\/question: is the sentiment of the 'text' column correct?\n\nIt's possible there is a percentage of rows where the sentiment may be incorrect or different from what we would expect (incorrect data or another reason)","81c5c5fe":"<i><p style=\"font-size:20px; background-color: #FFF1D7; border: 2px solid black; margin: 20px; padding: 20px;\">Conclusion: `41.3%` of rows mismatching sentiments, this could have a negative impact on the model inferencing aspects of the process. The mismatch is spread across all three sentiment types in different ratios. Comparing the observations above with the training set data to the test dataset, we can see the proportions look similar. And so the repeat question: will these rows will impact our machine learning or model building in any form, it is to be seen?","94510fcf":"## Original datasets","3ba08d84":"<i><p style=\"font-size:20px; background-color: #FFF1D7; border: 2px solid black; margin: 20px; padding: 20px;\">We can see that for `neutral` sentiment texts we have almost a `100%` (i.e. `97%`) similarity between the texts in the columns `selected_text` and `text`. Meaning for most of the rows `selected_text` and `text` are the same. But for `positive` and `negative` sentiments the degree of similar are between `31%` and `34%`, meaning that only a subset of the text from the columns `text` is present in `selected_text`.","6b4880c9":"### Hypothesis\/question: does the 'text' column have spelling mistakes?\n\nIt's possible there is a percentage of rows where the text in the text column have one or more spelling mistakes (incorrect data or another reason)","e15cb794":"<i><p style=\"font-size:20px; background-color: #FFF1D7; border: 2px solid black; margin: 20px; padding: 20px;\">Conclusion: A number of rows have one or more groups of characters (letters, digits, spaces, whitespace, punctuations) that are repeated two or more times in the text column of the test dataset. But on the whole these are a much smaller number of rows of text as compared to the whole corpus in the test dataset. That is from a total of `3,534` rows, we have only `266`, `69`, `1,231`, `1,231`, and `1,115` rows of repeated three or more repeated letters, two or more repeated digits,\ttwo or more repeated spaces, two or more repeated whitespaces and two or more repeated punctuations respectively. The split between **English** and **non-English** characters `3534` (`~99%`)\t and `18` (`<%1`) respectively, just like our previous observations. Again looking at the big-picture, if these rows will impact our machine learning or model building in any form, it is to be seen.","7f10afa6":"### Hypothesis\/question: are the 'text' or 'selected_text' columns ease to read?\n\nIt's possible there is a percentage of rows where the text in the text or selected_text columns are not easy to read for humans or ML models (incorrect data or another reason)","1f6955af":"We can further use the `jaccard` score function to measure the degree of similar between the two columns, even though we know that only a subset of the text in the `text` column will be present in the `selected_text` columns.","ac546ba8":"### Hypothesis\/question: (a) 'text' column contains two or more spaces, whitespaces and\/or (b) the 'text' column contains two or more repeated letters, digits, (c) and\/or two or more repeated punctuations marks\n\nIt's possible there is a percentage of rows where the there are repeated spaces, whitespaces, alphanumeric characters and\/or punctuations marks.","3a2ffbfe":"## Training dataset: selected_text column","72d7fb52":"<i><p style=\"font-size:20px; background-color: #FFF1D7; border: 2px solid black; margin: 20px; padding: 20px;\">Conclusion: A number of rows have one or more groups of characters (letters, digits, spaces, whitespace, punctuations) that are repeated two or more times in the text column of the training dataset. But on the whole these are a smaller number of rows of text as compared to the whole corpus in the training dataset. That is from a total of `27,481` rows, we have only `2,157` (`7.84%`), `550` (`2.00%`), `9,653`  (`35.13%`), `9,653` (`35.13%`), and `8,702` (`31.67%`) rows of repeated three or more repeated letters, two or more repeated digits, two or more repeated spaces, two or more repeated whitespaces and two or more repeated punctuations respectively. The split between **English** and **non-English** characters across the texts is `27,480` (`99.38%`), and `172` (`0.62%`) -- this particular statistics means, we don't need to worry about it at the moment. But looking at the big-picture if these rows will impact our machine learning or model building in any form, it is to be seen.","e76eb22d":"### Prequels\/sequels\n\n- [FastChai sessions: Tweet Sentiment Extraction (data-prep)](https:\/\/www.kaggle.com\/neomatrix369\/fastchai-tweet-sentiment-extraction-data-prep\/) | [Extended Dataset](https:\/\/www.kaggle.com\/neomatrix369\/tweet-sentiment-extraction-extended)\n- **FastChai sessions: Tweet Sentiment Extraction (analysis)**","37017702":"<i><p style=\"font-size:20px; background-color: #FFF1D7; border: 2px solid black; margin: 20px; padding: 20px;\">Conclusion: `41.2%` of rows have spelling mistakes. If we look at the `41.2%` of the texts where the spelling is `Bad`, this trait is split across the three sentiments in this order: `positive` (`13.2%`), `neutral` (`17.1%`) and `negative` (`10.9%`). `41.8` is still a big number -- and would this impact our machine learning or model training process in any form, it is to be seen.","5bd60281":"<i><p style=\"font-size:20px; background-color: #FFF1D7; border: 2px solid black; margin: 20px; padding: 20px;\">Conclusion: sentiment across a number of rows do not match for the 'text' column. The matching and mismatching ratios are `58.7%` and `41.3%` respectively. Also from the breakdown, it appears that a portion of the `neutral` and `negative` sentiment text do not match.  If we look at the `41.3%` of the sentiments that do not match up, they are split across the three sentiments in this order: `positive` (`6.3%`), `neutral` (`19.3%`) and `negative` (`15.5`). The mismatching rows could be earmarked to verify during model analysis and and if it this impact our machine learning or model building in any form, it is to be seen.","f2937939":"### Hypothesis\/question: does the 'text' or 'selected_text' column have spelling mistakes?\n\nIt's possible there is a percentage of rows where the text in the text or selected_text columns have one or more spelling mistakes (incorrect data or another reason)","ef2b2cc5":"### Hypothesis\/question: (a) 'text' column contains two or more spaces, whitespaces and\/or (b) the 'text' column contains two or more repeated letters, digits, (c) and\/or two or more repeated punctuations marks\n\nIt's possible there is a percentage of rows where the there are repeated spaces, whitespaces, alphanumeric characters and\/or punctuations marks.","80f50a05":"<i><p style=\"font-size:20px; background-color: #FFF1D7; border: 2px solid black; margin: 20px; padding: 20px;\">Looking at the jaccard similarity scores between the different sentiments it appears that the patterns of dissimilarity are the same between the `positive` and `negative` sentiments while that of `neutral` sentiment is clearly skewed right and also where majority of them are similar.","054cb9f4":"<i><p style=\"font-size:20px; background-color: #FFF1D7; border: 2px solid black; margin: 20px; padding: 20px;\">Conclusion: `41.633%` of rows mismatching sentiments, this could have a negative impact on the model inferencing aspects of the process. The issue is spread across multiple sentiment types. And similar to the rows in training dataset as observed previously.","86e39244":"## Loading datasets","1ef894ed":"<i><p style=\"font-size:20px; background-color: #FFF1D7; border: 2px solid black; margin: 20px; padding: 20px;\">Conclusion: It appears that `76.2%` (`65.4%` + `10.8%`) of the `text` (also would apply to the `selected_text` column) in the training dataset are readable (human\/machine understandable). It would be good to assess the rest of the `23.8%` across the different sentiments and tally with other findings above. If we look at the the texts, this trait is split across the three sentiments in this order: `Standard` or `Easy` to read: `positive` (`23.8%`), `neutral` (`30.9%`) and `negative` (`22.3%`), while `Confusing` or `Difficult` to read: `positive` (`5.4%`), `neutral` (`6.3%`) and `negative` (`0.8%`). This can help us determine that during the ML training\/inference, if these entries are going to impede the machine learning or model building process.","8aae1977":"### Hypothesis\/question: are the 'text' or 'selected_text' columns ease to read?\n\nIt's possible there is a percentage of rows where the text in the `text` or `selected_text` columns are not easy to read for humans or ML models (incorrect data or another reason)","6822ecc2":"### Hypothesis\/question: (a) 'selected_text' column contains two or more spaces, whitespaces and\/or (b) the 'selected_text' column contains two or more repeated letters, digits, (c) and\/or two or more repeated punctuations marks\n\nIt's possible there is a percentage of rows where the there are repeated spaces, whitespaces, alphanumeric characters and\/or punctuations marks.","b37c5079":"### Prequels\/sequels\n\n- [FastChai sessions: Tweet Sentiment Extraction (data-prep)](https:\/\/www.kaggle.com\/neomatrix369\/fastchai-tweet-sentiment-extraction-data-prep\/) | [Extended Dataset](https:\/\/www.kaggle.com\/neomatrix369\/tweet-sentiment-extraction-extended)\n- **FastChai sessions: Tweet Sentiment Extraction (analysis)**","ef857d89":"<i><p style=\"font-size:20px; background-color: #FFF1D7; border: 2px solid black; margin: 20px; padding: 20px;\">Conclusion: A number of rows have one or more groups of characters (letters, digits, spaces, whitespace, punctuations) that are repeated two or more times in the `selected_text` column of the training dataset. But on the whole these are a smaller number of rows of text as compared to the whole corpus in the training dataset. That is from a total of `27,481` rows, we have only `1,287`, `261`, `4,104`, `4,104`, and `4,593` rows of repeated three or more repeated letters, two or more repeated digits, two or more repeated spaces, two or more repeated whitespaces and\ttwo or more repeated punctuations respectively. The split between **English** and **non-English** characters is `27,480` (`~99%`) and `86` (`<1%`) respectively. We have seen similar numbers previously. But again looking at the big-picture, if these rows will impact our machine learning or model building in any form, it is to be seen.","8f30779a":"<i><p style=\"font-size:20px; background-color: #FFF1D7; border: 2px solid black; margin: 20px; padding: 20px;\">Conclusion: all the values in the `selected_text` column can be found in the `text` column","73bf6ec5":"## Training dataset: text column"}}