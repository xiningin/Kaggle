{"cell_type":{"72e20fce":"code","08de2108":"code","e9dfde80":"code","3db1b2d9":"code","47345fcb":"code","8b1efcbc":"code","f5cccd7f":"code","6bcd344c":"code","cf3e9a12":"code","176912f5":"code","b3e57059":"code","beaf02a1":"code","f2253240":"code","8d35cd36":"code","2307e140":"code","64237d3b":"code","edc09390":"code","de16bf6a":"code","8b51ec8a":"code","2f23ba33":"code","c9397e57":"markdown","eb26e6ae":"markdown","dc03d8ee":"markdown","5af79e2b":"markdown","d59c95db":"markdown","aa7b7e7f":"markdown","85a3b6e0":"markdown","94f71777":"markdown","991ddfe4":"markdown","96e55ea9":"markdown","281bb2b4":"markdown","e2724ce7":"markdown","988dcef6":"markdown","d87f252f":"markdown","dc87f4c3":"markdown","e3b68a17":"markdown","af685632":"markdown"},"source":{"72e20fce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","08de2108":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport pandas as pd\nimport sklearn.metrics as metrics\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nfrom sklearn.metrics import classification_report\ndf = pd.read_csv(\"..\/input\/online-shoppers-intention\/online_shoppers_intention.csv\")","e9dfde80":"def preprocessing(df,columns):\n    df.dropna(inplace=True) \n    df.replace(['Returning_Visitor', 'New_Visitor','Other'],[1,2,3], inplace=True)\n    df.replace(['Jan','Feb','Mar','Apr','May','June','Jul','Aug','Sep','Oct','Nov','Dec'],[1,2,3,4,5,6,7,8,9,10,11,12],inplace=True)\n    if columns!=[]: \n        df=df[columns]\n    return df\n\ndef Split(df):\n    X = df.drop('Revenue', axis=1)\n    y = df['Revenue']\n    obj_escalar = StandardScaler()\n    X_standardization = obj_escalar.fit_transform(X)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n    return X_train, X_test, Y_train, Y_test\n \ndf=preprocessing(df,[])\nX_train, X_test, Y_train, Y_test=Split(df)\ndf['Revenue'].value_counts().plot.bar()\n","3db1b2d9":"model1 = LogisticRegression(class_weight='balanced')\nmodel1.fit(X_train, Y_train)\ny_pred=model1.predict(X_test)\nprint(\"\\nAccuracy\\t{}\".format(round(metrics.accuracy_score(Y_test, y_pred),3)))  \nprint(\"\\nRecall\\t{}\".format(round(metrics.recall_score(Y_test, y_pred),3)))  \nmodel1.get_params()\n","47345fcb":"parametres = {\"C\": [0.001, 0.008, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06,0.07, 0.1]}\nmodel1_gs = GridSearchCV(model1, param_grid=parametres,\n                         cv = 10, scoring='accuracy')\nmodel1_gs.fit(X_train, Y_train)\nprint(model1_gs.best_params_, \"\\nAcc: {}\".format(round(model1_gs.best_score_,3)))","8b1efcbc":"df_search = pd.DataFrame.from_dict(model1_gs.cv_results_)\nplt.xlabel('C')\nplt.ylabel('Acc')\n_ = plt.plot(df_search['param_C'], df_search['mean_test_score'], 'x')\nprint('Best_parameter',model1_gs.best_params_, \"\\nAcc: {}\".format(round(model1_gs.best_score_,3)))","f5cccd7f":"model1 = LogisticRegression(class_weight='balanced', C=model1_gs.best_params_.get(\"C\"))\nmodel1.fit(X_train, Y_train)\ny_test_pred_prob = model1.predict_proba(X_test)\ny_test_pred_prob_pos = y_test_pred_prob[np.where(Y_test == 1)[0]]\ny_test_pred_prob_neg = y_test_pred_prob[np.where(Y_test == 0)[0]]\n\ndef representation_seuil(x_1, x_0, n_bins=11, title='This figure represents in blue the probabilities assigned by the model to data that are 1, and in red the probabilities assigned to the data that are 0', label_1='Clase 1', \n                          label_0='Clase 0', density=0):\n    bins = n_bins\n    plt.hist(x_1, bins, density = density, alpha=0.5, label=label_1, color='blue')    \n    plt.hist(x_0, bins, density = density, alpha=0.5, label=label_0, color='red')\n    plt.title(title)\n    plt.legend(loc='best') \n    \nrepresentation_seuil(y_test_pred_prob_pos[:, 1], y_test_pred_prob_neg[:, 1], n_bins=21, density=0)    \n","6bcd344c":"THRESHOLD = 0.5\ny_THRESHOLD = 1*(y_test_pred_prob[:, 1] > THRESHOLD)\n\nprint(u\"Matriz de confusi\u00f3n\\n\", metrics.confusion_matrix(Y_test, y_THRESHOLD))\nprint(\"\\nAccuracy\\t{}\".format(round(metrics.accuracy_score(Y_test, y_THRESHOLD),3)))  \nprint(\"Sensitivity\\t{}\".format(round(metrics.recall_score(Y_test, y_THRESHOLD),3)))\nprint(u\"Precision\\t{}\".format(round(metrics.precision_score(Y_test, y_THRESHOLD),3))) ","cf3e9a12":"THRESHOLD = 0.6\ny_THRESHOLD = 1*(y_test_pred_prob[:, 1] > THRESHOLD)\n\nprint(u\"Matrice de confusion\\n\", metrics.confusion_matrix(Y_test, y_THRESHOLD))\nprint(\"\\nAccuracy\\t{}\".format(round(metrics.accuracy_score(Y_test, y_THRESHOLD),3)))  \nprint(\"Sensitivity\\t{}\".format(round(metrics.recall_score(Y_test, y_THRESHOLD),3)))\nprint(u\"Precision\\t{}\".format(round(metrics.precision_score(Y_test, y_THRESHOLD),3))) ","176912f5":"THRESHOLD = 0.55\ny_THRESHOLD = 1*(y_test_pred_prob[:, 1] > THRESHOLD)\n\nprint(u\"Matrice de confusion\\n\", metrics.confusion_matrix(Y_test, y_THRESHOLD))\nprint(\"\\nAccuracy\\t{}\".format(round(metrics.accuracy_score(Y_test, y_THRESHOLD),3)))  \nprint(\"Sensitivity\\t{}\".format(round(metrics.recall_score(Y_test, y_THRESHOLD),3)))\nprint(u\"Precision\\t{}\".format(round(metrics.precision_score(Y_test, y_THRESHOLD),3)))","b3e57059":"importance = model1.coef_[0]\nimportant_features=[]\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature:',df.columns[i],'Score:', v)\n    if (v>0.04 or v<-0.04):\n        important_features.append(df.columns[i])\n# plot feature importance\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()\n\nimportant_features","beaf02a1":"df = pd.read_csv(\"..\/input\/online-shoppers-intention\/online_shoppers_intention.csv\")\ncolumns=important_features+['Revenue']\ndf_less_features=preprocessing(df,columns)\nX_train, X_test, Y_train, Y_test=Split(df_less_features)","f2253240":"model1 = LogisticRegression(class_weight='balanced')\nparametres = {\"C\": [0.001, 0.008, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06,0.07, 0.1]}\nmodel1_gs = GridSearchCV(model1, param_grid=parametres,\n                         cv = 10, scoring='accuracy')\nmodel1_gs.fit(X_train, Y_train)\nprint(model1_gs.best_params_, \"\\nAcc: {}\".format(round(model1_gs.best_score_,3)))\n","8d35cd36":"model1 = LogisticRegression(class_weight='balanced', C=model1_gs.best_params_.get(\"C\"))\nmodel1.fit(X_train, Y_train)\ny_test_pred_prob = model1.predict_proba(X_test)\ny_test_pred_prob_pos = y_test_pred_prob[np.where(Y_test == 1)[0]]\ny_test_pred_prob_neg = y_test_pred_prob[np.where(Y_test == 0)[0]]\n\ndef representation_seuil(x_1, x_0, n_bins=11, title='', label_1='Classe 1', \n                          label_0='Classe 0', density=0):\n    bins = n_bins\n    plt.hist(x_1, bins, density = density, alpha=0.5, label=label_1, color='blue')    \n    plt.hist(x_0, bins, density = density, alpha=0.5, label=label_0, color='red')\n    plt.title(title)\n    plt.legend(loc='best') \n    \nrepresentation_seuil(y_test_pred_prob_pos[:, 1], y_test_pred_prob_neg[:, 1], n_bins=21, density=0)    \n\n","2307e140":"THRESHOLD = 0.6\ny_threshold = 1*(y_test_pred_prob[:, 1] > THRESHOLD)\n\nprint(u\"Matriz de confusion\\n\", metrics.confusion_matrix(Y_test, y_threshold))\nprint(\"\\nAccuracy\\t{}\".format(round(metrics.accuracy_score(Y_test, y_threshold),3)))  \nprint(\"Sensitivity\\t{}\".format(round(metrics.recall_score(Y_test, y_threshold),3)))\nprint(u\"Precision\\t{}\".format(round(metrics.precision_score(Y_test, y_threshold),3)))","64237d3b":"THRESHOLD = 0.55\ny_threshold = 1*(y_test_pred_prob[:, 1] > THRESHOLD)\n\nprint(u\"Matriz de confusion\\n\", metrics.confusion_matrix(Y_test, y_threshold))\nprint(\"\\nAccuracy\\t{}\".format(round(metrics.accuracy_score(Y_test, y_threshold),3)))  \nprint(\"Sensitivity\\t{}\".format(round(metrics.recall_score(Y_test, y_threshold),3)))\nprint(u\"Precision\\t{}\".format(round(metrics.precision_score(Y_test, y_threshold),3)))","edc09390":"THRESHOLD = 0.5\ny_threshold = 1*(y_test_pred_prob[:, 1] > THRESHOLD)\n\nprint(u\"Matriz de confusion\\n\", metrics.confusion_matrix(Y_test, y_threshold))\nprint(\"\\nAccuracy\\t{}\".format(round(metrics.accuracy_score(Y_test, y_threshold),3)))  \nprint(\"Sensitivity\\t{}\".format(round(metrics.recall_score(Y_test, y_threshold),3)))\nprint(u\"Precision\\t{}\".format(round(metrics.precision_score(Y_test, y_threshold),3)))","de16bf6a":"model2 = RandomForestClassifier(n_estimators = 30,max_depth = 10,random_state = 101)\nmodel2.fit(X_train,Y_train)\npred = model2.predict(X_test)\nprint('Results Random Forest with no optimization')\nprint(classification_report(Y_test,pred))\nprint(model2.score(X_test,Y_test))","8b51ec8a":" param_grid = {\n    'n_estimators' : [60,100],\n    'max_depth' : [10,15],\n    'min_samples_leaf' : [2,4],\n    'min_samples_split': [2,4]\n}\n\ngridsearch = GridSearchCV(estimator=model2,param_grid=param_grid,verbose = 1)\ngridsearch.fit(X_train,Y_train)\ngridsearch.best_params_\n","2f23ba33":"model2 = RandomForestClassifier(n_estimators = 60,max_depth = 10,min_samples_leaf = 3, min_samples_split = 2,random_state = 101)\nmodel2.fit(X_train,Y_train)\npred = model2.predict(X_test)\nprint(classification_report(Y_test,pred))\n\nfrom sklearn.metrics import accuracy_score\nAcc = accuracy_score(Y_test,pred)\nprint('Accuracy',Acc)","c9397e57":" > We are keeping features that score> 0.04 or <-0.04 to see if our model's performance will improve.","eb26e6ae":"**At this stages, we developped three ML models to predict whether a visitor to the e-commerce page will make a purchase or not. \nThen, this model can help to find the right consumer - who have the intention to purchase - analyze the data of potential consumer in real time and push direct marketing strategies to all those people.**","dc03d8ee":"Find Best threshold","5af79e2b":"Optimization of the Random Forest Classifier using GridSearch","d59c95db":"> We will keep the threshold of 0.55 which allows to have a good trade-off between accuracy and recall","aa7b7e7f":"Import","85a3b6e0":"#  **Prediction PART**","94f71777":"**Features importance**\n> We want to observe which variables contribute the most to the model","991ddfe4":"**Preprocessing**\n> We will first transform the categorical variables into a numeric variable then split our data into train and test parts. Then we will plot the repartition of the two classes in the df\n","96e55ea9":"# > > MODEL 2 RANDOM FOREST","281bb2b4":"**Results**\n* We will keep the threshold of 0.5 which allows us to have the best deal between accuracy and recall.\n* We can observe that when we relaunch the model with only the most important features, we get better results. The other variables therefore made noise in the previous model. \n* We thus obtain in our best logistic regression model: 0,89 accuracy, 0,70 recall.","e2724ce7":"> Because the sensitivity represents the percentage of true positive on all the positives values, it is a data which is important for our cases since we need to find which visitors have the intention to buy (Revenue=True)","988dcef6":"**Results**\n* We thus obtain in our best logistic regression model: 0.91 accuracy, 0.62 recall.\n* The logistic regression is finally a better model than random forest to predict shopping intentions","d87f252f":"> * The \u201cbalanced\u201d mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples \/ (n_classes * np.bincount(y)).\n> * Cfloat, default=1.0 Inverse of regularization strength; must be a positive float --> smaller values specify stronger regularization.","dc87f4c3":"Find Best parameters using Grid Search","e3b68a17":"> We observe that the data are imbalanced. There is a lot more 'Revenue = False' observations than 'Revenue = True'. We will therefore have to take it into account in our models.","af685632":"# > **BASELINE LOGISTIC REGRESSION**"}}