{"cell_type":{"6e636ee8":"code","3217c42e":"code","9beccc19":"code","e076b6e9":"code","67a5ed5d":"code","d45658be":"code","070e8489":"code","20f9d3ab":"code","1feff246":"code","6616d031":"code","28be0c12":"code","a3e2e9a6":"code","df43cac2":"code","be9a6b54":"code","ee305fe2":"code","51e003ad":"code","3eb45bbb":"code","942b3cf2":"code","06ec7e35":"code","412d35f3":"markdown","897ddabb":"markdown","aa179953":"markdown","f8746602":"markdown","f98e73b8":"markdown","ccf28e89":"markdown","1b64ef1c":"markdown","d8e36c09":"markdown","6d25127f":"markdown","1c3046e3":"markdown","e0ff1d41":"markdown"},"source":{"6e636ee8":"# Imports\n# Data\nimport numpy as np\nimport pandas as pd \n# Viz\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n# Tensorflow\nimport tensorflow as tf \n# Scikit Learn\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer, MinMaxScaler, PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer, TransformedTargetRegressor\nfrom sklearn.ensemble import RandomForestRegressor, StackingRegressor\nfrom sklearn.linear_model import Ridge, Lasso, RidgeCV, LassoCV\nfrom sklearn.metrics import mean_squared_error\nimport sklearn\nsklearn.set_config(display='diagram')\n# XGBoost\nfrom xgboost import XGBRegressor\n\n# Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","3217c42e":"# Loading data\nroot = '\/kaggle\/input\/tabular-playground-series-feb-2021\/'\ntrain = pd.read_csv(f'{root}\/train.csv')\nassert all(train.isna().sum() == 0), 'Some NA in the data'\n\n# Features\nfeatures = [c for c in train.columns if ('cat' in c) or ('cont' in c)]\ntarget = 'target'\ncat_features = [c for c in train.columns if 'cat' in c]\ncont_features = [c for c in train.columns if 'cont' in c]\n\n# Train, val\ntrain, test = train_test_split(train)","9beccc19":"categories = np.sort(list(set([x for f in cat_features for x in train[f].unique()])))\ncategory_summary = pd.DataFrame(columns=cat_features, index=categories, data=np.zeros([len(categories), len(cat_features)]))\nfor f in cat_features: \n    for c in categories: \n        category_summary.loc[c, f] = (train[f] == c).sum()\ndisplay(category_summary.astype(int))","e076b6e9":"fig, axs = plt.subplots(3, 5, figsize=(30, 15))\nfor i, f in enumerate(cont_features + [target]): \n    xidx = i \/\/ 5\n    yidx = i % 5\n    sns.distplot(train[f], ax=axs[xidx, yidx])\nfig.show()","67a5ed5d":"# Helpers\ndef val_model(model, test): \n    # Validation Predictions\n    test_predict = model.predict(test[features])\n    # Viz Results\n    plt.figure(figsize=(10, 7))\n    plt.scatter(\n        test[target], \n        test_predict, \n        marker='+', \n        label='Prediction'\n    )\n    plt.plot(\n        [test[target].min(), test[target].max()], \n        [test[target].min(), test[target].max()],\n        c='r',\n        label='Perfect Prediction', \n    )\n    plt.xlabel('Target')\n    plt.ylabel('Prediction')\n    plt.legend()\n    plt.show()\n    # Metric\n    mse = mean_squared_error(test[target], test_predict)\n    print(f'MSE: {mse}\\tRMSE: {np.sqrt(mse)}')\n    \ndef create_submission(model, fp=None): \n    test = pd.read_csv(f'{root}\/test.csv')\n    assert all(test[features].isna().sum() == 0), 'Unexpected NAs'\n    prediction = model.predict(test[features])\n    submission = pd.DataFrame(\n        columns=['id', 'target'], \n    )\n    submission['id'] = test['id']\n    submission['target'] = prediction\n    if bool(fp): \n        display(submission.head())\n        print(f'Saving: {fp}')\n        submission.to_csv(fp, index=False)\n    else: \n        return submission","d45658be":"# Preprocessing\ncolumn_transformer = ColumnTransformer([\n    (\n        'numerical', \n        SimpleImputer(strategy='median'), \n        cont_features, \n    ), \n    (\n        'categorical', \n        OneHotEncoder(handle_unknown='ignore'), \n        cat_features, \n    )\n])\nscaler = StandardScaler()\npreprocessing = Pipeline([\n    ('column', column_transformer), \n    ('scaler', scaler), \n])","070e8489":"# Model \nmodel = TransformedTargetRegressor(\n    Pipeline(\n        [\n            ('preprocessing', preprocessing), \n            ('regressor', XGBRegressor(\n                n_estimators=100, \n                max_depth=2,\n                objective='reg:squarederror', \n                tree_method='gpu_hist'\n            )), \n        ], \n        verbose=False\n    ), \n    transformer=QuantileTransformer()\n)\n\nmodel.fit(train[features], train[target])","20f9d3ab":"val_model(model, test)","1feff246":"# Models\nrf = RandomForestRegressor(\n    n_estimators=50, \n    max_depth=2, \n    criterion='mse', \n    max_features=4,\n)\nxgb = XGBRegressor(\n    n_estimators=100, \n    max_depth=2,\n    objective='reg:squarederror', \n    tree_method='gpu_hist'\n)\nridge = Ridge()\nlasso = Lasso()\n\nstacked = StackingRegressor(\n    estimators=[\n        # ('random_forest', rf), \n        ('xgboost', xgb), \n        ('ridge', ridge), \n        ('lasso', lasso), \n    ], \n    final_estimator=Ridge(),\n    cv=5,\n    passthrough=False, \n    verbose=0\n)\n\n# Model \nstacked_model = TransformedTargetRegressor(\n    Pipeline(\n        [\n            ('preprocessing', preprocessing), \n            ('regressor', stacked), \n        ], \n        verbose=False\n    ), \n    transformer=StandardScaler()\n)\n\n\nstacked_model.fit(train[features], train[target])","6616d031":"val_model(stacked_model, test)","28be0c12":"# Get the list of all parameters to train on\nprint('\\n'.join(list(stacked_model.get_params().keys())))","a3e2e9a6":"param_grid = {\n    'regressor__regressor__ridge__alpha': [1],                        # Ridge Pen\n    'regressor__regressor__lasso__alpha': [1],                        # Lasso Pen\n    'regressor__regressor__xgboost__n_estimators': [100],             # XBB n estimators\n    'regressor__regressor__xgboost__max_depth': [2]                   # XGB depth\n}\n\ncross_val_stacked = GridSearchCV(\n    estimator=stacked_model, \n    param_grid=param_grid, \n    cv=5, \n    return_train_score=False,   \n    scoring='neg_mean_squared_error', \n    verbose=False\n)\n\ncross_val_stacked.fit(train[features], train[target])","df43cac2":"val_model(cross_val_stacked, test)","be9a6b54":"create_submission(stacked_model, fp='submission.csv')","ee305fe2":"# Prepropcessing the data the same as before\npreprocessing.fit(train[features])\ntrain_X =  preprocessing.transform(train[features])\ntrain_y =  train[target].values\ntest_X  =  preprocessing.transform(test[features])\ntest_y  =  test[target].values","51e003ad":"def build_tf_neural_network(input_shape=70, n_layers=3, units=1024, leak_relu=0.1, dropout=0, skip_conn=True, normalization=False):\n    inputs = tf.keras.Input(shape=(input_shape), name=\"input\")\n    x = inputs\n    for i_layer in range(n_layers): \n        if normalization: \n            x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dense(units, activation=tf.keras.layers.LeakyReLU(leak_relu))(x)\n        x = tf.keras.layers.Dropout(dropout)(x)\n        if skip_conn: \n            x = tf.keras.layers.Concatenate()([x, inputs])\n    # Some more layers\n    x = tf.keras.layers.Dense(64, activation=None)(x)\n    x = tf.keras.layers.Dense(64, activation=None)(x)\n    x = tf.keras.layers.Dense(64, activation=None)(x)\n    outputs = tf.keras.layers.Dense(1, activation=None)(x)\n    model = tf.keras.Model(inputs, outputs)\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(0.00001), \n        loss=tf.keras.losses.MeanSquaredError(), \n        metrics=[tf.keras.metrics.RootMeanSquaredError()]\n    )\n    return model","3eb45bbb":"nn_model = build_tf_neural_network()\nnn_model.summary()","942b3cf2":"# Training\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10)\nhistory = nn_model.fit(\n    x=train_X, y=train_y, \n    epochs=50, \n    batch_size=4096, \n    validation_data=(test_X, test_y), \n    callbacks=[early_stopping], \n    verbose=False\n)\n# History\nplt.figure(figsize=(10, 6))\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Val Loss')\nplt.legend()\nplt.show()\n\n# Test\ntest_predict = nn_model.predict(test_X)\n# Metric\nmse = mean_squared_error(test_y, test_predict)\nprint(f'MSE: {mse}\\tRMSE: {np.sqrt(mse)}')","06ec7e35":"nn_pipeline = Pipeline([\n        ('preprocessing', preprocessing), \n        ('regressor', nn_model), \n    ], \n    verbose=False\n)\nval_model(nn_pipeline, test)","412d35f3":"# Preprocessing","897ddabb":"### Model Submission","aa179953":"# Tabular Playground: Playing with sklearn\n\nIn this notebook, we will present: \n- A shallow EDA allowing to build baseline models\n- Build a simple model with Scikit Learn and XGBoost\n- Stacking models with Scikit Learn\n- Hyperparameters tuning with Scikit Learn\n- Submission and prediction visualization","f8746602":"# TensorFlow Example","f98e73b8":"### Target\n\nSee plot above\n* Not normally distributed, looks more bi-modal\n* Huge bias that needs to be taken into account, we will use: `sklearn.compose.TransformedTargetRegressor`","ccf28e89":"### Stacking","1b64ef1c":"# EDA\n\n### Categorical Features\n* Letters are the different classes\n* No NaNs\n* The features can be imbalanced (eg. `cat0`, `cat6`). Some even seem to have a Zipf law distribution. \n* One-Hot encoding is a good baseline to handle those features. ","d8e36c09":"### Hyperparameters","6d25127f":"# Scikit Learn\n### A simple model - XGB","1c3046e3":" # Conclusion\n \n Those models are not good as shown by the validation graphs. But the score achieved is close to the current top models. \n \n ##### Next steps\n * More EDA \/ Feature engineering: \n   * what are we supposed to do with the seemingly discrete `cont` feature ?\n   * can we fit a multi-modal model on each features? \n   * the target is bi-modal, what should we do with this? \n* Tune hyperparameters: I did not spend much time on this\n* Stack more models: Random Forest, SVM Regressor, etc\n","e0ff1d41":"### Continuous Features\n* They are all in the interval $[0, 1]$, somewhat already transformed. \n* Some features are discontinuous (eg. `cont1`)\n* Many features look like a mixture of gaussian, we could look into fitting those\n* No NaNs"}}