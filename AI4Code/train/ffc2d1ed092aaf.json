{"cell_type":{"21272d6d":"code","4c4beb99":"code","dfdc5186":"code","4ca6d1e6":"code","5be33a23":"code","e0d08476":"code","09477477":"code","41680c8d":"code","860665e0":"code","95c1ccd4":"code","03453cdc":"code","fa99669d":"code","bbce3b64":"code","126553b3":"code","50b45e37":"code","cbe576ae":"code","77a90095":"code","08d3fa79":"code","b5bcde86":"code","8b192902":"code","c0302dbb":"code","2f1de41c":"code","6ccf20b3":"code","1848ad3c":"code","5b3f2e22":"code","883326e4":"code","185287a9":"code","248b051f":"code","3db868bb":"code","68ffbb2d":"code","94817ce1":"code","870df7f6":"code","a0a9e126":"markdown","144b0d52":"markdown","83c1fa93":"markdown","1827ac66":"markdown","f7c3eb59":"markdown","1ee660cd":"markdown","6c1e06d6":"markdown","5d68b7aa":"markdown","a94c09a1":"markdown","48780500":"markdown","aeda3c6f":"markdown"},"source":{"21272d6d":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.rcParams[\"figure.figsize\"] = (15, 10)\nplt.rcParams[\"figure.dpi\"] = 125\nplt.rcParams[\"font.size\"] = 14\nplt.rcParams['font.family'] = ['sans-serif']\nplt.rcParams['font.sans-serif'] = ['DejaVu Sans']\nplt.style.use('ggplot')\nsns.set_style(\"whitegrid\", {'axes.grid': False})\nplt.rcParams['image.cmap'] = 'gray' # grayscale looks better\nfrom itertools import cycle\nprop_cycle = plt.rcParams['axes.prop_cycle']\ncolors = prop_cycle.by_key()['color']","4c4beb99":"from pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport os\nfrom skimage.io import imread as imread\nfrom skimage.util import montage\nmontage_rgb = lambda x: np.stack([montage(x[:, :, :, i]) for i in range(x.shape[3])], -1)\nfrom skimage.color import label2rgb\nimage_dir = Path('..') \/ 'input' \/ 'minigredients'\nmapping_file = Path('..') \/ 'input' \/ 'ingredients-to-allergies-mapping' \/ 'clean_list.json'\nalleg_df = pd.read_json(mapping_file)\nalleg_df['image_path'] = alleg_df['image_path'].map(lambda x: image_dir \/ 'subset' \/ x) \nprint(alleg_df['image_path'].map(lambda x: x.exists()).value_counts())\nallergens = alleg_df.columns[3:].tolist()\nalleg_df.sample(2)","dfdc5186":"co_all = np.corrcoef(np.stack(alleg_df[allergens].applymap(lambda x: 1 if x>0 else 0).values, 0).T)\nfig, ax1 = plt.subplots(1, 1, figsize=(10, 10))\nsns.heatmap(co_all, annot=True, fmt='2.1%', ax=ax1, cmap='RdBu', vmin=-1, vmax=1)\nax1.set_xticklabels(allergens, rotation=90)\nax1.set_yticklabels(allergens);","4ca6d1e6":"alleg_df['aller_vec'] = alleg_df[allergens].applymap(lambda x: 1 if x>0 else 0).values.tolist()\nalleg_df.sample(2)","5be33a23":"from sklearn.model_selection import train_test_split\nraw_train_df, valid_df = train_test_split(alleg_df.drop('ingredients_list', 1), \n                 test_size = 0.3, \n                  # hack to make stratification work                  \n                 stratify = alleg_df['aller_vec'].map(lambda x: x[0:3]))\nprint(raw_train_df.shape[0], 'training masks')\nprint(valid_df.shape[0], 'validation masks')","e0d08476":"GAUSSIAN_NOISE = 0.05\n# number of validation images to use\nVALID_IMG_COUNT = 1500\nBASE_MODEL='InceptionV3' # ['VGG16', 'RESNET52', 'InceptionV3', 'Xception', 'DenseNet169', 'DenseNet121']\nIMG_SIZE = (299, 299) # [(224, 224), (384, 384), (512, 512), (640, 640)]\nBATCH_SIZE = 64 # [1, 8, 16, 24]\nDROPOUT = 0.5\nDENSE_COUNT = 256\nSAMPLE_PER_GROUP = 1500\nLEARN_RATE = 3e-4\nEPOCHS = 30\nRGB_FLIP = 1 # should rgb be flipped when rendering images","09477477":"train_df = pd.concat([raw_train_df.\\\n     groupby(raw_train_df[k].map(lambda x: x>0)).\\\n     apply(lambda x: x.sample(SAMPLE_PER_GROUP\/\/2, replace=True)).\\\n     reset_index(drop=True) \n     for k in allergens])\ntrain_df.shape[0]","41680c8d":"from keras.preprocessing.image import ImageDataGenerator\nif BASE_MODEL=='VGG16':\n    from keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\nelif BASE_MODEL=='RESNET52':\n    from keras.applications.resnet50 import ResNet50 as PTModel, preprocess_input\nelif BASE_MODEL=='InceptionV3':\n    from keras.applications.inception_v3 import InceptionV3 as PTModel, preprocess_input\nelif BASE_MODEL=='Xception':\n    from keras.applications.xception import Xception as PTModel, preprocess_input\nelif BASE_MODEL=='DenseNet169': \n    from keras.applications.densenet import DenseNet169 as PTModel, preprocess_input\nelif BASE_MODEL=='DenseNet121':\n    from keras.applications.densenet import DenseNet121 as PTModel, preprocess_input\nelse:\n    raise ValueError('Unknown model: {}'.format(BASE_MODEL))","860665e0":"from keras.preprocessing.image import ImageDataGenerator\ndg_args = dict(featurewise_center = False, \n                  samplewise_center = False,\n                  rotation_range = 45, \n                  width_shift_range = 0.1, \n                  height_shift_range = 0.1, \n                  shear_range = 0.01,\n                  zoom_range = [0.9, 1.25],  \n                  brightness_range = [0.7, 1.3],\n                  horizontal_flip = True, \n                  vertical_flip = False,\n                  fill_mode = 'reflect',\n                   data_format = 'channels_last',\n              preprocessing_function = preprocess_input)\n\nvalid_args = dict(fill_mode = 'reflect',\n                   data_format = 'channels_last',\n                  preprocessing_function = preprocess_input)\n\ncore_idg = ImageDataGenerator(**dg_args)\nvalid_idg = ImageDataGenerator(**valid_args)","95c1ccd4":"def flow_from_dataframe(img_data_gen, raw_df, path_col, y_col, **dflow_args):\n    \"\"\"Keras update makes this much easier\"\"\"\n    in_df = raw_df.copy()\n    in_df[path_col] = in_df[path_col].map(str)\n    in_df[y_col] = in_df[y_col].map(lambda x: np.array(x))\n    df_gen = img_data_gen.flow_from_dataframe(in_df, \n                                              x_col=path_col,\n                                              y_col=y_col,\n                                    class_mode = 'raw',\n                                    **dflow_args)\n    # posthoc correction\n    df_gen._targets = np.stack(df_gen.labels, 0)\n    return df_gen","03453cdc":"train_gen = flow_from_dataframe(core_idg, train_df, \n                             path_col = 'image_path',\n                            y_col = 'aller_vec', \n                            target_size = IMG_SIZE,\n                             color_mode = 'rgb',\n                            batch_size = BATCH_SIZE)\n\n# used a fixed dataset for evaluating the algorithm\nvalid_x, valid_y = next(flow_from_dataframe(valid_idg, \n                               valid_df, \n                             path_col = 'image_path',\n                            y_col = 'aller_vec', \n                            target_size = IMG_SIZE,\n                             color_mode = 'rgb',\n                            batch_size = VALID_IMG_COUNT)) # one big batch\nprint(valid_x.shape, valid_y.shape)","fa99669d":"t_x, t_y = next(train_gen)\nprint('x', t_x.shape, t_x.dtype, t_x.min(), t_x.max())\nprint('y', t_y.shape, t_y.dtype, t_y.min(), t_y.max())\nfig, (ax1) = plt.subplots(1, 1, figsize = (10, 10))\nax1.imshow(montage_rgb((t_x-t_x.min())\/(t_x.max()-t_x.min()))[:, :, ::RGB_FLIP])\nax1.set_title('images')","bbce3b64":"plt.matshow(t_y.T)","126553b3":"base_pretrained_model = PTModel(input_shape =  t_x.shape[1:], \n                              include_top = False, weights = 'imagenet')\nbase_pretrained_model.trainable = False","50b45e37":"from keras import models, layers\nfrom keras.optimizers import Adam\nimg_in = layers.Input(t_x.shape[1:], name='Image_RGB_In')\nimg_noise = layers.GaussianNoise(GAUSSIAN_NOISE)(img_in)\npt_features = base_pretrained_model(img_noise)\npt_depth = base_pretrained_model.get_output_shape_at(0)[-1]\nbn_features = layers.BatchNormalization()(pt_features)\nfeature_dropout = layers.SpatialDropout2D(DROPOUT)(bn_features)\ngmp_dr = layers.GlobalAvgPool2D()(bn_features)\ndr_steps = layers.Dropout(DROPOUT)(layers.Dense(DENSE_COUNT, activation = 'relu')(gmp_dr))\nout_layer = layers.Dense(t_y.shape[1], activation = 'sigmoid')(dr_steps)\n\nallergen_model = models.Model(inputs = [img_in], outputs = [out_layer], name = 'full_model')\n\nallergen_model.compile(optimizer = Adam(lr=LEARN_RATE), \n                   loss = 'binary_crossentropy',\n                   metrics = ['binary_accuracy', 'mae'])\n\nallergen_model.summary()","cbe576ae":"from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nweight_path=\"{}_weights.best.hdf5\".format('allergen_detector')\n\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = True)\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\nearly = EarlyStopping(monitor=\"val_loss\", \n                      mode=\"min\", \n                      patience=15) # probably needs to be more patient, but kaggle time is limited\ncallbacks_list = [checkpoint, early, reduceLROnPlat]","77a90095":"from IPython.display import clear_output\ntrain_gen.batch_size = BATCH_SIZE\nfit_results = allergen_model.fit_generator(train_gen, \n                            steps_per_epoch = train_gen.samples\/\/BATCH_SIZE,\n                      validation_data = (valid_x, valid_y), \n                      epochs = EPOCHS, \n                      callbacks = callbacks_list,\n                      workers = 3)\nclear_output()","08d3fa79":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\nax1.plot(fit_results.history['loss'], label='Training')\nax1.plot(fit_results.history['val_loss'], label='Validation')\nax1.legend()\nax1.set_title('Loss')\nax2.plot(fit_results.history['binary_accuracy'], label='Training')\nax2.plot(fit_results.history['val_binary_accuracy'], label='Validation')\nax2.legend()\nax2.set_title('Binary Accuracy')\nax2.set_ylim(0, 1)","b5bcde86":"allergen_model.load_weights(weight_path)\nallergen_model.save('full_allergen_model.h5')","8b192902":"for k, v in zip(allergen_model.metrics_names, \n        allergen_model.evaluate(valid_x, valid_y)):\n    if k!='loss':\n        print('{:40s}:\\t{:2.1f}%'.format(k, 100*v))","c0302dbb":"t_x, t_y = next(train_gen)\nt_yp = allergen_model.predict(t_x)\nfig, (m_axs) = plt.subplots(4, 4, figsize = (20, 20))\nfor i, c_ax in enumerate(m_axs.flatten()):\n    c_ax.imshow(((t_x[i]-t_x.min())\/(t_x.max()-t_x.min()))[:, ::RGB_FLIP])\n    c_title = '\\n'.join(['{}: Pred: {:2.1f}%'.format(allergens[j], 100*t_yp[i, j]) \n                         for j, v in enumerate(t_y[i]) if v>0.5])\n    c_ax.set_title(c_title)\n    c_ax.axis('off')","2f1de41c":"t_x, t_y = valid_x, valid_y\nt_yp = allergen_model.predict(t_x)\nfig, (m_axs) = plt.subplots(8, 2, figsize = (10, 20))\nfor i, (c_ax, d_ax) in enumerate(m_axs):\n    c_ax.imshow(((t_x[i]-t_x.min())\/(t_x.max()-t_x.min()))[:, ::RGB_FLIP])\n    c_ax.axis('off')\n    d_ax.barh(np.arange(len(allergens))+0.1, t_yp[i], alpha=0.5, label='Predicted')\n    d_ax.barh(np.arange(len(allergens))-0.1, t_y[i]+0.001, alpha=0.5, label='Ground Truth')\n    d_ax.set_yticks(range(len(allergens)))\n    d_ax.set_yticklabels(allergens, rotation=0)\n    d_ax.legend();","6ccf20b3":"pred_df = pd.concat([\n    pd.DataFrame(t_yp, columns=allergens).assign(source='Prediction').assign(id=range(t_yp.shape[0])),\n    pd.DataFrame(t_y, columns=allergens).assign(source='Ground-truth').assign(id=range(t_yp.shape[0]))\n])\nflat_pred_df = pd.melt(pred_df, id_vars=['source', 'id']).pivot_table(index=['id', 'variable'], columns='source', values='value').reset_index()\nflat_pred_df['Ground-truth'] = flat_pred_df['Ground-truth'].map(lambda x: 'Positive' if x>0.5 else 'Negative')\nsns.catplot(data=flat_pred_df, x='Ground-truth', y='Prediction', col='variable', kind='swarm',  col_wrap=4)","1848ad3c":"fig, ax1 = plt.subplots(1, 1, figsize=(12, 5))\nsns.swarmplot(data=flat_pred_df, hue='Ground-truth', y='Prediction', x='variable', size=2.0, ax=ax1)","5b3f2e22":"fig, ax1 = plt.subplots(1, 1, figsize=(12, 5))\nsns.boxplot(data=flat_pred_df, hue='Ground-truth', y='Prediction', x='variable', ax=ax1)","883326e4":"from sklearn.metrics import roc_curve, roc_auc_score\nfig, ax1 = plt.subplots(1, 1, figsize=(10, 10))\nfor i, c_all in enumerate(allergens):\n    tpr, fpr, thresh = roc_curve(y_true=t_y[:, i], y_score=t_yp[:, i])\n    auc_roc = roc_auc_score(y_true=t_y[:, i], y_score=t_yp[:, i])\n    ax1.plot(tpr, fpr, '.-', label='{} (AUC:{:2.1%})'.format(c_all, auc_roc), lw=2)\nax1.legend()","185287a9":"allergen_model.get_input_at(0), allergen_model.get_output_at(0)","248b051f":"# install tensorflowjs\n!pip install -qq tensorflowjs","3db868bb":"!tensorflowjs_converter --input_format keras \\\n                       full_allergen_model.h5 \\\n                       .","68ffbb2d":"!ls -lh","94817ce1":"from IPython.display import FileLink\n!tar -czf workspace.tar.gz *","870df7f6":"FileLink('workspace.tar.gz')","a0a9e126":"# Prepare for Model","144b0d52":"# Export the Model\nWe can export the model to tensorflowjs to build a web-app that can automatically predict what allergens are in a given image ","83c1fa93":"# Build a Model","1827ac66":"## Detailed Performance by group","f7c3eb59":"# Goal\nThe goal is to make a simple model that can go from an image (taken with a smartphone) to a prediction of how likely different allergens are to be present in the food. It could be part of a helpful app for people trying to avoid foods they might be allergic to.\n\n## Setup\nWe basically take a pretrained model (Inception in this case) and add a few layers that we train ourselves in order to determine if the food contains any of the 8 different allergens identified [here](https:\/\/www.kaggle.com\/kmader\/ingredients-to-allergies-mapping\/). We try to create a balanced training group and a realistic validation group to know if the model is learning anything useful","1ee660cd":"## Correlations between allergens\nHere we can see which ones show up together. Most are expected like eggs and milk being together but interestingly tree-nuts have a negative (weak) correlation with peanuts.","6c1e06d6":"# Model Parameters","5d68b7aa":"### Show the labels\nHere we show the labels for the batch items and can see how frequent each one is","a94c09a1":"# Validation Data Results\nHere we show the results on validation data","48780500":"Split up the groups so we can validate our model on something besides the direct training data","aeda3c6f":"## Class-level ROC curves"}}