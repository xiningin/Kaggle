{"cell_type":{"ff73941b":"code","c04f679b":"code","aba19ed2":"code","ebce995d":"code","3759e4bb":"code","43809bfd":"code","be60896c":"code","01cf30d1":"code","1f7c8334":"code","4148e2ab":"code","bef51a10":"code","dba3a080":"code","09fde2cf":"code","ac2820fb":"code","a9328046":"code","900e93bc":"code","4f2b1964":"markdown","1c5b1e81":"markdown","145422d0":"markdown","8ab7c44e":"markdown","4dd9e6be":"markdown","32b67c7a":"markdown","2a4e6e9e":"markdown","42e65081":"markdown","bef3fb3a":"markdown","5587bb19":"markdown","d3ef511c":"markdown"},"source":{"ff73941b":"import numpy as np\nimport pandas as pd\nimport torch\n\n# images ids are already sorted by inchi length from a previous submission\ntest = pd.read_csv('..\/input\/inchiresnetlstmwithattentionstarter10epochs\/sample_submission_with_length.csv')\n# test = test.sort_values(by=['InChI_length'])\ntest = test.drop(columns=['InChI_length'])\n\ndef get_test_file_path(image_id):\n    return \"..\/input\/bms-molecular-translation\/test\/{}\/{}\/{}\/{}.png\".format(\n        image_id[0], image_id[1], image_id[2], image_id \n    )\n\ntest['file_path'] = test['image_id'].apply(get_test_file_path)\n\nprint(f'test.shape: {test.shape}')\ndisplay(test.head())","c04f679b":"class Tokenizer(object):\n    \n    def __init__(self):\n        self.stoi = {}\n        self.itos = {}\n\n    def __len__(self):\n        return len(self.stoi)\n    \n    def fit_on_texts(self, texts):\n        vocab = set()\n        for text in texts:\n            vocab.update(text.split(' '))\n        vocab = sorted(vocab)\n        vocab.append('<sos>')\n        vocab.append('<eos>')\n        vocab.append('<pad>')\n        for i, s in enumerate(vocab):\n            self.stoi[s] = i\n        self.itos = {item[1]: item[0] for item in self.stoi.items()}\n        \n    def text_to_sequence(self, text):\n        sequence = []\n        sequence.append(self.stoi['<sos>'])\n        for s in text.split(' '):\n            sequence.append(self.stoi[s])\n        sequence.append(self.stoi['<eos>'])\n        return sequence\n    \n    def texts_to_sequences(self, texts):\n        sequences = []\n        for text in texts:\n            sequence = self.text_to_sequence(text)\n            sequences.append(sequence)\n        return sequences\n\n    def sequence_to_text(self, sequence):\n        return ''.join(list(map(lambda i: self.itos[i], sequence)))\n    \n    def sequences_to_texts(self, sequences):\n        texts = []\n        for sequence in sequences:\n            text = self.sequence_to_text(sequence)\n            texts.append(text)\n        return texts\n    \n    def predict_caption(self, sequence):\n        caption = ''\n        for i in sequence:\n            if i == self.stoi['<eos>'] or i == self.stoi['<pad>']:\n                break\n            caption += self.itos[i]\n        return caption\n    \n    def predict_captions(self, sequences):\n        captions = []\n        for sequence in sequences:\n            caption = self.predict_caption(sequence)\n            captions.append(caption)\n        return captions\n\ntokenizer = torch.load('..\/input\/inchi-preprocess-2\/tokenizer2.pth')\nprint(f\"tokenizer.stoi: {tokenizer.stoi}\")","aba19ed2":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    debug=False\n    max_len=275\n    print_freq=1000\n    num_workers=4\n    model_name='resnet34'\n    size=224\n    scheduler='CosineAnnealingLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n    epochs=1 # not to exceed 9h\n    #factor=0.2 # ReduceLROnPlateau\n    #patience=4 # ReduceLROnPlateau\n    #eps=1e-6 # ReduceLROnPlateau\n    T_max=4 # CosineAnnealingLR\n    #T_0=4 # CosineAnnealingWarmRestarts\n    encoder_lr=1e-4\n    decoder_lr=4e-4\n    min_lr=1e-6\n    batch_size=256\n    weight_decay=1e-6\n    gradient_accumulation_steps=1\n    max_grad_norm=5\n    attention_dim=256\n    embed_dim=256\n    decoder_dim=512\n    dropout=0.5\n    seed=42\n    n_fold=5\n    trn_fold=[0] # [0, 1, 2, 3, 4]\n    train=True","ebce995d":"if CFG.debug:\n    test = test.head(1000)","3759e4bb":"# ====================================================\n# Library\n# ====================================================\nimport sys\nsys.path.append('..\/input\/pytorch-image-models\/pytorch-image-models-master')\n\nimport os\nimport gc\nimport re\nimport math\nimport time\nimport random\nimport shutil\nimport pickle\nfrom pathlib import Path\nfrom contextlib import contextmanager\nfrom collections import defaultdict, Counter\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nimport Levenshtein\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nfrom functools import partial\n\nimport cv2\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nimport torchvision.models as models\nfrom torch.nn.parameter import Parameter\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform\n\nimport timm\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","43809bfd":"# ====================================================\n# Utils\n# ====================================================\ndef get_score(y_true, y_pred):\n    scores = []\n    for true, pred in zip(y_true, y_pred):\n        score = Levenshtein.distance(true, pred)\n        scores.append(score)\n    avg_score = np.mean(scores)\n    return avg_score\n\n\ndef init_logger(log_file='inference.log'):\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = init_logger()\n\n\ndef seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False # True\n\nseed_torch(seed=CFG.seed)","be60896c":"from matplotlib import pyplot as plt\n\nplt.figure(figsize=(20, 20))\nfor i in range(20):\n    image = cv2.imread(test.loc[i, 'file_path'])\n    plt.subplot(5, 4, i + 1)\n    plt.imshow(image)\nplt.show()","01cf30d1":"from matplotlib import pyplot as plt\n\ntransform = A.Compose([A.Transpose(p=1), A.VerticalFlip(p=1)])\n\nplt.figure(figsize=(20, 20))\nfor i in range(20):\n    image = cv2.imread(test.loc[i, 'file_path'])\n    h, w, _ = image.shape\n    if h > w:\n        image = transform(image=image)['image']\n    plt.subplot(5, 4, i + 1)\n    plt.imshow(image)\nplt.show()","1f7c8334":"# ====================================================\n# Dataset\n# ====================================================\nclass TestDataset(Dataset):\n    def __init__(self, df, transform=None):\n        super().__init__()\n        self.df = df\n        self.file_paths = df['file_path'].values\n        self.transform = transform\n        self.fix_transform = A.Compose([A.Transpose(p=1), A.VerticalFlip(p=1)])\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        file_path = self.file_paths[idx]\n        image = cv2.imread(file_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        h, w, _ = image.shape\n        if h > w:\n            image = self.fix_transform(image=image)['image']\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n        return image","4148e2ab":"def get_transforms(*, data):\n    \n    if data == 'train':\n        return A.Compose([\n            A.Resize(CFG.size, CFG.size),\n            A.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(),\n        ])\n    \n    elif data == 'valid':\n        return A.Compose([\n            A.Resize(CFG.size, CFG.size),\n            A.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(),\n        ])","bef51a10":"import torch\nimport torch.nn.functional as F\n\n\ndef _inflate(tensor, times, dim):\n    # repeat_dims = [1] * tensor.dim()\n    # repeat_dims[dim] = times\n    # return tensor.repeat(*repeat_dims)\n    return torch.repeat_interleave(tensor, times, dim)\n\n\nclass TopKDecoder(torch.nn.Module):\n    r\"\"\"\n    Top-K decoding with beam search.\n\n    Args:\n        decoder_rnn (DecoderRNN): An object of DecoderRNN used for decoding.\n        k (int): Size of the beam.\n\n    Inputs: inputs, encoder_hidden, encoder_outputs, function, teacher_forcing_ratio\n        - **inputs** (seq_len, batch, input_size): list of sequences, whose length is the batch size and within which\n          each sequence is a list of token IDs.  It is used for teacher forcing when provided. (default is `None`)\n        - **encoder_hidden** (num_layers * num_directions, batch_size, hidden_size): tensor containing the features\n          in the hidden state `h` of encoder. Used as the initial hidden state of the decoder.\n        - **encoder_outputs** (batch, seq_len, hidden_size): tensor with containing the outputs of the encoder.\n          Used for attention mechanism (default is `None`).\n        - **function** (torch.nn.Module): A function used to generate symbols from RNN hidden state\n          (default is `torch.nn.functional.log_softmax`).\n        - **teacher_forcing_ratio** (float): The probability that teacher forcing will be used. A random number is\n          drawn uniformly from 0-1 for every decoding token, and if the sample is smaller than the given value,\n          teacher forcing would be used (default is 0).\n\n    Outputs: decoder_outputs, decoder_hidden, ret_dict\n        - **decoder_outputs** (batch): batch-length list of tensors with size (max_length, hidden_size) containing the\n          outputs of the decoder.\n        - **decoder_hidden** (num_layers * num_directions, batch, hidden_size): tensor containing the last hidden\n          state of the decoder.\n        - **ret_dict**: dictionary containing additional information as follows {*length* : list of integers\n          representing lengths of output sequences, *topk_length*: list of integers representing lengths of beam search\n          sequences, *sequence* : list of sequences, where each sequence is a list of predicted token IDs,\n          *topk_sequence* : list of beam search sequences, each beam is a list of token IDs, *inputs* : target\n          outputs if provided for decoding}.\n    \"\"\"\n\n    def __init__(self, decoder_rnn, k, decoder_dim, max_length, tokenizer):\n        super(TopKDecoder, self).__init__()\n        self.rnn = decoder_rnn\n        self.k = k\n        self.hidden_size = decoder_dim  # self.rnn.hidden_size\n        self.V = len(tokenizer)\n        self.SOS = tokenizer.stoi[\"<sos>\"]\n        self.EOS = tokenizer.stoi[\"<eos>\"]\n        self.max_length = max_length\n        self.tokenizer = tokenizer\n\n    def forward(self, inputs=None, encoder_hidden=None, encoder_outputs=None, function=F.log_softmax,\n                teacher_forcing_ratio=0, retain_output_probs=True):\n        \"\"\"\n        Forward rnn for MAX_LENGTH steps.  Look at :func:`seq2seq.models.DecoderRNN.DecoderRNN.forward_rnn` for details.\n        \"\"\"\n\n        # inputs, batch_size, max_length = self.rnn._validate_args(inputs, encoder_hidden, encoder_outputs,\n        #                                                         function, teacher_forcing_ratio)\n\n        batch_size = encoder_outputs.size(0)\n        max_length = self.max_length\n\n        self.pos_index = (torch.LongTensor(range(batch_size)) * self.k).view(-1, 1).cuda()\n\n        # Inflate the initial hidden states to be of size: b*k x h\n        # encoder_hidden = self.rnn._init_state(encoder_hidden)\n        if encoder_hidden is None:\n            hidden = None\n        else:\n            if isinstance(encoder_hidden, tuple):\n                # hidden = tuple([_inflate(h, self.k, 1) for h in encoder_hidden])\n                hidden = tuple([h.squeeze(0) for h in encoder_hidden])\n                hidden = tuple([_inflate(h, self.k, 0) for h in hidden])\n                hidden = tuple([h.unsqueeze(0) for h in hidden])\n            else:\n                # hidden = _inflate(encoder_hidden, self.k, 1)\n                raise RuntimeError(\"Not supported\")\n\n        # ... same idea for encoder_outputs and decoder_outputs\n        if True:  # self.rnn.use_attention:\n            inflated_encoder_outputs = _inflate(encoder_outputs, self.k, 0)\n        else:\n            inflated_encoder_outputs = None\n\n        # Initialize the scores; for the first step,\n        # ignore the inflated copies to avoid duplicate entries in the top k\n        sequence_scores = torch.Tensor(batch_size * self.k, 1)\n        sequence_scores.fill_(-float('Inf'))\n        sequence_scores.index_fill_(0, torch.LongTensor([i * self.k for i in range(0, batch_size)]), 0.0)\n        sequence_scores = sequence_scores.cuda()\n\n        # Initialize the input vector\n        input_var = torch.transpose(torch.LongTensor([[self.SOS] * batch_size * self.k]), 0, 1).cuda()\n\n        # Store decisions for backtracking\n        stored_outputs = list()\n        stored_scores = list()\n        stored_predecessors = list()\n        stored_emitted_symbols = list()\n        stored_hidden = list()\n\n        for i in range(0, max_length):\n\n            # Run the RNN one step forward\n            log_softmax_output, hidden, _ = self.rnn.forward_step(input_var, hidden,\n                                                                  inflated_encoder_outputs, function=function)\n            # If doing local backprop (e.g. supervised training), retain the output layer\n            if retain_output_probs:\n                stored_outputs.append(log_softmax_output)\n\n            # To get the full sequence scores for the new candidates, add the local scores for t_i to the predecessor scores for t_(i-1)\n            sequence_scores = _inflate(sequence_scores, self.V, 1)\n            sequence_scores += log_softmax_output.squeeze(1)\n            scores, candidates = sequence_scores.view(batch_size, -1).topk(self.k, dim=1)\n\n            # Reshape input = (bk, 1) and sequence_scores = (bk, 1)\n            input_var = (candidates % self.V).view(batch_size * self.k, 1)\n            sequence_scores = scores.view(batch_size * self.k, 1)\n\n            # Update fields for next timestep\n            predecessors = (candidates \/\/ self.V + self.pos_index.expand_as(candidates)).view(batch_size * self.k, 1)\n            if isinstance(hidden, tuple):\n                hidden = tuple([h.index_select(1, predecessors.squeeze()) for h in hidden])\n            else:\n                hidden = hidden.index_select(1, predecessors.squeeze())\n\n            # Update sequence scores and erase scores for end-of-sentence symbol so that they aren't expanded\n            stored_scores.append(sequence_scores.clone())\n            eos_indices = input_var.data.eq(self.EOS)\n            if eos_indices.nonzero().dim() > 0:\n                sequence_scores.data.masked_fill_(eos_indices, -float('inf'))\n\n            # Cache results for backtracking\n            stored_predecessors.append(predecessors)\n            stored_emitted_symbols.append(input_var)\n            stored_hidden.append(hidden)\n\n        # Do backtracking to return the optimal values\n        output, h_t, h_n, s, l, p = self._backtrack(stored_outputs, stored_hidden,\n                                                    stored_predecessors, stored_emitted_symbols,\n                                                    stored_scores, batch_size, self.hidden_size)\n\n        # Build return objects\n        decoder_outputs = [step[:, 0, :] for step in output]\n        if isinstance(h_n, tuple):\n            decoder_hidden = tuple([h[:, :, 0, :] for h in h_n])\n        else:\n            decoder_hidden = h_n[:, :, 0, :]\n        metadata = {}\n        metadata['inputs'] = inputs\n        metadata['output'] = output\n        metadata['h_t'] = h_t\n        metadata['score'] = s\n        metadata['topk_length'] = l\n        metadata['topk_sequence'] = p\n        metadata['length'] = [seq_len[0] for seq_len in l]\n        metadata['sequence'] = [seq[0] for seq in p]\n        return decoder_outputs, decoder_hidden, metadata\n\n    def _backtrack(self, nw_output, nw_hidden, predecessors, symbols, scores, b, hidden_size):\n        \"\"\"Backtracks over batch to generate optimal k-sequences.\n\n        Args:\n            nw_output [(batch*k, vocab_size)] * sequence_length: A Tensor of outputs from network\n            nw_hidden [(num_layers, batch*k, hidden_size)] * sequence_length: A Tensor of hidden states from network\n            predecessors [(batch*k)] * sequence_length: A Tensor of predecessors\n            symbols [(batch*k)] * sequence_length: A Tensor of predicted tokens\n            scores [(batch*k)] * sequence_length: A Tensor containing sequence scores for every token t = [0, ... , seq_len - 1]\n            b: Size of the batch\n            hidden_size: Size of the hidden state\n\n        Returns:\n            output [(batch, k, vocab_size)] * sequence_length: A list of the output probabilities (p_n)\n            from the last layer of the RNN, for every n = [0, ... , seq_len - 1]\n\n            h_t [(batch, k, hidden_size)] * sequence_length: A list containing the output features (h_n)\n            from the last layer of the RNN, for every n = [0, ... , seq_len - 1]\n\n            h_n(batch, k, hidden_size): A Tensor containing the last hidden state for all top-k sequences.\n\n            score [batch, k]: A list containing the final scores for all top-k sequences\n\n            length [batch, k]: A list specifying the length of each sequence in the top-k candidates\n\n            p (batch, k, sequence_len): A Tensor containing predicted sequence\n        \"\"\"\n\n        lstm = isinstance(nw_hidden[0], tuple)\n\n        # initialize return variables given different types\n        output = list()\n        h_t = list()\n        p = list()\n        # Placeholder for last hidden state of top-k sequences.\n        # If a (top-k) sequence ends early in decoding, `h_n` contains\n        # its hidden state when it sees EOS.  Otherwise, `h_n` contains\n        # the last hidden state of decoding.\n        if lstm:\n            state_size = nw_hidden[0][0].size()\n            h_n = tuple([torch.zeros(state_size).cuda(), torch.zeros(state_size).cuda()])\n        else:\n            h_n = torch.zeros(nw_hidden[0].size()).cuda()\n        l = [[self.max_length] * self.k for _ in range(b)]  # Placeholder for lengths of top-k sequences\n        # Similar to `h_n`\n\n        # the last step output of the beams are not sorted\n        # thus they are sorted here\n        sorted_score, sorted_idx = scores[-1].view(b, self.k).topk(self.k)\n        # initialize the sequence scores with the sorted last step beam scores\n        s = sorted_score.clone()\n\n        batch_eos_found = [0] * b  # the number of EOS found\n        # in the backward loop below for each batch\n\n        t = self.max_length - 1\n        # initialize the back pointer with the sorted order of the last step beams.\n        # add self.pos_index for indexing variable with b*k as the first dimension.\n        t_predecessors = (sorted_idx + self.pos_index.expand_as(sorted_idx)).view(b * self.k)\n        while t >= 0:\n            # Re-order the variables with the back pointer\n            current_output = nw_output[t].index_select(0, t_predecessors)\n            if lstm:\n                current_hidden = tuple([h.index_select(1, t_predecessors) for h in nw_hidden[t]])\n            else:\n                current_hidden = nw_hidden[t].index_select(1, t_predecessors)\n            current_symbol = symbols[t].index_select(0, t_predecessors)\n            # Re-order the back pointer of the previous step with the back pointer of\n            # the current step\n            t_predecessors = predecessors[t].index_select(0, t_predecessors).squeeze()\n\n            # This tricky block handles dropped sequences that see EOS earlier.\n            # The basic idea is summarized below:\n            #\n            #   Terms:\n            #       Ended sequences = sequences that see EOS early and dropped\n            #       Survived sequences = sequences in the last step of the beams\n            #\n            #       Although the ended sequences are dropped during decoding,\n            #   their generated symbols and complete backtracking information are still\n            #   in the backtracking variables.\n            #   For each batch, everytime we see an EOS in the backtracking process,\n            #       1. If there is survived sequences in the return variables, replace\n            #       the one with the lowest survived sequence score with the new ended\n            #       sequences\n            #       2. Otherwise, replace the ended sequence with the lowest sequence\n            #       score with the new ended sequence\n            #\n            eos_indices = symbols[t].data.squeeze(1).eq(self.EOS).nonzero()\n            if eos_indices.dim() > 0:\n                for i in range(eos_indices.size(0) - 1, -1, -1):\n                    # Indices of the EOS symbol for both variables\n                    # with b*k as the first dimension, and b, k for\n                    # the first two dimensions\n                    idx = eos_indices[i]\n                    b_idx = int(idx[0] \/\/ self.k)\n                    # The indices of the replacing position\n                    # according to the replacement strategy noted above\n                    res_k_idx = self.k - (batch_eos_found[b_idx] % self.k) - 1\n                    batch_eos_found[b_idx] += 1\n                    res_idx = b_idx * self.k + res_k_idx\n\n                    # Replace the old information in return variables\n                    # with the new ended sequence information\n                    t_predecessors[res_idx] = predecessors[t][idx[0]]\n                    current_output[res_idx, :] = nw_output[t][idx[0], :]\n                    if lstm:\n                        current_hidden[0][:, res_idx, :] = nw_hidden[t][0][:, idx[0], :]\n                        current_hidden[1][:, res_idx, :] = nw_hidden[t][1][:, idx[0], :]\n                        h_n[0][:, res_idx, :] = nw_hidden[t][0][:, idx[0], :].data\n                        h_n[1][:, res_idx, :] = nw_hidden[t][1][:, idx[0], :].data\n                    else:\n                        current_hidden[:, res_idx, :] = nw_hidden[t][:, idx[0], :]\n                        h_n[:, res_idx, :] = nw_hidden[t][:, idx[0], :].data\n                    current_symbol[res_idx, :] = symbols[t][idx[0]]\n                    s[b_idx, res_k_idx] = scores[t][idx[0]].data[0]\n                    l[b_idx][res_k_idx] = t + 1\n\n            # record the back tracked results\n            output.append(current_output)\n            h_t.append(current_hidden)\n            p.append(current_symbol)\n\n            t -= 1\n\n        # Sort and re-order again as the added ended sequences may change\n        # the order (very unlikely)\n        s, re_sorted_idx = s.topk(self.k)\n        for b_idx in range(b):\n            l[b_idx] = [l[b_idx][k_idx.item()] for k_idx in re_sorted_idx[b_idx, :]]\n\n        re_sorted_idx = (re_sorted_idx + self.pos_index.expand_as(re_sorted_idx)).view(b * self.k)\n\n        # Reverse the sequences and re-order at the same time\n        # It is reversed because the backtracking happens in reverse time order\n        output = [step.index_select(0, re_sorted_idx).view(b, self.k, -1) for step in reversed(output)]\n        p = [step.index_select(0, re_sorted_idx).view(b, self.k, -1) for step in reversed(p)]\n        if lstm:\n            h_t = [tuple([h.index_select(1, re_sorted_idx).view(-1, b, self.k, hidden_size) for h in step]) for step in reversed(h_t)]\n            h_n = tuple([h.index_select(1, re_sorted_idx.data).view(-1, b, self.k, hidden_size) for h in h_n])\n        else:\n            h_t = [step.index_select(1, re_sorted_idx).view(-1, b, self.k, hidden_size) for step in reversed(h_t)]\n            h_n = h_n.index_select(1, re_sorted_idx.data).view(-1, b, self.k, hidden_size)\n        s = s.data\n\n        return output, h_t, h_n, s, l, p\n\n    def _mask_symbol_scores(self, score, idx, masking_score=-float('inf')):\n        score[idx] = masking_score\n\n    def _mask(self, tensor, idx, dim=0, masking_score=-float('inf')):\n        if len(idx.size()) > 0:\n            indices = idx[:, 0]\n            tensor.index_fill_(dim, indices, masking_score)","dba3a080":"class Encoder(nn.Module):\n    def __init__(self, model_name='resnet18', pretrained=False):\n        super().__init__()\n        self.cnn = timm.create_model(model_name, pretrained=pretrained)\n        self.n_features = self.cnn.fc.in_features\n        self.cnn.global_pool = nn.Identity()\n        self.cnn.fc = nn.Identity()\n\n    def forward(self, x):\n        bs = x.size(0)\n        features = self.cnn(x)\n        features = features.permute(0, 2, 3, 1)\n        return features","09fde2cf":"class Attention(nn.Module):\n    \"\"\"\n    Attention network for calculate attention value\n    \"\"\"\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        \"\"\"\n        :param encoder_dim: input size of encoder network\n        :param decoder_dim: input size of decoder network\n        :param attention_dim: input size of attention network\n        \"\"\"\n        super(Attention, self).__init__()\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n\n    def forward(self, encoder_out, decoder_hidden):\n        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n        alpha = self.softmax(att)  # (batch_size, num_pixels)\n        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n        return attention_weighted_encoding, alpha\n\n\nclass DecoderWithAttention(nn.Module):\n    \"\"\"\n    Decoder network with attention network used for training\n    \"\"\"\n\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, device, encoder_dim=512, dropout=0.5):\n        \"\"\"\n        :param attention_dim: input size of attention network\n        :param embed_dim: input size of embedding network\n        :param decoder_dim: input size of decoder network\n        :param vocab_size: total number of characters used in training\n        :param encoder_dim: input size of encoder network\n        :param dropout: dropout rate\n        \"\"\"\n        super(DecoderWithAttention, self).__init__()\n        self.encoder_dim = encoder_dim\n        self.attention_dim = attention_dim\n        self.embed_dim = embed_dim\n        self.decoder_dim = decoder_dim\n        self.vocab_size = vocab_size\n        self.dropout = dropout\n        self.device = device\n        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n        self.dropout = nn.Dropout(p=self.dropout)\n        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n        self.init_weights()  # initialize some layers with the uniform distribution\n\n    def init_weights(self):\n        self.embedding.weight.data.uniform_(-0.1, 0.1)\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n\n    def load_pretrained_embeddings(self, embeddings):\n        self.embedding.weight = nn.Parameter(embeddings)\n\n    def fine_tune_embeddings(self, fine_tune=True):\n        for p in self.embedding.parameters():\n            p.requires_grad = fine_tune\n\n    def init_hidden_state(self, encoder_out):\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n        c = self.init_c(mean_encoder_out)\n        return h, c\n\n    def forward(self, encoder_out, encoded_captions, caption_lengths):\n        \"\"\"\n        :param encoder_out: output of encoder network\n        :param encoded_captions: transformed sequence from character to integer\n        :param caption_lengths: length of transformed sequence\n        \"\"\"\n        batch_size = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size = self.vocab_size\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n        num_pixels = encoder_out.size(1)\n        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n        encoder_out = encoder_out[sort_ind]\n        encoded_captions = encoded_captions[sort_ind]\n        # embedding transformed sequence for vector\n        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n        # initialize hidden state and cell state of LSTM cell\n        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n        # set decode length by caption length - 1 because of omitting start token\n        decode_lengths = (caption_lengths - 1).tolist()\n        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(self.device)\n        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(self.device)\n        # predict sequence\n        for t in range(max(decode_lengths)):\n            batch_size_t = sum([l > t for l in decode_lengths])\n            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t], h[:batch_size_t])\n            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n            attention_weighted_encoding = gate * attention_weighted_encoding\n            h, c = self.decode_step(\n                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n            predictions[:batch_size_t, t, :] = preds\n            alphas[:batch_size_t, t, :] = alpha\n        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n    \n    def predict(self, encoder_out, decode_lengths, tokenizer):\n        batch_size = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size = self.vocab_size\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n        num_pixels = encoder_out.size(1)\n        # embed start tocken for LSTM input\n        start_tockens = torch.ones(batch_size, dtype=torch.long).to(self.device) * tokenizer.stoi[\"<sos>\"]\n        embeddings = self.embedding(start_tockens)\n        # initialize hidden state and cell state of LSTM cell\n        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n        predictions = torch.zeros(batch_size, decode_lengths, vocab_size).to(self.device)\n        end_condition = torch.zeros(batch_size, dtype=torch.long).to(encoder_out.device)\n        # predict sequence\n        for t in range(decode_lengths):\n            attention_weighted_encoding, alpha = self.attention(encoder_out, h)\n            gate = self.sigmoid(self.f_beta(h))  # gating scalar, (batch_size_t, encoder_dim)\n            attention_weighted_encoding = gate * attention_weighted_encoding\n            h, c = self.decode_step(\n                torch.cat([embeddings, attention_weighted_encoding], dim=1),\n                (h, c))  # (batch_size_t, decoder_dim)\n            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n            predictions[:, t, :] = preds\n            end_condition |= (torch.argmax(preds, -1) == tokenizer.stoi[\"<eos>\"])\n            if end_condition.sum() == batch_size:\n                break\n            embeddings = self.embedding(torch.argmax(preds, -1))\n        return predictions\n    \n    def forward_step(self, prev_tokens, hidden, encoder_out, function):\n        assert len(hidden) == 2\n        h, c = hidden\n        h, c = h.squeeze(0), c.squeeze(0)\n\n        embeddings = self.embedding(prev_tokens)\n        if embeddings.dim() == 3:\n            embeddings = embeddings.squeeze(1)\n\n        attention_weighted_encoding, alpha = self.attention(encoder_out, h)\n        gate = self.sigmoid(self.f_beta(h))  # gating scalar, (batch_size_t, encoder_dim)\n        attention_weighted_encoding = gate * attention_weighted_encoding\n        h, c = self.decode_step(\n            torch.cat([embeddings, attention_weighted_encoding], dim=1),\n            (h, c))  # (batch_size_t, decoder_dim)\n        preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n\n        hidden = (h.unsqueeze(0), c.unsqueeze(0))\n        predicted_softmax = function(preds, dim=1)\n        return predicted_softmax, hidden, None","ac2820fb":"def inference(test_loader, encoder, decoder, tokenizer, device):\n    encoder.eval()\n    decoder.eval()\n    text_preds = []\n    \n    # k = 2\n    topk_decoder = TopKDecoder(decoder, 2, CFG.decoder_dim, CFG.max_len, tokenizer)\n    \n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for images in tk0:\n        images = images.to(device)\n        predictions = []\n        with torch.no_grad():\n            encoder_out = encoder(images)\n            batch_size = encoder_out.size(0)\n            encoder_dim = encoder_out.size(-1)\n            encoder_out = encoder_out.view(batch_size, -1, encoder_dim)\n            h, c = decoder.init_hidden_state(encoder_out)\n            hidden = (h.unsqueeze(0), c.unsqueeze(0))\n            \n            decoder_outputs, decoder_hidden, other = topk_decoder(None, hidden, encoder_out)\n            \n            for b in range(batch_size):\n                length = other['topk_length'][b][0]\n                tgt_id_seq = [other['topk_sequence'][di][b, 0, 0].item() for di in range(length)]\n                predictions.append(tgt_id_seq)\n            assert len(predictions) == batch_size\n            \n        predictions = tokenizer.predict_captions(predictions)\n        predictions = ['InChI=1S\/' + p.replace('<sos>', '') for p in predictions]\n        # print(predictions[0])\n        text_preds.append(predictions)\n    text_preds = np.concatenate(text_preds)\n    return text_preds","a9328046":"states = torch.load(f'..\/input\/inchiresnetlstmwithattentionstarter10epochs\/resnet34-fold0-10epochs.pth', map_location=torch.device('cpu'))\n\nencoder = Encoder(CFG.model_name, pretrained=False)\nencoder.load_state_dict(states['encoder'])\nencoder.to(device)\n\ndecoder = DecoderWithAttention(attention_dim=CFG.attention_dim,\n                               embed_dim=CFG.embed_dim,\n                               decoder_dim=CFG.decoder_dim,\n                               vocab_size=len(tokenizer),\n                               dropout=CFG.dropout,\n                               device=device)\ndecoder.load_state_dict(states['decoder'])\ndecoder.to(device)\n\ndel states; gc.collect()\n\ntest_dataset = TestDataset(test, transform=get_transforms(data='valid'))\ntest_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=CFG.num_workers)\npredictions = inference(test_loader, encoder, decoder, tokenizer, device)\n\ndel test_loader, encoder, decoder, tokenizer; gc.collect()","900e93bc":"# submission\ntest['InChI'] = [f\"{text}\" for text in predictions]\ntest[['image_id', 'InChI']].to_csv('submission.csv', index=False)\ntest[['image_id', 'InChI']].head()","4f2b1964":"# Top-K Decoder for Y.Nakama's notebook.\n\nThe original top-k decoder is taken from https:\/\/github.com\/IBM\/pytorch-seq2seq\/blob\/master\/seq2seq\/models\/TopKDecoder.py and fixed the batch beam inference.\n\n**Update**: with fast inferencing, now it should be 35% faster than previous.\n\n\n\n\n\n# About this notebook\n- PyTorch Resnet + LSTM with attention starter code\n- Preprocess notebook is [here](https:\/\/www.kaggle.com\/yasufuminakama\/inchi-preprocess-2)\n- Training notebook is [here](https:\/\/www.kaggle.com\/yasufuminakama\/inchi-resnet-lstm-with-attention-starter)\n- In this notebook, I use 2 epoch trained weight\n- There is much room to improve, for example more epochs, augmentation, larger models, larger size...\n\nIf this notebook is helpful, feel free to upvote :)\n\n# References\n- https:\/\/github.com\/sgrvinod\/a-PyTorch-Tutorial-to-Image-Captioning\n- https:\/\/github.com\/dacon-ai\/LG_SMILES_3rd\n- https:\/\/www.kaggle.com\/kaushal2896\/bms-mt-show-attend-and-tell-pytorch-baseline\n![%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202021-03-13%202.27.05.png](attachment:%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202021-03-13%202.27.05.png)\n(Figure from https:\/\/arxiv.org\/pdf\/1502.03044.pdf)","1c5b1e81":"- There are 90\u00b0 rotated images, but we trained horizontal compounds images, so we need to fix them","145422d0":"# Top-K Decoder","8ab7c44e":"# Transforms","4dd9e6be":"# Utils","32b67c7a":"# Data Loading","2a4e6e9e":"# Library","42e65081":"# Dataset","bef3fb3a":"# MODEL","5587bb19":"# Inference","d3ef511c":"# CFG"}}