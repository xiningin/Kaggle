{"cell_type":{"ce6e78e9":"code","7e7a7141":"code","9de3f7e3":"code","480bc10f":"code","f0e7ed90":"code","28628d01":"code","b8963a08":"code","b76482cc":"code","145c90cf":"code","5146e187":"code","1490e35f":"code","2dbeb468":"code","c9e511fa":"code","3935c96a":"code","2f1963f3":"code","4de9ecc8":"code","df9ea266":"markdown","c5b8c3fc":"markdown","9d80bdf3":"markdown","dda8c6f0":"markdown","45e52551":"markdown","ccda1832":"markdown","cb3d6bbb":"markdown","f30bb4a2":"markdown","e6f7f3ac":"markdown","93c1a204":"markdown","b23bd3f4":"markdown","473b60fa":"markdown","321f6f82":"markdown","8e331837":"markdown","b40d4663":"markdown","6c9c6918":"markdown","b95f627a":"markdown","cf3b8d2b":"markdown","f39800ed":"markdown"},"source":{"ce6e78e9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7e7a7141":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold # For creating folds\nfrom sklearn.metrics import log_loss # Evaluation metrics\n\nimport optuna\n#from optuna.integration.lightgbm import LGBMClassifier","9de3f7e3":"df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/test.csv\")\nss = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/sample_submission.csv\")","480bc10f":"print(f\"Shape of train : {df.shape}\")\nprint(f\"Shape of test : {test.shape}\")\nprint(f\"Shape of sample submission : {ss.shape}\")","f0e7ed90":"df.head()","28628d01":"df.info()","b8963a08":"test.info()","b76482cc":"sns.countplot(x= df.target)","145c90cf":"df[\"kfold\"] = -1\ndf = df.sample(frac=1,random_state=42).reset_index(drop=True)\ny = df.target\nkf = StratifiedKFold(n_splits=5)\nfor f, (t_,v_) in enumerate(kf.split(X=df,y=y)):\n  df.loc[v_,\"kfold\"] = f","5146e187":"lgbm = LGBMClassifier(random_state=42)\nlogloss = []\nlgbm_pred = 0\nfor f in range(5): # Looping around 5 folds\n    \n    #Splitting the data into train and validation set\n    train = df[df.kfold!= f].reset_index(drop=True) \n    valid = df[df.kfold== f].reset_index(drop=True)\n    \n    #Creating X_train and y_train\n    X_train = train.drop([\"id\",\"target\", \"kfold\"], axis=1)\n    y_train = train.target\n    X_valid = valid.drop([\"id\",\"target\", \"kfold\"], axis=1)\n    y_valid = valid.target\n    X_test = test.drop([\"id\"], axis=1)\n    \n    \n    #Fitting the model\n    lgbm.fit(X_train,y_train)\n    \n    #Predicting for valid and test datasets\n    valid_preds = lgbm.predict_proba(X_valid)\n    lgbm_pred += lgbm.predict_proba(X_test)\/5\n    \n    #Calculating log loss\n    logloss.append(log_loss(y_valid,valid_preds))\n    \nprint(logloss)\nprint(sum(logloss)\/len(logloss))","1490e35f":"def optimize(trial):\n    param = {\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'objective': 'multiclass',\n        #'metric' : ''\n        \"random_state\" : 42}\n\n\n\n    model = LGBMClassifier(**param)\n    logloss = []\n    for f in range(5):\n        train = df[df.kfold!= f].reset_index(drop=True)\n        valid = df[df.kfold== f].reset_index(drop=True)\n\n        X_train = train.drop([\"id\",\"target\", \"kfold\"], axis=1)\n        y_train = train[\"target\"]\n        X_valid = valid.drop([\"id\",\"target\", \"kfold\"], axis=1)\n        y_valid = valid[\"target\"]\n\n        model.fit(X_train,y_train)\n        pred = model.predict_proba(X_valid)\n        fold_logloss = log_loss(y_valid, pred)\n        logloss.append(fold_logloss)\n    \n    return np.mean(logloss)","2dbeb468":"study = optuna.create_study(direction=\"minimize\")\nstudy.optimize(optimize, n_trials=15)","c9e511fa":"print(study.best_params)","3935c96a":"param = {\n    'lambda_l1': 7.288238692320857, \n    'lambda_l2': 0.0011320378225972616, \n    'num_leaves': 23, \n    'feature_fraction': 0.8423047951534829, \n    'bagging_fraction': 0.8752188255110098, \n    'bagging_freq': 1, \n    'min_child_samples': 93,\n    'objective': 'multiclass',\n    'random_state' : 42\n}\nlgbm = LGBMClassifier(**param)\nlogloss = []\nlgbm_pred = 0\nfor f in range(5): # Looping around 5 folds\n    \n    #Splitting the data into train and validation set\n    train = df[df.kfold!= f].reset_index(drop=True) \n    valid = df[df.kfold== f].reset_index(drop=True)\n    \n    #Creating X_train and y_train\n    X_train = train.drop([\"id\",\"target\", \"kfold\"], axis=1)\n    y_train = train.target\n    X_valid = valid.drop([\"id\",\"target\", \"kfold\"], axis=1)\n    y_valid = valid.target\n    X_test = test.drop([\"id\"], axis=1)\n    \n    \n    #Fitting the model\n    lgbm.fit(X_train,y_train)\n    \n    #Predicting for valid and test datasets\n    valid_preds = lgbm.predict_proba(X_valid)\n    lgbm_pred += lgbm.predict_proba(X_test)\/5\n    \n    #Calculating log loss\n    logloss.append(log_loss(y_valid,valid_preds))\n    \nprint(logloss)\nprint(sum(logloss)\/len(logloss))","2f1963f3":"print(sum(logloss)\/len(logloss))","4de9ecc8":"ss[\"Class_1\"] = lgbm_pred[:,0]\nss[\"Class_2\"] = lgbm_pred[:,1]\nss[\"Class_3\"] = lgbm_pred[:,2]\nss[\"Class_4\"] = lgbm_pred[:,3]\nss[\"Class_5\"] = lgbm_pred[:,4]\nss[\"Class_6\"] = lgbm_pred[:,5]\nss[\"Class_7\"] = lgbm_pred[:,6]\nss[\"Class_8\"] = lgbm_pred[:,7]\nss[\"Class_9\"] = lgbm_pred[:,8]\nss.to_csv(\"\/kaggle\/working\/tuned_lgbm_sub.csv\", index=False)","df9ea266":"## 5. Basline model","c5b8c3fc":"1. Import libraries\n2. Read the data\n3. Check for missing values and target distribution\n4. Create folds for Cross Validation\n5. Fit with base LGBMClassifier\n6. Tune hyperparamters with Optuna\n7. Retrain the model with tuned hyperparameters.\n8. Create submission files","9d80bdf3":"### Hi everyone, in this notebook we will start with basic LGBMClassifier and then we will see how to optimize hyper paramters of LGBMClassifier with Optuna.","dda8c6f0":"Retraining with the tuned parameters","45e52551":"If you like the notebook kindly upvote it. It will motivate me to write more notebooks. :)","ccda1832":"## Approach","cb3d6bbb":"The best parameters are below.","f30bb4a2":"There are no missing values in the both train and test datasets and all are integers, so the categories might be encoded already.","e6f7f3ac":"## 2. Reading the train, test and sample submission file","93c1a204":"## Import Libraries","b23bd3f4":"## 4. Create folds for Cross Validation","473b60fa":"Creating folds for the train dataset, so that we can train the model for the n folds, to avoid overfitting.","321f6f82":"## 6. Optuna - Hyperparameter tuning","8e331837":"Target column is imbalanced, so I will use StratifiedKFold for cross validation.","b40d4663":"Since it is a baseline\/starter model, I am not doing EDA and directly moving onto model building part.","6c9c6918":"## 8. Submission file","b95f627a":"## 7. Tuned model","cf3b8d2b":"## 3. Basic data check","f39800ed":"### Thank you!"}}