{"cell_type":{"85cf958e":"code","55addded":"code","f3330cf8":"code","3e8b125e":"code","3d5965ee":"code","aa1a122a":"code","de01704f":"code","27d68b87":"code","5c41ce78":"code","1cb488c7":"code","8fbc9670":"code","0b376449":"code","d8674fc2":"code","4f99590b":"code","56aa4396":"code","c5b4a826":"code","6ed25fd0":"code","81edca30":"code","887a7595":"code","ce86f919":"code","b624a5d6":"code","0d915702":"code","3ae6f06b":"code","0ad478b0":"code","a5f70164":"code","0bc88366":"code","dae298c6":"code","54aa0b24":"markdown","d7da43a8":"markdown","0b51eb88":"markdown","6c4bbfb9":"markdown","9f342fea":"markdown","88c2aaef":"markdown","e264b1de":"markdown","dd057edf":"markdown","f82075c3":"markdown","7d6946e6":"markdown","1e28451e":"markdown","d3f3d1a8":"markdown","404a4937":"markdown","15305546":"markdown"},"source":{"85cf958e":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nplt.rcParams['figure.figsize'] = [16, 10]\nplt.rcParams['font.size'] = 16\n\nimport os\nfrom tqdm import tqdm # Fancy progress bars\n\nimport seaborn as sns\nfrom keras.preprocessing import image\nfrom keras.applications import xception\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# Our data files are available in the \"..\/input\/\" directory.\nprint(os.listdir(\"..\/input\"))\n# For Kaggle kernel purposes any results we write to the current directory is saved as output.","55addded":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'","f3330cf8":"# This project was done in Kaggle Kernels, where we'd need to copy the Keras pretrained models\n# into the cache directory (~\/.keras\/models) where keras is looking for them\n\n# Display the pretrained models that we have prepared in our file directory\n!ls ..\/input\/keras-pretrained-models\/","3e8b125e":"# Create keras cache directories in Kaggle Kernels to load the pretrained models into\ncache_dir = os.path.expanduser(os.path.join('~', '.keras')) # Cache directory\nif not os.path.exists(cache_dir):\n    os.makedirs(cache_dir)\nmodels_dir = os.path.join(cache_dir, 'models') # Models directory\nif not os.path.exists(models_dir):\n    os.makedirs(models_dir)","3d5965ee":"# Copy a selection of our pretrained models files onto the keras cache directory so Keras can access them\n!cp ..\/input\/keras-pretrained-models\/xception* ~\/.keras\/models","aa1a122a":"# Display our pretrained models that are located in the keras cache directory\n!ls ~\/.keras\/models","de01704f":"!ls ..\/input\/plant-seedlings-classification","27d68b87":"# Define Y-labels and NUM_CLASSES\nCATEGORIES = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent',\n             'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet']\nNUM_CATEGORIES = len(CATEGORIES)","5c41ce78":"SAMPLE_PER_CATEGORY = 200\nSEED = 7\ndata_dir = '..\/input\/plant-seedlings-classification\/'\ntrain_dir = os.path.join(data_dir, 'train')\ntest_dir = os.path.join(data_dir, 'test')\nsample_submission = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'))\n\nprint(train_dir)\nprint(test_dir)","1cb488c7":"sample_submission.head(10)","8fbc9670":"# Displaying the training data: Note that the training images are organized into sub-folders within the main folder,\n# organized by plant species. Hence, we are simply calling each directory name and printing out their lengths\n\nfor category in CATEGORIES:\n    print('{} {} images'.format(category, len(os.listdir(os.path.join(train_dir, category)))))\n    # \"Print length of this directory -- an integer output\"","0b376449":"# We are going to do a traversal over the directories and folders containing the training set data in order \n# to collate all the image-files and their corresponding class index and class_names into an aggregate training-set\n# collection, and convert it into a pandas DataFrame.\n\n# Note that this step simply concerns with aggregating the data (i.e.: filenames) and their labels.\n# We have not \"unpacked\" each example into its image-3D-matrix yet.\n\ntrain = []\nfor category_id, category in enumerate(CATEGORIES): # category_id is the integer index corresponding to each class_name\n    for file in os.listdir(os.path.join(train_dir, category)): # Means: \"for every \"file\" in this directory,:\"\n        train.append(['train\/{}\/{}'.format(category, file), category_id, category]) # Renaming the file names and\n        # adding to the train list\ntrain = pd.DataFrame(train, columns = ['file', 'category_id', 'category']) # Define a pandas DataFrame over training data\ntrain.head(5) # Print preview of the training DataFrame\ntrain.shape # Check shape: should be of dims (m, 3), where 3 represents file_name, category_id (int index), and class_name\n# for each example","d8674fc2":"# Remember: as this point, \"train\" is a pandas DataFrame object\n\ntrain = pd.concat([train[train['category'] == c][:SAMPLE_PER_CATEGORY] for c in CATEGORIES])\ntrain = train.sample(frac=1) # pandas function for returning a random sample of items from an axis\n# in this case, axis defaults to =0\n# The function can either return a specified number of random sample items, or a fraction of them from the selected axis\n# frac=1 means we want all of it\ntrain.index = np.arange(len(train)) # This specifies the DataFrame's index (the leftmost \"column\" counter for m)\ntrain.head(5)\ntrain.shape # Should be same as above, but m decreased due to us selected a random sample from the aggregate training-set","4f99590b":"# Similar procedure to creating the training set\n# Remember, the test examples are NOT labeled; the labels are not provided for competition purposes\n# So, the purpose of this is primarily to collate all the test examples into a neatly organized pandas DataFrame with\n# appropriate headers\n\n# Also note that the column 'filepath' is equivalent to the column 'file' in the training set DataFrame\n\ntest = []\nfor file in os.listdir(test_dir):\n    test.append(['test\/{}'.format(file), file])\ntest = pd.DataFrame(test, columns=['filepath', 'file'])\ntest.head(5)\ntest.shape # We would expect (m, 2) with m being the number of test examples, and 2 being the filepath and file columns","56aa4396":"# Image is a keras.preprocessing object containing function for preprocessing images for use in keras \/ tf models\n# Essentially, converting images into their corresponding 3-D numpy arrays\n\n# The data_dir is the root-project directory, and the filepaths we have set up nicely when preparing our datasets-in-name\n# Thus here, all we have to do is concat the filepaths, and the function will spit out the image file's array format\ndef read_img(filepath, size):\n    img = image.load_img(os.path.join(data_dir, filepath), target_size = size)\n    img = image.img_to_array(img)\n    return img","c5b4a826":"# Using matplotlib for this\n\nfig = plt.figure(1, figsize=(NUM_CATEGORIES, NUM_CATEGORIES)) # Displaying a square matrix with num_categories number of\n# images for each category, across all categories\ngrid = ImageGrid(fig, 111, nrows_ncols=(NUM_CATEGORIES, NUM_CATEGORIES), axes_pad=0.05) # Set-up grid using 'fig'\ni = 0 # Initialize counter\n\n# Iterate through the files in the categories\nfor category_id, category in enumerate(CATEGORIES):\n    for filepath in train[train['category'] == category]['file'].values[:NUM_CATEGORIES]:\n        ax = grid[i]\n        img = read_img(filepath, (224,224)) # read_img function call; filepath specified, img_size hard-coded\n        ax.imshow(img\/255.)\n        ax.axis('off')\n        if i % NUM_CATEGORIES == NUM_CATEGORIES - 1: # Labeling the row-categories (I believe)\n            ax.text(250, 112, filepath.split('\/')[1], verticalalignment='center')\n        i += 1\nplt.show();","6ed25fd0":"# A bit more sophisticated \/ randomized method of splitting train-dev than simply picking the split index to be\n# len(trainset) * split_percentage\n\nnp.random.seed(seed=SEED)\nrnd = np.random.random(len(train)) # Returns a list of len(train) of 'continuous uniform'\n# random distribution floats b\/t [0.0, 1.0)\ntrain_idx = rnd < 0.8 # Indices in which rnd is <0.8 (which should come out to roughly 80% of the dataset)\nvalid_idx = rnd >= 0.8\nytr = train.loc[train_idx, 'category_id'].values # pandas function calls\nyv = train.loc[valid_idx, 'category_id'].values\nlen(ytr)\nlen(yv)","81edca30":"# Specify parameters:\nINPUT_SIZE = 299\nPOOLING = 'avg'\nx_train = np.zeros((len(train), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\n# Initialize aggregate trainset object of shape (m_total, height, width, channels)\n\n# Fill the numpy array with image files converted into their image-3D arrays\nfor i, file in tqdm(enumerate(train['file'])): # tqdm is a progress bar\n    img = read_img(file, (INPUT_SIZE, INPUT_SIZE)) # Read the filepath into an array via our function call\n    x = xception.preprocess_input(np.expand_dims(img.copy(), axis=0)) # Pre-process that into a format for Xception model\n    x_train[i] = x # Set the i-th example in our initialized zero-4D-array to the particular example\nprint('Train Images shape: {} size: {:,}'.format(x_train.shape, x_train.size))","887a7595":"# Split X into training and validation, now that we have loaded the actual image arrays into x_train\n\nXtr = x_train[train_idx]\nXv = x_train[valid_idx]\nprint((Xtr.shape, Xv.shape, ytr.shape, yv.shape)) # Print shapes to confirm dims are correct","ce86f919":"# Forward propagation through pre-trained Xception model for feature-extraction\n# Note: No classification yet in this step\n\nxception_bottleneck = xception.Xception(weights='imagenet', include_top=False, pooling=POOLING) # Define Xception object\n    # based on \"off-the-shelf\" pre-trained Xception model\ntrain_x_bf = xception_bottleneck.predict(Xtr, batch_size=32, verbose=1) # Fwdprop through Xception for feature-extraction\nvalid_x_bf = xception_bottleneck.predict(Xv, batch_size=32, verbose=1)\n\n# Check output dims:\nprint(\"Xception train bottleneck-features shape: {} size: {:,}\".format(train_x_bf.shape, train_x_bf.size))\nprint(\"Xception valid bottleneck-features shape: {} size: {:,}\".format(valid_x_bf.shape, valid_x_bf.size))","b624a5d6":"# Define logistic regression object (that we have imported via sk-learn)\n\nlogreg = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=SEED)\nlogreg.fit(train_x_bf, ytr) # We need to fit the classifier to our (X,Y pairs)\nvalid_probs = logreg.predict_proba(valid_x_bf) # Classification on our dev set -- probabilities of various classes\nvalid_preds = logreg.predict(valid_x_bf) # Classification on our dev set -- predicted classes","0d915702":"# accuracy_score is an object we've imported from sk-learn\n\nprint(\"Validation Xception Accuracy: {}\".format(accuracy_score(yv, valid_preds)))","3ae6f06b":"cnf_matrix = confusion_matrix(yv, valid_preds) # Confusion matrix imported from sk-learn\nabbreviation = ['BG', 'Ch', 'Cl', 'CC', 'CW', 'FH', 'LSB', 'M', 'SM', 'SP', 'SFC', 'SB']\npd.DataFrame({'class': CATEGORIES, 'abbreviation': abbreviation})","0ad478b0":"# Plotting the confusion matrix to illustrate correct and incorrect predictions\n\nfig, ax = plt.subplots(1)\nax = sns.heatmap(cnf_matrix, ax=ax, cmap=plt.cm.Greens, annot=True)\nax.set_xticklabels(abbreviation)\nax.set_yticklabels(abbreviation)\nplt.title('Confusion Matrix')\nplt.ylabel('True Class')\nplt.xlabel('Predicted Class')\nfig.savefig('Confusion matrix.png', dpi=300)\nplt.show();","a5f70164":"# Creating the X input objects for the test data\n# (similar to how we constructed the 4D input x_train from the filepath object)\n\nx_test = np.zeros((len(test), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\nfor i, filepath in tqdm(enumerate(test['filepath'])):\n    img = read_img(filepath, (INPUT_SIZE, INPUT_SIZE))\n    x = xception.preprocess_input(np.expand_dims(img.copy(), axis=0))\n    x_test[i] = x\nprint(\"Test images dataset shape: {} size: {:,}\".format(x_test.shape, x_test.size))","0bc88366":"# Run forwardprop on the test set input through Xception to get encoded-feature-representation\n\ntest_x_bf = xception_bottleneck.predict(x_test, batch_size=32, verbose=1)\nprint('Xception test bottleneck features shape: {} size: {:,}'.format(test_x_bf.shape, test_x_bf.size))\n\n# Run encoded-feature-representations through the Logistic-Regression classifier (by sk-learn)\n\ntest_preds = logreg.predict(test_x_bf)","dae298c6":"# Note: test_preds have shape of (794, 1) that corresponds to the class for each of the test examples\n\n# Creating the submission file\n\ntest['category_id'] = test_preds\ntest['species'] = [CATEGORIES[c] for c in  test_preds]\ntest[['file', 'species']].to_csv('submission.csv', index=False)","54aa0b24":"**Creating Our Training Set**","d7da43a8":"**Loading up Keras Pretrained Models into Kaggle Kernels**","0b51eb88":"**Run Examples through the Pre-trained Xception Model to Extract Xception Features \/ Representations:  \n(pre-classification step)**","6c4bbfb9":"**Image Classification: Plant Seedlings Classification using Convolutional Neural Networks and Transfer Learning**\n<br><br>We will use a pre-trained state-of-the-art CNN architecture, Xception, to classify various species of crop seedlings taken at several growth stages. The ability to do so effectively can mean better crop yields and better stewardship of the environment.\n<br>-The inputs will be plant seedling images. We will use the Keras deep learning library and supported functionalities for our deep learning model.\n<br>-4,750 training and validation examples, 794 test examples\n<br>-12 classes of plant species\n<br>-(X,Y) supervised learning dataset where X is the input image and Y is the ground-true class label\n<br><br><u>Methodology:<\/u>\n<br>-Use a pre-trained CNN, Xception, to extract an input image's feature-learned-representation-vector during forward-pass\n<br>-Take said representation from the CNN architecture's \"bottleneck\" layer\n<br>-Run the extracted feature-representation-vectors through a linear Logistic Regression classifier for prediction purposes\n<br><br><u>Notes:<\/u>\n<br>-Instead of extracting the bottleneck feature-vector then running it through a [separate] LR classifier, we could alternatively have appended another fully-connected layer to the end of the CNN. This would have likely produced better results, but would be more time-consuming and resource-intensive to train and fine-tune. Classifying the feature-representations via a simple logistic regression classifier provides a good starting baseline\n<br><br><u>Sources:<\/u>\n<br>Kaggle Challenge Competition: https:\/\/www.kaggle.com\/c\/plant-seedlings-classification\n<br>Plant Seedlings Dataset and Paper: https:\/\/vision.eng.au.dk\/plant-seedlings-dataset\/ | https:\/\/arxiv.org\/abs\/1711.05458\n<br>Keras Pre-trained Models: https:\/\/keras.io\/applications\/#models-for-image-classification-with-weights-trained-on-imagenet","9f342fea":"**LogReg Classification on (\"using\") Resulting Xception-bottleneck Features:**","88c2aaef":"**Reading an Image to an Array**","e264b1de":"**Loading and Visualizing Sample Images (Training Examples)**","dd057edf":"**Import Dependencies**","f82075c3":"**Train-Validation Split**","7d6946e6":"**Preparing the Dataset for the Model**","1e28451e":"**Creating Our Test Set**","d3f3d1a8":"**Creating Our Aggregate Training Sample for the CNN:**","404a4937":"**Illustrating the Results: Confusion Matrix**","15305546":"**Finalization and Creating the Submission**"}}