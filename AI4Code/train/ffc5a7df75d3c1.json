{"cell_type":{"ac089062":"code","d4730a11":"code","bffaaa19":"code","694142e0":"code","922d02c7":"code","a7cee603":"code","25f0b3a2":"code","e150bffd":"code","04606926":"code","7eff0327":"code","1ad56c31":"code","87836a8a":"code","e1ad6d00":"code","a32c1bcf":"code","24af0e16":"code","aea23df5":"code","877589e7":"code","64196867":"code","8e6033d7":"code","fa3afb28":"code","655bf2c6":"code","753b78f3":"code","d0b5129b":"code","fbc6fd06":"code","d04d46b5":"code","fed42033":"markdown","2a82d657":"markdown","ce077086":"markdown","54615c3a":"markdown","1b4f3e71":"markdown","7f34c3dd":"markdown","2ec6ede1":"markdown","b73430bc":"markdown","5c2dd6eb":"markdown","cbf99b88":"markdown","371471e7":"markdown","d159138c":"markdown","b27e9903":"markdown"},"source":{"ac089062":"#------------------------------------------Libraries---------------------------------------------------------------#\n####################################################################################################################\n#-------------------------------------Boiler Plate Imports---------------------------------------------------------#\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n#---------------------------------------Text Processing------------------------------------------------------------#\nimport regex\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import WordPunctTokenizer\nfrom string import punctuation\nfrom nltk.stem import WordNetLemmatizer\n#####################################################################################################################","d4730a11":"names = []\nbase = '\/kaggle\/input\/topic-modelling-on-emails\/Data\/'\nwith os.scandir(base) as entries:\n    for entry in entries:\n        if(entry.is_file() == False):\n            names.append(entry.name)\nnames","bffaaa19":"names.sort()","694142e0":"files = {}\nunique = []\nfor name in names:\n    path = base + name+'\/'\n    x = []\n    with os.scandir(path) as entries:\n        for entry in entries:\n            if(entry.is_file()):\n                x.append(entry.name)\n    files[name] = x\n    files[name].sort()","922d02c7":"for k, v in files.items():\n    print(k, len(v))","a7cee603":"names","25f0b3a2":"for i in range(len(names)):\n    x = files[names[i]]\n    for j in x:\n        for k in range(i+1, len(names)):\n            key = names[k]\n            if j in files[key]:\n                files[key].remove(j)","e150bffd":"for k, v in files.items():\n    print(k, len(v))","04606926":"data = {}\ni = 0\n\nfor genre in files.keys() :\n    texts = files[genre]\n    for text in texts:\n        if text in files[genre]:\n            path = base + genre + '\/' + text\n            with open(path, \"r\", encoding = \"latin1\") as file:\n                data[i] = file.readlines()\n                i = i+1\n            data[i-1] = [\" \".join(data[i-1]), genre] \n\ndata = pd.DataFrame(data).T\nprint(data.shape)\ndata.columns = ['Text', 'Class']\ndata.head()","7eff0327":"data.info()","1ad56c31":"data.isna().sum()","87836a8a":"unique = list(data.Text.unique())\nlen(unique)","e1ad6d00":"dic = dict(data)","a32c1bcf":"uni = {}\ni = 0\nfor k in range(len(list(dic['Text']))):\n    if dic['Text'][k] in unique:\n        uni[i] = [dic['Text'][k], dic['Class'][k]]\n        unique.remove(dic['Text'][k])\n        i += 1","24af0e16":"data = pd.DataFrame(uni).T\nprint(data.shape)\ndata.columns = ['Text', 'Class']\ndata.head()","aea23df5":"plt.figure(figsize=(10,5))\nax = sns.countplot(data.Class, palette = sns.color_palette(\"mako\"))","877589e7":"def make_wordcloud(words,title):\n    cloud = WordCloud(width=1920, height=1080,max_font_size=200, max_words=300, background_color=\"white\").generate(words)\n    plt.figure(figsize=(20,20))\n    plt.imshow(cloud, interpolation=\"gaussian\")\n    plt.axis(\"off\") \n    plt.title(title, fontsize=60)\n    plt.show()","64196867":"wordnet_lemmatizer = WordNetLemmatizer()\n\nstop = stopwords.words('english')\n\nfor punct in punctuation:\n    stop.append(punct)\n\ndef filter_text(text, stop_words):\n    word_tokens = WordPunctTokenizer().tokenize(text.lower())\n    filtered_text = [regex.sub(u'\\p{^Latin}', u'', w) for w in word_tokens if w.isalpha() and len(w) > 3]\n    filtered_text = [wordnet_lemmatizer.lemmatize(w, pos=\"v\") for w in filtered_text if not w in stop_words] \n    return \" \".join(filtered_text)","8e6033d7":"data[\"filtered_text\"] = data.Text.apply(lambda x : filter_text(x, stop)) \ndata.head()","fa3afb28":"all_text = \" \".join(data[data.Class == \"Crime\"].filtered_text) \nmake_wordcloud(all_text, \"Crime\")","655bf2c6":"count = pd.DataFrame(all_text.split(), columns = ['words'])\ntop_10 = count[count['words'].isin(list(count.words.value_counts()[:10].index[:10]))]\nplt.figure(figsize=(10,5))\nsns.barplot(x = top_10.words.value_counts().index,\n            y = top_10.words.value_counts(), palette = sns.color_palette(\"mako\"))","753b78f3":"all_text = \" \".join(data[data.Class == \"Politics\"].filtered_text) \nmake_wordcloud(all_text, \"Politics\")","d0b5129b":"count = pd.DataFrame(all_text.split(), columns = ['words'])\ntop_10 = count[count['words'].isin(list(count.words.value_counts()[:10].index[:10]))]\nplt.figure(figsize=(10,5))\nsns.barplot(x = top_10.words.value_counts().index,\n            y = top_10.words.value_counts(), palette = sns.color_palette(\"mako\"))","fbc6fd06":"all_text = \" \".join(data[data.Class == \"Science\"].filtered_text) \nmake_wordcloud(all_text, \"Science\")","d04d46b5":"count = pd.DataFrame(all_text.split(), columns = ['words'])\ntop_10 = count[count['words'].isin(list(count.words.value_counts()[:10].index[:10]))]\nplt.figure(figsize=(10,5))\nsns.barplot(x = top_10.words.value_counts().index,\n            y = top_10.words.value_counts(), palette = sns.color_palette(\"mako\"))","fed42033":"* From the above result it is clearly implied that the class 'Entertainment' had no data unique to its class. \n* By not considering it, we are also eliminating any variance that can be caused by the duplicate data.","2a82d657":"### Top 10 words in the Crime Category","ce077086":"### Science","54615c3a":"### Top 10 words in the Politics Category","1b4f3e71":"# Introduction\n## Data\n* The input data comprises of files within the folders with class labels as their name.\n* The textual data are emails written from one journalist to another or to their source, regarding a story.\n* Emails can be seen labelled under multiple classes which would mislead the model.\n\n# Methodology\n* To clean our data and obtain some meaningful insights, we first need to make sure our data is properly labelled and stored\n* To do so:\n    1. Read the text from each file and while doing so, make sure we are not reading duplicate data.\n    2. Load the textual data into DataFrame for easier analysis.\n    3. Clean the raw text by:\n        * Removing special characters, punctuations, pronouns, stopwords.\n        * Tokeizing each data point, i.e segmenting text into sentences and further into words.\n        * Normalize the text by converting it into lower case.\n        * Extract the lemma for each word. Ex: Lemma(swimming) -> swim.","7f34c3dd":"### Politics","2ec6ede1":"### Top 10 words in the Science Category","b73430bc":"## Read Data from the '.txt' files","5c2dd6eb":"Now that we managed to load our data frame, we can move to the next step, i.e cleaning the data.","cbf99b88":"* We now know how many files are labelled under each class. Our job now is to remove the data points that are labelled under other classes. Ex: 14147.txt is labelled under 'Crime', 'Entertainment' and 'Science', so we will be removing the entry from 'Entertainment' and 'Science'.\n* The risk here is that we might have removed the entry from a correctly labelled class. Ex: 14147.txt may be labelled as 'Science' initially and was repeated in other classes, by removing it from 'Science' we are mislabelling the data as 'Crime'.\n* But 'Science' already contains the most no. of entries, making it easier for us to train the model for that particular class. Hence, our approach doesn't affect the analysis.","371471e7":"#### The data at this point is cleaned but not structured yet. This can be achieved by transforming data into numbers. We'll focus on rest of the pre-processing and modelling in other [notebook]('https:\/\/www.kaggle.com\/dipankarsrirag\/topic-modelling-using-ensemble-methods'). \n## Thank You.","d159138c":"There still exists few duplicate texts which might have been the result of poor data management or sending the same mail multiple times.","b27e9903":"We can now find some useful insights into the data set by constructing wordclouds and find term frequencies in each class.\n\n### Crime"}}