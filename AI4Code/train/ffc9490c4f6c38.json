{"cell_type":{"abda3958":"code","9fa0efdc":"code","61d369e3":"code","3bd84dbf":"code","a1c726b6":"code","f08401b9":"code","ee4f2f52":"code","6a3265e0":"code","9d0af336":"markdown","f0e6a1a7":"markdown","cb2c4433":"markdown","0d3430db":"markdown","48ab2e63":"markdown","5ee93f29":"markdown","bed166bd":"markdown","d6f3381e":"markdown"},"source":{"abda3958":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\ntrain = pd.read_csv('..\/input\/train.csv')","9fa0efdc":"train2=train[train['wheezy-copper-turtle-magic']==0]\n\nfeats = [f for f in train2.columns if f not in ['id','target']]\ndef plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(64,4,figsize=(15,100))\n\n    for feature in features:\n        i += 1\n        plt.subplot(32,8,i)\n        sns.distplot(df1[feature], hist=False,label=label1)\n        sns.distplot(df2[feature], hist=False,label=label2)\n        \n        plt.xlabel(feature, fontsize=9)\n        plt.xlim(-30,30)\n        plt.ylim(0,1)\n        \n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show();\n    \nt0 = train2[feats].loc[train['target'] == 0]\nt1 = train2[feats].loc[train['target'] == 1]\nfeatures = train2[feats].columns.values\nplot_feature_distribution(t0, t1, '0', '1', features);","61d369e3":"train2=train[train['wheezy-copper-turtle-magic']==0]\nmin_max = []\nfor x in train2.columns[1:-1][train2.columns[1:-1]!='wheezy-copper-turtle-magic']:\n    min_max.append(train2[x].values.max()-train2[x].values.min())        \nsns.distplot(min_max);\nplt.title('range histgram (wheezy-copper-turtle-magic=0)')\nplt.show()","3bd84dbf":"var = []\nfor x in train2.columns[1:-1][train2.columns[1:-1]!='wheezy-copper-turtle-magic']:\n    var.append(train2[x].var())        \nsns.distplot(var);\nplt.title('variance histgram (wheezy-copper-turtle-magic=0)')\nplt.show()","a1c726b6":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","f08401b9":"oof = np.zeros(len(train))\npreds = np.zeros(len(test))\nn_split = 5\n\nfor i in range(512):\n    cols = [c for c in train.columns if c not in ['id', 'target']]\n    cols.remove('wheezy-copper-turtle-magic')\n    train2 = train[train['wheezy-copper-turtle-magic']==i].copy()\n    test2 = test[test['wheezy-copper-turtle-magic']==i].copy()\n    \n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n    test2.reset_index(drop=True,inplace=True)\n\n    skf = StratifiedKFold(n_splits=n_split, random_state=42)\n    for train_index, test_index in skf.split(train2[train2.columns[train2.columns!='target']], train2['target']):\n\n        clf = LogisticRegression(solver='sag',penalty='l2',C=0.001)\n        clf.fit(train2.loc[train_index][cols],train2.loc[train_index]['target'])\n        oof[idx1[test_index]] = clf.predict_proba(train2.loc[test_index][cols])[:,1]\n        preds[idx2] += clf.predict_proba(test2[cols])[:,1] \/ n_split\n\nauc = roc_auc_score(train['target'],oof)\nprint('LR scores CV =',round(auc,5))","ee4f2f52":"oof = np.zeros(len(train))\npreds = np.zeros(len(test))\nn_split = 5\nprint('drop columnss whose data range is upper 15.')\nfor i in range(512):\n    cols = [c for c in train.columns if c not in ['id', 'target']]\n    cols.remove('wheezy-copper-turtle-magic')\n    train2 = train[train['wheezy-copper-turtle-magic']==i].copy()\n    test2 = test[test['wheezy-copper-turtle-magic']==i].copy()\n    \n    ##  Reduction by Range\n    for x in cols:\n        if train2[x].values.max()-train2[x].values.min() >= 15:\n            train2 = train2.drop([x],axis=1)\n            test2 = test2.drop([x],axis=1)\n            cols.remove(x)\n    \n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n    test2.reset_index(drop=True,inplace=True)\n\n    skf = StratifiedKFold(n_splits=n_split, random_state=42)\n    for train_index, test_index in skf.split(train2[train2.columns[train2.columns!='target']], train2['target']):\n\n        clf = LogisticRegression(solver='sag',penalty='l2',C=0.001)\n        clf.fit(train2.loc[train_index][cols],train2.loc[train_index]['target'])\n        oof[idx1[test_index]] = clf.predict_proba(train2.loc[test_index][cols])[:,1]\n        preds[idx2] += clf.predict_proba(test2[cols])[:,1] \/ n_split\n\nauc = roc_auc_score(train['target'],oof)\nprint('LR with dimention reduction by data range scores CV =',round(auc,5))","6a3265e0":"oof = np.zeros(len(train))\npreds = np.zeros(len(test))\nn_split = 5\nprint('drop columns whose data range is under 15.')\nfor i in range(512):\n    cols = [c for c in train.columns if c not in ['id', 'target']]\n    cols.remove('wheezy-copper-turtle-magic')\n    train2 = train[train['wheezy-copper-turtle-magic']==i].copy()\n    test2 = test[test['wheezy-copper-turtle-magic']==i].copy()\n    \n    ##  Reduction by Range\n    for x in cols:\n        if train2[x].values.max()-train2[x].values.min() < 15:\n            train2 = train2.drop([x],axis=1)\n            test2 = test2.drop([x],axis=1)\n            cols.remove(x)\n    \n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n    test2.reset_index(drop=True,inplace=True)\n\n    skf = StratifiedKFold(n_splits=n_split, random_state=42)\n    for train_index, test_index in skf.split(train2[train2.columns[train2.columns!='target']], train2['target']):\n\n        clf = LogisticRegression(solver='sag',penalty='l2',C=0.001)\n        clf.fit(train2.loc[train_index][cols],train2.loc[train_index]['target'])\n        oof[idx1[test_index]] = clf.predict_proba(train2.loc[test_index][cols])[:,1]\n        preds[idx2] += clf.predict_proba(test2[cols])[:,1] \/ n_split\n\nauc = roc_auc_score(train['target'],oof)\nprint('LR with dimention reduction by data range scores CV =',round(auc,5))","9d0af336":"'useless' columns are revealed?","f0e6a1a7":"Next, I drop columns whose data range is under 15.","cb2c4433":"## Predict with reduction\nCan I drop these cells?\n","0d3430db":"### Dimension reduction by data range\n\nFirst, I drop columns whose data range is upper 15.\n(The number 15 is from looking plots shown earlier)","48ab2e63":"Some plots show different from other. It looks like the range or variance is different.\n\nPlot these range and variance.","5ee93f29":"## feature histgram","bed166bd":"# Search 'useless columns'\n\nIn this kernel, suggest 'useless' columns.\nI believe these should be drop but may not be correct.\n\n\nbased on [LR great kernel](https:\/\/www.kaggle.com\/cdeotte\/logistic-regression-0-800).\n","d6f3381e":"plot code from [this kernel](https:\/\/www.kaggle.com\/donariumdebbie\/explore-funny-column-names#Target-distribution-of-group-of-column-names)\n\nThat kernel revealed <b>wheezy-copper-turtle-magic<\/b> columns shows different pattern.\n\nIn this cell, restrict only <b>wheezy-copper-turtle-magic<\/b>==0. Fix the axis and show histgram.\n"}}