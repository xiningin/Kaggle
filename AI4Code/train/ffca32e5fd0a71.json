{"cell_type":{"608da19b":"code","d94472b2":"code","1696e7e4":"code","49394a75":"code","cf424794":"code","78550d49":"code","ff13f7a0":"code","5936b8b9":"code","7150c214":"code","386b1f5e":"code","ccfefa09":"code","b1d4de89":"code","df5c1f42":"code","7946ba48":"code","8b3fc058":"code","099a548b":"code","fa5d90f8":"code","7914946f":"code","010f9ced":"code","42b72e02":"code","94613b53":"code","2e074de1":"code","90689e90":"markdown","eeab342f":"markdown","fcf8f18c":"markdown","2ceb5494":"markdown","c2227e26":"markdown","7c28fff7":"markdown","dd2da380":"markdown","585a7625":"markdown","5d84c44b":"markdown","dae77744":"markdown","31f61510":"markdown","9d4f006a":"markdown","fd6068e8":"markdown"},"source":{"608da19b":"import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport scipy.stats as sts\n%matplotlib inline\nimport seaborn as sns\nimport statistics\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#import machine learning\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris \n\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.externals import joblib #save model\n\nfrom xgboost import XGBClassifier\nimport xgboost\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, f1_score, recall_score\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\n\n%matplotlib inline","d94472b2":"df = pd.read_csv('..\/input\/telecom-users\/telecom_users.csv', delimiter=',')\ndf # View the table. Our Churn is at the end on the right","1696e7e4":"df.info()\n# Almost all values are object, which means the task is categorical. In the future we will take this into account when choosing training models","49394a75":"df.isnull().sum()# let's check if there are any null","cf424794":"df['InternetService'].value_counts(dropna=False) ","78550d49":"# delete the null column, it is not needed\n#df = df.drop(columns='Unnamed')\n\n# the gender attribute is converted to a number\ndf['gender'] = df['gender'].map({'Female': 0, \n                                 'Male': 1}).astype(int)\n\n# Using a loop for columns with 2 categories: 'yes' or 'no'\nlist_yes_no = ['Partner', 'Dependents', 'PhoneService', \n               'PaperlessBilling', 'Churn']\nfor column in list_yes_no:\n    df[column] = df[column].map({'No': 0, \n                                 'Yes': 1}).astype(int)\n\n# indication of phone lines in numbers (3 categories!)\ndf['MultipleLines'] = df['MultipleLines'].map({'No': 0, \n                                               'Yes': 1, \n                                               'No phone service': 2}).astype(int)\n\ndf['InternetService'] = df['InternetService'].map({'DSL': 0, \n                                                   'Fiber optic': 1, \n                                                   'No': 2}).astype(int)\n\n\nlist_3_categ = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', \n                'TechSupport', 'StreamingTV', 'StreamingMovies']\n\n# loop \nfor column in list_3_categ:\n    df[column] = df[column].map({'No': 0, 'Yes': 1, \n                                 'No internet service': 2}).astype(int)\n\ndf['Contract'] = df['Contract'].map({'Month-to-month': 0, \n                                     'One year': 1, \n                                     'Two year': 2}).astype(int)\n\n\ndf['PaymentMethod'] = df['PaymentMethod'].map({'Electronic check': 0, \n                                               'Mailed check': 1, \n                                               'Bank transfer (automatic)': 2, \n                                               'Credit card (automatic)': 3}).astype(int)\n","ff13f7a0":"fig, subplot = plt.subplots()\nsubplot.hist(df['tenure'].values, bins=4, histtype='bar',\n             align='mid', orientation='vertical');\nfig, subplot = plt.subplots()\nsubplot.hist(df['MonthlyCharges'].values, bins=4, histtype='bar',\n             align='mid', orientation='vertical');","5936b8b9":"# from the histograms around the dienes dipezona breakdown\n\ndf.loc[df['tenure'] <= 18, 'tenure'] = 0\ndf.loc[(df['tenure'] > 18) & (df['tenure'] <= 36), 'tenure'] = 1\ndf.loc[(df['tenure'] > 36) & (df['tenure'] <= 54), 'tenure'] = 2\ndf.loc[df['tenure'] > 54, 'tenure'] = 3\n\ndf.loc[df['MonthlyCharges'] <= 42, 'MonthlyCharges'] = 0\ndf.loc[(df['MonthlyCharges'] > 42) & (df['MonthlyCharges'] <= 70), 'MonthlyCharges'] = 1\ndf.loc[(df['MonthlyCharges'] > 70) & (df['MonthlyCharges'] <= 95), 'MonthlyCharges'] = 2\ndf.loc[df['MonthlyCharges'] > 95, 'MonthlyCharges'] = 3","7150c214":"df","386b1f5e":"# By the way, there is another way of splitting, without cycles. But in this case, it is not convenient.\n#df = pd.get_dummies(df_main, columns =['gender','Partner'])","ccfefa09":"import seaborn as sns\nsns.heatmap(df.corr(),annot=True,cmap='RdYlGn',linewidths=0.2)\nplt.title('Pearson Correlation of Features', y=1.05, size=18)\nfig=plt.gcf()\nfig.set_size_inches(20,12)\nplt.show()","b1d4de89":"# to avoid redundancy of features, we will delete some features:\n# just in case, we'll make a copy of the dataframe\ndf_main = df.copy()\ndrop_elements = ['PhoneService', 'StreamingMovies', 'StreamingTV', \n                 'TechSupport', 'DeviceProtection', 'OnlineBackup']\ndf_main = df_main.drop(drop_elements, axis=1)\n\n# And we will also delete 3 attributes that have:\n# - very weak \"connections\", and they can ruin our models by making noise.\n# SeniorCitizen, Partner, Dependents \ndrop_elements2 = ['customerID','SeniorCitizen', 'Partner', 'Dependents','TotalCharges']\ndf_main = df_main.drop(drop_elements2, axis=1)","df5c1f42":"X = df_main.drop(['Churn'], axis=1)\ny = df_main.Churn\n\nTEST_SIZE = 0.3 \nRAND_STATE = 42\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = TEST_SIZE, random_state=RAND_STATE)","7946ba48":"#train XGBoost model\n\nxgb = xgboost.XGBClassifier(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,colsample_bytree=1, max_depth=7)\nxgb.fit(X_train,y_train.squeeze().values)\ny_train_preds = xgb.predict(X_train)\ny_test_preds = xgb.predict(X_test) \nprint('XGBoost: {:.2f}'.format(xgb.score(X_test, y_test)))\n\ny_test_preds_itog = xgb.predict(X) ","8b3fc058":"# KNeighborsClassifier \n# First let's see how many neighbors to take for training\n\nneighbors = np.arange(1, 15)\n\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\nfor i, k in enumerate(neighbors):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    train_accuracy[i] = knn.score(X_train, y_train)\n    test_accuracy[i] = knn.score(X_test, y_test)\n\nplt.plot(neighbors, test_accuracy, label='Testing dataset Accuracy')\nplt.plot(neighbors, train_accuracy, label='Training dataset Accuracy')\nplt.legend()\nplt.xlabel('n_neighbors')\nplt.ylabel('Accuracy')\nplt.show()","099a548b":"knn = KNeighborsClassifier(n_neighbors=7)\nknn.fit(X_train, y_train)\nprint('Accuracy: {:.2f}'.format(knn.score(X_test, y_test)))\ny_test_preds_knn = knn.predict(X) ","fa5d90f8":"# GradientBoostingClassifier \ngbc = GradientBoostingClassifier()\ngbc.fit(X_train, y_train)\npredicted_y = gbc.predict(X_test)\nprint('Accuracy: {:.2f}'.format(gbc.score(X_test, y_test)))\ny_test_preds_gbc = gbc.predict(X) ","7914946f":"# LogisticRegression\nclassifier = LogisticRegression(solver='lbfgs',random_state=40)\nclassifier.fit(X_train, y_train)\npredicted_y = classifier.predict(X_test)\nprint('Accuracy: {:.2f}'.format(classifier.score(X_test, y_test)))\ny_test_preds_classifier = gbc.predict(X) ","010f9ced":"# SVC\nSVC_model = SVC()  \nSVC_model.fit(X_train, y_train)\npredicted_y = SVC_model.predict(X_test)\nprint('Accuracy: {:.2f}'.format(SVC_model.score(X_test, y_test)))     \ny_test_preds_SVC_model = SVC_model.predict(X) ","42b72e02":"# GaussianNB\nclf = GaussianNB()\nclf.fit(X_train, y_train)\npredicted_y = clf.predict(X_test)\ny_test_preds_clf = clf.predict(X) \nprint('Accuracy: {:.2f}'.format(clf.score(X_test, y_test)))","94613b53":"tree = DecisionTreeClassifier(criterion='entropy',max_depth=4,random_state=40)\ntree.fit(X_train, y_train)\npredicted_y = tree.predict(X_test)\nprint('Accuracy: {:.2f}'.format(tree.score(X_test, y_test)))\ny_test_preds_tree = tree.predict(X) ","2e074de1":"print('XGBoost: {:.2f}'.format(xgb.score(X_test, y_test)))\nprint('KNN: {:.2f}'.format(knn.score(X_test, y_test)))\nprint('GradientBoos: {:.2f}'.format(gbc.score(X_test, y_test)))\nprint('LogisticRegression: {:.2f}'.format(classifier.score(X_test, y_test)))\nprint('SVC: {:.2f}'.format(SVC_model.score(X_test, y_test)))     \nprint('GaussianNB: {:.2f}'.format(clf.score(X_test, y_test)))\nprint('tree: {:.2f}'.format(tree.score(X_test, y_test)))\ndf3 = pd.DataFrame({'|ACTUAL|': y, 'XGBoost': y_test_preds_itog,'KNN': y_test_preds_knn, 'Gradient': y_test_preds_gbc, 'LogisticR': y_test_preds_classifier, 'SVC_model': y_test_preds_SVC_model, 'GaussianNB': y_test_preds_clf, 'DecisionTree': y_test_preds_tree})\ndf3.sort_index().head(20) \n#df3.sort_index().tail(60) ","90689e90":"# Analysis of customer behavior of a telecommunications company\n\n#### Task.\nThe company provides Internet access and video content services. The task is to predict which customers will want to terminate the contract.\n\n#### Goals.\nKeeping an existing client is cheaper than attracting a new one. If a customer wants to leave, you can make a discount and help them solve the problems they face. This is a completely different approach than to a new client. To impose a new service on regular customers, increase the fee imperceptibly. Stop running ads on it.\n\n#### Incoming parameters.\n\n\ntelecom_users.csv customer database for training\n\ncustomerID \u2013 id of the client\n\ngender-gender of the client (male\/female)\n\nSeniorCitizen \u2013 whether the client is a pensioner (1, 0)\n\nPartner \u2013 whether the client is married (Yes, No)\n\nDependents \u2013 whether the client has dependents (Yes, No)\n\ntenure \u2013 how many months a person has been a client of the company\n\nPhoneService \u2013 whether the phone service is enabled (Yes, No)\n\nMultipleLines \u2013 whether multiple phone lines are connected (Yes, No, No phone service)\n\nInternetService-the client's Internet provider (DSL, Fiber optic, No)\n\nOnlineSecurity \u2013 whether the online security service is enabled (Yes, No, No internet service)\n\nOnlineBackup \u2013 whether the online backup service is enabled (Yes, No, No internet service)\n\nDeviceProtection \u2013 does the client have hardware insurance (Yes, No, No internet service)\n\nTechSupport \u2013 whether the technical support service is enabled (Yes, No, No internet service)\n\nStreamingTV \u2013 whether the streaming TV service is connected (Yes, No, No internet service)\n\nStreamingMovies \u2013 whether the streaming movie theater service is enabled (Yes, No, No internet service)\n\nContract \u2013 type of client's contract (Month-to-month, One year, Two year)\n\nPaperlessBilling \u2013 whether the client uses paperless billing (Yes, No)\n\nPaymentMethod-payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))\n\nMonthlyCharges \u2013 the current monthly payment amount\n\nTotalCharges \u2013 the total amount that the customer paid for the services for the entire time\n\nChurn-whether there was an outflow (Yes or No)\n\n &nbsp;&nbsp;&nbsp;&nbsp; <b>`Churn`<\/b> \u2013what we are investigating<br>","eeab342f":"### Search for correlation.\n\nTo train the model, leave only the columns that affect Churn. To find a correlation, you need to translate categorical features into numeric ones, since the model does not understand Yes \/ No, but it will mathematically understand 1 or 0.\n\nThe table shows that the main values are Yes \/ No, but there are more signs. Let's explore the data deeper so that we don't miss anything. Here is the' Internet Service ' suspicious column.","fcf8f18c":"### To download the correct libraries and data","2ceb5494":"##### We were lucky!\n \nNo missing values, no problems with duplicates, errors, and outliers. Since the task is categorical, you do not need to do normalization. The work will be reduced significantly.\nSearch for correlation.\n\nTo train the model, leave only the columns that affect Churn. To find a correlation, you need to translate categorical features into numeric ones, since the model does not understand Yes \/ No, but it will mathematically understand 1 or 0.\n\nThe table shows that the main values are Yes \/ No, but there are more signs. Let's explore the data deeper so that we don't miss anything. Here is the' Internet Service ' suspicious column.","c2227e26":"### Looking at the correlation","7c28fff7":"We see the following picture. Nothing strongly correlates with our investigated Churn trait. This is bad, but if there was such a connection, then you don't need a learning model.( For reference, 1 is a good correlation, 0 is a bad one)\n\nWe see a small negative correlation, especially with Contract. This is not for nothing, since this field indicates how much (month, year, or two) the client has signed the contract. Naturally, with a contract concluded for two years, the client is less likely to leave early.\n\nNegative correlation shows that when the values of one attribute increase, the associated one decreases. But in our case, when we encoded the Churn attribute, we assigned Yes ( the client left) to 1, just so it is accepted. If, as in the sense of assigned 0, the correlation was positive.\n\nSome features (green in the middle) strongly correlate with each other, but Churn is affected. They must be removed so that they do not knock down the model and do not affect the speed of operation.\n\nWell, despite the generally poor relationship of useful features with Churn, let's try to train the model.","dd2da380":"#### Let's go through the training models and look at their accuracy","585a7625":"Yes, 3 of the characteristic. There are others like it.\n\nThen the data is already grouped by the number of features. In cycles, categorical features are replaced by 0,1,2 depending on the number of categories.","5d84c44b":"Let's see what happened.\n\nACTUAL - actual data (Churn column). Other predicted values.\n\nNow it is important to choose the model we need. We see that in some cases, the model predicts the outflow of customers quite accurately, but also gives a lot of false predictions. In others, on the contrary, there are fewer false ones, but there are also few accurate ones.\n\nThe conclusion should be made by a person based not on mathematical estimates of accuracy, but on common sense. In this case, the best model is one that more accurately predicts outflow, even with false estimates. Additional work with clients will not hurt.\n\nThen the selected model is saved and later integrated into Analytics systems.\n\nhttp:\/\/web-inline.ru\/ Comprehensive implementation of Analytics.","dae77744":"## Training the model","31f61510":"#### Let's divide the model into a training test sample.","9d4f006a":"However, we have two more categorical criteria - duration and MonthlyCharges, the number of months you use the services, and the monthly payment amount.\n\nWithout analysis, it is clear that there are too many months to divide them into categories, also with the second sign. So we'll break them up.\n\nExperimentally, the graph shows that it makes no sense to divide the number into a larger number -4 is optimal. ","fd6068e8":"Let's see how the table looks now"}}