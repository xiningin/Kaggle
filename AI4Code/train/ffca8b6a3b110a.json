{"cell_type":{"6ebe4fa7":"code","ec26b8e3":"code","b82ce8ec":"code","e13cf8e9":"code","63a6f413":"code","49cb3bc5":"code","01efbf69":"code","05b29eff":"code","41a27c8f":"markdown","01420b45":"markdown","d5e5b006":"markdown","867ee4d9":"markdown","4e41ca2e":"markdown","42a7d2d7":"markdown","99eb4835":"markdown","a576547b":"markdown"},"source":{"6ebe4fa7":"import re\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.metrics import accuracy_score\nimport nltk\nimport math\nimport string\nimport seaborn as sns\nimport matplotlib.pyplot as plt","ec26b8e3":"frankenstein_txt = open(\"..\/input\/pg42324.txt\", \"r\").read().split('PREFACE.')[1].split('THE END.')[0].replace('\\n',' ').strip()\ntokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\nfrankenstein_sentences = [t.split(' ') for t in tokenizer.tokenize(frankenstein_txt) if len(t.split(' ')) > 10]\ndata = {\n    'left context': [], \n    'right context':[], \n    'predicted word':[]\n}\n\ndef clean_words(word):\n    return word.translate(str.maketrans('', '', string.punctuation))\n\nfrankenstein_words = ''\nfor sentence in frankenstein_sentences :\n    for i in range(5):\n        windows = sentence[i : i+5]\n        if len(windows) > 4:\n            frankenstein_words += clean_words(' '.join(windows)) + ' '\n            data['left context'].append(clean_words(' '.join(windows[0:2])))\n            data['right context'].append(clean_words(' '.join(windows[3:5])))\n            data['predicted word'].append(clean_words(windows[2]))\n            \nfrankenstein_dataframe = pd.DataFrame(data=data)\nfrankenstein_dataframe = frankenstein_dataframe.sample(frac=1).reset_index(drop=True)\nfrankenstein_words = set(frankenstein_words.split(' '))\nlist(frankenstein_words)[:20]","b82ce8ec":"# Split the corpus\ntraining_percentage = int(70*len(frankenstein_dataframe)\/100)\nvalidation_percentage = training_percentage + int(15*len(frankenstein_dataframe)\/100)\nfor i, row in frankenstein_dataframe.iterrows():\n    if i < training_percentage :\n        frankenstein_dataframe.at[i,'corpus'] = 'train'\n    elif i > training_percentage and i < validation_percentage :\n        frankenstein_dataframe.at[i,'corpus'] = 'validation'\n    else :\n        frankenstein_dataframe.at[i,'corpus'] = 'test'\nfrankenstein_dataframe.iloc[:10]","e13cf8e9":"class FrankensteinDataset(Dataset):\n    \n    # Build the FrankensteinDataset Class\n    def __init__(self, frankenstein, words):\n        self.words_index  = list(words)\n        self.corpus = {\n            'train' : frankenstein[frankenstein.corpus == 'train'],\n            'test' : frankenstein[frankenstein.corpus == 'test'],\n            'validation' : frankenstein[frankenstein.corpus == 'validation']\n        }\n        self.frankenstein_corpus = self.corpus['train']\n\n    # overcharge the getitem method -> it is used by pytorch dataloader \n    def __getitem__(self, index):\n        data = self.frankenstein_corpus.iloc[index]\n        return {\n            'x_data': (data['left context'] + ' ' + data['right context'], self.get_indexes(data['left context'] + ' ' + data['right context'])),\n            'y_data': (data['predicted word'], self.get_indexes(data['predicted word'])[0]),\n        }\n    \n    def get_indexes(self, context):\n        return torch.Tensor([self.words_index.index(i) for i in context.split(' ')]).long()\n    \n    # switch to another corpus  -> it is used by pytorch dataloader \n    def change_corpus(self, corpus_type):\n        self.frankenstein_corpus = self.corpus[corpus_type]\n        print(\"corpus switched to {} !\".format(corpus_type))\n    \n     # overcharge the len method\n    def __len__(self):\n        return len(self.frankenstein_corpus)","63a6f413":"def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n    \"\"\"\n    A generator function which wraps the PyTorch DataLoader. It will\n    ensure each tensor is on the write device location.\n    \"\"\"\n    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,shuffle=shuffle, drop_last=drop_last)\n    \n    for data_dict in dataloader:\n        out_data_dict = {}\n        for name, tensor in data_dict.items():\n            out_data_dict[name] = data_dict[name][1].to(device)\n        yield out_data_dict","49cb3bc5":"class EmbeddedPerceptron(nn.Module):\n    \n    def __init__(self, num_embeddings, embedding_dim):\n        super(EmbeddedPerceptron, self).__init__()\n        self.fc1 = nn.Embedding(num_embeddings,embedding_dim)\n        self.fc2 = nn.Linear(in_features=embedding_dim,out_features=num_embeddings)\n    def forward(self, x, in_softmax_pass=True):\n        embedding = self.fc1(x)\n        out = self.fc2(embedding.sum(1))\n        if in_softmax_pass:\n            out = F.softmax(out, dim=0)\n        return out","01efbf69":"# The Dataset\nfrankenstein_dataset = FrankensteinDataset(frankenstein_dataframe, frankenstein_words)\n# The model\nep = EmbeddedPerceptron(len(frankenstein_dataset.words_index), 128).to(\"cuda\")\n# The loss\nce_loss = nn.CrossEntropyLoss().to(\"cuda\")\n# The optimizer\noptimizer = optim.Adam(ep.parameters(), lr=0.001)","05b29eff":"running_loss = []\nrunning_acc = []\n\n# each epoch is a complete pass over the training data\nfor epoch in range(100):\n    # the inner loop is over the batches in the dataset\n    for batch in generate_batches(frankenstein_dataset, 128, device=\"cuda\"):\n        # Step 0: Get the data\n        x_data = batch['x_data']\n        y_target = batch['y_data']\n        # Step 1: Clear the gradients        \n        ep.zero_grad()\n        # Step 2: Compute the forward pass of the model\n        y_pred = ep(x_data, in_softmax_pass=False)\n        # Step 3: Compute the loss value that we wish to optimize\n        loss = ce_loss(y_pred, y_target)\n        # Step 4: Propagate the loss signal backward\n        loss.backward()\n        # Step 5: Trigger the optimizer to perform one update\n        optimizer.step()   \n    acc = accuracy_score(y_target.clone().detach().to(\"cpu\"), torch.argmax(y_pred.clone().detach(), dim=1).to(\"cpu\"))\n    running_loss.append(loss.item())\n    running_acc.append(acc)\nsns.set(font_scale=2)\nfig, axes = plt.subplots(ncols=2, figsize=(50,15))\nprint(\"train result :\")\nsns.lineplot(x=range(100),y=running_loss , ax=axes[0])\nsns.lineplot(x=range(100),y=running_acc , ax=axes[1])","41a27c8f":"## Dataloader","01420b45":"## Create the differents components","d5e5b006":"## Embedded Perceptron Model","867ee4d9":"# Frankenstein Embedding MLP Model (using CBOW)\n\ninteresting links to understand the model : \n* https:\/\/medium.com\/analytics-vidhya\/maths-behind-word2vec-explained-38d74f32726b\n* https:\/\/towardsdatascience.com\/introduction-to-word-embedding-and-word2vec-652d0c2060fa\n* http:\/\/jalammar.github.io\/illustrated-word2vec\/\n* https:\/\/papers.nips.cc\/paper\/2013\/file\/9aa42b31882ec039965f3c4923ce901b-Paper.pdf","4e41ca2e":"## Preprocessed and create the dataset","42a7d2d7":"## Label the frankenstein dataset","99eb4835":"## Training Loop","a576547b":"## Creating the Frankenstein Dataset"}}