{"cell_type":{"900d55ca":"code","2e95bfc7":"code","01c88b06":"code","ea8524ac":"code","73dd1513":"code","7a7bee2d":"code","4a92c790":"code","9b93fae4":"code","2e775426":"code","41af39c1":"code","c85b1d51":"code","7a2f7e4d":"code","a0492695":"code","1652c3f6":"code","e57b4344":"code","5eb81be9":"code","2a808e01":"code","aa9c12b1":"code","5794aa79":"code","e1d56707":"code","98566414":"code","b41c774d":"code","aa6641f0":"code","c8637573":"code","587eaf6a":"code","ad742650":"code","180ce942":"code","0ca16677":"code","cd337c5d":"code","191c04eb":"code","95338f32":"code","7714ae70":"code","ce6daecd":"code","80559cbb":"code","c5dec5d0":"code","58199d7e":"code","28ce56ad":"code","0dbf398d":"code","eb541e94":"code","356a4714":"code","aa66c99b":"code","c81f7103":"code","9f9165e2":"code","2d52c567":"code","7bc759e1":"code","a351f410":"code","0937d142":"code","a329ee38":"code","7e5c66f1":"code","fa4f39f1":"code","132d74a8":"code","a83b8b79":"code","bbbd4058":"code","5f399bb0":"code","e8b61b25":"code","0c144749":"code","b534b514":"code","9174c1b9":"code","fd9bb51e":"code","a6a4c593":"code","bc9f1f4c":"code","a40182f1":"code","69c02fe6":"code","94aec359":"code","39b0391f":"code","240108b6":"code","02e16e9d":"code","1ebbed85":"code","dc88907b":"code","18967b11":"code","6b069b86":"code","9a6b0f7a":"code","4f6f3bd4":"markdown","f120ed49":"markdown","651b25a9":"markdown","a9ea91c6":"markdown","cb7c205c":"markdown","05e92338":"markdown","82692d95":"markdown","9c5bdddb":"markdown","f17a0a9b":"markdown","171f0cc8":"markdown","1420e2e3":"markdown","6446c5ee":"markdown","411e289c":"markdown","33160265":"markdown","3b1b1195":"markdown","9646abe8":"markdown","e2b34bd9":"markdown","2edf8803":"markdown","50a8ee6c":"markdown","7f56c256":"markdown","ec1698e0":"markdown","c098e4d4":"markdown","c7b43f3d":"markdown","d6f5b0bd":"markdown","bc95c583":"markdown","1c643815":"markdown","fb9269a3":"markdown","674bbc6e":"markdown","08ca977a":"markdown","05883000":"markdown","811c58b4":"markdown","9ac22f4d":"markdown","f97d4128":"markdown","525fb964":"markdown"},"source":{"900d55ca":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math","2e95bfc7":"train_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")","01c88b06":"train_data.head()","ea8524ac":"train_data.info()","73dd1513":"test_data.info()","7a7bee2d":"train_data['Ticket'].unique().shape, test_data['Ticket'].unique().shape","4a92c790":"train_data.describe()","9b93fae4":"train_data.describe(include=['O'])","2e775426":"train_data[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean()","41af39c1":"train_data[['Pclass', 'Survived']].groupby('Pclass', as_index=False).mean()","c85b1d51":"train_data[['Embarked', 'Survived']].groupby('Embarked', as_index=False).mean().sort_values(by='Survived', ascending=False)","7a2f7e4d":"train_data[['SibSp', 'Survived']].groupby('SibSp', as_index=False).mean().sort_values(by='Survived', ascending=False)","a0492695":"train_data[['Parch', 'Survived']].groupby('Parch', as_index=False).mean().sort_values(by='Survived', ascending=False)","1652c3f6":"age_plt = sns.FacetGrid(train_data, col='Survived')\nage_plt.map(plt.hist, 'Age', bins=20);","e57b4344":"class_age_plt = sns.FacetGrid(train_data, col='Survived',row='Pclass', height=2.8, aspect=2.0)\nclass_age_plt.map(plt.hist, 'Age', bins=20);","5eb81be9":"cat = sns.FacetGrid(train_data, row='Embarked', height=2.2, aspect=1.6)\ncat.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')","2a808e01":"fare_plt = sns.FacetGrid(train_data, col='Survived')\nfare_plt.map(plt.hist, 'Fare')","aa9c12b1":"fare_embarked_plt = sns.FacetGrid(train_data, col='Survived', row='Embarked')\nfare_embarked_plt.map(sns.barplot, 'Sex', 'Fare', ci=None)","5794aa79":"train_data = train_data.drop(['Cabin', 'Ticket', 'PassengerId'], axis = 1)\ntest_data = test_data.drop(['Cabin', 'Ticket'], axis = 1)","e1d56707":"combine = [train_data, test_data]","98566414":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand = False)\npd.crosstab(train_data['Title'], train_data['Sex']), pd.crosstab(test_data['Title'], test_data['Sex'])","b41c774d":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    \n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\npd.crosstab(train_data['Title'], train_data['Sex'])","aa6641f0":"train_data[['Title', 'Survived']].groupby('Title', as_index=False).mean()","c8637573":"train_data = train_data.drop(['Name'], axis=1)\ntest_data = test_data.drop(['Name'], axis = 1)\ncombine = [train_data, test_data]\ntrain_data.head()","587eaf6a":"train_data = pd.concat([train_data, pd.get_dummies(train_data['Sex'])],axis=1)\ntest_data = pd.concat([test_data, pd.get_dummies(test_data['Sex'])],axis=1)\ntrain_data","ad742650":"train_data.drop(['female'],axis=1,inplace=True) \ntest_data.drop(['female'],axis=1,inplace=True)","180ce942":"train_data = pd.concat([train_data, pd.get_dummies(train_data['Title'])],axis=1)\ntest_data = pd.concat([test_data, pd.get_dummies(test_data['Title'])],axis=1)","0ca16677":"combine = [train_data, test_data]\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\ntrain_data.head()","cd337c5d":"train_data.info(), test_data.info()","191c04eb":"grid = sns.FacetGrid(train_data, row='Pclass', col='Title', height=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins = 30)\ngrid.add_legend();","95338f32":"grid = sns.FacetGrid(train_data, row='Pclass', col='Sex', height=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","7714ae70":"guess_ages = np.zeros((5,3))\nguess_ages","ce6daecd":"combine = [train_data, test_data]","80559cbb":"train_data","c5dec5d0":"for dataset in combine:\n    for title in range(1, 6):\n        for pclass in range(1,4):\n            guess_df = dataset[(dataset['Title'] == title) & \\\n                                  (dataset['Pclass'] == pclass)]['Age'].dropna()\n            age_guess = guess_df.median()\n            #rounding\n            if not math.isnan(age_guess):\n                guess_ages[title-1,pclass-1] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            else:\n                guess_ages[title-1,pclass-1] = dataset['Age'].median()\nguess_ages","58199d7e":"for dataset in combine:\n    for title in range(1, 6):\n        for pclass in range(1,4):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Title == title) & (dataset.Pclass == pclass),\\\n                    'Age'] = float(guess_ages[title-1,pclass-1])","28ce56ad":"train_data['AgeBand'] = pd.cut(train_data['Age'], 5)\ntrain_data[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean() \\\n                                .sort_values(by='AgeBand', ascending=True)","0dbf398d":"for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age_band'] = 'child'\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age_band'] = 'young'\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age_band'] = 'middle'\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age_band'] = 'old'\n    dataset.loc[ dataset['Age'] > 64, 'Age_band'] = 'v_old'\ntrain_data.head()","eb541e94":"train_data = train_data.drop(['AgeBand'], axis=1)\ncombine = [train_data, test_data]\ntrain_data.head()","356a4714":"train_data['Embarked'].fillna(train_data.Embarked.dropna().mode()[0], inplace=True)\ntest_data['Fare'].fillna(test_data['Fare'].dropna().median(), inplace=True)\ntrain_data.info(), test_data.info()","aa66c99b":"train_data = pd.concat([train_data, pd.get_dummies(train_data['Embarked'])],axis=1)\ntest_data = pd.concat([test_data, pd.get_dummies(test_data['Embarked'])],axis=1)","c81f7103":"train_data","9f9165e2":"combine = [train_data, test_data]","2d52c567":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    \ntrain_data[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values\\\n(by='Survived', ascending=False)","7bc759e1":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_data[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","a351f410":"train_data = train_data.drop(['Parch', 'SibSp'], axis=1)\ntest_data = test_data.drop(['Parch', 'SibSp'], axis=1)\ncombine = [train_data, test_data]\ntrain_data.head()","0937d142":"train_data['FareBand'] = pd.qcut(train_data['Fare'], 4)\ntrain_data[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().\\\n            sort_values(by='FareBand', ascending=True)","a329ee38":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_data = train_data.drop(['FareBand'], axis=1)\ncombine = [train_data, test_data]\ntrain_data.head()","7e5c66f1":"train_data = pd.concat([train_data, pd.get_dummies(train_data['Fare'],prefix=\"Fare\")],axis=1)\ntest_data = pd.concat([test_data, pd.get_dummies(test_data['Fare'],prefix=\"Fare\")],axis=1)\ntrain_data.drop(['Fare'],axis=1,inplace=True)\ntest_data.drop(['Fare'],axis=1, inplace=True)","fa4f39f1":"train_data","132d74a8":"train_data.drop(['Sex'],axis=1,inplace=True)\ntest_data.drop(['Sex'],axis=1, inplace=True)\ntrain_data.drop(['Embarked'],axis=1,inplace=True)\ntest_data.drop(['Embarked'],axis=1, inplace=True)\n#train_data.drop(['Title'],axis=1,inplace=True)\n#test_data.drop(['Title'],axis=1, inplace=True)","a83b8b79":"#train_data = pd.concat([train_data, pd.get_dummies(train_data['Pclass'],prefix=\"Pclass\")],axis=1)\n#test_data = pd.concat([test_data, pd.get_dummies(test_data['Pclass'],prefix=\"Pclass\")],axis=1)\n#train_data.drop(['Pclass'],axis=1,inplace=True)\n#test_data.drop(['Pclass'],axis=1, inplace=True)","bbbd4058":"train_data = pd.concat([train_data, pd.get_dummies(train_data['Age_band'],)],axis=1)\ntest_data = pd.concat([test_data, pd.get_dummies(test_data['Age_band'])],axis=1)\ntrain_data.drop(['Age_band'],axis=1,inplace=True)\ntest_data.drop(['Age_band'],axis=1, inplace=True)","5f399bb0":"train_data.info()","e8b61b25":"train_data.head()","0c144749":"test_data.head()","b534b514":"# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost as xgb","9174c1b9":"X_train = train_data.drop(\"Survived\", axis=1)\nY_train = train_data[\"Survived\"]\nX_test  = test_data.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","fd9bb51e":"KFold_Score = pd.DataFrame()\nclassifiers = ['Linear SVM', 'LogisticRegression', \n               'RandomForestClassifier', 'XGBoostClassifier', 'KNeighborsClassifier',\n               'GradientBoostingClassifier']\nestimators = [svm.SVC(kernel='linear'),\n              LogisticRegression(max_iter = 1000),\n              RandomForestClassifier(n_estimators=200, random_state=0),\n              xgb.XGBClassifier(n_estimators=100,use_label_encoder=False, objective= 'binary:logistic',\\\n                               verbosity=0),\n              KNeighborsClassifier(),\n              GradientBoostingClassifier(random_state=0)]\n","a6a4c593":"j = 0\nfor model in estimators:\n    cv = KFold(n_splits=5, random_state=0, shuffle=True)\n    KFold_Score[classifiers[j]] = (cross_val_score(model, X_train, Y_train,\\\n                                                   scoring = 'accuracy', cv=cv))\n    j+=1","bc9f1f4c":"mean = pd.DataFrame(KFold_Score.mean(), index= classifiers)\nKFold_Score = pd.concat([KFold_Score,mean.T])\nKFold_Score.index=['Fold 1','Fold 2','Fold 3','Fold 4','Fold 5','Mean']\nKFold_Score.T.sort_values(by=['Mean'], ascending = False)","a40182f1":"sc = StandardScaler()\ntrain_scaled = sc.fit_transform(X_train)\ntest_scaled = sc.transform(X_test)","69c02fe6":"KFold_Score = pd.DataFrame()\nj = 0\nfor model in estimators:\n    cv = KFold(n_splits=5, random_state=0, shuffle=True)\n    KFold_Score[classifiers[j]] = (cross_val_score(model, train_scaled, Y_train,\\\n                                                   scoring = 'accuracy', cv=cv))\n    j+=1","94aec359":"mean = pd.DataFrame(KFold_Score.mean(), index= classifiers)\nKFold_Score = pd.concat([KFold_Score,mean.T])\nKFold_Score.index=['Fold 1','Fold 2','Fold 3','Fold 4','Fold 5','Mean']\nKFold_Score.T.sort_values(by=['Mean'], ascending = False)","39b0391f":"#randomForestClassifier, tuning hyperparameters\nrfc = RandomForestClassifier(random_state=0)\nparam_grid = { \n    'n_estimators': [ 200,300],\n    'max_features': ['auto', 'sqrt'],\n    'max_depth' : [6,7,8],\n    'criterion' :['gini', 'entropy']\n}\nCV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\nCV_rfc.fit(train_scaled,Y_train )\nCV_rfc.best_params_","240108b6":"rfc = RandomForestClassifier(criterion='entropy',max_depth=7,max_features='auto',\\\n                             n_estimators=300)\nrfc.fit(train_scaled, Y_train)\nrand_for_pred = rfc.predict(test_scaled)\nrfc.score(train_scaled, Y_train) ","02e16e9d":"feature_names = list(test_data.drop(\"PassengerId\", axis=1).columns)\nforest_importances = pd.Series(rfc.feature_importances_, index=feature_names)\nfig, ax = plt.subplots()\nforest_importances.plot.bar(ax=ax)\nax.set_title(\"Feature importances using MDI\")\nax.set_ylabel(\"Mean decrease in impurity\")\nfig.tight_layout()","1ebbed85":"# gbm\ngbm = xgb.XGBClassifier(\n #learning_rate = 0.02,\n #gamma=1,\n objective= 'binary:logistic',\n verbosity=0, \n use_label_encoder=False\n )\nparam_grid = { \n    'n_estimators': [100,200,300],\n    'max_depth' : [5,6,7],\n}\nCV_gbm = GridSearchCV(estimator=gbm, param_grid=param_grid, cv= 5)\nCV_gbm.fit(train_scaled,Y_train )\nCV_gbm.best_params_","dc88907b":"gbm = xgb.XGBClassifier(\n#learning_rate = 0.02,\n n_estimators= 150,\n max_depth= 7,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(train_scaled, Y_train)\ngbm_pred = gbm.predict(test_scaled)\ngbm.score(train_scaled, Y_train)","18967b11":"# Linear svm\nsvc = svm.SVC(kernel='linear')\nsvc.fit(train_scaled, Y_train)\nsvc_pred = rfc.predict(test_scaled)\nsvc.score(train_scaled, Y_train) ","6b069b86":"#Y_pred = gbm_pred\nY_pred = rand_for_pred\n#Y_pred = svc_pred\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_data[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission","9a6b0f7a":"submission.to_csv('.\/submission.csv', index=False)","4f6f3bd4":"Now we can drop the *Name*","f120ed49":"Let's divide *Fare* into clusters in the same way as *Age*","651b25a9":"\nSince there are few gaps in the other traits, we will fill them with the median value\n","a9ea91c6":"##### Notes:\n\n * **Pclass**: many people with Pclass=1 survived. Strong correlation. \n * **Sex:** Sex=female has a high survival rate. Strong correlation. \n * **SibSp and Parch:** for some values there is no correlation. It is better to replace individual features with a combination or remove them.","cb7c205c":"## Correlation between single features and survival\n","05e92338":"Let's turn *Age* from a numerical value into a cluster","82692d95":"\nWe will restore the missing *Age* by a combination of *Title+Pclass*\n","9c5bdddb":"Here \".get_dummies()\" will convert this column and make 2 dummy columns of male and female. This is done in order to convert the categorical data into numerical.","f17a0a9b":"###  Create new attributes based on old ones\nLet's try to replace the *Name* attribute with a *Title*, which is present in every name","171f0cc8":"### Creating features derived as a combination of other\nLet's try to create a trait based on *Patch+Sibsp*, as well as combinations of *(Embarked, Fare)* and *(Embarked, Sex)*","1420e2e3":"**Notes**\n* For Embarked = C Fare strongly affects survival rate, for S city not strongly, and for Q city weakly.\n* Fare itself is weakly correlated with survivability\n\n\n**Conclusion**\n* Include Fare paired with Embarked, Fare itself can be removed","6446c5ee":"## Filling in missing values\n","411e289c":"Now let's repeat the comparison, but with Scaler","33160265":"Since *Embarked* is now full, we can recode it","3b1b1195":"Let's combine the rarely encountered titles","9646abe8":"Let's look at survivability by *Title*","e2b34bd9":"Remove *SibSp, Parch* in favor of *FamilySize*, *IsAlone*","2edf8803":"## Categorical features conversion","50a8ee6c":"We will drop *Title* and convert *Pclass* later","7f56c256":"# First glance on data","ec1698e0":"Note that *Age* is partially filled in, and in *Fare*, *Embarked* there are only a few missing values. Let's try to restore the missing *Age* values by other features","c098e4d4":"Let's combine *train* and *test* for general transformations","c7b43f3d":"#### Notes\n* **Cabin:** mostly incomplete, better to drop.\n* **Age:** partly incomplete.\n* **Ticket:** many duplicates, maybe dropped.\n* **PassengerId:** unique, dropped.\n* **Name:** need to split \"title\".\n* **Sex, Name, Embarked:** encode from strings to numbers.","d6f5b0bd":"## Removing unnecessary features\nAs we saw above, Cabin is too poorly filled and Ticket has too many duplicates","bc95c583":"**Notes**\n* The 3rd class is the most numerous, but has a low survival rate\n* \u0412 \u043a\u043b\u0430\u0441\u0441\u0435 2 \u0432\u044b\u0436\u0438\u043b\u0438 \u0432\u0441\u0435 \u0434\u0435\u0442\u0438.\n\n**Conclusion**\n* Pclass and (Pclass, age) are worthwhile to include\n","1c643815":"**Notes**\n* High correlation of Pclass+Embarked pair for men\n* Low Survival Rate for women Embarked = C\n\n**Conclusion**\n* Complete Embarked, include in model\n* Include a couple in the model (Embarked, Sex)","fb9269a3":"**TODO** extract some information from *Ticket*","674bbc6e":"*Pclass* is ordered category, so we don't need to encode it","08ca977a":"# Data processing","05883000":"# Fit & Predict","811c58b4":"Comparing models with each other","9ac22f4d":"## Pairwise correlation","f97d4128":"### TODO \n* add an ensemble of models\n* try to get something out of *ticket*\n* try to find some information using clustering algo-s. \/\/ **Not working!**","525fb964":"Let's divide the ages into groups and look at their correlation to the survival rate\n"}}