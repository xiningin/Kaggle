{"cell_type":{"62edcdbc":"code","2351af47":"code","e509fbd8":"code","bbda91ca":"code","de96d2f1":"code","43b6bf9c":"code","a1670243":"code","5b5ab8dc":"code","7bc5517d":"code","8e34a785":"code","71c91bdf":"code","36079820":"code","e445bea8":"code","14749e2f":"code","c95b97b3":"code","f354acf9":"code","3a1b980e":"code","da6d0ead":"code","b35caf0c":"code","e454c3cf":"code","d01cf7d7":"code","92c144e1":"code","abe638ef":"markdown","6a2aa5f5":"markdown","1e25dbef":"markdown","a55b4947":"markdown","fced0797":"markdown","b916e534":"markdown","45810e6b":"markdown","5a3835da":"markdown"},"source":{"62edcdbc":"# Basic libraries\nimport pandas as pd\nimport numpy as np\n\n# General imports\nimport re\nimport sys\nfrom collections import defaultdict\n\n# Viz. libraries\nimport missingno\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\n\n# NLP libraries\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nnltk.download(\"stopwords\", quiet=True)\nstopwords = stopwords.words(\"english\")","2351af47":"# Reading the CSV files\ntrain_df = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","e509fbd8":"train_df.head()","bbda91ca":"train_df.info()","de96d2f1":"train_df.shape, test_df.shape","43b6bf9c":"# Missing info matrix\nmissingno.matrix(train_df)","a1670243":"# Getting the number of null count\npd.DataFrame({\"count\": train_df.isnull().sum(), \"percent\":  train_df.isnull().sum()\/train_df.count()*100})","5b5ab8dc":"# Get all the keyword count\nkeywords_count = pd.DataFrame({\"count\": train_df[\"keyword\"].value_counts()[:30]})\nfigure(figsize=(10,5))\nsns.barplot(y=keywords_count.index, x=keywords_count[\"count\"], orient=\"h\")","7bc5517d":"# Get unique values\nlen(train_df[\"keyword\"].value_counts())","8e34a785":"# Visualization based on disaster and nondisaster keywords\ndisaster_keywords = train_df[train_df[\"target\"] == 1][\"keyword\"].value_counts()[:30]\nnondisaster_keywords = train_df[train_df[\"target\"] == 0][\"keyword\"].value_counts()[:30]\n\nfig, ax = plt.subplots(1, 2, figsize=(20, 8))\nsns.barplot(x=disaster_keywords, y=disaster_keywords.index, ax=ax[0])\nax[0].set_title(\"disaster keywords\")\nsns.barplot(x=nondisaster_keywords, y=nondisaster_keywords.index, ax=ax[1])\nax[1].set_title(\"nondisaster keywords\")","71c91bdf":"# Visualization based on location - Top 30 locations with maximum number of tweets\nlocation_count = train_df[\"location\"].value_counts()[:30]\nfigure(figsize=(10,5))\nsns.barplot(x=location_count, y=location_count.index)","36079820":"# Disaster and nondisaster tweets based of location - Which location is more likely to have a disaster tweet or not\ndisaster_location = train_df[train_df[\"target\"] == 1][\"location\"].value_counts()[:30]\nnondisaster_location = train_df[train_df[\"target\"] == 0][\"location\"].value_counts()[:30]\n\nfig, ax = plt.subplots(1, 2, figsize=(20,8))\nsns.barplot(x=disaster_location, y=disaster_location.index, ax=ax[0])\nsns.barplot(x=nondisaster_location, y=nondisaster_location.index, ax=ax[1])","e445bea8":"# Text lenght\/tweet length\ntrain_df[\"text_length\"] = train_df[\"text\"].apply(len)\nfigure(figsize=(6, 4))\nplt.title(\"Text lenght hist plot\")\nsns.distplot(train_df[\"text_length\"]);","14749e2f":"# Getting the min and max tweet length\nmin(train_df[\"text_length\"]), max(train_df[\"text_length\"])","c95b97b3":"g = sns.FacetGrid(train_df, col=\"target\")\ng = g.map(sns.distplot, \"text_length\")\nplt.suptitle(\"Tweet lenght distribution\")\nfigure(figsize=(8, 8))","f354acf9":"# Number of words\ndef count_words(x):\n    return len(x.split())\n\ntrain_df[\"num_words\"] = train_df[\"text\"].apply(count_words)\nsns.distplot(train_df[\"num_words\"], bins=10)","3a1b980e":"# Stop words\ndef create_corpus(target):\n    corpus = []\n    for w in train_df.loc[train_df[\"target\"] == target][\"text\"].str.split():\n        for i in w:\n            corpus.append(i)\n    return corpus\n\ndef create_corpus_dict(target):\n    corpus = create_corpus(target)\n    stop_dict = defaultdict(int)\n    for word in corpus:\n        if word in stopwords:\n            stop_dict[word] += 1\n    return sorted(stop_dict.items(), key=lambda x:x[1], reverse=True)","da6d0ead":"print(create_corpus_dict(1))","b35caf0c":"# Functions to remove pattern\n\ndef remove_pattern(pattern, input_text):\n    r = re.findall(pattern, input_text)\n    for i in r:\n        input_text = re.sub(i, \" \", input_text)\n    return input_text\n\ndef remove_stop_words(sentence):\n    return [w for w in sentence if w not in stopwords.words(\"english\")]","e454c3cf":"# Remove all the # keywords\ntrain_df[\"tweet\"] = np.vectorize(remove_pattern)(\"#[\\w]*\", train_df[\"text\"])\ntest_df[\"tweet\"] = np.vectorize(remove_pattern)(\"#[\\w]*\", test_df[\"text\"])\n\n# Remove all the non alphabets keywords\ntrain_df[\"tweet\"] = train_df[\"tweet\"].str.replace(\"[^a-zA-A]\", \" \")\ntest_df[\"tweet\"] = test_df[\"tweet\"].str.replace(\"[^a-zA-Z]\", \" \")\n\n# Dropping words whose lenght is lesser than 3\ntrain_df[\"tweet\"] = train_df[\"tweet\"].apply(lambda x: \" \".join([w for w in x.split() if len(w) > 3]))\ntest_df[\"tweet\"] = test_df[\"tweet\"].apply(lambda x: \" \".join([w for w in x.split() if len(w) > 3]))\n\n# Convert all the words to lower case\ntrain_df[\"tweet\"] = train_df[\"tweet\"].str.lower()\ntest_df[\"tweet\"] = test_df[\"tweet\"].str.lower()\n\n# Tokens of words\ntrain_df[\"tokenized_sent\"] = train_df.apply(lambda x: nltk.word_tokenize(x[\"tweet\"]), axis=1)\ntest_df[\"tokenized_sent\"] = test_df.apply(lambda x: nltk.word_tokenize(x[\"tweet\"]), axis=1)\n\n# Remove stop words\ntrain_df[\"cleaned_tweet\"] = train_df[\"tokenized_sent\"].apply(lambda x: \" \".join([word for word in x if word not in stopwords]))\ntest_df[\"cleaned_tweet\"] = test_df[\"tokenized_sent\"].apply(lambda x: \" \".join([word for word in x if word not in stopwords]))\n\n# Dropped extra columns\ntrain_df.drop([\"tokenized_sent\", \"tweet\"], axis=1, inplace=True)\ntest_df.drop([\"tokenized_sent\", \"tweet\"], axis=1, inplace=True)","d01cf7d7":"train_df.head(5)","92c144e1":"test_df.head(5)","abe638ef":"## 3: Preprocessing text data","6a2aa5f5":"# Natural Language Processing with Disaster Tweets\n\nTwitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Hence, twitter is one of the most popular way of announcing such distress signals. \n\nIn this notebook we will primarly focus on exploratory data analysis along with some different basic ways of pre processing the text data","1e25dbef":"Functions to preprocess data","a55b4947":"List of all the preprocessing\n- Remove all the # keywords\n- Remove all the non alphabets keywords\n- Dropping words whose lenght is lesser than 3\n- Convert all the words to lower case\n- Tokenize all the words\n- Removing all the stop words","fced0797":"## 1. Importing libraries and data reading","b916e534":"This kernal is based off this [notebook](https:\/\/www.kaggle.com\/ghaiyur\/ensemble-models-versiong) by Ghaiyur. It has some amazing modular code on data preproceesing and data visulization. Feel free to check it out!","45810e6b":"There are a lot of missing values in location, but we will not be using this to determine the target. We can ignore this","5a3835da":"## 2. Exploratory Data Analysis"}}