{"cell_type":{"a038f3f8":"code","70cadd11":"code","4e3e2964":"code","5f25657a":"code","90c4a891":"code","dfb4241c":"code","570acf06":"code","40991739":"code","2f663246":"code","d1da5f32":"code","7bb9f603":"code","b7138521":"code","3a226f13":"code","46e2efff":"code","1bd70be9":"code","3fcc6cff":"code","85851798":"code","b1d55b4f":"code","a1251a0d":"code","b1e2c328":"code","30552558":"code","03cde4d9":"code","ffdec745":"code","4a1f7323":"code","c04d2eb5":"code","12c9fc86":"code","ae0e28a3":"code","2e0282a0":"code","5cc6969a":"code","ecf9f7eb":"code","4213cd7a":"code","67e45249":"code","7c9c6372":"code","12352499":"code","fcc94420":"code","76dd04b1":"code","c47a0484":"code","0618abf1":"markdown","8c82d4d3":"markdown","dac38b9d":"markdown","5600d7b9":"markdown","03e1be51":"markdown","fe63a511":"markdown","3450ee3a":"markdown","f5294743":"markdown","8ed6adfd":"markdown","d09f0d36":"markdown","5ef40e6b":"markdown","7beeb55e":"markdown","859e804a":"markdown","824a1cd8":"markdown","3bf6afdb":"markdown","358c990b":"markdown","c204a2ef":"markdown","c6e50a2d":"markdown","3cc6c4aa":"markdown","a7e27a85":"markdown","65aa39e3":"markdown"},"source":{"a038f3f8":"!pip install -q swifter # faster pandas apply","70cadd11":"import numpy as np \nimport pandas as pd\nimport swifter \nimport seaborn as sns\nimport re\nfrom sklearn.model_selection import StratifiedKFold\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, Input, Conv1D, MaxPooling1D, Flatten, Dense\nfrom tensorflow.keras.layers import concatenate\nfrom tensorflow.keras.models import Model\nfrom sklearn.preprocessing import OneHotEncoder\nfrom tqdm import tqdm\ntqdm.pandas()","4e3e2964":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5f25657a":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","90c4a891":"train_df.head()","dfb4241c":"test_df.head()","570acf06":"import geopy\nimport pycountry\nimport math\nfrom geopy.geocoders import Nominatim\n\ngeolocator = Nominatim(user_agent=\"navneet\")\n\ndef get_location(region):\n    if pd.isnull(region):\n        return None\n    try:\n        return geolocator.geocode(region)[0].split(\",\")[-1] \n    except:\n        return \"no_country\"","40991739":"train_df[\"country\"] = train_df[\"location\"].swifter.progress_bar(enable=True).apply(get_location)","2f663246":"test_df[\"country\"] = test_df[\"location\"].swifter.progress_bar(enable=True).apply(get_location)","d1da5f32":"nan_values = {\"keyword\": \"no_keyword\", \"country\": \"no_location\"}\ntrain_df.fillna(value=nan_values, inplace=True)\ntest_df.fillna(value=nan_values, inplace=True)\ntrain_df.head()","7bb9f603":"sns.countplot(train_df.target)","b7138521":"train_df.isnull().sum()","3a226f13":"sns.countplot(y=train_df.keyword,  order=train_df.keyword.value_counts()[:20].index, orient='v')","46e2efff":"total_keywords = len(train_df.keyword.unique())\ntotal_locations = len(train_df.country.unique())","1bd70be9":"keyword_encoder = OneHotEncoder()\nkeyword_encoder.fit(train_df.keyword.values.reshape(-1,1))\nkeyword_encoder.fit(test_df.keyword.values.reshape(-1,1))\nencoded_keywords = keyword_encoder.transform(train_df.keyword.values.reshape(-1,1)).toarray()","3fcc6cff":"location_encoder = OneHotEncoder()\nlocation_encoder.fit(train_df.country.values.reshape(-1,1))\nlocation_encoder.fit(test_df.country.values.reshape(-1,1))\nencoded_locations = location_encoder.fit_transform(train_df.country.values.reshape(-1,1)).toarray()","85851798":"# https:\/\/stackoverflow.com\/a\/47091490\ndef decontracted(sentence):\n    \"\"\"Convert contractions like \"can't\" into \"can not\"\n    \"\"\"\n    # specific\n    sentence = re.sub(r\"won\\'t\", \"will not\", sentence)\n    sentence = re.sub(r\"can\\'t\", \"can not\", sentence)\n\n    # general\n    #phrase = re.sub(r\"n't\", \" not\", phrase) # resulted in \"ca not\" when sentence started with \"can't\"\n    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n    sentence = re.sub(r\"\\'s\", \" is\", sentence)\n    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n    sentence = re.sub(r\"\\'t\", \" not\", sentence)\n    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n    sentence = re.sub(r\"\\'m\", \" am\", sentence)\n    return sentence","b1d55b4f":"# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\n        \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U000024C2-\\U0001F251\"\n        \"]+\",\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n","a1251a0d":"tokenizer = RegexpTokenizer(r'\\w+')\ndef remove_punctuation(text):\n    text = tokenizer.tokenize(text)\n    return text","b1e2c328":"def remove_stopwords(text):\n    filtered_words = [w for w in text if not w in stopwords.words('english')]\n    return \" \".join(filtered_words)","30552558":"def clean_urls(text):\n    return re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)","03cde4d9":"def clean_tweet(text):\n    text = text.lower()\n    text = clean_urls(text)\n    text = decontracted(text)\n    text = remove_emoji(text)\n    text = remove_punctuation(text)\n    text = remove_stopwords(text)\n    return text","ffdec745":"train_df['text'] = train_df['text'].progress_apply(clean_tweet)","4a1f7323":"test_df['text'] = test_df['text'].progress_apply(clean_tweet)","c04d2eb5":"MAX_NB_WORDS=225\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS)\ntokenizer.fit_on_texts(train_df['text'])\ntokenizer.fit_on_texts(test_df['text'])\nsequences_data = tokenizer.texts_to_sequences(train_df['text'])","12c9fc86":"word_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","ae0e28a3":"MAX_SEQUENCE_LENGTH=225\nsequences_data = pad_sequences(sequences_data, maxlen=MAX_SEQUENCE_LENGTH)","2e0282a0":"embeddings_index = {}\nf = open(\"\/kaggle\/input\/embeddings\/glove-840B-300d.txt\", encoding='latin')\nfor line in f:\n    values = line.split(\" \")\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float16')\n    embeddings_index[word] = coefs\nf.close()\nprint('Found %s word vectors.' % len(embeddings_index))","5cc6969a":"EMBEDDING_DIM = 300 # glove-840B-300d\nembedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","ecf9f7eb":"embedding_layer = Embedding(len(word_index) + 1,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=False)","4213cd7a":"def build_model():\n    keyword_input = Input(shape=(total_keywords,), dtype='int32')\n    location_input = Input(shape=(total_locations,), dtype='int32')\n    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n    \n    embedded = embedding_layer(sequence_input)\n    embedded = Conv1D(64, 5, activation='relu')(embedded)\n    embedded = MaxPooling1D(5)(embedded)\n    embedded = Conv1D(32, 5, activation='relu')(embedded)\n    embedded = MaxPooling1D(5)(embedded)\n    embedded = Flatten()(embedded)\n    \n    x = concatenate([keyword_input, location_input, sequence_input])\n    x = Dense(512, activation='relu')(x)\n    x = Dense(128, activation='relu')(x)\n    preds = Dense(1, activation='sigmoid')(x)\n\n    model = Model([keyword_input, location_input, sequence_input], preds)\n\n    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n    return model","67e45249":"kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfor train, test in kfold.split(sequences_data, train_df.target):\n    model = build_model()\n\n    train_sequences = sequences_data[train]\n    train_keywords = encoded_keywords[train]\n    train_locations = encoded_locations[train]\n    train_target = train_df.target[train]\n    \n    test_sequences = sequences_data[test]\n    test_keywords = encoded_keywords[test]\n    test_locations = encoded_locations[test]\n    test_target = train_df.target[test]\n    \n    model.fit([train_keywords, train_locations, train_sequences], train_target)\n    loss, acc = model.evaluate([test_keywords, test_locations, test_sequences], test_target)\n    print(\"Loss: \", loss)\n    print(\"Accuracy: \", acc)","7c9c6372":"model = build_model()\nmodel.fit([encoded_keywords, encoded_locations, sequences_data], train_df.target)","12352499":"sequences_data = tokenizer.texts_to_sequences(test_df['text'])\nsequences_data = pad_sequences(sequences_data, maxlen=MAX_SEQUENCE_LENGTH)\nencoded_keywords = keyword_encoder.transform(test_df.keyword.values.reshape(-1, 1)).toarray()\nencoded_locations = location_encoder.transform(test_df.country.values.reshape(-1, 1)).toarray()\npredictions = model.predict([encoded_keywords, encoded_locations, sequences_data])","fcc94420":"submission = pd.DataFrame(\n    {\n        'id': test_df.id,\n        'target': predictions.flatten()\n    }, \n    columns = ['id', 'target']\n)\n","76dd04b1":"submission.head()","c47a0484":"submission.to_csv(\"submission.csv\")","0618abf1":"# Embedding Layer","8c82d4d3":"# Submission","dac38b9d":"Let see the given data","5600d7b9":"Check on null columns","03e1be51":"Check class balance by our target","fe63a511":"# Text Tokenization","3450ee3a":"### 1. Explor keywords","f5294743":"# Import libs","8ed6adfd":"# Cross Validation","d09f0d36":"# Our Model","5ef40e6b":"Create embedding Matrix","7beeb55e":"# Data Exploration","859e804a":"Get country","824a1cd8":"pad sequences to 225","3bf6afdb":"\n# Text Cleaning","358c990b":"There are no null in text and target. But we have 61 null in keywords and 2533 null in location","c204a2ef":"# Train Model","c6e50a2d":"# Prediction","3cc6c4aa":"# Preparing the Embedding layer","a7e27a85":"Total words","65aa39e3":"# One Hot Encoding"}}