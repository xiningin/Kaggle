{"cell_type":{"a70059fd":"code","b8206efc":"code","2f8bcf4f":"code","c64a3c66":"code","c32f3c64":"code","b7dca756":"code","42c4e0bc":"code","9a45659c":"code","d87f4c03":"code","59baa5f6":"code","5f91e525":"code","78d2fa98":"code","e45ebec6":"code","75493ba1":"code","8cc2e7c5":"code","b5d66726":"code","290b58e0":"code","d1fc1046":"code","a4b58c69":"code","80f2356d":"code","c0ba1138":"code","0d099368":"code","b77b1944":"code","94524b69":"code","0e93df3b":"code","8cb79dc4":"code","c1d23c0a":"code","cab0c703":"code","c94911e1":"code","5758d027":"code","31faff25":"code","fec7d76c":"code","f2d3e21e":"code","5a45a7d5":"code","70849a15":"code","bcc74111":"code","281d019e":"code","16b5a84f":"code","3a5fbc5b":"code","16bd27e0":"code","9a998d3e":"code","f3036a91":"code","94e96c01":"code","14667ee6":"code","36d10067":"code","5ed0e8df":"code","12a5fc3b":"code","a208de04":"code","b288474f":"markdown","6f207d91":"markdown","cbb90f7a":"markdown","4247be5d":"markdown","d3961503":"markdown","cff4f479":"markdown","c57b2a17":"markdown","bf2ae038":"markdown","d7413970":"markdown","c712dff3":"markdown","547a9e41":"markdown","f62d8eb5":"markdown","3bafcd53":"markdown","f1d47bd5":"markdown","b03d62e0":"markdown","82fcac28":"markdown","e76d9365":"markdown","1980e061":"markdown","21653330":"markdown","044ba042":"markdown","27e33db5":"markdown","47f59f44":"markdown","e2f2b145":"markdown","c30fcd2f":"markdown","876d1014":"markdown","b85671ef":"markdown","5a4ed6e8":"markdown","ea309135":"markdown","63bc0baf":"markdown","cc77e911":"markdown","88009696":"markdown","15c3f59f":"markdown","9d3ac127":"markdown","dbd736ea":"markdown","94deaec3":"markdown"},"source":{"a70059fd":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport lightgbm as lgb\n","b8206efc":"train = pd.read_csv('..\/input\/cat-in-the-dat\/train.csv')  \ntest = pd.read_csv('..\/input\/cat-in-the-dat\/test.csv')  \n\nprint(f'Train data Shape is {train.shape}')\nprint(f'Test data Shape is {test.shape}')","2f8bcf4f":"def Drop(feature) :\n    global data\n    data.drop([feature],axis=1, inplace=True)\n    data.head()\n    \ndef UniqueAll(show_value = True) : \n    global data\n    for col in data.columns : \n        print(f'Length of unique data for   {col}   is    {len(data[col].unique())} ')\n        if show_value == True  : \n            print(f'unique values ae {data[col].unique()}' )\n            print('-----------------------------')\n            \ndef Encoder(feature , new_feature, drop = True) : \n    global data\n    enc  = LabelEncoder()\n    enc.fit(data[feature])\n    data[new_feature] = enc.transform(data[feature])\n    if drop == True : \n        data.drop([feature],axis=1, inplace=True)\n        \ndef CPlot(feature) : \n    global data\n    sns.countplot(x=feature, data=data,facecolor=(0, 0, 0, 0),linewidth=5,edgecolor=sns.color_palette(\"dark\", 3))\n    \ndef Mapp(feature , new_feature ,f_dict, drop_feature = True) : \n    global data\n    data[new_feature] = data[feature].map(f_dict)\n    if drop_feature == True : \n        data.drop([feature],axis=1, inplace=True)\n    else :\n        data.head()\ndef Unique(feature) : \n    global data\n    print(f'Number of unique vaure are {len(list(data[feature].unique()))} which are : \\n {list(data[feature].unique())}')","c64a3c66":"train.head()","c32f3c64":"test.head()","b7dca756":"X_train = train.drop(['id' , 'target'], axis=1, inplace=False)\nX_test = test.drop(['id'], axis=1, inplace=False)\n\nX_train.shape , X_test.shape","42c4e0bc":"X = pd.concat([X_train , X_test])\nX.shape","9a45659c":"X.head()","d87f4c03":"data = X","59baa5f6":"CPlot('bin_0')","5f91e525":"CPlot('bin_1')","78d2fa98":"CPlot('bin_2')","e45ebec6":"CPlot('bin_3')","75493ba1":"Mapp('bin_3' , 'bin_03' , {'T':1 , 'F':0} , True)","8cc2e7c5":"data.head()","b5d66726":"CPlot('bin_03')","290b58e0":"Mapp('bin_4' , 'bin_04' , {'Y':1 , 'N':0} , True)","d1fc1046":"CPlot('bin_04')","a4b58c69":"UniqueAll(False)","80f2356d":"for C in ['nom_0' , 'nom_1' , 'nom_2' , 'nom_3' , 'nom_4'] : \n    enc  = LabelEncoder()\n    enc.fit(X[C])\n    X[C] = enc.transform(X[C])","c0ba1138":"data.head()","0d099368":"CPlot('nom_0')","b77b1944":"CPlot('nom_1')","94524b69":"CPlot('nom_2')","0e93df3b":"CPlot('nom_3')","8cb79dc4":"CPlot('nom_4')","c1d23c0a":"Unique('nom_5')","cab0c703":"Unique('nom_6')","c94911e1":"for C in ['nom_5' , 'nom_6' , 'nom_7' , 'nom_8' , 'nom_9']: \n    enc  = LabelEncoder()\n    enc.fit(X[C])\n    X[C] = enc.transform(X[C])","5758d027":"data.head()","31faff25":"CPlot('ord_0')","fec7d76c":"CPlot('ord_1')","f2d3e21e":"CPlot('ord_2')","5a45a7d5":"CPlot('ord_3')","70849a15":"CPlot('ord_4')","bcc74111":"CPlot('ord_5')","281d019e":"for C in ['ord_0' , 'ord_1' , 'ord_2' , 'ord_3' , 'ord_4' , 'ord_5']: \n    enc  = LabelEncoder()\n    enc.fit(X[C])\n    X[C] = enc.transform(X[C])\n\ndata.head()","16b5a84f":"train_data = data.iloc[:train.shape[0],:]\ntest_data=  data.iloc[train.shape[0]:,:]\ntrain_data.shape , test_data.shape","3a5fbc5b":"X = train_data\ny = train['target']\nX.shape , y.shape","16bd27e0":"scaler = MinMaxScaler(copy=True, feature_range=(0, 1))\nX = scaler.fit_transform(X)","9a998d3e":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=44, shuffle =True)\n\nprint('X_train shape is ' , X_train.shape)\nprint('X_test shape is ' , X_test.shape)\nprint('y_train shape is ' , y_train.shape)\nprint('y_test shape is ' , y_test.shape)","f3036a91":"num_round = 25000\n\nparameters = {'num_leaves': 128,\n             'min_data_in_leaf': 20, \n             'objective':'binary',\n             'max_depth': 8,\n             'learning_rate': 0.001,\n             \"min_child_samples\": 20,\n             \"boosting\": \"gbdt\",\n             \"feature_fraction\": 0.9,\n             \"bagging_freq\": 1,\n             \"bagging_fraction\": 0.9 ,\n             \"bagging_seed\": 44,\n             \"metric\": 'auc',\n             \"verbosity\": -1}\n\n\ntraindata = lgb.Dataset(X_train, label=y_train)\ntestdata = lgb.Dataset(X_test, label=y_test)\n\nLGBModel = lgb.train(parameters, traindata, num_round, valid_sets = [traindata, testdata],\n                     verbose_eval=50, early_stopping_rounds = 600)\n","94e96c01":"test = scaler.transform(test_data)\ntest.shape","14667ee6":"y_pred = LGBModel.predict(test)\ny_pred.shape","36d10067":"y_pred[:10]","5ed0e8df":"data = pd.read_csv('..\/input\/cat-in-the-dat\/sample_submission.csv')  \n\nprint(f'Test data Shape is {data.shape}')\ndata.head()","12a5fc3b":"idd = data['id']\nFinalResults = pd.DataFrame(y_pred,columns= ['target'])\nFinalResults.insert(0,'id',idd)\nFinalResults.head()","a208de04":"FinalResults.to_csv(\"sample_submission.csv\",index=False)","b288474f":"and it might not be helpful to plot features with very high number of unique values \n\n______\n\nnow to plot other features ","6f207d91":"& here is test data","cbb90f7a":"now it's time to apply lgb classification model , with round numbers = 25K & shown parameters","4247be5d":"plot them ","d3961503":"____\n\n# Forming the Data\n\nsince this example depend on categorical data , we have to slice features (X) from output (y) from training data , then concatenate X from training data to features from text data . \n\n& this step to make same data processing (like label encoder & so ) for all features \n\nso first to slice X_train & X_test","cff4f479":"300K sample size for training & 200K for testing , great . \n\nnow to define needed functions","c57b2a17":"let's apply minmaxscalerfrom sklearn , to make the model faster ","bf2ae038":"at last we concatenate id column with the result","d7413970":"ok , now predicting testing data","c712dff3":"_____\n\nfor nom_5 & nom_6 , let's have a look to their unique values","547a9e41":"great , now to open sample_submission , to read id columns from it","f62d8eb5":"how it looks now","3bafcd53":"how it looks","f1d47bd5":"then to split it into training & testing data","b03d62e0":"____\n\nwe'll repeat it for bin 4 , Yes & No will be 1 & 0","82fcac28":"____\n\nas usual , start with heading data to have a look to it ","e76d9365":"plot it","1980e061":"looks fine , may be except few features which got a high number of unique values , so it might not be very helpful in training\n\n_____\n\n# Label Encoding\n\nnow we need to apply label encoding to some categorical features , so it be ready for training \n\nlet's start with features : 'nom_0' , 'nom_1' , 'nom_2' , 'nom_3' , 'nom_4'\n","21653330":"plot it ","044ba042":"now for plotting some features , to be sure its values are well represented","27e33db5":"how it looks like ? ","47f59f44":"how it looks ? ","e2f2b145":"_____\n\n& since we use number of unique values for feature on alot of things , let's show them ","c30fcd2f":"______\n\n# Data Processing\n\n\nno we'll call it data , so it be suitable for all functions we define , which depend on global data","876d1014":"ok , we'll continue label encode them","b85671ef":" again to label encode them ","5a4ed6e8":"now to define X & y ","ea309135":"& export the result file","63bc0baf":"_____\n\ngreat . \n\nfor bin 3 , since it got T for True & F for False , let's map it to new feature bin 03, with values 1 , 0","cc77e911":"# Is There a Cat in the Dat ? \nBy : Hesdham Asem\n\n______\n\na simple clean data , which depend on categorical featurs , & we need to classify it to know whether there will be a cat or not\n\nlet's start by importing libraries","88009696":"then read the data","15c3f59f":"how it looks now","9d3ac127":"so now we are ready for Build the model & train the data \n\n______\n\n# Build the Model\n\nfirst to prepare the data for training by defining trainging & testing data again \n","dbd736ea":"now to concatenate them together into X","94deaec3":"accuracy might be better by using more round numbers \n\n_____\n\nnow to predict test data , but first we have to apply same scaler model to test data"}}