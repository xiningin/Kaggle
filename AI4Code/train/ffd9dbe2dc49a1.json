{"cell_type":{"5f47fd9d":"code","0a242946":"code","263100b6":"code","73736567":"code","84bc14c8":"code","60161add":"code","e15f19cd":"code","5701263c":"code","7735f58b":"code","c131c86b":"code","4fcf2785":"code","4b8dd641":"code","30eaea27":"code","f6e5d1ee":"code","4127e019":"code","a7fe20a9":"code","5881e708":"code","e94fea5e":"code","109dbddf":"code","700e76b4":"code","fa9aa22e":"code","56361ae0":"code","bbcd53d0":"code","62bf0e93":"code","114b8c55":"code","de984125":"code","9a922f43":"code","8ec30e14":"code","825c3cac":"markdown","cee00077":"markdown","d0e64ba8":"markdown","a35c558a":"markdown","1978199d":"markdown","ca6f7a4a":"markdown","d7a75314":"markdown","e05686c9":"markdown","ac9143f4":"markdown","b27c2af0":"markdown","ffaaaee1":"markdown","e0dc5c8c":"markdown","cac33c5a":"markdown","55dca407":"markdown","071021c8":"markdown","7f568ed3":"markdown","471ac3a2":"markdown","fc73e74c":"markdown","7cf08c39":"markdown"},"source":{"5f47fd9d":"!apt update\n!apt install --yes python-opencv\n!apt install --yes libopencv-dev\n!\/bin\/bash -c 'echo \"\/opt\/conda\/lib\/\" > \/etc\/ld.so.conf.d\/opencv.conf'\n!ldconfig\n!pip install imagesize","0a242946":"import pandas as pd\nimport os\nimport pickle\nimport matplotlib.pyplot as plt\nimport ast\nimport glob\nimport shutil\nimport sys\nimport numpy as np\nimport imagesize\nimport cv2\nfrom tqdm.notebook import tqdm","263100b6":"!git clone https:\/\/github.com\/AlexeyAB\/darknet.git\n\n%cd darknet\n\n!cp '..\/..\/input\/libcuda\/libcuda.so' .\n\n!sed -i 's\/OPENCV=0\/OPENCV=1\/g' Makefile\n!sed -i 's\/GPU=0\/GPU=1\/g' Makefile\n!sed -i 's\/CUDNN=0\/CUDNN=1\/g' Makefile\n!sed -i 's\/CUDNN_HALF=0\/CUDNN_HALF=1\/g' Makefile\n!sed -i 's\/LIBSO=0\/LIBSO=1\/' Makefile\n!sed -i \"s\/ARCH= -gencode arch=compute_60,code=sm_60\/ARCH= ${ARCH_VALUE}\/g\" Makefile\n\n!sed -i 's\/LDFLAGS+= -L\\\/usr\\\/local\\\/cuda\\\/lib64 -lcuda -lcudart -lcublas -lcurand\/LDFLAGS+= -L\\\/usr\\\/local\\\/cuda\\\/lib64 -lcudart -lcublas -lcurand -L\\\/kaggle\\\/working\\\/darknet -lcuda\/' Makefile\n!make &> compile.log","73736567":"# Check there were no error\n!tail compile.log","84bc14c8":"!.\/darknet detector train","60161add":"ROOT_DIR  = '\/kaggle\/input'\nWORKING_DIR  = '\/kaggle\/working'\ndef get_path(row):\n    row['image_path'] = f'{ROOT_DIR}\/tensorflow-great-barrier-reef\/train_images\/video_{row.video_id}\/{row.video_frame}.jpg'\n    row['label_path'] = f'{WORKING_DIR}\/darknet\/data\/obj\/video_{row.video_id}_{row.video_frame}.txt'\n    return row","e15f19cd":"df = pd.read_csv(f'{ROOT_DIR}\/tensorflow-great-barrier-reef\/train.csv')\ndf = df.apply(get_path, axis=1)\ndf['annotations'] = df['annotations'].apply(lambda x: ast.literal_eval(x))\ndisplay(df.head(2))","5701263c":"df['num_bbox'] = df['annotations'].apply(lambda x: len(x))\ndata = (df.num_bbox>0).value_counts()\/len(df)*100\nprint('% images without annotations: {}'.format(data[0]))\nprint('% images with annotations: {} '.format(data[1]))","7735f58b":"# Remove data without boxes\ndf = df.query(\"num_bbox>0\")","c131c86b":"# COCO <--> YOLO conversion\ndef coco2yolo(image_height, image_width, bboxes):\n    \"\"\"\n    coco => [xmin, ymin, w, h]\n    yolo => [xmid, ymid, w, h] (normalized)\n    \"\"\"\n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # normalizinig\n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]\/ image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]\/ image_height\n    \n    # converstion (xmin, ymin) => (xmid, ymid)\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\/2\n    \n    return bboxes\n\ndef yolo2coco(image_height, image_width, bboxes):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    coco => [xmin, ymin, w, h]\n    \n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # denormalizing\n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]* image_height\n    \n    # converstion (xmid, ymid) => (xmin, ymin) \n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]\/2\n    \n    return bboxes\n\ndef load_image(image_path):\n    return cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n\n\ndef plot_one_box(x, img, color=None, label=None, line_thickness=None):\n    # Plots one bounding box on image img\n    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) \/ 2) + 1  # line\/font thickness\n    color = color or [random.randint(0, 255) for _ in range(3)]\n    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n    if label:\n        tf = max(tl - 1, 1)  # font thickness\n        t_size = cv2.getTextSize(label, 0, fontScale=tl \/ 3, thickness=tf)[0]\n        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl \/ 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n\ndef draw_bboxes(img, bboxes, classes, class_ids, colors = None, show_classes = None, bbox_format = 'yolo', class_name = False, line_thickness = 2):  \n     \n    image = img.copy()\n    show_classes = classes if show_classes is None else show_classes\n    colors = (0, 255 ,0) if colors is None else colors\n    \n    if bbox_format == 'yolo':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:\n            \n                x1 = round(float(bbox[0])*image.shape[1])\n                y1 = round(float(bbox[1])*image.shape[0])\n                w  = round(float(bbox[2])*image.shape[1]\/2) #w\/2 \n                h  = round(float(bbox[3])*image.shape[0]\/2)\n\n                voc_bbox = (x1-w, y1-h, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(get_label(cls)),\n                             line_thickness = line_thickness)\n    else:\n        raise ValueError('wrong bbox format')\n\n    return image\n\ndef get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\ndef get_imgsize(row):\n    row['width'], row['height'] = imagesize.get(row['image_path'])\n    return row","4fcf2785":"df['bboxes'] = df.annotations.apply(get_bbox)\ndf = df.apply(get_imgsize,axis=1)\ndisplay(df.width.unique(), df.height.unique())\ndisplay(df.head(2))","4b8dd641":"%cd data\/\n!mkdir obj test\n\ncnt = 0\nfor row_idx in tqdm(range(df.shape[0])):\n    row = df.iloc[row_idx]\n    image_height = row.height\n    image_width = row.width\n    bboxes_coco = np.asarray(row.bboxes).astype(np.float32).copy()\n    num_bbox = len(bboxes_coco)\n    labels = [0]*num_bbox\n  \n    f = open(row.label_path, 'w')\n\n    if num_bbox < 1:\n        annot = ''\n        f.write(annot)\n        f.close()\n        cnt += 1\n        continue\n  \n    bboxes_yolo  = coco2yolo(image_height, image_width, bboxes_coco)\n\n    for i in range(len(bboxes_yolo)):\n        annot = [str(labels[i])] + list(bboxes_yolo[i].astype(str)) + (['\\n'] if num_bbox!=(i+1) else [''])\n        annot = ' '.join(annot)\n        annot = annot.strip(' ')\n        f.write(annot)\n    f.close()\n\nprint('Missing boxes ', cnt)","30eaea27":"!cat obj\/video_0_1000.txt","f6e5d1ee":"from sklearn.model_selection import GroupKFold\nkf = GroupKFold(n_splits = 5) \ndf = df.reset_index(drop=True)\ndf['fold'] = -1\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df, y = df.video_id.tolist(), groups=df.sequence)):\n    df.loc[val_idx, 'fold'] = fold\ndisplay(df.fold.value_counts())","4127e019":"val_df = df[df['fold']==2]\ntrain_df = df[df['fold']!=2]\nprint(train_df.shape)\nprint(val_df.shape)","a7fe20a9":"# # Move labels from obj\/ to test\/ directory\ndef mv_labels (row):\n    old_path = row.label_path\n    filename = row.label_path.split('\/')[-1]\n    new_path = '\/'.join(row.label_path.split('\/')[:-2]) + '\/test\/' + filename\n    row['label_path'] = new_path\n    shutil.move(old_path, new_path)\n    return row\n\nval_df = val_df.apply(lambda x: mv_labels(x), axis=1)\nval_df.head(2)","5881e708":"val_df.head(2)","e94fea5e":"# Copy images to working dir\n'''\nLabels and images must have the same name:\nImages: obj\/image_XX.jpg\nLabels: obj\/image_XX.txt\n'''\ndef copy_images (row):\n    old_path = row.image_path\n    filename = row.label_path.split('\/')[-1][:-4] + '.jpg'\n    new_path = '\/'.join(row.label_path.split('\/')[:-1]) + '\/' + filename\n    shutil.copy(old_path, new_path)\nval_df.apply(lambda x: copy_images(x), axis=1)\ntrain_df.apply(lambda x: copy_images(x), axis=1)","109dbddf":"!ls obj\/*.jpg | wc -l\n!ls obj\/*.txt | wc -l\n!ls test\/*.jpg | wc -l\n!ls test\/*.txt | wc -l","700e76b4":"# Generate train.txt and test.txt\n%cd ..\/\ntrain_images = glob.glob('data\/obj\/*.jpg')\nf = open('.\/data\/train.txt', 'w')\nannot = [os.path.join(os.getcwd(),t) + ('\\n' if i<len(train_images)-1 else '') for i, t in enumerate(train_images)]\nannot = ''.join(annot)\nannot = annot.strip()\nf.write(annot)\n\nval_images = glob.glob('data\/test\/*.jpg')\nf = open('.\/data\/test.txt', 'w')  \nannot = [os.path.join(os.getcwd(),t) + ('\\n' if i<len(val_images)-1 else '') for i, t in enumerate(val_images)]\nannot = ''.join(annot)\nannot = annot.strip()\nf.write(annot)","fa9aa22e":"!cat data\/train.txt | wc -l\n!cat data\/test.txt | wc -l","56361ae0":"\nnp.random.seed(32)\ncolors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n          for idx in range(1)]\n\ndf2 = train_df[(train_df.num_bbox>0)].sample(100) # takes samples with bbox\n\nfor idx in range(10):\n    row = df2.iloc[idx]\n    img           = load_image(row.image_path)\n    image_height  = row.height\n    image_width   = row.width\n    f = open(row.label_path)\n    bboxes_yolo = np.asarray([[float(a) for a in l[1:].strip().split(' ')] for l in f.readlines()])\n\n    names         = ['starfish']*len(bboxes_yolo)\n    labels        = [0]*len(bboxes_yolo)\n\n    plt.figure(figsize = (12, 8))\n    plt.imshow(draw_bboxes(img = img,\n                           bboxes = bboxes_yolo, \n                           classes = names,\n                           class_ids = labels,\n                           class_name = True, \n                           colors = colors, \n                           bbox_format = 'yolo',\n                           line_thickness = 2))\n    plt.axis('OFF')\n    plt.show()","bbcd53d0":"# Adapt yolov4-custom.cfg to one-class model\n# If subdivisions=16 runs into memory issues use 32, otherwise 16 is the optimal\n!sed -i 's\/subdivisions=16\/subdivisions=32\/g' .\/cfg\/yolov4-custom.cfg\n# To avoid memory issues with downsized image size from 608 to 416. \n!sed -i 's\/width=608\/width=416\/g' .\/cfg\/yolov4-custom.cfg\n!sed -i 's\/height=608\/height=416\/g' .\/cfg\/yolov4-custom.cfg\n\n# Make the rest of the changes to the cfg based on how many classes you are training your detector on.\n'''\nheight = 416 (these can be any multiple of 32, 416 is standard, you can sometimes \nimprove results by making value larger like 608 but will slow down training)\n\nmax_batches = (# of classes) * 2000 (but no less than 6000 so if you are training \nfor 1, 2, or 3 classes it will be 6000, however detector for 5 classes would have max_batches=10000)\n\nsteps = (80% of max_batches), (90% of max_batches) \n(so if your max_batches = 10000, then steps = 8000, 9000)\n\nfilters = (# of classes + 5) * 3 (so if you are training for one class then your \nfilters = 18, but if you are training for 4 classes then your filters = 27)\n'''\n!sed -i 's\/max_batches = 500500\/max_batches = 6000\/g' .\/cfg\/yolov4-custom.cfg\n!sed -i 's\/steps=400000,450000\/steps=4800,5400\/g' .\/cfg\/yolov4-custom.cfg\n!sed -i 's\/classes=80\/classes=1\/g' .\/cfg\/yolov4-custom.cfg\n!sed -i 's\/filters=255\/filters=18\/g' .\/cfg\/yolov4-custom.cfg\n\n# Let's build obj.data and obj.names needed by darknet\nf = open('.\/data\/obj.data', 'w')\nf.write('classes = 1\\ntrain = data\/train.txt\\nvalid = data\/test.txt\\nnames = data\/obj.names\\nbackup = backup\\n')\nf.close()\nf = open('.\/data\/obj.names', 'w')\nf.write('starfish')\nf.close()","62bf0e93":"# Download a pre-trained model\n!wget https:\/\/github.com\/AlexeyAB\/darknet\/releases\/download\/darknet_yolo_v3_optimal\/yolov4.conv.137","114b8c55":"# Start training from pre-trained model\n!.\/darknet detector train data\/obj.data cfg\/yolov4-custom.cfg yolov4.conv.137 -dont_show -map","de984125":"# need to set our custom cfg to test mode \n%cd cfg\n!sed -i 's\/batch=64\/batch=1\/' yolov4-custom.cfg\n!sed -i 's\/subdivisions=16\/subdivisions=1\/' yolov4-custom.cfg\n%cd ..","9a922f43":"test_image = '.\/data\/obj\/video_1_3921.jpg'\n\n!.\/darknet detector test data\/obj.data cfg\/yolov4-custom.cfg {ROOT_DIR}\/yolov4-cots\/yolov4-custom_last.weights {test_image} -thresh 0.1","8ec30e14":"from PIL import Image\n\nnp.random.seed(32)\ncolors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n          for idx in range(1)]\n\nIMAGEPATH = test_image\n\ndef change_path(row):\n    filename = row.image_path.split('\/')[-1]\n    videoname = row.image_path.split('\/')[-2]\n    return os.path.join('.\/data\/obj', videoname + '_' + filename)\n\ntmp_df = train_df.copy()\ntmp_df['image_path'] = tmp_df.apply(lambda x: change_path(x), 1)\n\ndf2 = tmp_df[(tmp_df.image_path==IMAGEPATH)]\nrow = df2.iloc[0]\nimg           = load_image(row.image_path)\nimage_height  = row.height\nimage_width   = row.width\nf = open(row.image_path[:-4] + '.txt')\nbboxes_yolo = np.asarray([[float(a) for a in l[1:].strip().split(' ')] for l in f.readlines()])\n\nnames         = ['starfish']*len(bboxes_yolo)\nlabels        = [0]*len(bboxes_yolo)\n\n\n# 2. Plot in same line, on two rows\nplt.figure(figsize = (19, 8))\nplt.subplot(1, 2, 1)\n\nplt.imshow(draw_bboxes(img = img,\n                      bboxes = bboxes_yolo, \n                      classes = names,\n                      class_ids = labels,\n                      class_name = True, \n                      bbox_format = 'yolo',\n                       colors = colors,\n                      line_thickness = 2))\n\nplt.axis('OFF')\nplt.title('Ground truth test set')\n\nplt.subplot(1, 2, 2)\n#plt.figure(figsize = (12, 8))\nplt.axis('OFF')\nplt.title('Prediction test set')\nimg = Image.open('predictions.jpg')\nplt.imshow(img)\nplt.show()","825c3cac":"# \ud83d\udcda Import Libraries","cee00077":"# Visualization","d0e64ba8":"- More details in [Build YOLOv4 with darknet \ud83d\udd28](https:\/\/www.kaggle.com\/gimarcecaml\/build-darknet-yolo4)","a35c558a":"We need to have the following dir structure according to [YOLOv4 tutorial](https:\/\/colab.research.google.com\/drive\/1_GdoqCJWXsChrOiY8sZMr_zbr_fH-0Fg#scrollTo=POozxsvFdXTu)\n```\n\/Kaggle\/working\/darknet\n    \/data\n         \/obj\n             \/video_X_XXX.jpg\n             \/video_X_XXX.txt\n         \/test\n             \/video_X_XXX.jpg\n             \/video_X_XXX.txt\n         \/train.txt\n         \/test.txt\n        \/obj.data\n        \/obj.names\n    \/cfg\n        \/yolov4-custom.cfg\n```\n- `video_X_XXX.txt`: contains the YOLO normalized annotations (one per line)\n- `train(test).txt`: contains the filenames of the images `data\/obj(test)\/video_X_XXX.jpg`\n- `yolov4-custom.cfg`: YOLO config provided by darknet. We updated some values accordingly for this challenge.","1978199d":"# \ud83d\udee0 Install Libraries","ca6f7a4a":"## \ud83d\udcd2 Notebooks:\n* Install darknet: [Build YOLOv4 with darknet \ud83d\udd28](https:\/\/www.kaggle.com\/gimarcecaml\/build-darknet-yolo4)","d7a75314":"## Please Upvote if you find this Helpful","e05686c9":"# Split dataset","ac9143f4":"# \ud83d\ude80 Train a pre-trained model","b27c2af0":"We modify `yolov4-custom.cfg` provided by darknet to adapt it to one-class classification","ffaaaee1":"### Please if this kernel is useful, <font color='red'>please upvote !!<\/font>","e0dc5c8c":"# \ud83d\udd0e Predictions vs ground-truth","cac33c5a":"# Directory Structure","55dca407":"# Install \ud83d\udee0 darknet framework","071021c8":"# \ud83d\udcd6 Meta Data\n* `train_images\/` - Folder containing training set photos of the form `video_{video_id}\/{video_frame}.jpg`.\n\n* `[train\/test].csv` - Metadata for the images. As with other test files, most of the test metadata data is only available to your notebook upon submission. Just the first few rows available for download.\n\n* `video_id` - ID number of the video the image was part of. The video ids are not meaningfully ordered.\n* `video_frame` - The frame number of the image within the video. Expect to see occasional gaps in the frame number from when the diver surfaced.\n* `sequence` - ID of a gap-free subset of a given video. The sequence ids are not meaningfully ordered.\n* `sequence_frame` - The frame number within a given sequence.\n* `image_id` - ID code for the image, in the format `{video_id}-{video_frame}`\n* `annotations` - The bounding boxes of any starfish detections in a string format that can be evaluated directly with Python. Does not use the same format as the predictions you will submit. Not available in test.csv. A bounding box is described by the pixel coordinate `(x_min, y_min)` of its lower left corner within the image together with its `width` and `height` in pixels --> (COCO format).","7f568ed3":"# Config files update & creation","471ac3a2":"# \u2757\ufe0f Inputs needed\n- `libcuda.so` imported from [here](https:\/\/www.kaggle.com\/denispotapov\/libcuda)\n- Don't forget to turn on your GPU!\n- For visualizing predictions (see at the bottom) a custom trained YOLOv4 model in COTS dataset is provided [here](https:\/\/www.kaggle.com\/gimarcecaml\/yolov4-cots)","fc73e74c":"# \ud83d\udd2e Inference\nA custom trained model is provided as an example [here](https:\/\/www.kaggle.com\/gimarcecaml\/yolov4-cots) to test some predictions","7cf08c39":"# \ud83d\udea9 Version Info\n| Version | Model | mAP@0.50| Comment\n|---|---|---|---|\n| v0 | YOLOv4 | 33.14%| fold 2 \/ 20 |"}}