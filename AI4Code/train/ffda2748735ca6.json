{"cell_type":{"fcc59463":"code","1644125b":"code","5169b8ab":"code","a7577ec7":"code","31b91e7a":"code","c34643ff":"code","ffbc2685":"code","e9d70f25":"code","6d7ca280":"code","d900c674":"code","b3a8af94":"code","b2fa5256":"code","cb8d43f0":"code","3fa48468":"code","659cd671":"code","d8c6fb8f":"code","45a21b2e":"code","1bcd2421":"code","7bbfaba0":"code","19668ac4":"code","b0c48760":"code","7cac28db":"code","8985576c":"code","37a2dfd6":"code","059e3bb1":"code","c4e60624":"code","98a77ec6":"code","47617cd7":"code","fb517f48":"code","07857b32":"code","d5d7fb7c":"code","c5a6640e":"code","0e478e18":"code","14d71a90":"code","7382d593":"code","3b605403":"code","496dda98":"markdown","aeeee3b1":"markdown","933f1faf":"markdown","caedd39f":"markdown","e9f70a82":"markdown","290e62d6":"markdown","1fcd42ad":"markdown","673d92fe":"markdown","980f9e81":"markdown"},"source":{"fcc59463":"!pip install jovian -q","1644125b":"import jovian","5169b8ab":"import tensorflow as tf\n# get device name\ndevice_name = tf.test.gpu_device_name()\n\nif device_name == '\/device:GPU:0':\n  print(f'Found gpu as {device_name}')\nelse:\n  raise SystemError('GPU device not found')","a7577ec7":"import torch\n\n# check if gpu available \nif torch.cuda.is_available():\n  # set pytorch to use gpu\n  device = torch.device(\"cuda\")\n  print(f\" There are {torch.cuda.device_count()} GPU(s) available. We will use GPU {torch.cuda.get_device_name(0)}.\" )\nelse:\n  print(\"Np GPU available. Using CPU instead.\")\n  device = torch.device('cpu')","31b91e7a":"# installing huggingface transformer library\n!pip install transformers -q","c34643ff":"import pandas as pd\ndf = pd.read_csv('..\/input\/wine-reviews\/winemag-data-130k-v2.csv')\ndf.head()","ffbc2685":"df['points'] = df['points'] - 80 # setting points from 0 to 21","e9d70f25":"train_df = df[['description','title', 'points']]","6d7ca280":"from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n\n# Load bert tokenizer\nprint('Load DistilBERT tokenizer...')\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased',do_lower_case = True)","d900c674":"# Let us take a look at the first description and tokenize it\ntext = train_df.iloc[0].description\n# Run tokenizer and count number of tokens\ntokens = tokenizer.tokenize(text)\nprint(f'First description contains {len(tokens)} wordpiece tokens.')","b3a8af94":"import numpy as np\n#tokenize all the descriptions and map tokens to their word IDs\n\ninput_ids = []\n\n# Record length of each sequence (truncate to 512 -> BERT limit of for input sequence)\nlengths = []\n\nfor des in train_df.description:\n  if (len(input_ids) % 30000 == 0) and len(input_ids)!=0:\n    print(f'Completed reading {len(input_ids)} lines.')\n\n  # tokenizer.enode does\n    # Tokenize\n    # Prepend [CLS] token\n    # Append [SEP] token\n    # Map token to their IDs\n\n  encoded_sent = tokenizer.encode(\n      des, # description\n      add_special_tokens = True, #add [CLS] and [SEP]\n\n  )\n  # add encoded text to list\n  input_ids.append(encoded_sent)\n  # record the truncated length\n  lengths.append(len(encoded_sent))\n\nprint('Done')","b2fa5256":"# Get labels from train_df \nlabels = train_df.points.to_numpy().astype(int)","cb8d43f0":"print(f'Minimum length of tokens is {min(lengths)}.')\nprint(f'Maximum length of tokens is {max(lengths)}.')\nprint(f'Median length of tokens is {np.median(lengths)}.')","3fa48468":"import matplotlib.pyplot as plt \nimport seaborn as sns\nsns.set(style = 'darkgrid', font_scale = 1.5)\nplt.rcParams['figure.figsize'] = (10,5)","659cd671":"sns.distplot(lengths);\nplt.title('Description tokens distribution');\nplt.xlabel('Description token length')\nplt.ylabel('# of sentences')","d8c6fb8f":"# PAD the tokens for future use\nfrom keras.preprocessing.sequence import pad_sequences\n# Most of sequences have token length smaller than 128\nMAX_LEN = 128\nprint(f'\\nPadding and truncating sentences to {MAX_LEN} values...')\nprint(f'\\nPadding token: {tokenizer.pad_token}, ID: {tokenizer.pad_token_id}')\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype='long',\n                          value=tokenizer.pad_token_id, truncating = 'post', padding = 'post')\n\nprint('Done')","45a21b2e":"# create attention mask\nattention_masks = []\nfor sent in input_ids:\n  # if token id == 0 then its padding so set mask to 0\n  # else its a real token and hence set mask == 1\n  attn_mask = [int(token_id > 0) for token_id in sent]\n  attention_masks.append(attn_mask)","1bcd2421":"from sklearn.model_selection import train_test_split\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,random_state = 1234, test_size = 0.1)\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,random_state = 1234, test_size = 0.1)","7bbfaba0":"# converting all the numpy arrays to pytorch tensors\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\n\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\n\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks)","19668ac4":"# Using pytorch dataloader class to help with training\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\nbatch_size = 32\n# train data loader\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size)\n# test data loader\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = RandomSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler = validation_sampler, batch_size = batch_size)","b0c48760":"from transformers import BertForSequenceClassification, AdamW, BertConfig\n\nmodel = DistilBertForSequenceClassification.from_pretrained(\n    'distilbert-base-uncased', # use 12 layer base Distil-BERT with uncased vocab\n    num_labels = 21, # Linear regression unique points\n    output_attentions = False, # Do not return attention weights\n    output_hidden_states = False )# do not retun all hidden states\n\nmodel.cuda()","7cac28db":"optimizer = AdamW(model.parameters(), lr = 2e-4, eps = 1e-8)","8985576c":"from transformers import get_linear_schedule_with_warmup\n# number of training epochs\nepochs = 4\n# number of training steps\ntotal_steps = len(train_dataloader)* epochs\n# learning rate scheduler\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)","37a2dfd6":"# helper functions \nimport time\nimport datetime\ndef flat_accuracy(preds, labels):\n  preds_flat = np.argmax(preds, axis=1).flatten()\n  labels_flat = labels.flatten()\n  return np.sum(preds_flat == labels_flat) \/ len(labels_flat)\n\ndef format_time(elapsed):\n  '''\n  retunr time in hh:mm:ss from seconds\n  '''\n  elapsed_round = int(round(elapsed))\n  return str(datetime.timedelta(seconds = elapsed_round))","059e3bb1":"import random\nimport numpy as np\nimport os\n\n# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n\noutput_dir = '.\/model'\n\n# Create output directory if needed\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n\n# This training code is based on the `run_glue.py` script here:\n# https:\/\/github.com\/huggingface\/transformers\/blob\/5bfcd0485ece086ebcbed2d008813037968a9e58\/examples\/run_glue.py#L128\n\n# Set the seed value all over the place to make this reproducible.\nseed_val = 1234\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# We'll store a number of quantities such as training and validation loss, \n# validation accuracy, and timings.\ntraining_stats = []\n\n# Measure the total training time for the whole run.\ntotal_t0 = time.time()\n\n# For each epoch...\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    \n    # Perform one full pass over the training set.\n\n    print(\"\")\n    print('======== Epoch {:} \/ {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    total_train_loss = 0\n\n    # Put the model into training mode. Don't be mislead--the call to \n    # `train` just changes the *mode*, it doesn't *perform* the training.\n    # `dropout` and `batchnorm` layers behave differently during training\n    # vs. test (source: https:\/\/stackoverflow.com\/questions\/51433378\/what-does-model-train-do-in-pytorch)\n    model.train()\n\n    # For each batch of training data...\n    for step, batch in enumerate(train_dataloader):\n\n        # Progress update every 40 batches.\n        if step % 40 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n        # `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        # Always clear any previously calculated gradients before performing a\n        # backward pass. PyTorch doesn't do this automatically because \n        # accumulating the gradients is \"convenient while training RNNs\". \n        # (source: https:\/\/stackoverflow.com\/questions\/48001598\/why-do-we-need-to-call-zero-grad-in-pytorch)\n        model.zero_grad()        \n\n        # Perform a forward pass (evaluate the model on this training batch).\n        # The documentation for this `model` function is here: \n        # https:\/\/huggingface.co\/transformers\/v2.2.0\/model_doc\/bert.html#transformers.BertForSequenceClassification\n        # It returns different numbers of parameters depending on what arguments\n        # arge given and what flags are set. For our useage here, it returns\n        # the loss (because we provided labels) and the \"logits\"--the model\n        # outputs prior to activation.\n        loss, logits = model(b_input_ids,\n                           attention_mask = b_input_mask,\n                           labels = b_labels)\n#         loss, logits = output.loss, output.logits\n\n        # Accumulate the training loss over all of the batches so that we can\n        # calculate the average loss at the end. `loss` is a Tensor containing a\n        # single value; the `.item()` function just returns the Python value \n        # from the tensor.\n        total_train_loss += loss.item()\n\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # Update parameters and take a step using the computed gradient.\n        # The optimizer dictates the \"update rule\"--how the parameters are\n        # modified based on their gradients, the learning rate, etc.\n        optimizer.step()\n\n        # Update the learning rate.\n        scheduler.step()\n\n    # Calculate the average loss over all of the batches.\n    avg_train_loss = total_train_loss \/ len(train_dataloader)            \n    \n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(training_time))\n        \n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on\n    # our validation set.\n\n    print(\"\")\n    print(\"Running Validation...\")\n\n    t0 = time.time()\n\n    # Put the model in evaluation mode--the dropout layers behave differently\n    # during evaluation.\n    model.eval()\n\n    # Tracking variables \n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    nb_eval_steps = 0\n\n    # Evaluate data for one epoch\n    for batch in validation_dataloader:\n        \n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using \n        # the `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        \n        # Tell pytorch not to bother with constructing the compute graph during\n        # the forward pass, since this is only needed for backprop (training).\n        with torch.no_grad():        \n\n            # Forward pass, calculate logit predictions.\n            # token_type_ids is the same as the \"segment ids\", which \n            # differentiates sentence 1 and 2 in 2-sentence tasks.\n            # The documentation for this `model` function is here: \n            # https:\/\/huggingface.co\/transformers\/v2.2.0\/model_doc\/bert.html#transformers.BertForSequenceClassification\n            # Get the \"logits\" output by the model. The \"logits\" are the output\n            # values prior to applying an activation function like the softmax.\n            loss, logits = model(b_input_ids,\n                           attention_mask = b_input_mask,\n                           labels = b_labels)\n#         loss, logits = output.loss, output.logits\n            \n        # Accumulate the validation loss.\n        total_eval_loss += loss.item()\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        # Calculate the accuracy for this batch of test sentences, and\n        # accumulate it over all batches.\n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n        \n\n    # Report the final accuracy for this validation run.\n    avg_val_accuracy = total_eval_accuracy \/ len(validation_dataloader)\n    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n\n    # Calculate the average loss over all of the batches.\n    avg_val_loss = total_eval_loss \/ len(validation_dataloader)\n    \n    # Measure how long the validation run took.\n    validation_time = format_time(time.time() - t0)\n    \n    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n    print(\"  Validation took: {:}\".format(validation_time))\n\n    # Record all statistics from this epoch.\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Valid. Accur.': avg_val_accuracy,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n\n    torch.save(model, (f\".\/model\/model_{epoch_i}.pth\"))\nprint(\"\")\nprint(\"Training complete!\")\n\nprint(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))","c4e60624":"# Display floats with two decimal places.\npd.set_option('precision', 2)\n\n# Create a DataFrame from our training statistics.\ndf_stats = pd.DataFrame(data=training_stats)\n\n# Use the 'epoch' as the row index.\ndf_stats = df_stats.set_index('epoch')\n\n# A hack to force the column headers to wrap.\n#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n\n# Display the table.\ndf_stats","98a77ec6":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\n\n# Use plot styling from seaborn.\nsns.set(style='darkgrid')\n\n# Increase the plot size and font size.\nsns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\n# Plot the learning curve.\nplt.plot(df_stats['Training Loss'], '-', label=\"Training\")\nplt.plot(df_stats['Valid. Loss'], '--', label=\"Validation\")\n\n# Label the plot.\nplt.title(\"Training & Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.xticks([1, 2, 3, 4])\n\nplt.show()","47617cd7":"import os\n\n# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n\noutput_dir = '.\/model'\n\n# Create output directory if needed\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\nprint(\"Saving model to %s\" % output_dir)\n\n# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n# They can then be reloaded using `from_pretrained()`\nmodel_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed\/parallel training\nmodel_to_save.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\n# Good practice: save your training arguments together with the trained model\ntorch.save(model, output_dir+\"\/model.pth\")","fb517f48":"jovian.commit(artifacts = ['.\/model\/model.pth'],project = 'semantic_similarity_wine')","07857b32":"import torch\nfrom keras.preprocessing.sequence import pad_sequences\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\nimport numpy as np\n# Load bert tokenizer\nprint('Load DistilBERT tokenizer...')\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased',do_lower_case = True)\n# Loading model and setting output hidden states flag to true\nprint('Load DistilBERT model...')\nmodel = DistilBertForSequenceClassification.from_pretrained('..\/input\/distilbertmodel-trained-wine-reviews', output_hidden_states = True)\nmodel.cuda()\ndef text_to_embedding(tokenizer,model, input_text):\n    # Step 1 = Tokenization\n    MAX_LEN = 128 #Max token length\n    input_ids = tokenizer.encode(\n      input_text, # description\n      add_special_tokens = True, #add [CLS] and [SEP]\n        max_length = MAX_LEN,\n        truncation=True\n  )\n    result = pad_sequences([input_ids], maxlen=MAX_LEN, dtype='long',\n                          value=tokenizer.pad_token_id, truncating = 'post', padding = 'post')\n    input_ids = result[0]\n    attn_mask = [int(token_id > 0) for token_id in input_ids]\n    \n    # to tensor\n    input_ids = torch.tensor(input_ids)\n    attn_mask = torch.tensor(attn_mask)\n    \n    # Adding extra dimension for processing\n    input_ids = input_ids.unsqueeze(0)\n    attn_mask = attn_mask.unsqueeze(0)\n    \n    # to GPU\n    input_ids = input_ids.to(device)\n    attn_mask = attn_mask.to(device)\n    # step 2 evaluaring text on DistilBERT model\n    model.eval()\n    # No gradient tracking mode\n    with torch.no_grad():\n        _ , encoded_layer = model(input_ids = input_ids,attention_mask = attn_mask)\n         \n        \n    layer_i = 6 # last layer for DistilBERT\n    batch_i = 0 # Only One input in the batch\n    token_i = 0 # Get token for the [CLS] \n    # Get sentece emedding \n    vec = encoded_layer[layer_i][batch_i][token_i]\n    vec = vec.detach().cpu().numpy() # move to cpu and convert to numpy\n    norm = np.linalg.norm(vec) # for normalizing the vector\n    return vec\/norm\n    \n    ","d5d7fb7c":"text = train_df.iloc[0].description\nvec = text_to_embedding(tokenizer, model, text)\nprint(vec.shape)","c5a6640e":"# Converting all Texts to vectors\nimport time\n# Setting initial time\nt0 = time.time()\n\nembeddings = []\n# Number of comments\nnum_text = len(train_df.description)\nfor text in train_df.description:\n    if (len(embeddings) % 2000 == 0) and len(embeddings)!=0:\n        elapsed = format_time(time.time() - t0)\n        print(f'Completed reading {len(embeddings)} lines. Took time {elapsed}.')\n        \n    vec = text_to_embedding(tokenizer, model, text)\n    embeddings.append(vec)\n    \n    ","0e478e18":"import numpy as np\nvecs = np.vstack(embeddings)\nprint(vecs.shape)","14d71a90":"import os\n\n# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n\noutput_dir = '.\/model'\n\n# Create output directory if needed\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\nnp.save(\".\/model\/Text_embeddings.npy\",vecs)","7382d593":"import numpy as np\n# Loading previously saved numpy embeddings\nembeddings = np.load('..\/input\/distilbertmodel-trained-wine-reviews\/Text_embeddings.npy')\n# Enter qualities of wine that you would like to get recommendations for\ntext_to_search = \"\"\"Vintage, old, red, pinot, white, cherry, fruit\"\"\"\nprint(\"Description to look for -\", text_to_search)\nvec = text_to_embedding(tokenizer, model, text_to_search)\nresult = np.matmul(embeddings, vec)\ntop_5_row_nums = result.argsort()[-5:][::-1]","3b605403":"print(\"====== Top 5 related Wines with reviews ======\")\nfor row in top_5_row_nums:\n    print('Name - ', df.iloc[row].title,'\\n', '')\n    print('Review - ' , train_df.iloc[row].description,'\\n')\n    ","496dda98":"## Saving model and training arguments","aeeee3b1":"## Following cell uses trained model and uses outputs of last hidden layer to get sentence embeddings for all the reviews (pretrained embeddings are already in data do not run unless model is trained on new data. Takes about 18 minutes)","933f1faf":"## Download wine reviews data from kaggle","caedd39f":"### performing train and validation split for sentences and attention masks","e9f70a82":"Lengths of all the tokens are less than 512 which is standerd limit for BERT. Hence we can use our tokens without dropping any of the data samples.","290e62d6":"### Lets plot this distribution using matplotlib","1fcd42ad":"### Let us now analyze how the description length is distributed","673d92fe":"## Bert fine tuning by using Bert for squence classification using huggingface library","980f9e81":"## Following cell takes upto 2 hr depending on dataset size and GPU variety (Do not run unless really necessary)"}}