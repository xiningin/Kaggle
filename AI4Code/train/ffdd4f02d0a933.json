{"cell_type":{"2aee2799":"code","7b5c25ec":"code","59304a18":"code","3dfd70dc":"code","a1580c4d":"code","3773994f":"code","1bafebc5":"code","7848f995":"code","d4a130b8":"code","e5bfb5f3":"code","3703c70e":"code","bd96d082":"code","ee99d8af":"code","ff0a211e":"code","0af62a3a":"code","a3d06511":"code","d7c04423":"code","246b15cb":"code","38ddb500":"code","b9ecf564":"code","049c062b":"code","5a6b869f":"code","ad2b4c46":"code","1eaae65c":"code","6cbe0f5f":"code","090521af":"code","a74eda6b":"code","58c0540b":"code","02ce1ff1":"code","90647635":"code","744e4b0d":"code","3d9e3a51":"code","3c8b4a5c":"code","0c14b7c1":"code","4b24b1f8":"code","33313a33":"code","65db29fd":"code","b8fbc1c3":"code","566616f6":"code","f7e05c35":"code","0958f778":"code","393de214":"code","aa505840":"code","510fe73f":"code","9f101ec2":"markdown","535a067f":"markdown","5fe12b5d":"markdown","b2e93877":"markdown","6d1df3f1":"markdown","f8fb3072":"markdown"},"source":{"2aee2799":"!pip install comet_ml\n!pip install --upgrade wandb","7b5c25ec":"!wandb login ee9416edde558c322450d0ec80266d2c0db81f45","59304a18":"from comet_ml import Experiment\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport wandb\nwandb.init(project=\"text-augmentation-quora\")\n\nimport os\nimport zipfile\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/quora-insincere-questions-classification\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport time\nimport os\nimport pandas as pd\nimport numpy as np\nimport random\nimport copy\n\nimport re\nfrom gensim.models import KeyedVectors\nimport gc\n\nfrom keras.preprocessing.text import Tokenizer\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.utils.data import Dataset\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\nt0 = time.time()\n\ndata_path = '\/kaggle\/input\/quora-insincere-questions-classification\/'\nseed = 2077\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# Any results you write to the current directory are saved as output.","3dfd70dc":"for dirname, _, filenames in os.walk('\/kaggle\/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a1580c4d":"api = 'GGF21Vtrnid3Cgat9n1nL9Vcc'\nexperiment = Experiment(api_key=api, project_name=\"GRU classificaton\", workspace=\"comet-ml testing\")","3773994f":"# Preprocessing\nmax_len = 50\nlower = True\ntrunc = 'pre'\nmax_features = 120000\nn_vocab = max_features\nclean_num = 0\n\n# Training\n#n_models = 6\nn_models = 5\naug_epochs = 8\nbatch_size = 512\ndrop_last = True\n\nhidden_dim = 128\n\n# Embedding\nfix_embedding = True\nunk_uni = True  # Initializer for unknown words \u0434\u0435\u043b\u0430\u0435\u0442 \u0432\u0435\u043a\u0442\u043e\u0440 \u0434\u043b\u044f \u043d\u0435\u0437\u043d\u0430\u043a\u043e\u043c\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430 \u0438\u0437 N(emb.mean(), emb.std())\nn_embed = 2\nembed_dim = n_embed * 300 \nproj_dim = hidden_dim\n\n# GRU\nbidirectional = True\nn_layers = 1\nrnn_dim = hidden_dim\n\n# The second last Linear layer\ndense_dim = 2 * rnn_dim if bidirectional else rnn_dim\n\n# EMA\nmu = 0.9\n#mu = 0\nupdates_per_epoch = 10\n\n# Test set\nthreshold = 0.353\ntest_batch_size = 8 * batch_size\ntest_batch_size = batch_size\n\ndef seed_torch(seed=1):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\nseed_torch(seed)\ndevice = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n\n\ndef get_param_size(model, trainable=True):\n    if trainable:\n        psize = np.sum([np.prod(p.size()) for p in model.parameters() if p.requires_grad])\n    else:\n        psize = np.sum([np.prod(p.size()) for p in model.parameters()])\n    return psize","1bafebc5":"# https:\/\/discuss.pytorch.org\/t\/how-to-apply-exponential-moving-average-decay-for-variables\/10856\nclass EMA():\n    def __init__(self, model, mu, level='batch', n=1):\n        \"\"\"\n        level: 'batch' or 'epoch'\n          'batch': Update params every n batches.\n          'epoch': Update params every epoch.\n        \"\"\"\n        # self.ema_model = copy.deepcopy(model)\n        self.mu = mu\n        self.level = level\n        self.n = n\n        self.cnt = self.n\n        self.shadow = {}\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                self.shadow[name] = param.data\n\n    def _update(self, model):\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                new_average = (1 - self.mu) * param.data + self.mu * self.shadow[name]\n                self.shadow[name] = new_average.clone()\n\n    def set_weights(self, ema_model):\n        for name, param in ema_model.named_parameters():\n            if param.requires_grad:\n                param.data = self.shadow[name]\n\n    def on_batch_end(self, model):\n        if self.level is 'batch':\n            self.cnt -= 1\n            if self.cnt == 0:\n                self._update(model)\n                self.cnt = self.n\n\n    def on_epoch_end(self, model):\n        if self.level is 'epoch':\n            self._update(model)\n\n","7848f995":"class GlobalMaxPooling1D(nn.Module):\n    def __init__(self):\n        super(GlobalMaxPooling1D, self).__init__()\n\n    def forward(self, inputs):\n        z, _ = torch.max(inputs, 1)\n        return z\n\n    def __repr__(self):\n        return self.__class__.__name__ + '()'\n\n\nclass GRUModel(nn.Module):\n    def __init__(self, n_vocab, embed_dim, proj_dim, rnn_dim, n_layers, bidirectional, dense_dim,\n                 padding_idx=0, pretrained_embedding=None, fix_embedding=True,\n                 n_out=1):\n        super(GRUModel, self).__init__()\n        self.n_vocab = n_vocab\n        self.embed_dim = embed_dim\n        self.n_layers = n_layers\n        self.dense_dim = dense_dim\n        self.n_out = n_out\n        self.bidirectional = bidirectional\n        self.fix_embedding = fix_embedding\n        self.padding_idx = padding_idx\n        if pretrained_embedding is not None:\n            self.embed = nn.Embedding.from_pretrained(pretrained_embedding, freeze=fix_embedding)\n            self.embed.padding_idx = self.padding_idx\n        else:\n            self.embed = nn.Embedding(self.n_vocab, self.embed_dim, padding_idx=self.padding_idx)\n        self.proj = nn.Linear(embed_dim, proj_dim)\n        self.proj_act = nn.ReLU()\n        self.gru = nn.GRU(proj_dim, rnn_dim, self.n_layers,\n                          batch_first=True, bidirectional=bidirectional)\n        self.pooling = GlobalMaxPooling1D()\n        in_dim = 2 * rnn_dim if self.bidirectional else rnn_dim\n        self.dense = nn.Linear(in_dim, dense_dim)\n        self.dense_act = nn.ReLU()\n        self.out_linear = nn.Linear(dense_dim, n_out)\n        self.init_weights()\n\n    def init_weights(self):\n        for name, param in self.named_parameters():\n            if name.find('embed') > -1:\n                continue\n            elif name.find('weight') > -1 and len(param.size()) > 1:\n                nn.init.xavier_uniform_(param)\n\n    def forward(self, inputs):\n        # inputs: (bs, max_len)\n        x = self.embed(inputs)\n        x = self.proj_act(self.proj(x))\n        x, hidden = self.gru(x)\n        x = self.pooling(x)\n        x = self.dense_act(self.dense(x))\n        x = self.out_linear(x)\n        return x\n\n    def fit(self, dataloader, epochs, optimizer, callbacks=None):\n        for i in range(epochs):\n            run_epoch(model, dataloader, optimizer, callbacks=callbacks)\n\n    def predict(self, dataloader):\n        preds = []\n        with torch.no_grad():\n            for batch in dataloader:\n                batch = tuple(t.to(device) for t in batch)\n                X_batch = batch[0]\n                preds.append(self.forward(X_batch).data.cpu())\n        return torch.cat(preds)\n\n    def predict_proba(self, dataloader):\n        return torch.sigmoid(self.predict(dataloader)).data.numpy()","d4a130b8":"!pip install torchsummary","e5bfb5f3":"class TextClassificationDataset(Dataset):\n    \"\"\"\n    token_ids_s : List of token ids\n    labels   : Target labels\n    training: Sort token_ids_s by length if training is True.\n    \"\"\"\n    def __init__(self, token_ids_s, labels=None, max_len=1000, training=True, sort=True):\n        self.training = training\n\n        if labels is None:\n            self.labels = torch.ones(len(token_ids_s), dtype=torch.long)  # dummy\n        else:\n            self.labels = torch.LongTensor(labels)\n\n        seq_lens = []\n        self.inputs = []\n        for e, token_ids in enumerate(token_ids_s):\n            seq_lens.append(len(token_ids))\n            input_ids = torch.LongTensor(token_ids[:max_len])\n            self.inputs.append(input_ids)\n\n        if self.training and sort:\n            self.indices = np.argsort(seq_lens)\n            self.inputs = [self.inputs[i] for i in self.indices]\n            self.labels = self.labels[self.indices]\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n        return self.inputs[idx], self.labels[idx]\n\n    def set_labels(self, labels):\n        self.labels = torch.tensor(labels, dtype=torch.long)\n\n\n# Always drop last\nclass BatchIterator(object):\n\n    def __init__(self, dataset, collate_fn, batch_size,\n                 shuffle=True, drop_last=True):\n        self.dataset = dataset\n        self.collate_fn = collate_fn\n        self.batch_size = batch_size\n        self.size = len(dataset)\n        self.shuffle = shuffle\n        if drop_last:\n            self.num_batches = self.size \/\/ batch_size\n        else:\n            self.num_batches = (self.size + self.batch_size - 1) \/\/ self.batch_size\n\n    def __iter__(self):\n        if self.shuffle:\n            indices = np.random.choice(self.num_batches, self.num_batches, replace=False)\n        else:\n            indices = range(self.num_batches)\n        for idx in indices:\n            left = self.batch_size * idx\n            yield(self.collate_fn(self.dataset[left: left + self.batch_size]))\n\n    def __len__(self):\n        return self.num_batches\n\n\ndef collate_fn(batch):\n    xy_batch = [pad_sequence(batch[0], batch_first=True), batch[1]]\n    return xy_batch\n\n\ndef run_epoch(model, dataloader, optimizer, callbacks=None,\n              criterion=nn.BCEWithLogitsLoss(), verbose_step=10000):\n    t1 = time.time()\n    tr_loss = 0\n    for step, batch in enumerate(dataloader):\n        batch = tuple(t.to(device) for t in batch)\n        x_batch, y_batch = batch\n        optimizer.zero_grad()\n        outputs = model(x_batch)\n        loss = criterion(outputs[:, 0], y_batch.float())\n        loss.backward()\n        optimizer.step()\n        tr_loss += loss.item()\n        if callbacks is not None:\n            for func in callbacks:\n                func.on_batch_end(model)\n        if (step + 1) % verbose_step == 0:\n            loss_now = tr_loss \/ (step + 1)\n            print(f'step:{step+1} loss:{loss_now:.7f} time:{time.time() - t1:.1f}s')\n    if callbacks is not None:\n        for func in callbacks:\n            func.on_epoch_end(model)\n    experiment.log_metric(\"train loss\", tr_loss \/ (step + 1))\n    wandb.log({\"epoch train loss\": tr_loss \/ (step + 1)})\n    return tr_loss \/ (step + 1)","3703c70e":"def load_glove(word_index, max_features, unk_uni):\n    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n    EMBEDDING_FILE = '\/kaggle\/input\/glove840b300dtxt\/glove.840B.300d.txt'\n    embeddings_index = dict(get_coefs(*o.split(' ')) for o in open(EMBEDDING_FILE))\n\n    all_embs = np.stack(list(embeddings_index.values()))\n    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    unknown_words = []\n    nb_words = min(max_features, len(word_index))\n\n    if unk_uni:\n        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    else:\n        embedding_matrix = np.zeros((nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= nb_words:\n            continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is None:\n            embedding_vector = embeddings_index.get(word.lower())\n            if embedding_vector is None:\n                unknown_words.append((word, i))\n            else:\n                embedding_matrix[i] = embedding_vector\n        else:\n            embedding_matrix[i] = embedding_vector\n    print('\\nTotal unknowns glove', len(unknown_words))\n    print(unknown_words[-10:])\n\n    del embeddings_index\n    gc.collect()\n    return embedding_matrix, unknown_words\n\n\ndef load_wiki(word_index, max_features, unk_uni):\n    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n    EMBEDDING_FILE = '\/kaggle\/working\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec'\n    embeddings_index = dict(get_coefs(*o.split(' ')) for o in open(EMBEDDING_FILE) if len(o) > 100)\n\n    all_embs = np.stack(list(embeddings_index.values()))\n    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    unknown_words = []\n    nb_words = min(max_features, len(word_index))\n\n    if unk_uni:\n        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    else:\n        embedding_matrix = np.zeros((nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= nb_words:\n            continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is None:\n            embedding_vector = embeddings_index.get(word.lower())\n            if embedding_vector is None:\n                unknown_words.append((word, i))\n            else:\n                embedding_matrix[i] = embedding_vector\n        else:\n            embedding_matrix[i] = embedding_vector\n    print('\\nTotal unknowns wiki', len(unknown_words))\n    print(unknown_words[-10:])\n\n    del embeddings_index\n    gc.collect()\n    return embedding_matrix, unknown_words\n\n\ndef load_parag(word_index, max_features, unk_uni):\n    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n    EMBEDDING_FILE = '\/kaggle\/working\/embeddings\/paragram_300_sl999\/paragram_300_sl999.txt'\n    embeddings_index = dict(get_coefs(*o.split(' '))\n                            for o in open(EMBEDDING_FILE, encoding='utf8', errors='ignore')\n                            if len(o) > 100)\n\n    all_embs = np.stack(list(embeddings_index.values()))\n    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    unknown_words = []\n    nb_words = min(max_features, len(word_index))\n    if unk_uni:\n        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    else:\n        embedding_matrix = np.zeros((nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= nb_words:\n            continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is None:\n            embedding_vector = embeddings_index.get(word.lower())\n            if embedding_vector is None:\n                unknown_words.append((word, i))\n            else:\n                embedding_matrix[i] = embedding_vector\n        else:\n            embedding_matrix[i] = embedding_vector\n    print('\\nTotal unknowns parag', len(unknown_words))\n    print(unknown_words[-10:])\n\n    del embeddings_index\n    gc.collect()\n    return embedding_matrix, unknown_words\n\n\n# https:\/\/www.kaggle.com\/strideradu\/word2vec-and-gensim-go-go-go\ndef load_ggle(word_index, max_features, unk_uni):\n    EMBEDDING_FILE = data_path + 'embeddings\/GoogleNews-vectors-negative300\/GoogleNews-vectors-negative300.bin'\n    embeddings_index = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n    embed_size = embeddings_index.get_vector('known').size\n\n    unknown_words = []\n    nb_words = min(max_features, len(word_index))\n    if unk_uni:\n        embedding_matrix = (np.random.rand(nb_words, embed_size) - 0.5) \/ 5.0\n    else:\n        embedding_matrix = np.zeros((nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= nb_words:\n            continue\n        if word in embeddings_index:\n            embedding_vector = embeddings_index.get_vector(word)\n            embedding_matrix[i] = embedding_vector\n        else:\n            word_lower = word.lower()\n            if word_lower in embeddings_index:\n                embedding_matrix[i] = embeddings_index.get_vector(word_lower)\n            else:\n                unknown_words.append((word, i))\n\n    print('\\nTotal unknowns ggle', len(unknown_words))\n    print(unknown_words[-10:])\n\n    del embeddings_index\n    gc.collect()\n    return embedding_matrix, unknown_words\n\n\ndef load_all_embeddings(tokenizer, max_features, clean_num=False, unk_uni=True):\n    word_index = tokenizer.word_index\n    if clean_num == 2:\n        ggle_word_index = {}\n        for word, i in word_index.items():\n            ggle_word_index[clean_numbers(word)] = i\n    else:\n        ggle_word_index = word_index\n\n    embedding_matrix_1, u1 = load_glove(word_index, max_features, unk_uni)\n    embedding_matrix_2, u2 = load_wiki(word_index, max_features, unk_uni)\n    embedding_matrix_3, u3 = load_parag(word_index, max_features, unk_uni)\n    embedding_matrix_4, u4 = load_ggle(ggle_word_index, max_features, unk_uni)\n    embedding_matrix = np.concatenate((embedding_matrix_1,\n                                       embedding_matrix_2,\n                                       embedding_matrix_3,\n                                       embedding_matrix_4), axis=1)\n    del embedding_matrix_1, embedding_matrix_2, embedding_matrix_3, embedding_matrix_4\n    gc.collect()\n    return embedding_matrix\n\n\ndef setup_emb(tr_X, max_features=50000, clean_num=2, unk_uni=True):\n    tokenizer = Tokenizer(num_words=max_features, lower=True, filters='')\n    tokenizer.fit_on_texts(tr_X)\n    print(tokenizer)\n    print('len(vocab)', len(tokenizer.word_index))\n    embedding_matrix = load_all_embeddings(tokenizer, max_features=max_features,\n                                           clean_num=clean_num, unk_uni=unk_uni)\n    # np.save(embed_path, embedding_matrix)\n    return tokenizer, embedding_matrix\n","bd96d082":"puncts = ',.\":)(-!?|;\\'$&\/[]>%=#*+\\\\\u2022~@\u00a3\u00b7_{}\u00a9^\u00ae`<\u2192\u00b0\u20ac\u2122\u203a\u2665\u2190\u00d7\u00a7\u2033\u2032\u00c2\u2588\u00bd\u00e0\u2026\u201c\u2605\u201d\u2013\u25cf\u00e2\u25ba\u2212\u00a2\u00b2\u00ac\u2591\u00b6\u2191\u00b1\u00bf\u25be\u2550\u00a6\u2551\\\n\u2015\u00a5\u2593\u2014\u2039\u2500\u2592\uff1a\u00bc\u2295\u25bc\u25aa\u2020\u25a0\u2019\u2580\u00a8\u2584\u266b\u2606\u00e9\u00af\u2666\u00a4\u25b2\u00e8\u00b8\u00be\u00c3\u22c5\u2018\u221e\u2219\uff09\u2193\u3001\u2502\uff08\u00bb\uff0c\u266a\u2569\u255a\u00b3\u30fb\u2566\u2563\u2554\u2557\u25ac\u2764\u00ef\u00d8\u00b9\u2264\u2021\u221a'\n\n\ndef clean_text(x, puncts=puncts): #\u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442 \u043f\u0440\u043e\u0431\u0435\u043b\u044b \u0432\u043e\u043a\u0440\u0443\u0433 \u043f\u0443\u043d\u043a\u0442\u0443\u0430\u0446\u0438\u0438\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\n\ndef prepare_data(train_df, test_df, max_len, max_features, trunc='pre',\n                 lower=False, clean_num=2, unk_uni=True):\n    train_df = train_df.copy()\n    train_y = train_df['target'].values\n\n    # lower\n    if lower:\n        train_df['question_text'] = train_df['question_text'].apply(lambda x: x.lower())\n        test_df['question_text'] = test_df['question_text'].apply(lambda x: x.lower())\n\n    # Clean the text\n    train_df['question_text'] = train_df['question_text'].apply(\n        lambda x: clean_text(x))\n    test_df['question_text'] = test_df['question_text'].apply(\n        lambda x: clean_text(x))\n\n    # Clean numbers\n    if clean_num == 1:\n        train_df['question_text'] = train_df['question_text'].apply(\n            lambda x: clean_numbers(x))\n        test_df['question_text'] = test_df['question_text'].apply(\n            lambda x: clean_numbers(x))\n\n    # fill up the missing values\n    train_df['question_text'] = train_df['question_text'].fillna('_##_')\n    test_df['question_text'] = test_df['question_text'].fillna('_##_')\n\n    train_X = train_df['question_text'].values\n    test_X = test_df['question_text'].values\n\n    tokenizer = Tokenizer(num_words=max_features, lower=True, filters='')\n    tokenizer.fit_on_texts(train_X)\n    print(tokenizer)\n    print('len(vocab)', len(tokenizer.word_index))\n\n    train_X_ids = tokenizer.texts_to_sequences(train_X)\n    train_dataset = TextClassificationDataset(train_X_ids, train_y, sort=False)\n    train_loader = BatchIterator(train_dataset, collate_fn, batch_size)\n\n    test_X_ids = tokenizer.texts_to_sequences(test_X)\n    test_dataset = TextClassificationDataset(test_X_ids, training=False)\n    test_loader = BatchIterator(test_dataset, collate_fn, batch_size, shuffle=False, drop_last=False)\n    return train_loader, test_loader, tokenizer","ee99d8af":"ids_s = [list(range(300)), list(range(300, 600)),\n         list(range(600, 900)), list(range(900, 1200))]\n\ncols_s = [ids_s[0] + ids_s[1],\n          ids_s[0] + ids_s[2],\n          ids_s[1] + ids_s[2],\n          ids_s[1] + ids_s[3],\n          ids_s[2] + ids_s[3]]","ff0a211e":"train_df = pd.read_csv(data_path  + 'train.csv')","0af62a3a":"train_df, test_df = train_test_split(train_df.copy(), train_size=0.7, random_state=seed)\noneline = train_df[~train_df['question_text'].str.contains('\\n')]\nmultiline = train_df[train_df['question_text'].str.contains('\\n')]\noneline_size = len(oneline)\ntrain_idx, valid_idx = train_test_split(np.arange(oneline_size), train_size=0.7, random_state=seed)","a3d06511":"train_df.head()","d7c04423":"import pandas as pd\naugmented = pd.read_csv(\"..\/input\/augmented-fairseq\/augmented (1).csv\")","246b15cb":"augmented['question_text'] = augmented['question_text'].str[2:-2]","38ddb500":"insincere = augmented[augmented['target'] == 1]\ndisplay(insincere.head())","b9ecf564":"oneliners = int(len(augmented) \/ 5)","049c062b":"augmented.head()","5a6b869f":"validation_df = oneline.iloc[valid_idx]\nvalidation_df.head()","ad2b4c46":"train_aug = pd.DataFrame({\n    'question_text' : \n    list(oneline.iloc[train_idx]['question_text'].values) +\n    list(augmented.iloc[train_idx]['question_text'].values) +\n    list(multiline['question_text'].values),\n    'target' :\n    list(oneline.iloc[train_idx]['target'].values) +\n    list(augmented.iloc[train_idx]['target'].values) +\n    list(multiline['target'].values)\n})\n\nvalid_df = oneline.iloc[valid_idx]\n\ntrain_aug = train_aug.sample(frac=1)","1eaae65c":"'''\ntrain_aug = pd.DataFrame({'question_text' : list(train_df['question_text'].values),\n                               'target' : list(train_df['target'].values)})\n'''","6cbe0f5f":"del augmented\ndel train_df\n#del insincere\ngc.collect()","090521af":"#print(len(train_df.index) \/ len(train_aug.drop_duplicates().index))","a74eda6b":"#val_ans = val_df['target']\ntest_ans = test_df['target']\n\nprint('Train : ', train_aug.shape)\nprint('Validition : ', test_df.shape)","58c0540b":"data_path = '\/kaggle\/working\/'\nobj = prepare_data(train_aug, test_df[['question_text']], max_len, max_features,\n                   trunc=trunc, lower=lower, clean_num=clean_num, unk_uni=unk_uni)\n\ntrain_aug_loader, test_loader, tokenizer = obj\n","02ce1ff1":"word_index = tokenizer.word_index","90647635":"embedding_matrix_1, _ = load_glove(word_index, max_features, unk_uni)","744e4b0d":"\n!unzip \/kaggle\/input\/quora-insincere-questions-classification\/embeddings.zip 'wiki-news-300d-1M\/*' -d \/kaggle\/working\/embeddings\/\n","3d9e3a51":"embedding_matrix_2, _ = load_wiki(word_index, max_features, unk_uni)","3c8b4a5c":"!rm -r \/kaggle\/working\/embeddings","0c14b7c1":"!unzip \/kaggle\/input\/quora-insincere-questions-classification\/embeddings.zip 'paragram_300_sl999\/*' -d \/kaggle\/working\/embeddings\/\n","4b24b1f8":"embedding_matrix_3, _ = load_parag(word_index, max_features, unk_uni)","33313a33":"!rm -r \/kaggle\/working\/embeddings","65db29fd":"!unzip \/kaggle\/input\/quora-insincere-questions-classification\/embeddings.zip 'GoogleNews-vectors-negative300\/*' -d \/kaggle\/working\/embeddings\/\n","b8fbc1c3":"embedding_matrix_4, _ = load_ggle(word_index, max_features, unk_uni)","566616f6":"!rm -r \/kaggle\/working\/embeddings","f7e05c35":"embedding_matrix = np.concatenate((embedding_matrix_1,\n                                   embedding_matrix_2,\n                                   embedding_matrix_3,\n                                   embedding_matrix_4), axis=1)\ndel embedding_matrix_1, embedding_matrix_2, embedding_matrix_3, embedding_matrix_4\ngc.collect()","0958f778":"embedding_matrix = torch.Tensor(embedding_matrix)","393de214":"ema_n = int(train_aug.shape[0] \/ (updates_per_epoch * batch_size))\ntest_pr = np.zeros((len(test_df), 1))\ntest_probas = np.zeros((len(test_df), 1))\nfor i in range(n_models):\n    cols_in_use = cols_s[i % len(cols_s)]\n    model = GRUModel(n_vocab, embed_dim, proj_dim, rnn_dim, n_layers, bidirectional, dense_dim,\n                     pretrained_embedding=embedding_matrix[:, cols_in_use],\n                     fix_embedding=fix_embedding, padding_idx=0)\n    if i == 0:\n        print(model)\n        print('#Trainable params', get_param_size(model))\n    model.to(device)\n    #ema_model = copy.deepcopy(model)\n    #ema_model.eval()\n    optimizer = Adam(model.parameters())\n    #ema = EMA(model, mu, n=ema_n)\n\n    t2 = time.time()\n    wandb.watch(model)\n    model.train()\n    #model.fit(train_aug_loader, aug_epochs, optimizer, callbacks=[ema])\n    model.fit(train_aug_loader, aug_epochs, optimizer)\n    experiment.log_metric(\"aug model\", i + 1)\n    print(f'n_model:{i + 1} {(time.time() - t2) \/ (aug_epochs):.1f}s\/epoch')\n    #ema.set_weights(ema_model)\n    #ema_model.gru.flatten_parameters()\n    t3 = time.time()\n    #test_pr += ema_model.predict_proba(test_loader)\n    test_pr += model.predict_proba(test_loader)\n    test_probas += model.predict_proba(test_loader)\n    print(f'{time.time() - t3:.1f}s')\ntest_pr \/= n_models\ntest_probas \/= n_models\ntest_pr = (test_pr > threshold).astype(int)\nprint(f'Done:{time.time() - t0:.1f}s')","aa505840":"'''\nema_n = int(train_aug.shape[0] \/ (updates_per_epoch * batch_size))\ntest_pr = np.zeros((len(val_df), 1))\ntest_probas = np.zeros((len(val_df), 1))\nfor i in range(n_models):\n    cols_in_use = cols_s[i % len(cols_s)]\n    model = GRUModel(n_vocab, embed_dim, proj_dim, rnn_dim, n_layers, bidirectional, dense_dim,\n                     pretrained_embedding=embedding_matrix[:, cols_in_use],\n                     fix_embedding=fix_embedding, padding_idx=0)\n    if i == 0:\n        print(model)\n        print('#Trainable params', get_param_size(model))\n    model.to(device)\n    ema_model = copy.deepcopy(model)\n    ema_model.eval()\n    optimizer = Adam(model.parameters())\n    ema = EMA(model, mu, n=ema_n)\n\n    t2 = time.time()\n    wandb.watch(model)\n    model.train()\n    model.fit(train_aug_loader, aug_epochs, optimizer, callbacks=[ema])\n    experiment.log_metric(\"aug model\", i + 1)\n    print(f'n_model:{i + 1} {(time.time() - t2) \/ (aug_epochs):.1f}s\/epoch')\n    ema.set_weights(ema_model)\n    ema_model.gru.flatten_parameters()\n    t3 = time.time()\n    test_pr += ema_model.predict_proba(test_loader)\n    test_probas += ema_model.predict_proba(test_loader)\n    print(f'{time.time() - t3:.1f}s')\ntest_pr \/= n_models\ntest_probas \/= n_models\ntest_pr = (test_pr > threshold).astype(int)\nprint(f'Done:{time.time() - t0:.1f}s')\n'''","510fe73f":"from sklearn.metrics import roc_auc_score\nf1 = f1_score(test_ans, test_pr)\nroc_auc = roc_auc_score(test_ans, test_probas)\nprint('validation f1-score: {}'.format(f1))\nprint('validation roc-auc score: {}'.format(roc_auc))","9f101ec2":"## GRU Model","535a067f":"## Data preprocessing","5fe12b5d":"## Loading embeddings","b2e93877":"## EMA","6d1df3f1":"## Constants","f8fb3072":"## Data loading"}}