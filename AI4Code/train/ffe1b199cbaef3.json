{"cell_type":{"01e14cf3":"code","2875a33a":"code","6887cb13":"code","ee5edc96":"code","39e1626f":"code","c2fb852b":"code","3f29449c":"code","b74d311d":"code","c5f23387":"code","6246c368":"code","da3fa418":"code","209ecb8b":"code","1e8b3c4f":"code","84be0964":"code","700e1024":"code","5874d241":"code","61968ab2":"code","8cd20b92":"code","62e014d4":"code","6427f140":"code","a73ad952":"markdown","d9edc9fb":"markdown","7b881286":"markdown","b63555f8":"markdown","4750d629":"markdown","7636f1c1":"markdown","66a3133a":"markdown","661c8058":"markdown","e92444be":"markdown","e4449236":"markdown","785950e6":"markdown","971453e2":"markdown"},"source":{"01e14cf3":"import os\nimport re\nimport numpy as np\nimport pandas as pd\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, Embedding, Conv1D, MaxPooling1D, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer, RobertaTokenizerFast\n\nfrom tqdm.notebook import tqdm\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n\n","2875a33a":"def clean_sentence(sentence):\n    ''' \n    Author: louilghada@gmail.com | kaggle.com\/swannnn\n    Removes punctuation, digits, special characters, stopwords and words of 1 character\n    '''\n    clean_sent = ''''''\n    for word in sentence.split():\n        if len(word) > 1 and not re.match(r'.*[0-9]+', word):\n            clean_sent = \"{} {}\".format(clean_sent, word)\n    clean_sent = re.sub(r'[.,:;?!\/\\|@#$%^&-_(){}]', '', clean_sent)\n    clean_sent = text = re.sub(r'(http|www)\\S*', '', clean_sent)\n    return clean_sent.strip()\n\ndef clean_df(df, column):\n    '''\n    louilghada@gmail.com | kaggle.com\/swannnn\n    Cleans text in specified column of dataframe df\n    '''\n    df = df.apply(lambda x: x.astype(str).str.lower())\n    df[column] = df[column].apply(lambda x: clean_sentence(x))\n\ndef regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_masks=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])\n\ndef build_model(transformer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model\n\ndef build_CNN_model():\n    input_layer = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_words_ids\")\n    embedding = Embedding(VOCAB_SIZE[0], VOCAB_SIZE[1], input_length=MAX_LEN, name='embed')(input_layer)\n    conv_1 = Conv1D(256, (100), activation='relu')(embedding)\n    max_pool = MaxPooling1D()(conv_1)\n    conv_1 = Conv1D(128, (5), activation='relu')(max_pool)\n    max_pool = MaxPooling1D()(conv_1)\n    dense = Dense(128, activation='relu')(max_pool)\n    dense = Dense(128, activation='relu')(dense)\n    flatten = Flatten()(dense)\n    out = Dense(1, activation='sigmoid')(flatten)\n    \n    \n    model = Model(inputs = input_layer, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    return model","6887cb13":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","ee5edc96":"AUTO = tf.data.experimental.AUTOTUNE\n\n# Configuration\nEPOCHS = 1\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 192\nMODEL = 'jplu\/tf-xlm-roberta-large'","39e1626f":"# First load the real tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL)","c2fb852b":"train1 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv\")\ntrain2 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv\")\ntrain3 = pd.read_csv(\"\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-es-cleaned.csv\")\ntrain4 = pd.read_csv(\"\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-fr-cleaned.csv\")\ntrain5 = pd.read_csv(\"\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-it-cleaned.csv\")\ntrain6 = pd.read_csv(\"\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-pt-cleaned.csv\")\ntrain7 = pd.read_csv(\"\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-ru-cleaned.csv\")\ntrain8 = pd.read_csv(\"\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-tr-cleaned.csv\")\n\nvalid1 = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv')\nvalid2 = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-test-translated\/jigsaw_miltilingual_valid_translated.csv')\ntest1 = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv')\ntest2 = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-test-translated\/jigsaw_miltilingual_test_translated.csv')\nsub = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv')\n","3f29449c":"# Combine train1 with a subset of train2\ntrain = pd.concat([\n    train1[['comment_text', 'toxic']].query('toxic==1'),\n    train1[['comment_text', 'toxic']].query('toxic==0').sample(n=50000, random_state=0),\n    train2[['comment_text', 'toxic']].query('toxic==1'),\n    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=50000, random_state=0),\n    train3[['comment_text', 'toxic']].query('toxic==1'),\n    train3[['comment_text', 'toxic']].query('toxic==0').sample(n=50000, random_state=0),\n    train4[['comment_text', 'toxic']].query('toxic==1'),\n    train4[['comment_text', 'toxic']].query('toxic==0').sample(n=50000, random_state=0),\n    train5[['comment_text', 'toxic']].query('toxic==1'),\n    train5[['comment_text', 'toxic']].query('toxic==0').sample(n=50000, random_state=0),\n    train6[['comment_text', 'toxic']].query('toxic==1'),\n    train6[['comment_text', 'toxic']].query('toxic==0').sample(n=50000, random_state=0),\n    train7[['comment_text', 'toxic']].query('toxic==1'),\n    train7[['comment_text', 'toxic']].query('toxic==0').sample(n=50000, random_state=0),\n    train8[['comment_text', 'toxic']].query('toxic==1'),\n    train8[['comment_text', 'toxic']].query('toxic==0').sample(n=50000, random_state=0),\n    \n])\n\nvalid = pd.concat([valid1, valid2])","b74d311d":"clean_df(train, 'comment_text')\ntrain.toxic = train.toxic.round().astype(int)\ntrain = train.sample(frac = 1)\n\nclean_df(valid, 'comment_text')\n\nclean_df(test1, 'content')\nclean_df(test2, 'content')","c5f23387":"x_train = regular_encode(train.comment_text.values, tokenizer, maxlen=MAX_LEN)\nx_valid = regular_encode(valid.comment_text.values, tokenizer, maxlen=MAX_LEN)\n\nx_test1 = regular_encode(test1.content.values, tokenizer, maxlen=MAX_LEN)\nx_test2 = regular_encode(test2.content.values, tokenizer, maxlen=MAX_LEN)\n\ny_train = train.toxic.values\ny_valid = valid.toxic.values\n","6246c368":"x_train1 = regular_encode(train1.comment_text.values, tokenizer, maxlen=MAX_LEN)\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n","da3fa418":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest1_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test1)\n    .batch(BATCH_SIZE)\n)\n\ntest2_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test2)\n    .batch(BATCH_SIZE)\n)","209ecb8b":"VOCAB_SIZE = x_train.shape","1e8b3c4f":"with strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    roberta_model = build_model(transformer_layer, max_len=MAX_LEN)\nroberta_model.summary()","84be0964":"with strategy.scope():\n    cnn_model = build_CNN_model()\ncnn_model.summary()","700e1024":"n_steps = x_train.shape[0] \/\/ BATCH_SIZE\n\ntrain_history = roberta_model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n)","5874d241":"n_steps = x_valid.shape[0] \/\/ BATCH_SIZE\n\ntrain_history_2 = roberta_model.fit(\n    valid_dataset.repeat(),\n    steps_per_epoch=n_steps,\n    epochs=EPOCHS\n)","61968ab2":"n_steps = x_train.shape[0] \/\/ BATCH_SIZE\n\ntrain_history = cnn_model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS*5\n)","8cd20b92":"n_steps = x_valid.shape[0] \/\/ BATCH_SIZE\n\ntrain_history_2 = cnn_model.fit(\n    valid_dataset.repeat(),\n    steps_per_epoch=n_steps,\n    epochs=EPOCHS\n)","62e014d4":"multi_ling_sub = roberta_model.predict(test1_dataset, verbose=1)\neng_sub = roberta_model.predict(test2_dataset, verbose=1)\n\nsub['toxic'] = multi_ling_sub*0.5 + eng_sub*0.5","6427f140":"sub.to_csv('submission.csv', index=False)","a73ad952":"## Helper Functions","d9edc9fb":"## Train Models","7b881286":"## Load text data into memory","b63555f8":"## Load models into the TPU","4750d629":"## Build datasets objects","7636f1c1":"## TPU Configs","66a3133a":"## Imports","661c8058":"## Create fast tokenizer","e92444be":"First, we train the XLM-Roberta Model.","e4449236":"## Submission","785950e6":"## blending","971453e2":"Then we train the CNN model"}}