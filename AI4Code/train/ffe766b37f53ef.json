{"cell_type":{"5b06bc9c":"code","78f37b07":"code","8d47fc5f":"code","2a1caeed":"code","27c35f58":"code","b641c07b":"code","2081d84e":"code","101f84c0":"code","dd26519d":"code","d602ba84":"code","5e9e3418":"code","2ddcf9c9":"code","ff659ed7":"code","22db6132":"code","aef1e740":"code","7b9942ea":"code","e2fe6a25":"code","cdfaea11":"code","d551b9d1":"code","f95beb1a":"code","80f4f84b":"code","a4672dac":"code","07bdd352":"code","e1216466":"code","957d295a":"code","eaf3ae4e":"code","c36adaa8":"code","ff9f5484":"code","60ff954d":"code","0597c01b":"code","adc9dfda":"code","61539189":"code","d3e881a5":"code","c83387be":"code","5575cd46":"code","0c142094":"code","3af3c16c":"code","1a27e0f6":"code","79094f55":"code","9ae7951b":"code","0f3e5b46":"code","6dccd3f3":"code","efd3a2bb":"code","7ba702bf":"code","a643cec4":"code","ceada20f":"code","23c4407d":"code","ff1cf1bd":"code","a8c5c03d":"code","4bba39db":"code","be1c34b5":"code","23f4d5b4":"code","c63b9e7d":"code","ce9f1284":"code","138d7b3b":"code","c9c63556":"code","01796cc3":"code","588d7fe9":"code","ed4bbdcc":"code","a62645d6":"code","471a451f":"code","f6cb652b":"code","0f7cf5df":"code","b3614491":"code","82debd7c":"code","fd679412":"code","9aed1f20":"code","92bc0769":"code","b23ffe89":"code","41ab3ffb":"code","7eeafc6b":"code","f97e6de7":"code","0f14cd74":"code","4299d3cb":"code","c4388a51":"code","49f93726":"code","da0bf907":"code","b08ecd82":"code","83ae7178":"code","7e85a86d":"code","0858d5a6":"code","2ec3ea9f":"code","9b5d10cf":"code","3ca0b5b2":"code","c07edd86":"code","0e4c645d":"code","ee9692c5":"code","b0484757":"code","a4b7b43e":"code","86e291b1":"code","f1e4a4f3":"code","330d3a1e":"code","837f0033":"code","aadbdff0":"code","79e8cf8b":"code","86796f5b":"code","bae22d89":"code","b3485683":"code","eb4ec4ba":"code","22cf68c6":"code","582b15a7":"code","3b019556":"code","4cdf07d6":"code","2e2c250e":"code","48398dba":"code","41b07538":"code","7a8553cc":"code","5ffd2845":"markdown","ac5435c9":"markdown","2152f112":"markdown","94e478d1":"markdown","ca818242":"markdown","664614a2":"markdown"},"source":{"5b06bc9c":"import os\nimport torch\nimport torch.nn as nn\nimport pandas as pd\n\n\nfrom PIL import Image\nimport numpy as np\nimport cv2\nimport gc\nimport time\nfrom torch.utils.data import DataLoader\nfrom torch.autograd import Variable\nfrom torch.optim import Adam,Adagrad,SGD\nimport torch.nn.functional as F\n\nimport random\n\nfrom torch.utils.data import Dataset, DataLoader, Subset\n\nfrom torch.utils import data\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nimport tqdm\nfrom torch.nn import functional as fnn\nfrom torch.utils import data\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn import Module, Sequential, Conv2d, AvgPool2d, GRU, Linear\nfrom torch.nn.functional import ctc_loss, log_softmax\nfrom torchvision import models\n\n\nimport torchvision\nimport pickle\n\nimport json\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nfrom torchvision.transforms import *\n\nfrom itertools import chain\nfrom pandas.io.json import json_normalize\n\nimport torch.distributed as dist\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nimport matplotlib.patches as patches\n\n\nimport os\nimport tqdm\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nfrom string import digits, ascii_uppercase\n\n#import utils\nimport math \n","78f37b07":"# \u0441\u0438\u0434\n\nSEED = 1489\n\n\nrandom.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nnp.random.seed(SEED)","8d47fc5f":"# \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0434\u0435\u0432\u0430\u0439\u0441\u0430\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","2a1caeed":"TEST_PATH = \".\/data\/test\/\" \nTRAIN_PATH = \".\/data\/train\/\"\nSUBMISSION_PATH = \".\/data\/submission.csv\"\nTRAIN_INFO = \".\/data\/train.json\"\n\n\nIMAGE_WIDTH = 412\nIMAGE_HEIGHT = 412\n\n\nVAL_SIZE = 0.3\n\n\nN_ITER = 2\nBATCH_SIZE = 32\n\nBATCH_SIZE_VAL = 8\nLR = 3e-5\n\n\nCOOR_COUNT = 4\n\n\nEXP_NAME = \"resnet34\"","27c35f58":"file_object = open(TRAIN_INFO, \"r\")\n\ntrain = json_normalize(json.load(file_object))\n\ntest = pd.read_csv(SUBMISSION_PATH)","b641c07b":"train['box'] = train.nums.apply(lambda x: list(chain.from_iterable(x[0]['box'])))\ntrain['text'] = train.nums.apply(lambda x: x[0]['text'])\ntrain['file'] = train.file.apply(lambda x: x.split(\"\/\")[1])\n\ntrain.drop(['nums'], axis =1, inplace = True)\n\ntest['text'] = test.file_name.apply(lambda x: x.split(\"\/\")[1])\n\ntest.drop(['file_name'], axis =1, inplace = True)","2081d84e":"train = train.head(25631)","101f84c0":"train.head()","dd26519d":"train.shape","d602ba84":"train.shape","5e9e3418":"img_names = train.file.values\nshapes = [cv2.imread(os.path.join(TRAIN_PATH, name), 0).shape for name in img_names]\n","2ddcf9c9":"train['shapes'] = shapes","ff659ed7":"def replace(x):\n    for i in range(len(x['box'])):\n        if i % 2 == 0:\n            x['box'][i] = (float(x['box'][i])\/x['shapes'][1]) *  IMAGE_WIDTH\n        else:\n            x['box'][i] = (float(x['box'][i])\/x['shapes'][0]) * IMAGE_HEIGHT\n\n    return x\n\ntrain = train.apply(lambda x: replace(x), axis = 1)","22db6132":"train['xmin'] = train.apply(lambda x: min(x['box'][0], int(x['box'][6])), axis=1)\ntrain['xmax'] = train.apply(lambda x: max(x['box'][2], int(x['box'][4])), axis=1) \ntrain['ymin'] = train.apply(lambda x: min(x['box'][1], int(x['box'][3])), axis=1) \ntrain['ymax'] = train.apply(lambda x: max(x['box'][5], int(x['box'][7])), axis=1)","aef1e740":"train[((train.ymax <= train.ymin + 5) | (train.xmax <= train.xmin + 5))]\n\ntrain = train[~((train.ymax <= train.ymin + 5) | (train.xmax <= train.xmin + 5))]\ntrain = train[~((train.xmin < 0 )| (train.ymin < 0))]","7b9942ea":"train.head()","e2fe6a25":"test.head()","cdfaea11":"valid_images = np.random.choice(train.file.unique(), size=int(VAL_SIZE * train.file.nunique()), replace=False)","d551b9d1":"valid_set = train[train.file.isin(valid_images)]\n\ntrain_set = train[~train.file.isin(valid_images)]","f95beb1a":"print(valid_set.shape, train_set.shape)","80f4f84b":"test_ids = test.text","a4672dac":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ndef get_device():\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n","07bdd352":"class ShapeDataset(Dataset):\n\n    def __init__(self, IMAGE_DIR, IMAGE_WIDTH, IMAGE_HEIGHT, data,\n                 transform=transforms.Compose([ToTensor(), \n                                               Normalize(\n                                                   mean=[0.485, 0.456, 0.406],\n                                                   std=[0.229, 0.224, 0.225]\n                                               )])):\n        self.IMAGE_DIR = IMAGE_DIR\n        self.IMAGE_WIDTH = IMAGE_WIDTH\n        self.IMAGE_HEIGHT = IMAGE_HEIGHT\n        \n        \n        self.data = data\n        self.transform = transform\n\n\n\n    def num_classes(self):\n        return len(self.class2index)\n\n    \n    def __len__(self, ):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        \n              \n        def get_boxes(obj):\n            boxes = [[obj[f] for f in ['xmin', 'ymin', 'xmax', 'ymax'] ]]\n            return torch.as_tensor(boxes, dtype=torch.float)\n\n\n        def get_areas(obj):\n            areas = [(obj['xmax'] - obj['xmin']) * (obj['ymax'] - obj['ymin']) ]\n            return torch.as_tensor(areas, dtype=torch.int64)\n\n        img_name = self.data.iloc[idx]['file']\n        \n        path = os.path.join(self.IMAGE_DIR, img_name)\n\n        img = cv2.imread(path)\n        \n        shapes  = img.shape\n        \n\n        \n        img = cv2.resize(img, (IMAGE_WIDTH, IMAGE_HEIGHT)) \n        \n\n\n        \n        \n        img_bbox = self.data.iloc[idx]['box']#.copy()\n        \n        #print(img_bbox, shapes, img_name)\n\n        obj = {}        \n        \n        obj['xmin'] = np.min([int(img_bbox[0]), int(img_bbox[6])])\n        obj['xmax'] = np.max([int(img_bbox[2]), int(img_bbox[4])])\n        obj['ymin'] = np.min([int(img_bbox[1]), int(img_bbox[3])])\n        obj['ymax'] = np.max([int(img_bbox[5]), int(img_bbox[7])])\n\n        if self.transform:\n            image = self.transform(img)\n            \n        print(img_name, obj)\n        \n        target = {}\n        target['boxes'] = get_boxes(obj)\n\n        target['labels'] = torch.ones((1,), dtype=torch.int64)#torch.as_tensor(1, dtype=torch.int64)\n        target['image_id'] = torch.as_tensor([idx], dtype=torch.int64)\n        target['area'] = get_areas(obj)\n        target['iscrowd'] = torch.ones((1,), dtype=torch.int64)#get_iscrowds(annot)\n\n        return image, target\n\n\n\n\n    \nclass ShapeDatasetTest(Dataset):\n\n    def __init__(self, IMAGE_DIR, IMAGE_WIDTH, IMAGE_HEIGHT, data,\n                 transform=transforms.Compose([ToTensor(),\n                                               Normalize(\n                                                   mean=[0.485, 0.456, 0.406],\n                                                   std=[0.229, 0.224, 0.225]\n                                               ) ])):\n        self.IMAGE_DIR = IMAGE_DIR\n        self.IMAGE_WIDTH = IMAGE_WIDTH\n        self.IMAGE_HEIGHT = IMAGE_HEIGHT\n        \n        \n        self.data = data\n        self.transform = transform\n\n\n    def num_classes(self):\n        return len(self.class2index)\n\n    \n    def __len__(self, ):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        \n        \n        img_name = self.data.iloc[idx]['text']\n        \n        path = os.path.join(self.IMAGE_DIR, img_name)\n\n        img = cv2.imread(path)\n        \n        shapes  = img.shape\n\n        img = cv2.resize(img, (IMAGE_WIDTH, IMAGE_HEIGHT)) \n        \n        \n        if self.transform:\n            image = self.transform(img)\n        return image \n\n\n","e1216466":"train_data = ShapeDataset(TRAIN_PATH, IMAGE_WIDTH, IMAGE_HEIGHT, train_set)\nvalid_data = ShapeDataset(TRAIN_PATH, IMAGE_WIDTH, IMAGE_HEIGHT, valid_set)\n\n\n","957d295a":"test_data  = ShapeDatasetTest(TEST_PATH, IMAGE_WIDTH, IMAGE_HEIGHT, test)","eaf3ae4e":"dataloader_train = DataLoader(\n    train_data, batch_size=2, shuffle=True, num_workers=0, collate_fn=collate_fn)\ndataloader_valid = DataLoader(\n    valid_data, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)\n\n\n","c36adaa8":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\ndevice = get_device()\n\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\nnum_classes = 2\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\nmodel = model.to(device)","ff9f5484":"# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n\n# and a learning rate scheduler\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)","60ff954d":"optimizer = optim.Adam(model.parameters(), lr=LR, amsgrad=True)\nloss_fn = fnn.mse_loss\n\n\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=5,\n                                               gamma=0.1)","0597c01b":"train_losses = []\ntest_losses = []","adc9dfda":"# best_val_loss = 1\n# with open(f\"model_{best_val_loss}.pth\", \"rb\") as fp:\n#     best_state_dict = torch.load(fp, map_location=\"cpu\")\n#     model.load_state_dict(best_state_dict)","61539189":"\ndef train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n    model.train()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    header = 'Epoch: [{}]'.format(epoch)\n\n    lr_scheduler = None\n    if epoch == 0:\n        warmup_factor = 1. \/ 1000\n        warmup_iters = min(1000, len(data_loader) - 1)\n\n        lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n\n    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n\n        losses = sum(loss for loss in loss_dict.values())\n\n        # reduce losses over all GPUs for logging purposes\n        loss_dict_reduced = utils.reduce_dict(loss_dict)\n        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n\n        loss_value = losses_reduced.item()\n\n        if not math.isfinite(loss_value):\n            print(\"Loss is {}, stopping training\".format(loss_value))\n            print(loss_dict_reduced)\n            #sys.exit(1)\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        if lr_scheduler is not None:\n            lr_scheduler.step()\n\n        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n\n    return metric_logger\n\n\ndef _get_iou_types(model):\n    model_without_ddp = model\n    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n        model_without_ddp = model.module\n    iou_types = [\"bbox\"]\n    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n        iou_types.append(\"segm\")\n    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n        iou_types.append(\"keypoints\")\n    return iou_types\n\n\n@torch.no_grad()\ndef evaluate(model, data_loader, device):\n    n_threads = torch.get_num_threads()\n    # FIXME remove this and make paste_masks_in_image run on the GPU\n    torch.set_num_threads(1)\n    cpu_device = torch.device(\"cpu\")\n    model.eval()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    header = 'Test:'\n\n    coco = get_coco_api_from_dataset(data_loader.dataset)\n    iou_types = _get_iou_types(model)\n    coco_evaluator = CocoEvaluator(coco, iou_types)\n\n    for images, targets in metric_logger.log_every(data_loader, 100, header):\n        images = list(img.to(device) for img in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        torch.cuda.synchronize()\n        model_time = time.time()\n        with torch.no_grad():\n            outputs = model(images)\n\n            outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n        model_time = time.time() - model_time\n\n        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n        evaluator_time = time.time()\n        coco_evaluator.update(res)\n        evaluator_time = time.time() - evaluator_time\n        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n\n    # gather the stats from all processes\n    metric_logger.synchronize_between_processes()\n    print(\"Averaged stats:\", metric_logger)\n    coco_evaluator.synchronize_between_processes()\n\n    # accumulate predictions from all images\n    coco_evaluator.accumulate()\n    coco_evaluator.summarize()\n    torch.set_num_threads(n_threads)\n    return coco_evaluator","d3e881a5":"num_epochs = 2\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, dataloader_train, device, epoch, print_freq=1)\n    # update the learning rate\n    lr_scheduler.step()\n\n    with open(f\"model_{epoch}.pth\", \"wb\") as fp:\n        torch.save(model.state_dict(), fp)\n    #evaluate(model, dataloader_valid, device=device)\n\n\n\n\n","c83387be":"gc.collect()","5575cd46":"def get_rects(boxes):\n    rect = lambda x, y, w, h: patches.Rectangle((x, y), w - x, h - y, linewidth=1, edgecolor='r', facecolor='none')\n\n    return [rect(box[0], box[1], box[2], box[3]) for box in boxes]\n\ndef get_clazzes(labels, boxes, index2class):\n    return [{'x': box[0].item(), 'y': box[1].item() - 5.0, 's': index2class[label.item()], 'fontsize': 10}\n            for label, box in zip(labels, boxes)]\n\ndef show_prediction(img, index2class, fig, ax):\n    pil_image = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n\n    ax.imshow(pil_image)\n    ax.axes.xaxis.set_visible(False)\n    ax.axes.yaxis.set_visible(False)\n\n    for rect in get_rects(prediction[0]['boxes']):\n        ax.add_patch(rect)\n\n\n    for label in get_clazzes(prediction[0]['labels'], prediction[0]['boxes'], index2class):\n        ax.text(**label)\n        \n        \ndef get_prediction(dataset,  model):\n  #  img, _ = dataset[idx]\n\n    model.eval()\n    cpu_device = torch.device(\"cpu\")\n\n    \n    preds = []\n    for images in metric_logger.log_every(dataset, 100, header):\n\n        images = torch.stack([images[0][0].to(device), images[1][0].to(device), images[2][0].to(device) ], dim=0).unsqueeze(0)\n\n        torch.cuda.synchronize()\n        model_time = time.time()\n        with torch.no_grad():\n            outputs = model(images)\n\n            outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n        model_time = time.time() - model_time\n\n        preds.append(outputs)\n\n        evaluator_time = time.time()\n        evaluator_time = time.time() - evaluator_time\n        \n    return preds#img, prediction\n","0c142094":"import utils\nmetric_logger = utils.MetricLogger(delimiter=\"  \")\n\nheader = \"Test:\"\ndataloader_test = DataLoader(\n    test_data, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)\n\n\npredictions = get_prediction(dataloader_test, model) \n\n","3af3c16c":"len(predictions)\n","1a27e0f6":"ids =[]\nboxes = []","79094f55":"test_ids = test.text","9ae7951b":"for index, i in enumerate(predictions):\n    for j in i:\n        for k in j['boxes'].cpu().detach().numpy():\n            boxes.append(k)\n            ids.append(test_ids[index])","0f3e5b46":"print(len(boxes), len(ids))","6dccd3f3":"test.head()","efd3a2bb":"d = {'file': ids, 'text': boxes}\n\ntest = pd.DataFrame(data=d)","7ba702bf":"test.head()","a643cec4":"img_names = test.file.values\nshapes = [cv2.imread(os.path.join(TEST_PATH, name), 0).shape for name in img_names]\n","ceada20f":"test['shapes']  = shapes","23c4407d":"test.head()","ff1cf1bd":"def restore(x, col_name):\n    \n    box = x[col_name].copy()\n    #print(type(box[0]), IMAGE_WIDTH)\n    box[0] = int((box[0]\/IMAGE_WIDTH) * x['shapes'][1])\n    box[1] = int((box[1]\/IMAGE_HEIGHT) * x['shapes'][0])\n    box[2] = int((box[2]\/IMAGE_WIDTH) * x['shapes'][1])\n    box[3] = int((box[3]\/IMAGE_HEIGHT) * x['shapes'][0])\n    \n    return box\n\n\ntest['fixed_text'] = test.apply(lambda x: restore(x, 'text'), axis = 1)","a8c5c03d":"test['xmin'] = test.fixed_text.apply(lambda x: x[0])\ntest['ymin'] = test.fixed_text.apply(lambda x: x[1])\ntest['xmax'] = test.fixed_text.apply(lambda x: x[2])\ntest['ymax'] = test.fixed_text.apply(lambda x: x[3])","4bba39db":"test.to_csv(\"test_first.csv\", index= None)","be1c34b5":"test = pd.read_csv(\"test_first.csv\")","23f4d5b4":"test.head()","c63b9e7d":"file_object = open(TRAIN_INFO, \"r\")\n\ntrain = json_normalize(json.load(file_object))","ce9f1284":"train['box'] = train.nums.apply(lambda x: list(chain.from_iterable(x[0]['box'])))\ntrain['text'] = train.nums.apply(lambda x: x[0]['text'])\ntrain['file'] = train.file.apply(lambda x: x.split(\"\/\")[1])\n\ntrain.drop(['nums'], axis =1, inplace = True)\n","138d7b3b":"train","c9c63556":"train = train.head(25631)","01796cc3":"\n\nimg_names = train.file.values\nshapes = [cv2.imread(os.path.join(TRAIN_PATH, name), 0).shape for name in img_names]\n\ntrain['shapes'] = shapes","588d7fe9":"train['xmin'] = train.apply(lambda x: min(x['box'][0], int(x['box'][6])), axis=1)\ntrain['xmax'] = train.apply(lambda x: max(x['box'][2], int(x['box'][4])), axis=1) \ntrain['ymin'] = train.apply(lambda x: min(x['box'][1], int(x['box'][3])), axis=1) \ntrain['ymax'] = train.apply(lambda x: max(x['box'][5], int(x['box'][7])), axis=1)","ed4bbdcc":"train[((train.ymax <= train.ymin + 5) | (train.xmax <= train.xmin + 5))]\n\ntrain = train[~((train.ymax <= train.ymin + 5) | (train.xmax <= train.xmin + 5))]\ntrain = train[~((train.xmin < 0 )| (train.ymin < 0))]","a62645d6":"train.head()","471a451f":"#train['fixed_text'] = train.apply(lambda x: restore(x,  'box'), axis = 1)","f6cb652b":"train = train[~((train.xmin < 0 )| (train.ymin < 0))]","0f7cf5df":"abc = \"0123456789ABEKMHOPCTYX\" ","b3614491":"def compute_mask(text):\n    \"\"\"Compute letter-digit mask of text.\n    Accepts string of text. \n    Returns string of the same length but with every letter replaced by 'L' and every digit replaced by 'D'.\n    e.g. 'E506EC152' -> 'LDDDLLDDD'.\n    Returns None if non-letter and non-digit character met in text.\n    \"\"\"\n    # YOUR CODE HERE\n    mask = []\n    for char in text:\n        if char in digits:\n            mask.append(\"D\")\n        elif char in ascii_uppercase:\n            mask.append(\"L\")\n        else:\n            return None\n    return \"\".join(mask)\n","82debd7c":"def check_in_alphabet(text, alphabet=abc):\n    \"\"\"Check if all chars in text come from alphabet.\n    Accepts string of text and string of alphabet. \n    Returns True if all chars in text are from alphabet and False else.\n    \"\"\"\n    # YOUR CODE HERE\n    for char in text:\n        if char not in alphabet:\n            return False\n    return True\n","fd679412":"def filter_data(config):\n    \"\"\"Filter config keeping only items with correct text.\n    Accepts list of items.\n    Returns new list.\n    \"\"\"\n    config_filtered = []\n    for item in tqdm.tqdm(config):\n        text = item[\"text\"]\n        mask = compute_mask(text)\n        if check_in_alphabet(text) and (mask == \"LDDDLLDD\" or mask == \"LDDDLLDDD\"):\n            config_filtered.append({\"file\": item[\"file\"],\n                                    \"text\": item[\"text\"]})\n    return config_filtered","9aed1f20":"class RecognitionDataset(Dataset):\n    \"\"\"Class for training image-to-text mapping using CTC-Loss.\"\"\"\n\n    def __init__(self, df, alphabet=abc, transforms=None):\n        \"\"\"Constructor for class.\n        Accepts:\n        - config: list of items, each of which is a dict with keys \"file\" & \"text\".\n        - alphabet: string of chars required for predicting.\n        - transforms: transformation for items, should accept and return dict with keys \"image\", \"seq\", \"seq_len\" & \"text\".\n        \"\"\"\n        super(RecognitionDataset, self).__init__()\n        self.df = df\n        self.alphabet = abc\n        self.image_names, self.texts = self._parse_root_()\n        self.transforms = transforms\n\n    def _parse_root_(self):\n        image_names, texts = [], []\n        #for item in self.train:\n        for index, item in self.df.iterrows():\n            #print(config)\n            \n            #print(item)\n            image_name = item[\"file\"]\n            text = item['text']\n            texts.append(text)\n            image_names.append(image_name)\n        return image_names, texts\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        \"\"\"Return dict with keys \"image\", \"seq\", \"seq_len\" & \"text\".\n        Image is a numpy array, float32, [0, 1].\n        Seq is list of integers.\n        Seq_len is an integer.\n        Text is a string.\n        \"\"\"\n\n        row = self.df[self.df.file == self.image_names[item]]\n\n        image = cv2.imread(os.path.join(TRAIN_PATH, self.image_names[item]))#[row['ymin']:row['ymax'], row['xmin']: row['xmax']].astype(np.float32) \/ 255.\n        #print(row['shapes'], int(row['ymin'].values[0]),int(row['ymax'].values[0]), int(row['xmin'].values[0]), int(row['xmax'].values[0]))\n        image = image[int(row['ymin'].values[0]):int(row['ymax'].values[0]), int(row['xmin'].values[0]): int(row['xmax'].values[0]) ]\n        iamge = image.astype(np.float32) \/ 255.\n\n        text = self.texts[item]\n        seq = self.text_to_seq(text)\n        seq_len = len(seq)\n        output = dict(image=image, seq=seq, seq_len=seq_len, text=text)\n        if self.transforms is not None:\n            output = self.transforms(output)\n        return output\n\n    def text_to_seq(self, text):\n        \"\"\"Encode text to sequence of integers.\n        Accepts string of text.\n        Returns list of integers where each number is index of corresponding characted in alphabet + 1.\n        \"\"\"\n        # YOUR CODE HERE\n        seq = [self.alphabet.find(c) + 1 for c in text]\n        return seq","92bc0769":"class RecognitionDatasetTest(Dataset):\n    \"\"\"Class for training image-to-text mapping using CTC-Loss.\"\"\"\n\n    def __init__(self, test, alphabet=abc, transforms=None):\n        \"\"\"Constructor for class.\n        Accepts:\n        - config: list of items, each of which is a dict with keys \"file\" & \"text\".\n        - alphabet: string of chars required for predicting.\n        - transforms: transformation for items, should accept and return dict with keys \"image\", \"seq\", \"seq_len\" & \"text\".\n        \"\"\"\n        super(RecognitionDatasetTest, self).__init__()\n        self.test = test\n        self.alphabet = abc\n        self.image_names, self.texts = self._parse_root_()\n        self.transforms = transforms\n\n    def _parse_root_(self):\n        image_names, texts = [], []\n        #for item in self.train:\n        for index, item in test.iterrows():\n\n            image_name = item[\"file\"]\n            text = item['text']\n            texts.append(text)\n            image_names.append(image_name)\n        return image_names, texts\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        \"\"\"Return dict with keys \"image\", \"seq\", \"seq_len\" & \"text\".\n        Image is a numpy array, float32, [0, 1].\n        Seq is list of integers.\n        Seq_len is an integer.\n        Text is a string.\n        \"\"\"\n        \n\n        row = test.loc[item]\n        \n\n        image = cv2.imread(os.path.join(TEST_PATH, self.image_names[item]))#[row['ymin']:row['ymax'], row['xmin']: row['xmax']].astype(np.float32) \/ 255.\n        image = image[int(row['ymin']):int(row['ymax']), int(row['xmin']): int(row['xmax']) ]\n        iamge = image.astype(np.float32) \/ 255.\n\n        text = self.texts[item]\n        seq = self.text_to_seq(text)\n        seq_len = len(seq)\n        output = dict(image=image, seq=seq, seq_len=seq_len, text=text)\n        \n        if self.transforms is not None:\n            output = self.transforms(output)\n        return output\n\n    def text_to_seq(self, text):\n        \"\"\"Encode text to sequence of integers.\n        Accepts string of text.\n        Returns list of integers where each number is index of corresponding characted in alphabet + 1.\n        \"\"\"\n        # YOUR CODE HERE\n        seq = [self.alphabet.find(c) + 1 for c in text]\n        return seq","b23ffe89":"class Resize(object):\n\n    def __init__(self, size=(320, 64)):\n        self.size = size\n\n    def __call__(self, item):\n        \"\"\"Accepts item with keys \"image\", \"seq\", \"seq_len\", \"text\".\n        Returns item with image resized to self.size.\n        \"\"\"\n        # YOUR CODE HERE\n        item['image'] = cv2.resize(item['image'], self.size, interpolation=cv2.INTER_AREA)\n        return item","41ab3ffb":"transforms = Resize(size=(320, 64))\ndataset = RecognitionDataset(train, alphabet=abc, transforms=transforms)","7eeafc6b":"def collate_fn(batch):\n    \"\"\"Function for torch.utils.data.Dataloader for batch collecting.\n    Accepts list of dataset __get_item__ return values (dicts).\n    Returns dict with same keys but values are either torch.Tensors of batched images, sequences, and so.\n    \"\"\"\n    images, seqs, seq_lens, texts = [], [], [], []\n    for sample in batch:\n        images.append(torch.from_numpy(sample[\"image\"]).permute(2, 0, 1).float())\n        seqs.extend(sample[\"seq\"])\n        seq_lens.append(sample[\"seq_len\"])\n        texts.append(sample[\"text\"])\n    images = torch.stack(images)\n    seqs = torch.Tensor(seqs).int()\n    seq_lens = torch.Tensor(seq_lens).int()\n    batch = {\"image\": images, \"seq\": seqs, \"seq_len\": seq_lens, \"text\": texts}\n    return batch\n\ndef collate_fn(batch):\n    \"\"\"Function for torch.utils.data.Dataloader for batch collecting.\n    Accepts list of dataset __get_item__ return values (dicts).\n    Returns dict with same keys but values are either torch.Tensors of batched images, sequences, and so.\n    \"\"\"\n    images, seqs, seq_lens, texts = [], [], [], []\n    for sample in batch:\n        images.append(torch.from_numpy(sample[\"image\"]).permute(2, 0, 1).float())\n        seqs.extend(sample[\"seq\"])\n        seq_lens.append(sample[\"seq_len\"])\n        texts.append(sample[\"text\"])\n    images = torch.stack(images)\n    seqs = torch.Tensor(seqs).int()\n    seq_lens = torch.Tensor(seq_lens).int()\n    batch = {\"image\": images, \"seq\": seqs, \"seq_len\": seq_lens, \"text\": texts}\n    return batch\n","f97e6de7":"class FeatureExtractor(Module):\n    \n    def __init__(self, input_size=(64, 320), output_len=20):\n        super(FeatureExtractor, self).__init__()\n        \n        h, w = input_size\n        resnet = getattr(models, 'resnet18')(pretrained=True)\n        self.cnn = Sequential(*list(resnet.children())[:-2])\n        \n        self.pool = AvgPool2d(kernel_size=(h \/\/ 32, 1))        \n        self.proj = Conv2d(w \/\/ 32, output_len, kernel_size=1)\n  \n        self.num_output_features = self.cnn[-1][-1].bn2.num_features    \n    \n    def apply_projection(self, x):\n        \"\"\"Use convolution to increase width of a features.\n        Accepts tensor of features (shaped B x C x H x W).\n        Returns new tensor of features (shaped B x C x H x W').\n        \"\"\"\n        # YOUR CODE HERE\n        x = x.permute(0, 3, 2, 1).contiguous()\n        x = self.proj(x)\n        x = x.permute(0, 2, 3, 1).contiguous()\n        return x\n   \n    def forward(self, x):\n        # Apply conv layers\n        features = self.cnn(x)\n        \n        # Pool to make height == 1\n        features = self.pool(features)\n        \n        # Apply projection to increase width\n        features = self.apply_projection(features)\n        \n        return features","0f14cd74":"feature_extractor = FeatureExtractor()","4299d3cb":"x = torch.randn(1, 3, 64, 320)\ny = feature_extractor(x)","c4388a51":"class SequencePredictor(Module):\n    \n    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3, bidirectional=False):\n        super(SequencePredictor, self).__init__()\n        \n        self.num_classes = num_classes        \n        self.rnn = GRU(input_size=input_size,\n                       hidden_size=hidden_size,\n                       num_layers=num_layers,\n                       dropout=dropout,\n                       bidirectional=bidirectional)\n        \n        fc_in = hidden_size if not bidirectional else 2 * hidden_size\n        self.fc = Linear(in_features=fc_in,\n                         out_features=num_classes)\n    \n    def _init_hidden_(self, batch_size):\n        \"\"\"Initialize new tensor of zeroes for RNN hidden state.\n        Accepts batch size.\n        Returns tensor of zeros shaped (num_layers * num_directions, batch, hidden_size).\n        \"\"\"\n        # YOUR CODE HERE\n        num_directions = 2 if self.rnn.bidirectional else 1\n        return torch.zeros(self.rnn.num_layers * num_directions, batch_size, self.rnn.hidden_size)\n        \n    def _prepare_features_(self, x):\n        \"\"\"Change dimensions of x to fit RNN expected input.\n        Accepts tensor x shaped (B x (C=1) x H x W).\n        Returns new tensor shaped (W x B x H).\n        \"\"\"\n        # YOUR CODE HERE\n        x = x.squeeze(1)\n        x = x.permute(2, 0, 1)\n        return x\n    \n    def forward(self, x):\n        x = self._prepare_features_(x)\n        \n        batch_size = x.size(1)\n        h_0 = self._init_hidden_(batch_size)\n        h_0 = h_0.to(x.device)\n        x, h = self.rnn(x, h_0)\n        \n        x = self.fc(x)\n        return x","49f93726":"sequence_predictor = SequencePredictor(input_size=512, \n                                       hidden_size=128, \n                                       num_layers=2, \n                                       num_classes=len(abc) + 1)","da0bf907":"x = torch.randn(1, 1, 512, 20)","b08ecd82":"y = sequence_predictor(x)","83ae7178":"class CRNN(Module):\n    \n    def __init__(self, alphabet=abc,\n                 cnn_input_size=(64, 320), cnn_output_len=20,\n                 rnn_hidden_size=128, rnn_num_layers=2, rnn_dropout=0.3, rnn_bidirectional=False):\n        super(CRNN, self).__init__()\n        self.alphabet = alphabet\n        self.features_extractor = FeatureExtractor(input_size=cnn_input_size, output_len=cnn_output_len)\n        self.sequence_predictor = SequencePredictor(input_size=self.features_extractor.num_output_features,\n                                                    hidden_size=rnn_hidden_size, num_layers=rnn_num_layers,\n                                                    num_classes=len(alphabet)+1, dropout=rnn_dropout,\n                                                    bidirectional=rnn_bidirectional)\n    \n    def forward(self, x):\n        features = self.features_extractor(x)\n        sequence = self.sequence_predictor(features)\n        return sequence","7e85a86d":"def pred_to_string(pred, abc):\n    seq = []\n    for i in range(len(pred)):\n        label = np.argmax(pred[i])\n        seq.append(label - 1)\n    out = []\n    for i in range(len(seq)):\n        if len(out) == 0:\n            if seq[i] != -1:\n                out.append(seq[i])\n        else:\n            if seq[i] != -1 and seq[i] != seq[i - 1]:\n                out.append(seq[i])\n    out = ''.join([abc[c] for c in out])\n    return out\n\ndef decode(pred, abc):\n    pred = pred.permute(1, 0, 2).cpu().data.numpy()\n    outputs = []\n    for i in range(len(pred)):\n        outputs.append(pred_to_string(pred[i], abc))\n    return outputs","0858d5a6":"crnn = CRNN()","2ec3ea9f":"x = torch.randn(1, 3, 64, 320)\ny = crnn(x)","9b5d10cf":"decode(y, abc)","3ca0b5b2":"ACTUALLY_TRAIN = True","c07edd86":"crnn = CRNN()","0e4c645d":"num_epochs =  8\nbatch_size = 128\nnum_workers = 0","ee9692c5":"device = torch.device(\"cuda: 0\") if torch.cuda.is_available() else torch.device(\"cpu\")\ncrnn.to(device);","b0484757":"optimizer = torch.optim.Adam(crnn.parameters(), lr=3e-4, amsgrad=True, weight_decay=1e-4)","a4b7b43e":"\ntrain_size = int(len(train) * 0.8)\nconfig_train = train[:train_size]\nconfig_val = train[train_size:]\n\ntransforms = Resize(size=(320, 64))\n\n","86e291b1":"config_val.head()","f1e4a4f3":"train_dataset = RecognitionDataset(config_train, alphabet=abc, transforms=Resize())\nval_dataset = RecognitionDataset(config_val, alphabet=abc, transforms=Resize())","330d3a1e":"train_dataloader = DataLoader(train_dataset, \n                              batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, \n                              drop_last=True, collate_fn=collate_fn)\nval_dataloader = DataLoader(val_dataset, \n                            batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, \n                            drop_last=False, collate_fn=collate_fn)","837f0033":"crnn.train()\nif ACTUALLY_TRAIN:\n    for i, epoch in enumerate(range(num_epochs)):\n        epoch_losses = []\n\n        for j, b in enumerate(tqdm.tqdm(train_dataloader, total=len(train_dataloader))):\n            images = b[\"image\"].to(device)\n            seqs_gt = b[\"seq\"]\n            seq_lens_gt = b[\"seq_len\"]\n\n            seqs_pred = crnn(images).cpu()\n            log_probs = log_softmax(seqs_pred, dim=2)\n            seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n\n            loss = ctc_loss(log_probs=log_probs,  # (T, N, C)\n                            targets=seqs_gt,  # N, S or sum(target_lengths)\n                            input_lengths=seq_lens_pred,  # N\n                            target_lengths=seq_lens_gt)  # N\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            epoch_losses.append(loss.item())\n\n        print(i, np.mean(epoch_losses))\n\n        val_losses = []\n        for i, b in enumerate(tqdm.tqdm(val_dataloader, total=len(val_dataloader))):\n            images = b[\"image\"].to(device)\n            seqs_gt = b[\"seq\"]\n            seq_lens_gt = b[\"seq_len\"]\n\n            with torch.no_grad():\n                seqs_pred = crnn(images).cpu()\n            log_probs = log_softmax(seqs_pred, dim=2)\n            seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n            loss = ctc_loss(log_probs=log_probs,  # (T, N, C)\n                            targets=seqs_gt,  # N, S or sum(target_lengths)\n                            input_lengths=seq_lens_pred,  # N\n                            target_lengths=seq_lens_gt)  # N\n\n            val_losses.append(loss.item())\n\n        print(np.mean(val_losses))\nelse:\n    image_train_log = cv2.imread(\".\/resources\/train_log.png\")\n    plt.figure(figsize=(15, 20))\n    plt.imshow(image_train_log[:, :, ::-1], interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()","aadbdff0":"crnn.eval()\nif ACTUALLY_TRAIN:\n    val_losses = []\n    for i, b in enumerate(tqdm.tqdm(val_dataloader, total=len(val_dataloader))):\n        images = b[\"image\"].to(device)\n        seqs_gt = b[\"seq\"]\n        seq_lens_gt = b[\"seq_len\"]\n\n        with torch.no_grad():\n            seqs_pred = crnn(images).cpu()\n        log_probs = log_softmax(seqs_pred, dim=2)\n        seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n        loss = ctc_loss(log_probs=log_probs,  # (T, N, C)\n                        targets=seqs_gt,  # N, S or sum(target_lengths)\n                        input_lengths=seq_lens_pred,  # N\n                        target_lengths=seq_lens_gt)  # N\n\n        val_losses.append(loss.item())\n\n    print(np.mean(val_losses))\nelse:\n    image_val_log = cv2.imread(\".\/resources\/val_log.png\")\n    plt.figure(figsize=(15, 20))\n    plt.imshow(image_val_log[:, :, ::-1], interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()","79e8cf8b":"# test['xmin'] = test.fixed_text.apply(lambda x: x[0])\n# test['ymin'] = test.fixed_text.apply(lambda x: x[1])\n# test['xmax'] = test.fixed_text.apply(lambda x: x[2])\n# test['ymax'] = test.fixed_text.apply(lambda x: x[3])","86796f5b":"test['text'] = \"A232BC41\"","bae22d89":"test = test[~((test.ymin == test.ymax) | (test.xmin == test.xmax) )].reset_index()","b3485683":"test.head()","eb4ec4ba":"dataset_test = RecognitionDatasetTest(test, alphabet=abc, transforms=transforms)","22cf68c6":"test_dataloader = DataLoader(dataset_test , \n                              batch_size=1, shuffle=False, num_workers=num_workers, pin_memory=True, \n                              drop_last=False, collate_fn=collate_fn)","582b15a7":"numbers = {}\n\nfor i, b in enumerate(tqdm.tqdm(test_dataloader, total=len(test_dataloader))):\n    #print(b)\n    images = b[\"image\"].to(device)\n    seqs_gt = b[\"seq\"]\n    seq_lens_gt = b[\"seq_len\"]\n\n    with torch.no_grad():\n        seqs_pred = crnn(images).cpu()\n    log_probs = log_softmax(seqs_pred, dim=2)\n    seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n    img_name = test[test.index == i]['file'].values[0]\n    if (img_name not in numbers.keys()):\n\n        #numbers[img_name] =  str(decode(log_probs, crnn.alphabet)[0]) +\" \" +  numbers[img_name]\n        numbers[img_name] = str(decode(log_probs, crnn.alphabet)[0])\n#     else:\n#         numbers[img_name] = str(decode(log_probs, crnn.alphabet)[0])\n    print(img_name, test[test.index == i]['file'].values[0], decode(log_probs, crnn.alphabet))\n\n","3b019556":"test.head()","4cdf07d6":"ids_final = []\nnum_final = []\n\nfor i in numbers.keys():\n    \n    ids_final.append(\"test\/\" + i)\n    num_final.append(numbers[i])","2e2c250e":"d = {'file_name': ids_final, 'plates_string':num_final}\n\n\noutput = pd.DataFrame(data=d)\n","48398dba":"sub = pd.read_csv(\"submission.csv\")","41b07538":"final = pd.merge(sub.drop(['plates_string'], axis =1 ) , output, how = 'left', left_on = \"file_name\", right_on = \"file_name\" )","7a8553cc":"final.to_csv(\"output.csv\", index = None)","5ffd2845":"# Import libraries","ac5435c9":"## \u041d\u0435\u0439\u0440\u043e\u043d\u043d\u0430\u044f \u0441\u0435\u0442\u044c","2152f112":"## Settings","94e478d1":"## NN prepapation","ca818242":"## Data","664614a2":"\n## Part II"}}