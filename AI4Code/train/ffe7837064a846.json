{"cell_type":{"e4c10f4b":"code","49ede398":"code","abdf3dc4":"code","6aaca551":"code","6da679de":"code","292f7d89":"code","56e8fc0a":"code","97bbb4fb":"code","c6c25baa":"code","fee8e065":"code","d6e5852b":"code","d4c9ddf7":"code","6158a65b":"code","9b5d7d0b":"code","12b9c647":"code","0e965fb2":"code","aa33f7f6":"code","93ed189e":"code","c971e414":"code","6dbea776":"code","2201c390":"code","e8a90c38":"code","9e329bdb":"code","1ef43d9b":"code","263b49a9":"code","84533578":"code","35a06839":"code","83ce86fe":"code","d1257bd2":"code","c562ba0a":"code","87deb3eb":"code","4192304f":"code","8fb6747b":"code","7adc43cd":"code","0a0004fa":"code","8a8a1439":"code","793a27a2":"code","8de38450":"code","7e8e77f3":"code","c80b18b3":"code","6060a23f":"code","27046a94":"code","a680dee6":"code","af48516f":"code","b5b604cd":"code","65783d56":"code","331c93c0":"code","d167f7cf":"code","d42f5d5a":"code","39c319e3":"code","cfe3f2ce":"code","254071e6":"code","921e57c6":"code","d398adcf":"code","35ee3ab6":"code","96562c4e":"code","0e6b2ec3":"code","75d12a34":"code","1a6dfae9":"code","81645e3f":"code","6e13a7cb":"code","d5d03aea":"markdown","2c199b7d":"markdown","956cbf46":"markdown","e3eef198":"markdown","81b501f3":"markdown","4a55e90b":"markdown","fffe2b2b":"markdown","a6da730e":"markdown","b65a7208":"markdown","6c7492d8":"markdown","9c503344":"markdown","34018251":"markdown","594a80c6":"markdown","7cd7d978":"markdown","10e6ea82":"markdown","f1e77bbb":"markdown","a8d6051e":"markdown","1c93a29b":"markdown","61d5dcdf":"markdown","9166dcf3":"markdown","e0f62049":"markdown","fd8f5517":"markdown","e2c85bd8":"markdown","07122bda":"markdown","02df856e":"markdown","78341b31":"markdown","1c8cf997":"markdown","6cf91c6c":"markdown","b9e23001":"markdown","a120b502":"markdown","39c2f561":"markdown","e0d6c52c":"markdown","bbb416dd":"markdown","7fdb7784":"markdown","2e4e788c":"markdown","76f1d250":"markdown","54a4b12a":"markdown","3ea58d58":"markdown","08e80dab":"markdown","c43e5739":"markdown","481e0194":"markdown"},"source":{"e4c10f4b":"#Problem Statement","49ede398":"# Steps for solving the problem","abdf3dc4":"#Loading Packages\nimport pandas as pd \nimport numpy as np                     # For mathematical calculations \nimport seaborn as sns                  # For data visualization \nimport matplotlib.pyplot as plt        # For plotting graphs \n%matplotlib inline \nimport warnings                        # To ignore any warnings warnings.filterwarnings(\"ignore\")","6aaca551":"train_original = pd.read_csv(\"..\/input\/loan-prediction-train-data\/train_ctrUa4K.csv\")\ntrain = train_original\ntrain.head()","6da679de":"# Looking into the number of rows and columns of data set train which syas training data set have 614\n#rows and 13 columns\ntrain.shape","292f7d89":"#Reading the name of columns in data set \ntrain.columns","56e8fc0a":"test = pd.read_csv(\"..\/input\/loan-prediction-test-data\/test_lAUu6dG.csv\")\ntest.head()","97bbb4fb":"test.shape  #","c6c25baa":"test.columns","fee8e065":"# print data types for each variable in train data set \ntrain.dtypes","d6e5852b":"# looking for  the target variable distribution in the train data set\ntrain['Loan_Status'].value_counts()","d4c9ddf7":"# Normalize can be set to True to print proportions instead of number \ntrain['Loan_Status'].value_counts(normalize=True)","6158a65b":"# Add title and axis names\nplt.title('Loan Status Bar Plot')\nplt.xlabel('Loan Status Y - Yes or N- No')\nplt.ylabel('Loan Status Count')\n\ntrain['Loan_Status'].value_counts().plot.bar(color=['green', 'red'],edgecolor='blue')","9b5d7d0b":"plt.figure(1)\nplt.subplot(221)\ntrain['Gender'].value_counts(normalize=True).plot.bar(title='Gender')\nplt.subplot(222)\ntrain['Married'].value_counts(normalize=True).plot.bar(title='Married')\nplt.subplot(223)\ntrain['Self_Employed'].value_counts(normalize=True).plot.bar(figsize=(20,10),title='Self Employed')\nplt.subplot(224)\ntrain['Credit_History'].value_counts(normalize=True).plot.bar(title='Credit History')","12b9c647":"plt.figure(2)\nplt.subplot(1,3,1)\ntrain['Dependents'].value_counts(normalize=True).plot(figsize=(30,10),kind='bar',title= 'Dependents')\nplt.subplot(1,3,2)\ntrain['Education'].value_counts(normalize=True).plot(kind='bar',title= 'Education')\nplt.subplot(1,3,3)\ntrain['Property_Area'].value_counts(normalize=True).plot(kind='bar',title= 'Property Area')\n","0e965fb2":"plt.figure(1)\nplt.subplot(131)\nsns.distplot(train['ApplicantIncome'],label=\"Applicant Income analysis\")\nplt.subplot(133)\ntrain['ApplicantIncome'].plot(kind='box',figsize=(16,5),label=\"Applicant Income analysis\")","aa33f7f6":"train.boxplot(column='ApplicantIncome',by='Education')\nplt.suptitle(\"\")","93ed189e":"plt.figure(1)\nplt.subplot(121)\nsns.distplot(train['CoapplicantIncome'])\nplt.subplot(122)\ntrain['CoapplicantIncome'].plot(kind='box', figsize=(16,5))","c971e414":"plt.figure()\nplt.subplot(121)\nsns.distplot(train['LoanAmount'])\nplt.subplot(122)\ntrain['LoanAmount'].plot(kind='box',figsize=(16,5))","6dbea776":"Gender = pd.crosstab(train['Gender'],train['Loan_Status'])\nGender.div(Gender.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\n\nMarried = pd.crosstab(train['Married'],train['Loan_Status'])\nMarried.div(Married.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True)\n\nDependents = pd.crosstab(train['Dependents'],train['Loan_Status'])\nDependents.div(Dependents.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True)\n\nEducation = pd.crosstab(train['Education'],train['Loan_Status'])\nEducation.div(Education.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True)\n\nSelf_Employed = pd.crosstab(train['Self_Employed'],train['Loan_Status'])\nSelf_Employed.div(Self_Employed.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True)\n\nCredit_History = pd.crosstab(train['Credit_History'],train['Loan_Status'])\nCredit_History.div(Credit_History.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True)\n\nProperty_Area = pd.crosstab(train['Property_Area'],train['Loan_Status'])\nProperty_Area.div(Property_Area.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True)\n","2201c390":"train.groupby('Loan_Status')['ApplicantIncome'].mean().plot(kind='bar',title=\"Loan Status w.r.t Applicant Income\")","e8a90c38":"#Declared four bins for for diff group\nbins=[0,2500,4000,6000,81000] \ngroup=['Low','Average','High', 'Very high']\ntrain['Income_bin']=pd.cut(train['ApplicantIncome'],bins,labels=group)\ntrain.head()","9e329bdb":"income_bin = pd.crosstab(train['Income_bin'],train['Loan_Status'])\nincome_bin.div(income_bin.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True,grid=True)\nplt.xlabel('Income group')\nplt.ylabel('Percentage')\n\n","1ef43d9b":"#train.groupby('Loan_Status')['CoapplicantIncome'].mean().plot(kind='bar')\nbins=[0,1000,3000,42000] \ngroup=['Low','Average','High'] \ntrain['Coapplicant_Income_bin']=pd.cut(train['CoapplicantIncome'],bins,labels=group)\nCoapplicant_Income_bin=pd.crosstab(train['Coapplicant_Income_bin'],train['Loan_Status'])\nCoapplicant_Income_bin.div(Coapplicant_Income_bin.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True)\nplt.xlabel('Income group')\nplt.ylabel('Percentage')","263b49a9":"train['total_income'] = train['ApplicantIncome'] + train['CoapplicantIncome']\nbins = [0,2500,4000,6000,8100]\ngroup = ['Low','Average','High', 'Very high']\ntrain['total_income_bin'] = pd.cut(train['total_income'],bins,labels=group)\n\n# crosstab plot between total_income_bin vs Loasn_Status\ntotal_income_bin = pd.crosstab(train['total_income_bin'],train['Loan_Status'])\n\ntotal_income_bin.div(total_income_bin.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True)","84533578":"train.groupby('Loan_Status')['LoanAmount'].mean().plot(kind='bar')","35a06839":"bins=[0,100,200,700] \ngroup=['Low','Average','High'] \ntrain['LoanAmount_bin']=pd.cut(train['LoanAmount'],bins,labels=group)\nLoanAmount_bin=pd.crosstab(train['LoanAmount_bin'],train['Loan_Status']) \nLoanAmount_bin.div(LoanAmount_bin.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True) \nplt.xlabel('LoanAmount')\nplt.ylabel('Percentage')","83ce86fe":"train.columns","d1257bd2":"train = train.drop(['Income_bin', 'Coapplicant_Income_bin', 'total_income_bin', 'LoanAmount_bin', 'total_income'], axis=1)","c562ba0a":"train['Dependents'].replace('3+',3,inplace=True)\ntest['Dependents'].replace('3+',3,inplace=True)\ntrain['Loan_Status'].replace('Y',1,inplace=True)\ntrain['Loan_Status'].replace('N',0,inplace=True)\n","87deb3eb":"matrix = train.corr() \nf, ax = plt.subplots(figsize=(9, 6))\nsns.heatmap(matrix, vmax=.8, square=True, cmap=\"BuPu\");","4192304f":"train.isnull().sum()","8fb6747b":"train['Gender'].fillna(train['Gender'].mode()[0],inplace=True)\ntrain['Married'].fillna(train['Married'].mode()[0],inplace=True)\ntrain['Dependents'].fillna(train['Dependents'].mode()[0],inplace=True)\ntrain['Self_Employed'].fillna(train['Self_Employed'].mode()[0],inplace=True)\ntrain['Credit_History'].fillna(train['Credit_History'].mode()[0],inplace=True)\n\n#----------- For Test Data\ntest['Gender'].fillna(train['Gender'].mode()[0], inplace=True)\ntest['Dependents'].fillna(train['Dependents'].mode()[0], inplace=True)\ntest['Self_Employed'].fillna(train['Self_Employed'].mode()[0], inplace=True)\ntest['Credit_History'].fillna(train['Credit_History'].mode()[0], inplace=True)\ntest['Loan_Amount_Term'].fillna(train['Loan_Amount_Term'].mode()[0], inplace=True)\ntest['LoanAmount'].fillna(train['LoanAmount'].median(), inplace=True)","7adc43cd":"train['Loan_Amount_Term'].value_counts()","0a0004fa":"train['Loan_Amount_Term'].fillna(train['Loan_Amount_Term'].mode()[0], inplace=True)","8a8a1439":"train['LoanAmount'].fillna(train['LoanAmount'].median(),inplace=True) \n# Replaced null value by Median as mean won't be best representative as outliers are there","793a27a2":"train.isnull().sum()","8de38450":"train['LoanAmount_log'] = np.log(train['LoanAmount'])\ntrain['LoanAmount_log'].hist(bins=100) \n#Below graph is approx normal distributed. For the test data set also we need to take log of LoanAmount field \n#to make the distribution normal\ntest['LoanAmount_log'] = np.log(test['LoanAmount'])","7e8e77f3":"train.head()","c80b18b3":"train = train.drop('Loan_ID',axis=1)\ntest = test.drop('Loan_ID',axis=1)","6060a23f":"x = train.drop('Loan_Status',axis=1)\ny = train['Loan_Status']","27046a94":"y.head()","a680dee6":"x = pd.get_dummies(x)\ntrain = pd.get_dummies(train)\ntest = pd.get_dummies(test)","af48516f":"#We will use the train_test_split function from sklearn to divide our train dataset. So, first let us import train_test_split.\n# x_cv and y_cv is validation data set component\nfrom sklearn.model_selection import train_test_split\nx_train, x_cv, y_train, y_cv = train_test_split(x,y, test_size =0.3)","b5b604cd":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(x_train,y_train)","65783d56":"#Let\u2019s predict the Loan_Status for validation set and calculate its accuracy.\npred_cv = model.predict(x_cv)\n\naccuracy_score(y_cv,pred_cv)","331c93c0":"pred_test = model.predict(test)\n","d167f7cf":"#Lets import result from test file.\n\nsubmission=pd.read_csv(\"..\/input\/submission\/sample_submission_49d68Cx.csv\")","d42f5d5a":"test_original = pd.read_csv(\"..\/input\/loan-prediction-test-data\/test_lAUu6dG.csv\")\nsubmission['Loan_Status']=pred_test \nsubmission['Loan_ID']=test_original['Loan_ID']","39c319e3":"submission['Loan_Status'].replace(0,'N',inplace=True)\nsubmission['Loan_Status'].replace(1,'Y',inplace=True)","cfe3f2ce":"#converting submission fiel to csv file\npd.DataFrame(submission, columns=['Loan_ID','Loan_Status']).to_csv('logistic.csv')","254071e6":"#Let\u2019s import StratifiedKFold from sklearn and fit the model.\nfrom sklearn.model_selection import StratifiedKFold\ni = 1\naccuracy = []\n\nskf = StratifiedKFold(n_splits=5, random_state=1, shuffle = True)\n#skf.get_n_splits(x,y)\n\nfor train_index,test_index in skf.split(x,y):\n    #print(\"train: \",train_index,\" Validation : \",test_index)\n    print('\\n{} of kfold {}'.format(i,skf.n_splits))\n    x1_train,x1_val = x.iloc[train_index],x.iloc[test_index]\n    y1_train,y1_val = y.iloc[train_index],y.iloc[test_index]\n    model = LogisticRegression(max_iter=200 ,random_state=1)\n    model.fit(x1_train,y1_train)\n    prediction_test = model.predict(x1_val)\n    score = accuracy_score(y1_val,prediction_test)\n    print('accuracy_score',score)\n    i+=1 \n    pred_test = model.predict(test) \n    #print(\"pred_test\",pred_test)\n    pred=model.predict_proba(x1_val)[:,1]\n    #print(\"pred\",pred)\n    \n#print(score)\n","921e57c6":"from sklearn import metrics\nfpr, tpr, _ = metrics.roc_curve(y1_val, pred)\nauc = metrics.roc_auc_score(y1_val,pred)\nplt.figure(figsize=(12,8)) \nplt.plot(fpr,tpr,label=\"validation, auc=\"+str(auc)) \nplt.xlabel('False Positive Rate') \nplt.ylabel('True Positive Rate') \nplt.legend(loc=4) \nplt.show()","d398adcf":"#We got an auc value of 0.721\n\nsubmission['Loan_Status']=pred_test \nsubmission['Loan_ID']=test_original['Loan_ID']\n\n#Remember we need predictions in Y and N. So let\u2019s convert 1 and 0 to Y and N.\n\nsubmission['Loan_Status'].replace(0, 'N',inplace=True) \nsubmission['Loan_Status'].replace(1, 'Y',inplace=True)\n#Lets convert the submission to .csv format and make submission to check the accuracy on the leaderboard.\n\npd.DataFrame(submission, columns=['Loan_ID','Loan_Status']).to_csv('Logistickfold.csv',index=False)","35ee3ab6":"submission.head","96562c4e":"train['Total_Income']=train['ApplicantIncome']+train['CoapplicantIncome'] \ntest['Total_Income']=test['ApplicantIncome']+test['CoapplicantIncome']","0e6b2ec3":"#Let\u2019s check the distribution of Total Income.\n\nsns.distplot(train['Total_Income']);\n","75d12a34":"train['Total_Income_log'] = np.log(train['Total_Income']) \nsns.distplot(train['Total_Income_log']); \ntest['Total_Income_log'] = np.log(test['Total_Income'])\n","1a6dfae9":"train['EMI']=train['LoanAmount']\/train['Loan_Amount_Term'] \ntest['EMI']=test['LoanAmount']\/test['Loan_Amount_Term']\n\n#Let\u2019s check the distribution of EMI variable.\n\nsns.distplot(train['EMI']);","81645e3f":"# us create Balance Income feature now and check its distribution.\n\ntrain['Balance Income']=train['Total_Income']-(train['EMI']*1000) # Multiply with 1000 to make the units equal test['Balance Income']=test['Total_Income']-(test['EMI']*1000)\nsns.distplot(train['Balance Income']);\n","6e13a7cb":"train_final=train.drop(['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term'], axis=1) \ntest_final=test.drop(['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term'], axis=1)","d5d03aea":"It can be inferred that Applicant income does not affect the chances of loan approval which contradicts our hypothesis in which we assumed that if the applicant income is high the chances of loan approval will also be high. We aill do the same way to loo onto coapplicant Income.","2c199b7d":"**Let us visualize the roc curve.**","956cbf46":"------------------------------------------------------------------------------------------------------\n# Let's visualize other features  of the data set by graph and get the analysis \n-----------------------------------------------------------------------------------------------------","e3eef198":"- --\nWe found simolar distribution for ApplicantIncome and CoApplicantIncome. Moreover distribution is skewed in nature . From box plot we can say in both the feature number of outliers are much . Let's look the distribution of loan amount\n- --","81b501f3":"-----------------\nNow we will make dummy variables for the categorical variables. Dummy variable turns categorical variables into a series of 0 and 1, making them lot easier to quantify and compare. Let us understand the process of dummies first:\n\nOnce we apply dummies to this variable, it will convert the \u201cGender\u201d variable into two variables(Gender_Male and Gender_Female), one for each class, i.e. Male and Female. Gender_Male will have a value of 0 if the gender is Female and a value of 1 if the gender is Male.\n- --------\n","4a55e90b":"# Data Dictionary\nTrain file: CSV containing the customers for whom loan eligibility is known as 'Loan_Status'\n\n**Variable\tDescription**\n* Loan_ID\tUnique Loan ID\n* Gender\tMale\/ Female\n* Married\tApplicant married (Y\/N)\n* Dependents\tNumber of dependents\n* Education\tApplicant Education (Graduate\/ Under Graduate)\n* Self_Employed\tSelf employed (Y\/N)\n* ApplicantIncome\tApplicant income\n* CoapplicantIncome\tCoapplicant income\n* LoanAmount\tLoan amount in thousands\n* Loan_Amount_Term\tTerm of loan in months\n* Credit_History\tcredit history meets guidelines\n* Property_Area\tUrban\/ Semi Urban\/ Rural\n* Loan_Status\t(Target) Loan approved (Y\/N)\n\n**Test file:** CSV containing the customer information for whom loan eligibility is to be predicted","fffe2b2b":"We have missing values for Gender,Married, Dependents, Self_Employed etc as per above data . We can consider these methods to fill the missing values:\n\n1. For numerical variables: imputation using mean or median\n2. For categorical variables: imputation using mode\n","a6da730e":"# **Step 4 Bivariate Analysis**\n\nIn Bivariate analysis we will analyse the independent variable with respect to target variable.","b65a7208":"## Step 1 : Reading and understanding Train and Test Data set ","6c7492d8":"Using this we found our result were classifying only 78% in right category .\n\nBased on the domain knowledge, we can come up with new features that might affect the target variable. We will create the following three new features:\n\n* Total Income - As discussed during bivariate analysis we will combine the Applicant Income and Coapplicant Income. If the total income is high, chances of loan approval might also be high.\n\n* EMI - EMI is the monthly amount to be paid by the applicant to repay the loan. Idea behind making this variable is that people who have high EMI\u2019s might find it difficult to pay back the loan. We can calculate the EMI by taking the ratio of loan amount with respect to loan amount term.\n\n* Balance Income - This is the income left after the EMI has been paid. Idea behind creating this variable is that if this value is high, the chances are high that a person will repay the loan and hence increasing the chances of loan approval.","9c503344":"- --\nWe can see that there are a higher number of graduates with very high incomes, which are appearing to be the outliers.\n\nLet\u2019s look at the Coapplicant income distribution.\n- --","34018251":"**Analysis done from above stacked histogram -**\n1. Gender : Proportion for both male and female applicant for approved \/ unapproved loan are approx same.\n2. Married : Proportion of Married applicant is more for approved loan\n3. dependents : Distribution of applicants with 1 or 3+ dependents is similar across both the categories of Loan_Status.\n4. Self_Employed : There is nothing significant we can infer from Self_Employed vs Loan_Status plot\n5. Credit History : people with credit history as 1 are more likely to get their loans approved.\n6. Property_Area : Loan approval in semiurban area is more likely than Rural and Urban area\n","594a80c6":"----\nWe see that the most correlated variables are (ApplicantIncome - LoanAmount) and (Credit_History - Loan_Status). LoanAmount is also correlated with CoapplicantIncome.\n- --","7cd7d978":"**Categorical Independent Variable vs Target Variable**","10e6ea82":"The shape of train data set is - (614,13) while that of test data set is (367,12). We need to predict the target variable 'Loan_Status' from the test data set.\n","f1e77bbb":"# Missing Value and Outlier Treatment","a8d6051e":"- --\nIt can be seen that the proportion of approved loans is higher for Low and Average Loan Amount as compared to that of High Loan Amount which supports our hypothesis in which we considered that the chances of loan approval will be high when the loan amount is less.\n\nLet\u2019s drop the bins which we created for the exploration part. We will change the 3+ in dependents variable to 3 to make it a numerical variable.We will also convert the target variable\u2019s categories into 0 and 1 so that we can find its correlation with numerical variables. One more reason to do so is few models like logistic regression takes only numeric values as input. We will replace N with 0 and Y with 1.\n- --","1c93a29b":"----------\nWe can see it is shifted towards left, i.e., the distribution is right skewed. So, let\u2019s take the log transformation to make the distribution normal.\n\n- -----","61d5dcdf":"As there are no clear inference from the loan_status with respect to Applicant Income as we don't see any change in mean income for both loan_status category. We need to make different bins for different income level. Let's create four bins as below and proceed further -\n\n- -----\nUse cut when you need to segment and sort data values into bins. This function is also useful for going from a continuous variable to a categorical variable. For example, cut could convert ages to groups of age ranges. Supports binning into an equal number of bins, or a pre-specified array of bins.\n- ----","9166dcf3":"So our predictions are almost 80% accurate, i.e. we have identified 80% of the loan status correctly.\n\nLet\u2019s make predictions for the test dataset.","e0f62049":"- --\nWe see the loanamount distribution is fairly ok  but at the same time box plot says presence of outliers which we need to solve in later stage. Now we will look how well each features corelate with loan_status\n- --","fd8f5517":"**Following inferences can be made from the above bar plots:**\n* \u2022\tMost of the applicants don\u2019t have any dependents.\n* \u2022\tAround 80% of the applicants are Graduate.\n* \u2022\tMost of the applicants are from Semiurban area.\n\n","e2c85bd8":"**It can be inferred from the above bar plots that:**\n* \u2022\t80% applicants in the dataset are male.\n* \u2022\tAround 65% of the applicants in the dataset are married.\n* \u2022\tAround 15% applicants in the dataset are self employed.\n* \u2022\tAround 85% applicants have repaid their debts.\n\n**Now let\u2019s visualize the ordinal variables.**\n\n","07122bda":"# Independent Variable (Numerical)\n\nTill now we have seen the categorical and ordinal variables and now lets visualize the numerical variables. Lets look at the distribution of Applicant income first.","02df856e":"------------------\nIt can be inferred that most of the data in the distribution of applicant income is towards left which means it is not normally distributed. We will try to make it normal in later sections as algorithms works better if the data is normally distributed.\n\nThe boxplot confirms the presence of a lot of outliers\/extreme values. This can be attributed to the income disparity in the society. Part of this can be driven by the fact that we are looking at people with different education levels. Let us segregate them by Education\n- -----","78341b31":"**In** above step we had found the more outlier values for variable LoanAmount. it was right skewed . I will try to make the distribtion normal by taking logarithmic values of LoanAmount field ","1c8cf997":"Dream Housing Finance company deals in all kinds of home loans. They have presence across all urban, semi urban and rural areas. Customer first applies for home loan and after that company validates the customer eligibility for loan.\n\nCompany wants to automate the loan eligibility process (real time) based on customer detail provided while filling online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. To automate this process, they have provided a dataset to identify the customers segments that are eligible for loan amount so that they can specifically target these customers. ","6cf91c6c":"**Numerical Independent Variable vs Target Variable**","b9e23001":"# **Logistic Regression using stratified k-folds cross validation**","a120b502":"---------------\nWe will use scikit-learn (sklearn) for making different models which is an open source library for Python. It is one of the most efficient tool which contains many inbuilt functions that can be used for modeling in Python.\n\nSklearn requires the target variable in a separate dataset. So, we will drop our target variable from the train dataset and save it in another dataset.\n- --------------","39c2f561":"# Step3: Univariate Analysis\n\nIn this section, we will do univariate analysis. It is the simplest form of analyzing data where we examine each variable individually. For categorical features we can use frequency table or bar plots which will calculate the number of each category in a particular variable. For numerical features, probability density plots can be used to look at the distribution of the variable","e0d6c52c":"## Hypothesis Generation\n\nThis is a very important stage in any data science\/machine learning pipeline. It involves understanding the problem in detail by brainstorming as many factors as possible which can impact the outcome. It is done by understanding the problem statement thoroughly and before looking at the data.\nBelow are some of the factors which I think can affect the Loan Approval (dependent variable for this loan prediction problem):\n* \u2022\tSalary: Applicants with high income should have more chances of loan approval.\n* \u2022\tPrevious history: Applicants who have repayed their previous debts should have higher chances of loan approval.\n* \u2022\tLoan amount: Loan approval should also depend on the loan amount. If the loan amount is less, chances of loan approval should be high.\n* \u2022\tLoan term: Loan for less time period and less amount should have higher chances of approval.\n* \u2022\tEMI: Lesser the amount to be paid monthly to repay the loan, higher the chances of loan approval\n\nThese are the factors (not limited to ) which can be helpful in deciding whether loan should be approved or not ?","bbb416dd":"# ** Related Information**\nIt is a classification problem where we have to predict whether a loan would be approved or not. In a classification problem, we have to predict discrete values based on a given set of independent variable(s). Classification can be of two types:\n\n**Binary Classification** : In this classification we have to predict either of the two given classes. For example: classifying the gender as male or female, predicting the result as win or loss, etc.\n\n**Multiclass Classification** : Here we have to classify the data into three or more classes. For example: classifying a movie's genre as comedy, action or romantic, classify fruits as oranges, apples, or pears, etc.\nLoan prediction is a very common real-life problem that each retail bank faces atleast once in its lifetime. If done correctly, it can save a lot of man hours at the end of a retail bank.\n\nLoan prediction is a very common real-life problem that each retail bank faces atleast once in its lifetime. If done correctly, it can save a lot of man hours at the end of a retail bank. This problem comes under **Binary Classification Problem**","7fdb7784":"# **The new model for this will be coming soon** ","2e4e788c":"- -----\nIt shows that if coapplicant\u2019s income is less the chances of loan approval are high. But this does not look right. The possible reason behind this may be that most of the applicants don\u2019t have any coapplicant so the coapplicant income for such applicants is 0 and hence the loan approval is not dependent on it. So we can make a new variable in which we will combine the applicant\u2019s and coapplicant\u2019s income to visualize the combined effect of income on loan approval.\n- ---","76f1d250":"We will go throgh various steps to solve this problem. These steps are as below - \n\n1.\tProblem Statement\n2.\tHypothesis Generation\n3.\tGetting the system ready and loading the data\n4.\tUnderstanding the data\n5.\tExploratory Data Analysis (EDA)\n\n        o\tUnivariate Analysis\n        o\tBivariate Analysis\n        \n6.\tMissing value and outlier treatment\n7.\tEvaluation Metrics for classification problems\n8.\tModel Building : Part I\n9.\tLogistic Regression using stratified k-folds cross validation\n10.\tFeature Engineering\n11.\tModel Building : Part II\n\n        o\tLogistic Regression\n        o\tDecision tree\n        o\tRandom Forest\n        o\tXGBoost\n\n","54a4b12a":"Now we will draw correltion between all the numeric varables . We will use the heat map to visualoize teh corelation. \n\n** Variable with daarker color represents more correlation ","3ea58d58":"\n# ****We can see there are three format of data types:****\n* \u2022\tobject: Object format means variables are categorical. Categorical variables in our dataset are: Loan_ID, Gender, Married, Dependents, Education, Self_Employed, Property_Area, Loan_Status\n* \u2022\tint64: It represents the integer variables. ApplicantIncome is of this format.\n* \u2022\tfloat64: It represents the variable which have some decimal values involved. They are also numerical variables. Numerical variables in our dataset are: CoapplicantIncome, LoanAmount, Loan_Amount_Term, and Credit_History\n\n","08e80dab":"## Study of categorical features such as Gender, Married, Self_Employed and Credit_History","c43e5739":"- ---\nWe can see that Proportion of loans getting approved for applicants having low Total_Income is very less as compared to that of applicants with Average, High and Very High Income.\n\nWe will look into LoanAmount variable w.r.t Loan_Status now\n- --","481e0194":"# **Model Building Part I**\n\nLet us make our first model to predict the target variable. We will start with Logistic Regression which is used for predicting binary outcome.\n- ----\n**FYI:**\n* Logistic Regression is a classification algorithm. It is used to predict a binary outcome (1 \/ 0, Yes \/ No, True \/ False) given a set of independent variables.\n* Logistic regression is an estimation of Logit function. Logit function is simply a log of odds in favor of the event.\n* This function creates a s-shaped curve with the probability estimate, which is very similar to the required step wise function\n- ---"}}