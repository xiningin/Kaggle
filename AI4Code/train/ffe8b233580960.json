{"cell_type":{"cc31d63e":"code","be94155f":"code","dee2480a":"code","67d18395":"code","cad2be7a":"code","e10bcdc6":"code","0ed75842":"code","390f25fd":"code","6f904247":"code","134ca3b5":"code","ddfe97f3":"code","6e4629ba":"code","af7dcf9f":"code","de70b7db":"code","5f0e7c2b":"code","8cbe2b1e":"code","dff92865":"code","25827d92":"code","66cf6e7a":"code","06826c40":"code","899ec4d1":"code","42143505":"code","88de0880":"code","c9dc5af7":"code","f3e9527c":"code","40ff0d46":"code","6d82542a":"code","eae37df0":"code","9e488343":"code","5847293d":"code","16423ae2":"code","421ae3fe":"markdown","8b288c8b":"markdown","2b375431":"markdown"},"source":{"cc31d63e":"# Basic Data Preprocessing\nimport numpy as np\nimport pandas as pd \n\n# Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\n# Data Preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler,MinMaxScaler\nfrom scipy.stats import uniform, randint\n\n# Feature Selection\nfrom sklearn.feature_selection import SelectKBest, chi2, SelectFromModel\nfrom sklearn.decomposition import PCA\n\n#Modelling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Metrics\nfrom sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,accuracy_score\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score,GridSearchCV, KFold, RandomizedSearchCV,RepeatedStratifiedKFold\nfrom skopt import BayesSearchCV\n","be94155f":"# Reading Test and Train data \n# Dropping ID column\ntrain_df= pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv').iloc[:,1:]\ntest_df= pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')","dee2480a":"# Create new DF without ID column from scoring dataset\ntest_df_X = test_df.iloc[:,1:]\n# Create DF of ID column to be used later while submission\ntest_df_id = test_df.iloc[:,:1]","67d18395":"# Function to Remove outliers from data\ndef remove_outliers(x):\n    upper_limit = x.mean() + (2*x.std())\n    lower_limit = x.mean() - (2*x.std())\n    return np.where(x > upper_limit,upper_limit,np.where(x <lower_limit,lower_limit,x))","cad2be7a":"# Remove outliers for Train dataset\ntrain_df = train_df.apply(lambda x: remove_outliers(x))\n\n# Seperate target from Train Dataset\ntrain_df_Y = train_df.target\ntrain_df_X = train_df.iloc[:,:100]","e10bcdc6":"train_df_X.dtypes.unique()","0ed75842":"df_cat_variables = train_df_X.select_dtypes('int64').astype('int8')\ndf_cont_variables = train_df_X.select_dtypes('float64').astype('float32')","390f25fd":"# Scale and transform dataset\ndef data_scaler_fit(option,df):\n    if option == 1:\n        transformer = StandardScaler().fit(df)\n    if option == 2 :\n        transformer = RobustScaler().fit(df)\n    if option ==3 :\n        transformer = MinMaxScaler().fit(df)\n    return transformer","6f904247":"\"\"\"\n# Tanh estimator : https:\/\/stackoverflow.com\/questions\/43061120\/tanh-estimator-normalization-in-python\nm = np.mean(unnormalizedData, axis=0) # array([16.25, 26.25])\nstd = np.std(unnormalizedData, axis=0) # array([17.45530005, 22.18529919])\n\ndata = 0.5 * (np.tanh(0.01 * ((unnormalizedData - m) \/ std)) + 1)\n\"\"\"","134ca3b5":"transformer = data_scaler_fit(3,df_cont_variables)","ddfe97f3":"train_df_X = transformer.transform(df_cont_variables)\ntest_df_X = transformer.transform(test_df_X[df_cont_variables.columns])\n\n#train_df_X = df_cont_variables.to_numpy()\n#test_df_X = test_df_X[df_cont_variables.columns].to_numpy()","6e4629ba":"# Use PCA for dimention reduction\n\"\"\"\npca = PCA(0.95)\npca.fit(train_df_X)\n\ntrain_df_X = pca.transform(train_df_X)\ntest_df_X = pca.transform(test_df_X)\n\"\"\"","af7dcf9f":"\"\"\"\nmodel = XGBClassifier(objective = 'binary:logistic',eval_metric=\"auc\",random_state=1542,use_label_encoder=False,tree_method = 'gpu_hist')\n\n\n# define evaluation\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n# params = {\n  #      \"colsample_bytree\": uniform(0.7, 0.3),\n   #     \"gamma\": uniform(0, 0.5),\n    #    \"learning_rate\": uniform(0.03, 0.3), # default 0.1 \n     #   \"max_depth\": randint(2, 8), # default 3\n      #  \"n_estimators\": randint(100, 1000), # default 100\n       # \"subsample\": uniform(0.6, 0.4)\n  #  }\n\n\nparam_grid = {\n    \"eta\":(0.01,0.2,'uniform') ,\n    \"learning_rate\": (0.0001, 0.3, \"log-uniform\"),\n    \"n_estimators\": (100,  1000) ,\n    \"max_depth\": (2, 12) ,\n    \"colsample_bytree\": (0.3, 0.7,'uniform'),\n    \"gamma\": (0, 0.5,'uniform'),\n    \"subsample\": (0.4, 1.0)\n}\n\n\nsearch = GridSearchCV(model, param_grid, cv=cv, verbose=1, n_jobs=1, scoring='roc_auc',return_train_score=True,refit=True)\n\nsearch.fit(X_train, y_train,eval_set=[(X_test, y_test)])\n\nprint(\" Results from Random Search \" )\nprint(\"\\n The best estimator across ALL searched params:\\n\", search.best_estimator_)\nprint(\"\\n The best score across ALL searched params:\\n\", search.best_score_)\nprint(\"\\n The best parameters across ALL searched params:\\n\", search.best_params_)\n\"\"\"","de70b7db":"\"\"\"\ndef training_models(model_type):\n    # Stochastic Gradient Descent\n    if model_type == 'SGD':\n        model = SGDClassifier(loss=\"log\", penalty=\"l2\", max_iter=100)\n    \n    # Multi-layer Perceptron\n    if model_type == 'MLP':\n        model = MLPClassifier(alpha=1e-5,solver = 'sgd',learning_rate = 'adaptive',warm_start=True,early_stopping = True,\n                              max_iter=500,random_state=1)\n    # Decision Tree\n    if model_type == 'DTC':\n        model = DecisionTreeClassifier(max_depth = 10,max_features = 'auto', random_state = 1)\n        \n    # Random Forest\n    if model_type == 'RFC':\n        model = RandomForestClassifier(n_jobs = -1,n_estimators=100, warm_start=True,random_state = 1)\n    \n    # Gradient Boosting Classifier\n    if model_type == 'GBC':\n        model = GradientBoostingClassifier(loss= 'exponential',learning_rate = 0.05,n_estimators=500,max_depth=10,criterion='squared_error')\n        \n    # XG Boost\n    if model_type == 'XGB':\n        model = XGBClassifier(objective = 'binary:logistic',n_estimators=1000,eval_metric=\"auc\",random_state=1542,tree_method = 'gpu_hist',use_label_encoder=False)\n    \n    \n    return model\n\"\"\"","5f0e7c2b":"\"\"\"\ndef feature_selection(model_type,train_X,train_Y):\n    embeded_selector = SelectFromModel(training_models(model_type), threshold='1.25*median')\n    embeded_selector.fit(train_X,train_Y)\n    embeded_support = embeded_selector.get_support()\n    embeded_feature = pd.DataFrame(train_X).loc[:,embeded_support].columns.tolist()\n    return embeded_feature\n\n\"\"\"","8cbe2b1e":"# final_features = feature_selection('GBC',train_df_X, train_df_Y)","dff92865":"# Divide into train and test\nX_train, X_test, y_train, y_test = train_test_split(train_df_X, train_df_Y, test_size=0.20, random_state=151)","25827d92":"\"\"\"\nparameter_space = {\n    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n    'activation': ['tanh', 'relu'],\n    'solver': ['sgd', 'adam'],\n    'alpha': [0.0001, 0.05],\n    'learning_rate': ['constant','adaptive'],\n}\n\nmodel = MLPClassifier(warm_start=True,early_stopping = True,max_iter=500,random_state=1)\n\nsearch = GridSearchCV(model, parameter_space, cv=3, verbose=1, n_jobs=-1, scoring='roc_auc',return_train_score=True,refit=True)\n\nsearch.fit(X_train, y_train)\n\n\nprint(\" Results from Random Search \" )\nprint(\"\\n The best estimator across ALL searched params:\\n\", search.best_estimator_)\nprint(\"\\n The best score across ALL searched params:\\n\", search.best_score_)\nprint(\"\\n The best parameters across ALL searched params:\\n\", search.best_params_)\n\nmodel = search\n\"\"\"","66cf6e7a":"model = keras.models.Sequential([\nkeras.layers.Flatten(input_shape=[X_train.shape[1],]),\nkeras.layers.Dense(512, activation=\"relu\"),\nkeras.layers.Dropout(0.5),\nkeras.layers.Dense(128, activation=\"relu\"),\nkeras.layers.Dropout(0.5),\nkeras.layers.Dense(64, activation=\"relu\"),\nkeras.layers.Dense(1, activation=\"sigmoid\")\n])","06826c40":"model.summary()","899ec4d1":"callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\nepochs = 100\n\nmodel.compile(loss=\"binary_crossentropy\",\noptimizer=\"sgd\",\nmetrics=[tf.keras.metrics.AUC()])","42143505":"history = model.fit(X_train, y_train, epochs=epochs,validation_data=(X_test, y_test),callbacks=[callback])","88de0880":"#Train Model\n\n#model = training_models('MLP',X_train,y_train,X_test, y_test)\n#del(model)","c9dc5af7":"# list all data in history\nprint(history.history.keys())","f3e9527c":"# summarize history for accuracy\nplt.plot(history.history[list(history.history.keys())[1]])\nplt.plot(history.history[list(history.history.keys())[3]])\nplt.title('Model AUC')\nplt.ylabel('AUC')\nplt.xlabel('Epoch')\nplt.legend(['train', 'test'], loc='lower right')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","40ff0d46":"# Predict on interim test df\ntarget_predict = pd.DataFrame(model.predict(X_test).round().astype(int),columns = ['pred_target'])","6d82542a":"# Calculate and Show Confusion Matrix\nconf_metrix = confusion_matrix(y_test,target_predict.to_numpy(), normalize= 'true')\ndisp = ConfusionMatrixDisplay(confusion_matrix = conf_metrix )\ndisp.plot()\nplt.show()","eae37df0":"# Print Accuracy\nprint('Accuracy: '+ str(accuracy_score(y_test,target_predict.to_numpy()) * 100) + '%')","9e488343":"#Train Model\n#model = training_models('MLP',train_df_X,train_df_Y.to_numpy(),X_test, y_test)","5847293d":"# Predict on actual test dataset\nprobability = pd.DataFrame(model.predict(test_df_X),columns = ['target'])","16423ae2":"# Submit Predictions\nsubmission = pd.concat([test_df_id,probability],axis = 1)\nsubmission.to_csv('submission.csv',index=False)","421ae3fe":"## Tabular Playground - November 2021","8b288c8b":"### Predict and Submit to leaderboard","2b375431":"### Train Model on Whole data before predicting actual test data"}}