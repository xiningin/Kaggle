{"cell_type":{"391322cd":"code","3b522e31":"code","73264773":"code","10f5dd92":"code","6c9eeeba":"code","54d274d2":"code","09bb2837":"code","ff2a8bd0":"code","971aafab":"code","5d69d616":"code","cd0f3a8c":"code","31498f78":"code","fcc1a15b":"code","09ab6e22":"code","4ff18dac":"code","fe9ca865":"code","913cf344":"code","99bf998f":"code","3efa985b":"code","9ca328a1":"code","6b49a704":"code","1ffc684e":"code","97925ede":"code","dddd2e8d":"code","48e837c8":"code","40453a61":"code","f71cbe6c":"code","6287d211":"code","a03d47b0":"code","67a287f3":"code","610a6bb5":"code","b92558cd":"code","baff34df":"code","1bc4ba7f":"code","0733e127":"code","bdd8f06f":"code","5eaa57ee":"code","ece0d075":"code","d594894e":"code","fed92e3a":"code","4dd2f6f8":"code","d549ac37":"code","b4f0e84a":"code","ac408365":"code","10855935":"code","6a955ead":"code","ab5618f0":"markdown","2eee28b3":"markdown","eee13440":"markdown","0e372ca6":"markdown","da4d393c":"markdown","ed119ac7":"markdown","e4be92ba":"markdown","4576dded":"markdown","9c93bf3c":"markdown","4b09e4cd":"markdown","3c8a8d7a":"markdown","79d5374a":"markdown","551e61ca":"markdown","f6206b04":"markdown","d9798ab0":"markdown","ff57bbe4":"markdown","f5c6e5bd":"markdown","fb838940":"markdown","86f8117b":"markdown","af6da6af":"markdown","c099e531":"markdown","3e68f1fd":"markdown","0ded755d":"markdown","d9e7ba51":"markdown","71859e7a":"markdown","640bb06d":"markdown","662d36c6":"markdown","5573db7b":"markdown","d2fa25c0":"markdown","e23b21de":"markdown","b9f0782a":"markdown","3492f280":"markdown","eea09e6e":"markdown","54ffd613":"markdown","3c645477":"markdown"},"source":{"391322cd":"import os\nimport sys\nimport math\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nfrom scipy import stats\nimport scipy.stats as ss\nfrom scipy.special import boxcox1p\nsns.set(font_scale=2)\nwarnings.filterwarnings('ignore')\n%matplotlib inline","3b522e31":"train_dataset = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv') # train dataset\ntest_dataset = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv') # test dataset\nsample_submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv') # sample file submission","73264773":"# view train dataset\ntrain_dataset.head()","10f5dd92":"# Description of dataset\ndef remove_ID(df, columns : list):\n    df.drop(columns = columns,axis = 1, inplace = True)\n    df.head()\n    return df\n\nremove_ID(train_dataset, columns = ['Id'])","6c9eeeba":"print(train_dataset.shape) # check the shape of the dataset\ntrain_dataset.describe()","54d274d2":"## description of dependent feature\ntrain_dataset['SalePrice'].describe()\n\n## min price of house : 34900.000000\n## max pirce of house : 755000.000000","09bb2837":"# Now, we will handling our dependent feature i.e, salesprice feature which is our output feature as well.\n\n# we are creating the histogram of our dependent variable because we know that it is a regresssion task, and we want that\n# our output feature must be in normal distribution. If we want to solve any regression task especially using linear model\n# we need that our output feature will be in normal distribution\nsns.set(rc={'figure.figsize':(11.7,8.27)})\nsns.distplot(train_dataset['SalePrice'], bins = 30, fit = ss.norm)\nplt.title(\"Sale Price Distribution\")\n\n# calculate mean and std of the sales price\nmean = train_dataset['SalePrice'].describe()[1]\nstd = train_dataset['SalePrice'].describe()[2]\n\nplt.legend([\"Mean : {} \\n Std : {}\".format(mean, std)], loc = 'best')\nplt.show()\n\n# plot QQ Plot to check for skewness\nresult = stats.probplot(train_dataset['SalePrice'], plot = plt)\nplt.show()\nprint(\"Skewness : {}\".format(train_dataset['SalePrice'].skew()))\nprint(\"Mean : {}\".format(mean))\nprint(\"Median : {}\".format(np.median(train_dataset['SalePrice'])))\nprint(\"Median : {}\".format(stats.mode(train_dataset['SalePrice'])[0][0]))","ff2a8bd0":"# Clearly we can see that skewness is > 0.5 and mean > median > mode, which represent that it is +ve skewness in the data\n# we can solve this by using log transformation of data\n\n# convert the data into its log form\ntrain_dataset['SalePrice'] = np.log1p(train_dataset['SalePrice'])\n\nsns.set(rc={'figure.figsize':(11.7,8.27)})\nsns.distplot(train_dataset['SalePrice'], bins = 30, fit = ss.norm)\nplt.title(\"Sale Price Distribution\")\n\n# calculate mean and std of the sales price\nmean = train_dataset['SalePrice'].describe()[1]\nstd = train_dataset['SalePrice'].describe()[2]\n\nplt.legend([\"Mean : {} \\n Std : {}\".format(mean, std)], loc = 'best')\nplt.show()\n\n# plot QQ Plot to check for skewness\nresult = stats.probplot(train_dataset['SalePrice'], plot = plt)\nplt.show()\nprint(\"Skewness : {}\".format(train_dataset['SalePrice'].skew()))\nprint(\"Mean : {}\".format(mean))\nprint(\"Median : {}\".format(np.median(train_dataset['SalePrice'])))\nprint(\"Median : {}\".format(stats.mode(train_dataset['SalePrice'])[0][0]))","971aafab":"corelation_matrix = train_dataset.corr()\n\ncorelation_matrix\n# plot the heatmap of co-relation matrix to visualize the graph, and select the features.\nplt.figure(figsize = (50,50))\nsns.reset_defaults()\n# It will plot only those features which are highly co-related to each other, and which are in use for our model\nsns.heatmap(corelation_matrix[(corelation_matrix >= 0.45)|(corelation_matrix <= -0.45)], annot = True,cmap='Blues')\nplt.show()\n# Here, we can see that co-relation ranges from -1 to 1, 1- highly positive co-related to each other, and -1 represent \n# highly negatively co-related to each other.\n\n# we will choose those features which are highly co-related to each other, it doesn't matter whether they are positive\n# or negative, but it should highly co-related with our target variable.\n\n# There may be  some features which are highly co-related to each other but we will not choose those \n# features because our linear regression model checks for multi-collinearity. \n# and multi-collinearity results in unstable parameter which makes it very difficult to assess \n# the effect of independent variables on dependent variables.","5d69d616":"# Features that are highly co-related to our 'SalePrice' features are : \nfeatures_highly_co_related = ['OverallQual','YearBuilt','YearRemodAdd','TotalBsmtSF','1stFlrSF','GrLivArea','FullBath','TotRmsAbvGrd','Fireplaces','GarageYrBlt','GarageCars','GarageArea','SalePrice']\nsns.set(rc={'figure.figsize':(10.7,8.27)})\n\nfig, axes = plt.subplots(6,2)\n\naxes[0,0].scatter(train_dataset[features_highly_co_related[0]], train_dataset['SalePrice'])\naxes[0,0].set_title(features_highly_co_related[0])\naxes[0,1].scatter(train_dataset[features_highly_co_related[1]], train_dataset['SalePrice'])\naxes[0,1].set_title(features_highly_co_related[1])\naxes[1,0].scatter(train_dataset[features_highly_co_related[2]], train_dataset['SalePrice'])\naxes[1,0].set_title(features_highly_co_related[2])\naxes[1,1].scatter(train_dataset[features_highly_co_related[3]], train_dataset['SalePrice'])\naxes[1,1].set_title(features_highly_co_related[3])\naxes[2,0].scatter(train_dataset[features_highly_co_related[4]], train_dataset['SalePrice'])\naxes[2,0].set_title(features_highly_co_related[4])\naxes[2,1].scatter(train_dataset[features_highly_co_related[5]], train_dataset['SalePrice'])\naxes[2,1].set_title(features_highly_co_related[5])\naxes[3,0].scatter(train_dataset[features_highly_co_related[6]], train_dataset['SalePrice'])\naxes[3,0].set_title(features_highly_co_related[6])\naxes[3,1].scatter(train_dataset[features_highly_co_related[7]], train_dataset['SalePrice'])\naxes[3,1].set_title(features_highly_co_related[7])\naxes[4,0].scatter(train_dataset[features_highly_co_related[8]], train_dataset['SalePrice'])\naxes[4,0].set_title(features_highly_co_related[8])\naxes[4,1].scatter(train_dataset[features_highly_co_related[9]], train_dataset['SalePrice'])\naxes[4,1].set_title(features_highly_co_related[9])\naxes[5,0].scatter(train_dataset[features_highly_co_related[10]], train_dataset['SalePrice'])\naxes[5,0].set_title(features_highly_co_related[10])\naxes[5,1].scatter(train_dataset[features_highly_co_related[11]], train_dataset['SalePrice'])\naxes[5,1].set_title(features_highly_co_related[11])\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.9, \n                    right=0.9, \n                    top=3, \n                    wspace=0.4, \n                    hspace=0.4)","cd0f3a8c":"# fisrt we find the kurtosis of each features, and check whose kurt>3, which implies that there is high chances of outliers\ndef kurt(df):\n    kurt_ = list()\n    df = df.select_dtypes(exclude = \"O\")\n    for feature in df.columns:\n        kurtosis = df[feature].kurt()\n        if kurtosis > 3:\n            kurt_.append((feature, kurtosis))\n    \n    return kurt_\nk = kurt(train_dataset)\nprint(len(k))\n# k = 17 there are more chances that these 17 features have outliers in it.\n# lets see what are those 17 features.\nprint(k)","31498f78":"# lets plot the pairplot of all the k features including our target feature\ndef get_data(df, required_features):\n    data = list()\n    for feature,_ in required_features:\n        data.append(df[feature])\n    return data\ntemp_df = pd.DataFrame(data = get_data(train_dataset,k)).T\ntemp_df = pd.concat([temp_df,train_dataset['SalePrice']], axis = 1)\n\nsns.set()\nsns.pairplot(temp_df, size = 4)","fcc1a15b":"temp_df.shape","09ab6e22":"def plotBoxplot(df):\n    for i in df.columns:\n        sns.boxplot(x = i, hue = 'SalePrice', data = df)\n        plt.show()\nplotBoxplot(temp_df)","4ff18dac":"# By viewing the pair we have conclude that the following data points can be removed, which may decrease our accuracy\n# LotFrontage > 300, LotArea > 150000, TotalBsmtSF > 5900, GrLivArea > 4000\ntrain_dataset.drop(train_dataset[(train_dataset['LotFrontage']>300) & (train_dataset['SalePrice']>2.55)].index, inplace = True)\ntrain_dataset.drop(train_dataset[(train_dataset['LotArea'] > 150000) & (train_dataset['SalePrice'] >  2.55)].index, inplace = True)\ntrain_dataset.drop(train_dataset[(train_dataset['TotalBsmtSF'] > 5900) & (train_dataset['SalePrice'] >  2.55)].index, inplace = True)\ntrain_dataset.drop(train_dataset[(train_dataset['GrLivArea'] > 4000) & (train_dataset['SalePrice'] >  2.55)].index, inplace = True)","fe9ca865":"fig, axes = plt.subplots(2,2)\n\naxes[0,0].scatter(train_dataset['LotFrontage'], train_dataset['SalePrice'])\naxes[0,0].set_title('LotFrontage')\naxes[0,1].scatter(train_dataset['LotArea'], train_dataset['SalePrice'])\naxes[0,1].set_title('LotArea')\naxes[1,0].scatter(train_dataset['TotalBsmtSF'], train_dataset['SalePrice'])\naxes[1,0].set_title('TotalBsmtSF')\naxes[1,1].scatter(train_dataset['GrLivArea'], train_dataset['SalePrice'])\naxes[1,1].set_title('GrLivArea')\n# yes all the outliers are removed from our data, in total there were 8 outliers, and we have successfully remove them. ","913cf344":"train_dataset.head()","99bf998f":"test_dataset.head()","3efa985b":"# first we remove the ID column, so that we can concat our both the data, and handle the missing values in it.\nremove_ID(test_dataset, ['Id'])\ntest_dataset.head()","9ca328a1":"# make our y_data\ny_data = train_dataset['SalePrice'].values # it will convert our pandas series into numpy array.","6b49a704":"# now we will concat our train and test dataset\ntrain_dataset.drop(columns = \"SalePrice\", inplace = True)\ndata  = pd.concat([train_dataset, test_dataset], axis = 0)\nprint(data.shape)\ndata.head()","1ffc684e":"# check for null values\ntemp = data.isnull().sum()\ntemp = temp[temp > 0]\ntemp = pd.DataFrame(temp, columns = ['Null Values'])\ntemp.reset_index(inplace=True,)\ntemp","97925ede":"# there are 34 features that contains null values, so there are some categorical as well as numerical fetaures, we will handle every feature seperately\n\n# now, plot how many missing values are there in data with respect to the total data\nfig, axes = plt.subplots(1,2)\nfig.autofmt_xdate(rotation='90' ) \nsns.barplot(x = 'index', y = 'Null Values', data = temp, ax = axes[0])\naxes[0].set_title(\"Number of Missing Values in the data\")\n# % of missing values in the data\ndef percent(df):\n    percent_values = list()\n    for i in range(df.shape[0]):\n        percent_values.append( (int(df.iloc[i][1])\/data.shape[0]) * 100 )\n        \n    df_ = pd.concat([df,pd.Series(percent_values, name = \"%values\", dtype = float)], axis = 1)\n    return df_\n\ntemp_ = percent(temp)\nsns.barplot(x = 'index', y = '%values', data = temp_, ax = axes[1])\naxes[1].set_title(\"% of Missing Values in the data\")\n\nplt.subplots_adjust(left=0.1, bottom = 0.3, wspace = 0.2)","dddd2e8d":"data['MSZoning'].fillna(value = 'RL', inplace = True)\ndata['LotFrontage'] = data.groupby('Neighborhood')['LotFrontage'].transform(lambda x : x.fillna(x.median()))\ndata['Alley'].fillna(value = 'NA',inplace = True)\ndata['Utilities'].fillna(value = 'AllPub', inplace = True)\ndata['Exterior1st'].fillna(value = 'VinylSd', inplace = True)\ndata['Exterior2nd'].fillna(value = 'VinylSd', inplace = True)\ndata['MasVnrType'].fillna(value = 'None', inplace = True)\ndata['MasVnrArea'].fillna(value = 0.0, inplace = True)\ndata['BsmtQual'].fillna(value = 'NA',inplace = True)\ndata['BsmtCond'].fillna(value = 'NA',inplace = True)\ndata['BsmtExposure'].fillna(value = 'NA',inplace = True)\ndata['BsmtFinType1'].fillna(value = 'NA',inplace = True)\ndata['BsmtFinType2'].fillna(value = 'NA',inplace = True)\ndata['BsmtFinSF1'].fillna(value = 0.0, inplace = True)\ndata['BsmtFinSF2'].fillna(value = 0.0, inplace = True)\ndata['TotalBsmtSF'].fillna(value = 0.0, inplace = True)\ndata['Electrical'].fillna(value = 'SBrkr', inplace = True)\ndata['BsmtFullBath'].fillna(value = 0.0, inplace = True)\ndata['BsmtHalfBath'].fillna(value = 0.0, inplace = True)\ndata['KitchenQual'].fillna(value = 'TA', inplace = True)\ndata['Functional'].fillna(value = 'Typ', inplace = True)\ndata['FireplaceQu'].fillna(value = 'NA',inplace = True)\ndata['GarageType'].fillna(value = 'NA',inplace = True)\ndata['GarageYrBlt'].fillna(value = 0.0,inplace = True)\ndata['GarageFinish'].fillna(value = 'NA',inplace = True)\ndata['GarageCars'].fillna(value = 0.0,inplace = True)\ndata['GarageArea'].fillna(value = 0.0,inplace = True)\ndata['GarageQual'].fillna(value = 'NA',inplace = True)\ndata['GarageCond'].fillna(value = 'NA',inplace = True)\ndata['PoolQC'].fillna(value = 'NA',inplace = True)\ndata['Fence'].fillna(value = 'NA',inplace = True)\ndata['MiscFeature'].fillna(value = 'NA',inplace = True)\ndata['SaleType'].fillna(value = 'WD',inplace = True)\ndata['BsmtUnfSF'].fillna(value = 0.0,inplace = True)","48e837c8":"temp = data.isnull().sum()\ntemp = temp[temp > 0]\ntemp","40453a61":"def skewness(df):\n    df_ = df.select_dtypes(exclude = \"O\")\n    skew = list()\n    for i in df_.columns:\n        skew.append( (i,df[i].skew()) )\n    \n    df_ = pd.DataFrame(skew, columns = [\"Feature\",\"Skewness\"], dtype = float)\n    return df_\ndf = skewness(data)\nskew_data = df[(df['Skewness']>0.6) | (df['Skewness']<-0.6)]\nskew_data.head(10)","f71cbe6c":"skew_data.shape","6287d211":"def convert_boxcox(data, df):\n    for i in data.Feature:\n        df[i] = boxcox1p(df[i], 0.2)\n    return df\ndata = convert_boxcox(skew_data,data)\ndata","a03d47b0":"# get dummies data\ndata = pd.get_dummies(data)\ndata","67a287f3":"len_train = len(y_data)\nX_data = data[:len_train]\nX_test = data[len_train:]\nprint(X_data.shape)\nprint(X_test.shape)\nX_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.15, random_state=42)\nprint(X_train.shape)\nprint(X_val.shape)","610a6bb5":"# calculate rmse value\ndef rmse(y_pred,y_true):\n    return np.sqrt(mean_squared_error(y_true,y_pred))\n\ndef pipeline_model(scale = None, model = None):\n    # scale feature\n    if scale == 'MinMaxScaler':\n        scale_feature = ('scale',MinMaxScaler())\n    if scale == 'StandardScaler':\n        scale_feature = ('scaler',StandardScaler())\n        \n    # model feature\n    if model == \"lr\" or model == \"LinearRegression\" or model == \"linearregression\":\n        model_feature = ('lr',LinearRegression())\n    if model == \"dt\" or model == \"DecisionTree\" or model == \"decisiontree\":\n        model_feature = ('dt',DecisionTreeRegressor())\n    if model == 'rf' or model == \"RandomForest\" or model == \"randomforest\":\n        model_feature = ('rf',RandomForestRegressor())\n    if model == 'xgb' or model == \"xgboast\":\n        model_feature = ('xgb',GradientBoostingRegressor())\n        \n    # create model\n    model_pipe = Pipeline([scale_feature, model_feature])\n    \n    return model_pipe\n\n#  model function\ndef create_model(model_name,predict = True, draw = True, cal_error = True, **kwargs):\n    \n    keys = kwargs.keys()\n    \n    # create model\n    try:\n        if model_name == \"lr\" or model_name == \"LinearRegression\" or model_name == \"linearregression\":\n            model = LinearRegression()\n        if model_name == \"dt\" or model_name == \"DecisionTree\" or model_name == \"decisiontree\":\n            model = DecisionTreeRegressor()\n        if model_name == 'rf' or model_name == \"RandomForest\" or model_name == \"randomforest\":\n            model = RandomForestRegressor()\n        if model_name == 'xgb' or model_name == \"xgboast\":\n            model = GradientBoostingRegressor()\n        if model_name == 'pipe' or model_name == 'Pipline':\n            if 'scale' in list(keys) and 'model' in list(keys):\n                model = pipeline_model(scale = kwargs['scale'],model = kwargs['model'])\n                      \n    except:\n        raise InputError(\"You have Entered wrong input, Please enter a valid input,\")\n    \n    # fitting the model\n    try:\n        if \"X_train\" in list(keys) and \"y_train\" in list(keys):\n            model.fit(kwargs['X_train'], kwargs['y_train'])        \n    except :\n        raise InputError(\"Enter the correct data.\")\n        \n    # prediction of data\n    if predict:\n        try:\n            # for validation data\n            if 'X_val' in list(keys):\n                y_val_pred = model.predict(kwargs['X_val'])\n                \n                if cal_error:\n                    try:\n                        if 'y_val' in list(keys):\n                            y_val_error = rmse(y_val_pred, kwargs['y_val'])\n                            y_val_score = r2_score(kwargs['y_val'],y_val_pred)\n                        \n                    except:\n                        raise ValueError(\"Data not found, Please enter the correct data.\")\n            \n            # for training data\n            if 'X_train' in list(keys):\n                y_train_pred = model.predict(kwargs['X_train'])\n                \n                if cal_error:\n                    try:\n                        if 'y_train' in list(keys):\n                            y_train_error = rmse(y_train_pred, kwargs['y_train'])\n                            y_train_score = r2_score(kwargs['y_train'],y_train_pred)\n                        \n                    except:\n                        raise ValueError(\"Data not found, Please enter the correct data.\")\n                        \n        except:\n            raise InputError(\"Its is boolen expression, or might the parameter doesn't satisfy the needs.\")\n            \n    \n    result = {'val_pred' : y_val_pred,\n              'val_error' : y_val_error,\n              'val_score' : y_val_score,\n              'train_pred' : y_train_pred,\n              'train_error' : y_train_error,\n              'train_score' : y_train_score}\n    if draw:\n        # plot the scatter plot of y_actual and y_prediction, to show the trend.\n        \n        # make subplots for training and validation data\n        \n        # training data\n        fig,axes = plt.subplots(1,2)\n        sns.set(rc={'figure.figsize':(10.7,8.27)})\n        axes[0].scatter(y_train_pred,kwargs['y_train'])\n        axes[0].set_title(\"Prediction Graph of Training Data\")\n        axes[0].set_xlabel(\"Actual Values\")\n        axes[0].set_ylabel(\"Prediction Values\")\n        # validation data\n        axes[1].scatter(y_val_pred,kwargs['y_val'])\n        axes[1].set_title(\"Prediction Graph of Validation Data\")\n        axes[1].set_xlabel(\"Actual Values\")\n        axes[1].set_ylabel(\"Prediction Values\")\n    \n    return model, result","b92558cd":"model_lr,result_lr = create_model('lr',draw = False, X_train = X_train, y_train = y_train, X_val = X_val, y_val = y_val)\nmodel_dt,result_dt = create_model('dt', draw = False,X_train = X_train, y_train = y_train, X_val = X_val, y_val = y_val)\nmodel_rf,result_rf = create_model('rf', draw = False,X_train = X_train, y_train = y_train, X_val = X_val, y_val = y_val)\nmodel_xgb,result_xgb = create_model('xgb', draw = False,X_train = X_train, y_train = y_train, X_val = X_val, y_val = y_val)\nmodel_pipe, result_pipe = create_model('pipe', draw = False,X_train = X_train, y_train = y_train, X_val = X_val, y_val = y_val, scale = 'StandardScaler', model = 'rf')","baff34df":"print(\"Linear Regression :  Training Error : {} and Validation Error : {}\".format(round(result_lr['train_error'],5),round(result_lr['val_error'],5)))\nprint(\"Decision Tree     :  Training Error : {} and Validation Error : {}\".format(round(result_dt['train_error'],5),round(result_dt['val_error'],5)))\nprint(\"Random Forest     :  Training Error : {} and Validation Error : {}\".format(round(result_rf['train_error'],5),round(result_rf['val_error'],5)))\nprint(\"XgBoast           :  Training Error : {} and Validation Error : {}\".format(round(result_xgb['train_error'],5),round(result_xgb['val_error'],5)))\nprint(\"PipeLine          :  Training Error : {} and Validation Error : {}\".format(round(result_pipe['train_error'],5),round(result_pipe['val_error'],5)))","1bc4ba7f":"# in ridge and lasso alpha values are important so we wil use some random alpha values.\n\n#for ridge\nalpha_r = [0,0.1,0.4,0.8,1,1.8,2.5,4,5,10,12,18,30,50,65,75,100]\n# this much is enough\nparameters_r = {'alpha' : alpha_r}\n\n# for lasso\nalpha_l = [3e-5, 2e-8, 1e-5, 0.00001, 0.00005, 0.00009, 0.0001, 0.0005, 0.001, 0.005, 0.007, 0.01, 0.07,0.1]\nparameters_l = {'alpha' : alpha_l}","0733e127":"lasso = Lasso()\n\nlasso_reg = GridSearchCV(lasso, param_grid = parameters_l)\nlasso_reg.fit(X_train,y_train)\nprint(\"Best Value of Alpha is : {}\".format(lasso_reg.best_params_))\nprint(\"Best score : {}\".format(lasso_reg.best_score_))","bdd8f06f":"ridge = Ridge()\nridge_reg = GridSearchCV(ridge, param_grid = parameters_r)\nridge_reg.fit(X_train,y_train)\nprint(\"Best Value of Alpha is : {}\".format(ridge_reg.best_params_))\nprint(\"Best score : {}\".format(ridge_reg.best_score_))","5eaa57ee":"model_lr, predictions= create_model('lr',draw = False, X_train = X_train, y_train = y_train, X_val = X_val, y_val = y_val)\nprint(\"Training Error : {}\".format(predictions['train_error']))\nprint(\"Training Score : {}\".format(predictions['train_score']))\nprint(\"Validation Error : {}\".format(predictions['val_error']))\nprint(\"Validation Score : {}\".format(predictions['val_score']))","ece0d075":"ridge = Ridge(alpha = 10)\nridge.fit(X_train,y_train)\nridge_train_pred = ridge.predict(X_train)\nridge_val_pred = ridge.predict(X_val)\nridge_train_error = rmse(ridge_train_pred,y_train)\nridge_val_error = rmse(ridge_val_pred,y_val)\nridge_train_score = r2_score(y_train,ridge_train_pred)\nridge_val_score = r2_score(y_val,ridge_val_pred)","d594894e":"print(\"Training Error : {}\".format(ridge_train_error))\nprint(\"Training Score : {}\".format(ridge_train_score))\nprint(\"Validation Error : {}\".format(ridge_val_error))\nprint(\"Validation Score : {}\".format(ridge_val_score))","fed92e3a":"lasso = Lasso(alpha = 0.0005)\nlasso.fit(X_train,y_train)\nlasso_train_pred = lasso.predict(X_train)\nlasso_val_pred = lasso.predict(X_val)\nlasso_train_error = rmse(lasso_train_pred,y_train)\nlasso_val_error = rmse(lasso_val_pred,y_val)\nlasso_train_score = r2_score(y_train,lasso_train_pred)\nlasso_val_score = r2_score(y_val,lasso_val_pred)","4dd2f6f8":"print(\"Training Error : {}\".format(lasso_train_error))\nprint(\"Training Score : {}\".format(lasso_train_score))\nprint(\"Validation Error : {}\".format(lasso_val_error))\nprint(\"Validation Score : {}\".format(lasso_val_score))","d549ac37":"# linear model prediction on x test data\ny_predictions = ridge.predict(X_test) # by checking all the 3 models we cam up that ridge is the best of all the 3 models in it, we are using ridge model for predictions","b4f0e84a":"print(X_test.shape)\nprint(y_predictions.shape)","ac408365":"sample_submission.head()","10855935":"sample_submission.drop(columns = 'SalePrice', inplace = True)\nsample_submission = pd.concat([sample_submission, pd.Series(np.expm1(y_predictions), name = \"SalePrice\", dtype = float)], axis = 1)\nsample_submission","6a955ead":"sample_submission.to_csv(\"submission.csv\",index = False )","ab5618f0":"### Load Data","2eee28b3":"### Regularization Method","eee13440":"### Description of Features that contains missing Values","0e372ca6":"### Remove Skewness","da4d393c":"In the above histogram we can visualize the blue and the black line, blue line represent the actual data, and black line represent the norm data, here is a twist, here we can see that there is some skewness in the data, and now we have to handle the skewness of the data, and make it symmterical and with 0.0 skewness.","ed119ac7":"### Kurtosis\n\nIt refers to the degree of outliers in the data.Kurtosis is a statistical measure, whether the data is heavy-tailed or light-tailed in a normal distribution. \n\n##### Excess Kurtosis\nIt is used in statistics and probability to compute the kurtosis with the normal distribution. It can be +ve, -ve or 0. \n* Note : Normal distribution has kurtosis = 3. \n$$ Excess Kurtosis = Kurtosis - 3 $$\n\n#### Types of Excess Kurtosis\n- +ve or Leptokurtic\n    In this Kurtosis > 3, in this case there are more chances of outliers. It indicates that distribution is peaked and possess thick tails. In this case more of the data is located in the tails of distribution instead of around the mean.\n- -ve or Platykurtic\n    In this Kurtosis < 3, having a lower tail and stretched around center tails means most of the data points are present in high proximity with mean.\n- Zero or Mesokurtic\n    In this Kurtosis = 3, It is just the normal distribution.","e4be92ba":"Till now, we have been working on numerical data, but from now onwards, we will working on both numerical as well as categorical data.","4576dded":"Now we can see that mean median and mode are nearly equall to each other and skewness ~ 0, so we can assume that our data is skew free.","9c93bf3c":"### Splitting the Data","4b09e4cd":"Till now, we have seen that our co-related features and remove skewness from our target feature, now we proceed further and see how we can handle the outliers, but before moving forward I want to say that while making plots between target feature and its co-related feature, we can conclude that 'OverallQuad' is most highly co-related with target feature, which shows that if overall quality of house increses then price of house will increase, and other features also related to target feature, and they all are contributing good enough for prediction.","3c8a8d7a":"#### Lasso Model","79d5374a":"Now we have 3 models for our prediction, now we will check all the 3 models, and check with is suitable for our data, and results, then we will finalize one model, and predict our test data.","551e61ca":"Now we remove all the possible outliers from our dataset, we can plot them and verify them as well so for confirmation, but we will plotting only whose in which outliers are being removed.","f6206b04":"### Handling Missing Values","d9798ab0":"### Make Train Test Data","ff57bbe4":"### Import Necessary Modules","f5c6e5bd":"### Outliers","fb838940":"All the three models are giving approximately equal results, so it is too confusing too choose which model is the best, so what we will do we are picking the random model, any of the model we can choose from it, I am choosing Linear Regression Model, you can choose any of the three.","86f8117b":"### Linear Regression","af6da6af":"Now we have remove our Id feature which is of no use, as it was just telling the id number of the row, and which is a primary key also, and it wasn't effecting our data, so we prefer to remove it, rather than to keep it\n\nFeatures left after remving Id are : 79","c099e531":"- Logarithmic Transformation of Data","3e68f1fd":"### CoRelation ","0ded755d":"### Analyse Dependent Feature","d9e7ba51":"Now, we have done with our EDA and Feature Engineering Task, now its time to prepare our train and test dataset.","71859e7a":"Now the data has no missing values. ","640bb06d":"### Lasso","662d36c6":"### Skewness\n\nSkewness esentially measures the symmetry of the distribution of the curve. Skewness is the degree of asymmetry observed in the probability distribution that deviates from the symmetrical normal distribution of the data.\n\nThe normal distribution helps us to know about the skewness in the data. In the normal distribution plot, the data is symmetrical. The symmetrical distribution has zero skewness. i.e., mean = median = mode = 0. if the normal distribution plot is asymmetrical, at that time skewness comes into picture.\n\n* Note : If the values of a feature are skewed, depending on the model, skewness may violate model assumptions or may reduce the interpretation of feature importance.\n\n#### Types of Skewness\n- 1 : Postive or right skewness\n        In this type of distribution where mean, median and mode are +ve rather than -ve or zero. generally mean > median > mode (It means that most of the data is bent over lower side of plot). This type of skewness isn't desirable for distribution as it may leads to the wrong results. We can solve this problem by transformimg our data into its log form. We will convert our data into its natural logarithmic form.\n\n- 2 : Negative or left skewness\n        This is just an opposite of positive skewness, mean, median and mode are -ve rather than +ve or zero. generally mean < median < mode.\n        \n### Calculate skewness\nThere are 2 methods to calculate the skewness:\n- Pearson\u2019s first coefficient of skewness\n    $$ = \\frac{(Mean - Mode)}{(Standard deviation)}$$\n    Range  : -1 to +1\n    If the mode is very high or very low mode, this method is not preferred\n- Pearson\u2019s second coefficient of skewness\n    $$ = \\frac{3(Mean - Median)}{Standard deviation)}$$\n    Range : \n       - -0.5 to +0.5 the data is symmetrical\n       - -1 to -0.5 -ve skewed\n       - 0.5 to 1 +ve skewed.","5573db7b":"Till now, we have seen various plot of our train dataset, we have handle outliers, learn about skewness and kurtosis, and make normal distribution plot, and much more. But from now onwards we will working on both the dataset i.e., train as well test dataset. In this section, we will see how we will missing values in our dataset.","d2fa25c0":"### Getting Result of test data","e23b21de":"Now we check for the co-relation between the features with respect to our dependent features.","b9f0782a":"### Modelling","3492f280":"Now one of the important step in this task, we know that this is a regression task, previously we seen skewness in our target data, but now we sill see the skewness in our independent variables.See how we can handle it.","eea09e6e":"We will made our regression model, now we have come to end our task, in this section we will capture some parameter that can increase the efficiency of model, without overfitting it, we have used Linear Regression model for prediction, now we will using Lasso and Ridge method for regularization.","54ffd613":"### Ridge","3c645477":"#### Ridge Model"}}