{"cell_type":{"3eb98627":"code","5ffc3ac2":"code","a72b2703":"code","517fc418":"code","023d0b40":"code","7c236899":"code","f2273c50":"code","0809c33f":"code","81ba9e64":"code","ecf3fe3f":"code","f633d75f":"code","8016c195":"code","be57e2d6":"code","d84c8fe5":"code","ac0f24e0":"code","1dd84bbf":"code","cdd9c8e7":"code","01c8b9c0":"code","a9dbdf50":"code","7787db51":"code","795ba1d8":"markdown","02611a7d":"markdown","cbf880de":"markdown","81cb2bba":"markdown","29a77fc5":"markdown","97a1f0e9":"markdown","a9c733b5":"markdown","af7d9a57":"markdown","0dddcc75":"markdown","dbbcc8ff":"markdown","909fcbcf":"markdown","a6e5d011":"markdown","d2e7c939":"markdown","949c3863":"markdown","af17dc79":"markdown","7898a8cc":"markdown","38d2f7d6":"markdown","c1641d29":"markdown"},"source":{"3eb98627":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures, MinMaxScaler\n%matplotlib inline","5ffc3ac2":"# Lendo os dados\ndf = pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv')\ndf.head()","a72b2703":"# Informa\u00e7\u00f5es b\u00e1sicas sobre o conjunto de dados\nprint('Formato do DataFrame: ', df.shape)\nprint('N\u00famero de registros: ', df.shape[0], '\\n')\nprint('Localiza\u00e7\u00f5es: ', df['Location'].unique())","517fc418":"#Informa\u00e7\u00f5es sobre valores nulos e tipos de dados\nprint(df.info())","023d0b40":"# Convertendo 'Date' para DateTime\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Selecionando Sydney\ndf_sidney = df[df['Location'] == 'Sydney']\n\n# Definindo a data como index e selecionando apenas a coluna MaxTemp\n# Note que [[\"MaxTemp\"]] foi usado para obter um DataFrame. Se [\"MaxTemp\"]\n# fosse utilizado, a operacao retornaria um objeto Series do pandas.\ndf_sidney = df_sidney.set_index('Date')[[\"MaxTemp\"]].dropna().sort_index()\n\nprint(df_sidney.head(), '\\n')\nprint(type(df_sidney))\nprint(df_sidney.shape)","7c236899":"df_sidney.plot(figsize=(20, 10))","f2273c50":"# Separando treino e valida\u00e7\u00e3o\ntrain_df = df_sidney[:'2015-12-31']\nvalid_df = df_sidney['2016-01-01':]\n\nfig, ax = plt.subplots(figsize=(20, 10))\n\nax.plot(train_df.index, train_df['MaxTemp'], color='b', label='Treino')\nax.plot(valid_df.index, valid_df['MaxTemp'], color='r', label='Valida\u00e7\u00e3o')\nax.legend()\nax.set_title('Divis\u00e3o entre treino e valida\u00e7\u00e3o')\nax.set_xlabel('Data')\nax.set_ylabel('Temperatura M\u00e1xima (MaxTemp)')","0809c33f":"# Resetando os \u00edndices para extrair features da data\ntrain_df.reset_index(inplace=True)\nvalid_df.reset_index(inplace=True)","81ba9e64":"def mostrar_predicoes(train_df_features, valid_df_features, model = None):\n    \"\"\"\n    Recebe os DataFrames de treino e validacao, treina o modelo,\n    e gera uma visualizacao grafica das predicoes para esses conjuntos de dados.\n    \"\"\"\n    if model == None:\n        # Instanciando a regress\u00e3o linear\n        model = LinearRegression()\n    \n    # Obtendo as features e alvos a partir dos dados\n    train_target = train_df_features['MaxTemp'].values\n    train_data = train_df_features.drop(['MaxTemp', 'Date'], axis=1).values\n    \n    valid_target = valid_df_features['MaxTemp'].values\n    valid_data = valid_df_features.drop(['MaxTemp', 'Date'], axis=1).values\n    \n    # Treinando o modelo\n    model.fit(train_data, train_target)\n    \n    # Computando as predi\u00e7\u00f5es e erro de valida\u00e7\u00e3o\n    predictions_train = model.predict(train_data)\n    predictions_valid = model.predict(valid_data)\n    validation_mse = mean_squared_error(valid_target, predictions_valid)\n    \n    fig, ax = plt.subplots(figsize=(20, 10))\n    ax.plot(train_df['Date'], train_df['MaxTemp'], color='b', marker='.', linestyle='', markersize=2, alpha=0.4)\n    ax.plot(valid_df['Date'], valid_df['MaxTemp'], color='r', marker='.', linestyle='', markersize=2, alpha=0.4)\n    \n    ax.plot(train_df['Date'], predictions_train, color='b', label='Treino')\n    ax.plot(valid_df['Date'], predictions_valid, color='r', label='Valida\u00e7\u00e3o')\n    ax.legend()\n    \n    ax.set_title('Predi\u00e7\u00f5es de temperatura em fun\u00e7\u00e3o do tempo')\n    \n    plt.show()\n    \n    print(\"MSE (Erro quadr\u00e1tico m\u00e9dio): \", validation_mse)","ecf3fe3f":"# Copiando os conjuntos de treino e valida\u00e7\u00e3o\ntrain_1 = train_df.copy()\nvalid_1 = valid_df.copy()\n\n# Extraindo o m\u00eas\ntrain_1['mes'] = train_1['Date'].dt.month\nvalid_1['mes'] = valid_1['Date'].dt.month\n    \nmostrar_predicoes(train_1, valid_1)","f633d75f":"# Copiando dos conjuntos anteriores\ntrain_2 = train_1.copy()\nvalid_2 = valid_1.copy()\n\n# Extraindo o dia\ntrain_2['dia'] = train_2['Date'].dt.day\nvalid_2['dia'] = valid_2['Date'].dt.day\n\n# Extraindo o ano\ntrain_2['ano'] = train_2['Date'].dt.day\nvalid_2['ano'] = valid_2['Date'].dt.day\n\nmostrar_predicoes(train_2, valid_2)","8016c195":"# Copiando dos conjuntos s\u00f3 com o m\u00eas\ntrain_3 = train_1.copy()\nvalid_3 = valid_1.copy()\n\n# Aplicando one-hot encoding\ntrain_3 = pd.get_dummies(train_3, columns=['mes'])\nvalid_3 = pd.get_dummies(valid_3, columns=['mes'])\ntrain_3.head()","be57e2d6":"# Predi\u00e7\u00f5es usando apenas o m\u00eas\nmostrar_predicoes(train_3, valid_3)","d84c8fe5":"# Copiando dos conjuntos anteriores\ntrain_3 = train_2.copy()\nvalid_3 = valid_2.copy()\n\n# Aplicando one-hot encoding\ntrain_3 = pd.get_dummies(train_3, columns=['dia', 'mes', 'ano'])\nvalid_3 = pd.get_dummies(valid_3, columns=['dia', 'mes', 'ano'])\ntrain_3.head()\n\n# Mostrando as predicoes usando dia, m\u00eas e ano\nmostrar_predicoes(train_3, valid_3)","ac0f24e0":"# Realizando uma regress\u00e3o linear com regulariza\u00e7\u00e3o (Regress\u00e3o de Ridge)\nridge_model = Ridge()\nmostrar_predicoes(train_3, valid_3, ridge_model)","1dd84bbf":"train_4 = train_2.copy()\nvalid_4 = valid_2.copy()\n\n# Instanciando o gerador de features do sklearn\npoly = PolynomialFeatures(degree=2, include_bias=False)\n\n# Adicionando as features geradas conjunto de treino e valida\u00e7\u00e3o\ntrain_4 = train_4.join(pd.DataFrame(poly.fit_transform(train_4[['dia', 'mes', 'ano']])))\nvalid_4 = valid_4.join(pd.DataFrame(poly.fit_transform(valid_4[['dia', 'mes', 'ano']])))\n\n# Removendo colunas que ficaram duplicadas\ntrain_4.drop(['dia', 'mes', 'ano'], axis=1, inplace=True)\nvalid_4.drop(['dia', 'mes', 'ano'], axis=1, inplace=True)\n\n# Observe que o sklearn opera utilizando o numpy, logo, os valores gerados pelo fit_transform\n# ser\u00e3o arrays do numpy. Para contornar isso, \u00e9 poss\u00edvel usar pd.DataFrame() pra gerar um novo\n# DataFrame a partir desse array, mas ele n\u00e3o possuir\u00e1 nomes nas colunas, apenas n\u00fameros sequenciais\ntrain_4.head()","cdd9c8e7":"# Mostrando as predi\u00e7\u00f5es geradas pela regress\u00e3o polinomial\nmostrar_predicoes(train_4, valid_4)","01c8b9c0":"train_4 = train_2.copy()\nvalid_4 = valid_2.copy()\n\n# Instanciando o gerador de features do sklearn\npoly = PolynomialFeatures(degree=4, include_bias=False)\n\n# Adicionando as features geradas conjunto de treino e validacao\ntrain_4 = train_4.join(pd.DataFrame(poly.fit_transform(train_4[['dia', 'mes', 'ano']])))\nvalid_4 = valid_4.join(pd.DataFrame(poly.fit_transform(valid_4[['dia', 'mes', 'ano']])))\n\ntrain_4.drop(['dia', 'mes', 'ano'], axis=1, inplace=True)\nvalid_4.drop(['dia', 'mes', 'ano'], axis=1, inplace=True)\n\n# Mostrando as predi\u00e7\u00f5es geradas pela regress\u00e3o polinomial\nmostrar_predicoes(train_4, valid_4)","a9dbdf50":"# Regress\u00e3o de Ridge com features polinomiais\nridge_model = Ridge()\nmostrar_predicoes(train_4, valid_4, ridge_model)","7787db51":"# Regress\u00e3o de Lasso com features polinomiais\nlasso_model = Lasso()\nmostrar_predicoes(train_4, valid_4, lasso_model)","795ba1d8":"## 3 - Extraindo features","02611a7d":"## 2 - Tratando os dados e preparando fun\u00e7\u00f5es","cbf880de":"Nesse caso, j\u00e1 foi poss\u00edvel observar uma melhoria significante no desempenho do modelo. Isso acontece, porque ele pode tratar cada m\u00eas separadamente durante a regress\u00e3o linear. Dessa forma, cada m\u00eas est\u00e1 devolvendo um valor diferente, enquanto o modelo anterior fazia com que a predi\u00e7\u00e3o fosse o produto do n\u00famero do m\u00eas por um valor somado a uma constante (linear).\n\n- Sem one-hot encoding: MaxTemp = c * m\u00eas + constante.\n- Com one-hot encoding: MaxTemp = c1 * \u00e9_janeiro + c2 * \u00e9_fevereiro + c3 * \u00e9_mar\u00e7o + ... + constante","81cb2bba":"Com isso, observamos que nem sempre a regulariza\u00e7\u00e3o proporciona um desempenho melhor para o modelo.","29a77fc5":"## Conclus\u00e3o\n\nA partir dessa an\u00e1lise, \u00e9 claro como extra\u00e7\u00e3o de features e seu tratamento adequado melhoram consideravelmente o desempenho de um modelo. Tamb\u00e9m vale a pena destacar que modelos diferentes funcionam melhor com outros tipos de features (por exemplo modelos baseados em \u00e1rvores).\n\nEm casos de dados com s\u00e9ries temporais, extrair informa\u00e7\u00f5es da data fornece ao modelo um comportamento em rela\u00e7\u00e3o a periodicidade, como foi o caso desse notebook, afinal a temperatura segue um padr\u00e3o com o decorrer dos meses. O mesmo poderia ser aplicado, por exemplo, \u00e0s vendas de uma loja. Nesse caso, existiriam outras informa\u00e7\u00f5es \u00fateis, como saber se o dia \u00e9 um feriado.\n\n\u00c9 importante ressaltar que tudo que foi apresentado \u00e9 apenas uma parte pequena do campo da engenharia de features. Cada tipo de dado possui m\u00faltiplos tratamentos e codifica\u00e7\u00f5es poss\u00edveis, com resultados diferentes. Por\u00e9m, espero que esse material tenha te ajudado a compreender mais sobre esse campo t\u00e3o amplo.","97a1f0e9":"Percebe-se que o modelo ficou mais complexo, mas ainda n\u00e3o obtemos melhoria na performance do modelo (na verdade o erro aumentou). Logo, temos que fazer algum tratamento adicional nesses dados. Uma op\u00e7\u00e3o \u00e9 aplicar one-hot encoding nas features, mesmo com elas sendo num\u00e9ricas.","a9c733b5":"Uma observa\u00e7\u00e3o importante \u00e9 que n\u00e3o \u00e9 faz sentido utilizar one-hot encoding seguido de gera\u00e7\u00e3o de features polinomiais, pois todos os valores ser\u00e3o 0 ou 1, e esses n\u00fameros elevados a qualquer pot\u00eancias s\u00e3o eles mesmos.\n\nOutra quest\u00e3o a prestar aten\u00e7\u00e3o na regress\u00e3o polinomial \u00e9 o grau da mesma, o gr\u00e1fico acima foi feito com uma de grau 2. Podemos usar graus maiores para gerar modelos mais complexos, mas n\u00fameros muito altos tamb\u00e9m podem causar overfitting.","af7d9a57":"Nota-se que a temperatura possui uma rela\u00e7\u00e3o forte com a data. Portanto, vamos extrair features a partir da data para tentar prever a temperatura em datas futuras.\n\nAntes disso, precisamos separar um conjunto de valida\u00e7\u00e3o, como estamos tentando prever o clima em datas futuras, n\u00e3o podemos extrair observa\u00e7\u00f5es em datas aleat\u00f3rias.\nPara obter um conjunto de valida\u00e7\u00e3o robusto, vamos treinar o model com observa\u00e7\u00f5es at\u00e9 o final de 2015. Os dados a partir de 2016 ser\u00e3o utilizados para valida\u00e7\u00e3o.","0dddcc75":"No modelo acima, usamos a regress\u00e3o de Ridge, que \u00e9 basicamente uma regress\u00e3o linear simples com regulariza\u00e7\u00e3o L2. Observamos uma redu\u00e7\u00e3o na complexidade do modelo, reduzindo o ru\u00eddo causado pelos dias, com isso tamb\u00e9m obtemos um erro menor que as tentativas anteriores.\n\nLeia mais sobre isso nesse link: https:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/a-comprehensive-guide-for-linear-ridge-and-lasso-regression\/","dbbcc8ff":"## Introdu\u00e7\u00e3o\n\nFeature engineering \u00e9 o processo no qual \u00e9 poss\u00edvel transformar, extrair e criar novas features a partir dos dados dispon\u00edveis, com o objetivo de melhorar o desempenho de algoritmos de aprendizado de m\u00e1quina. Isso faz com que seja essencial conhecer essas t\u00e9cnicas, para aqueles que pretendem participar em competi\u00e7\u00f5es de ci\u00eancia de dados.\n\nComo ser\u00e1 mostrado nesse notebook, esse procedimento pode aumentar significamente a precis\u00e3o at\u00e9 mesmo dos modelos mais simples. O objetivo desse projeto ser\u00e1 gerar um modelo para predizer a temperatura m\u00e1xima em um dia qualquer no futuro. \n\nPor\u00e9m, s\u00f3 ser\u00e1 utilizada uma feature fornecida nos dados: a data na qual a temperatura foi registrada. Al\u00e9m disso, o modelo utilizado ser\u00e1 uma regress\u00e3o linear simples ou com regulariza\u00e7\u00e3o. A data foi escolhida, pois muitas competi\u00e7\u00f5es utilizam s\u00e9ries temporais, logo as t\u00e9cnicas abordadas nesse material poder\u00e3o ser \u00fateis em uma variedade de projetos.","909fcbcf":"Utilizando features polinomiais de grau 4 obtemos o menor erro at\u00e9 agora, mas ainda podemos tentar aplicar regulariza\u00e7\u00e3o, por exemplo com Ridge e Lasso.","a6e5d011":"## 0 - Importando os m\u00f3dulos necess\u00e1rios","d2e7c939":"Novamente, o modelo se tornou mais complexo, mas o erro aumentou. Isso foi um caso de overfitting, que pode ser resolvido selecionando melhor as features utilizadas ou usando outros m\u00e9todos, como regulariza\u00e7\u00e3o.","949c3863":"## 1 - Examinando os dados\nNesse notebook, ser\u00e1 utilizado o dataset de clima da Austr\u00e1lia, que possui 10 anos de observa\u00e7\u00f5es di\u00e1rias do clima em diversas regi\u00f5es do pa\u00eds.\n\nLink do dataset: https:\/\/www.kaggle.com\/jsphyg\/weather-dataset-rattle-package","af17dc79":"## 4 - Features polinomiais","7898a8cc":"Uma desvantagem da regress\u00e3o linear \u00e9 a fato dela s\u00f3 extrair rela\u00e7\u00f5es lineares das features fornecidas, mas isso pode ser contornado adicionando features polinomiais ao modelo (transformando ele em uma regress\u00e3o polinomial), com isso, conseguimos extrair rela\u00e7\u00f5es mais complexas dos mesmos dados. ","38d2f7d6":"Para simplificar a an\u00e1lise, vamos utilizar apenas a coluna que representa a temperatura m\u00e1xima (MaxTemp) em Sydney, realizando regress\u00f5es tentar capturar as tend\u00eancias dessa feature em rela\u00e7\u00e3o ao tempo.","c1641d29":"Gr\u00e1ficos semelhantes ao que foi criado acima ser\u00e3o utilizados nesse notebook para demonstrar como as predi\u00e7\u00f5es se ajustam \u00e0s features fornecidas.\nNele, os pontos representam os dados reais do clima de Sydney, enquanto as linhas cont\u00ednuas representam as predi\u00e7\u00f5es obtidas pela regress\u00e3o linear.\n\nNessa primeira visualiza\u00e7\u00e3o, notamos que o gr\u00e1fico obtido est\u00e1 muito distante dos dados reais, afinal, fornecemos apenas uma feature (m\u00eas) para um modelo de regress\u00e3o linear simples. Para tentar melhorar isso, vamos extrair novas informa\u00e7\u00f5es da data."}}