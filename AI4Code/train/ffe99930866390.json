{"cell_type":{"3eafa216":"code","5c69d415":"code","cf59f9bb":"code","fa43078f":"code","048a9a47":"code","83b130cc":"code","2edea585":"code","c575b853":"code","2d0bc87f":"code","d78516af":"code","d0aec741":"code","5e6ae82b":"code","f746dcf9":"code","019cf86d":"code","06d3eb9a":"code","b64846e2":"code","a9c1739d":"code","55d9f75e":"code","21c42c7d":"code","f7539605":"code","ad05ab2d":"code","a4f1ad77":"code","d87c3295":"code","2b0991c3":"code","ccf60e03":"code","1e8ce884":"code","d187a037":"code","2f0fc3e0":"code","71a186bc":"code","7c902077":"code","40a489b7":"code","1ffaff89":"code","0906289c":"code","51437565":"code","e8e98ade":"code","e7bc6983":"code","f54a565f":"code","fb450b41":"code","6dbe740c":"markdown","4d5968f8":"markdown","03611dfe":"markdown","8a59ae95":"markdown","cdf0d8ec":"markdown","7986a4f0":"markdown","b41091ef":"markdown","f6e8ce68":"markdown","fce41320":"markdown","912e1d51":"markdown","255bf560":"markdown","2b9a9621":"markdown","3c7643fc":"markdown","5c53d14f":"markdown"},"source":{"3eafa216":"%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport pandas as pd\n\nimport os\n\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import f1_score\n","5c69d415":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nimport lightgbm as lgb","cf59f9bb":"# pip install lazypredict","fa43078f":"dataset_path = '..\/input\/dry-beans-classification-iti-ai-pro-intake01'\n\ntrain_df = pd.read_csv(os.path.join(dataset_path, 'train.csv'))\ntest_df = pd.read_csv(os.path.join(dataset_path, 'test.csv'))","048a9a47":"# Transform and inv transform function\n\ndef transform_to_num(data):\n    for i in range(len(data)):\n        if data[i] == \"BARBUNYA\":\n            data[i] = 1\n        elif data[i] == \"BOMBAY\":\n            data[i] = 2\n        elif data[i] == \"CALI\":\n            data[i] = 3\n        elif data[i] == \"DERMASON\":\n            data[i] = 4\n        elif data[i] == \"HOROZ\":\n            data[i] = 5\n        elif data[i] == \"SEKER\":\n            data[i] = 6\n        elif data[i] == \"SIRA\":\n            data[i] = 7\n    return (data)\n\ndef transform_to_cat(data):\n    for i in range(len(data)):\n        if data[i] == 1:\n            data[i] = \"BARBUNYA\"\n        elif data[i] == 2:\n            data[i] = \"BOMBAY\"\n        elif data[i] == 3:\n            data[i] = \"CALI\"\n        elif data[i] == 4:\n            data[i] = \"DERMASON\"\n        elif data[i] == 5:\n            data[i] = \"HOROZ\"\n        elif data[i] == 6:\n            data[i] = \"SEKER\"\n        elif data[i] == 7:\n            data[i] = \"SIRA\"\n    return (data)","83b130cc":"train_df.describe().T","2edea585":"colors = ['#550000', '#BB0000', '#FF0000', '#EEEE00', '#00FF00', '#00BB00', '#005500']\ndisplay(train_df[\"y\"].value_counts(),\n        px.histogram(data_frame=train_df,template='plotly_dark',x=\"y\",color=\"y\",color_discrete_sequence= colors));","c575b853":"px.scatter(train_df,hover_name=\"y\",template='plotly_dark',\n           x=\"MajorAxisLength\",y=\"MinorAxisLength\",color=\"y\",symbol=\"y\",color_discrete_sequence= colors)","2d0bc87f":"\nfig1 = px.box(data_frame=train_df,template='plotly_dark',\n       x=\"y\",y=[\"EquivDiameter\"],color=\"y\",width=800,height=350,title=\"EquivDiameter\")\nfig2 = px.box(data_frame=train_df,template='plotly_dark',\n       x=\"y\",y=[\"ConvexArea\"],color=\"y\",width=800,height=350,title=\"ConvexArea\")\nfig1.show()\nfig2.show()","d78516af":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ncorrl = train_df.copy()\ncorrl['y'] = le.fit_transform(corrl.y)\n\ncorr_martix = corrl.drop('ID',axis=1).corr()\nplt.subplots(figsize=(15,10))\nplt.title('Correlation between Features', size=18)\nsns.heatmap(corr_martix,linewidths=0.01,cmap=colors , annot=True)\nplt.show()","d0aec741":"from sklearn.preprocessing import LabelEncoder\n\n# le = LabelEncoder()\n# train_df['y'] = le.fit_transform(train_df.y)\ntrain_df['y'] = transform_to_num(train_df[\"y\"])\n#==============================================================\ndf = train_df.copy(deep=True) # Make a copy of original data, just in case\n\n# Create new dataframe for each type\n\ntypes = df['y'].unique()\nd = {type: df[df['y'] == type] for type in types}\n#==============================================================\n# Set the quantile\n\nlow = .25\nhigh = .75\n\nbounds = {}\nfor type in types:\n    filt_df = d[type].loc[:, d[type].columns != 'y'] # Remove 'Type' Column\n    quant_df = filt_df.quantile([low, high])\n    IQR = quant_df.iloc[1,:]-  quant_df.iloc[0,:]\n    quant_df.iloc[0,:] = quant_df.iloc[0,:] - 1.5*IQR\n    quant_df.iloc[1,:] = quant_df.iloc[1,:] + 1.5*IQR\n    bounds[type] = quant_df\n#==============================================================\n# Define our new dataset by removing the outliers \n\nfilt_df = d[1].loc[:, d[1].columns != 'y'] # Remove 'Type' Column\nfilt_df = filt_df.apply(lambda x: x[(x>bounds[1].loc[low,x.name]) & (x < bounds[1].loc[high,x.name])], axis=0)\nfilt_df = pd.concat([filt_df,d[1].loc[:,'y']], axis=1)\n\n# filt_df\n#==============================================================\n# Let's remove the outliers from the dataset \ndf_new = {}\n\nfor type in types:\n    filt_df = d[type].loc[:, d[type].columns != 'y'] # Remove 'Type' Column\n    filt_df = filt_df.apply(lambda x: x[(x>bounds[type].loc[low,x.name]) & (x < bounds[type].loc[high,x.name])], axis=0)\n    df_new[type] = pd.concat([filt_df,d[type].loc[:,'y']], axis=1)\n\n\nglassdata_new = result = pd.concat(df_new)\n# glassdata_new\n#==============================================================\n\n\ncols = glassdata_new.columns.drop('ID').drop('y')\n\n\nfor col in cols:\n  \n  glassdata_new[col] = glassdata_new[col].fillna(glassdata_new.groupby('y')[col].transform('mean'))\n\n#=========================================================================","5e6ae82b":"import imblearn\nfrom imblearn.over_sampling import SMOTE \nfrom collections import Counter\n# transform the dataset\noversample = SMOTE(random_state=42)\nnumerical_cat = glassdata_new.drop(\"ID\",axis=1).copy()\n# numerical_cat[\"y\"] = transform_to_num(numerical_cat[\"y\"]) #if u use toka's transformation","f746dcf9":"numerical_cat['y'] = numerical_cat['y'].astype('category')\nX, y = oversample.fit_resample(numerical_cat.drop(\"y\",axis=1), numerical_cat['y'])\n\nresampled_df = X.merge(y, left_index=True, right_index=True)\n\nresampled_df['y'] = resampled_df['y'].astype('int')\nresampled_df[\"y\"] = transform_to_cat(resampled_df[\"y\"])\n# counter = Counter(y)\n# print(counter)\n# resampled_df","019cf86d":"px.histogram(data_frame=resampled_df,template='plotly_dark',x=\"y\",color=\"y\",color_discrete_sequence= colors)","06d3eb9a":"px.scatter(resampled_df,hover_name=\"y\",template='plotly_dark',\n           x=\"MajorAxisLength\",y=\"MinorAxisLength\",color=\"y\",symbol=\"y\",color_discrete_sequence= colors)","b64846e2":"train_df.columns","a9c1739d":"glassdata_new = train_df.drop(['MajorAxisLength', 'MinorAxisLength','Perimeter','ConvexArea', 'EquivDiameter',],axis=1)\ntest_df = test_df.drop(['MajorAxisLength', 'MinorAxisLength','Perimeter','ConvexArea', 'EquivDiameter',],axis=1)\nresampled_df = resampled_df.drop(['MajorAxisLength', 'MinorAxisLength','Perimeter','ConvexArea', 'EquivDiameter',],axis=1)","55d9f75e":"# normalize the data\nfrom sklearn.preprocessing import StandardScaler\nstand = StandardScaler()\nstandard_df = np.copy(resampled_df.drop(columns=['y'])) ### use resampled_df -> drop y or train_df -> drop y and ID\nstandard_df\nstand.fit(standard_df)\nstandard_df = stand.transform(standard_df)               ### make changes down too","21c42c7d":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(standard_df,resampled_df[\"y\"], test_size=0.2,random_state=104)\nprint(np.shape(X_train),np.shape(X_test),np.shape(y_train),np.shape(y_test))","f7539605":"# ### importing lazypredict library\n# import lazypredict\n# ### importing LazyClassifier for classification problem\n# from lazypredict.Supervised import LazyClassifier","ad05ab2d":"# clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric = None)\n# ### fitting data in LazyClassifier\n# models,predictions = clf.fit(X_train, X_test, y_train, y_test)\n# ### lets check which model did better on Breast Cancer Dataset\n# print(models)","a4f1ad77":"# clf1 = LogisticRegression(max_iter=41,multi_class='multinomial',n_jobs=-1)\nclf2 = RandomForestClassifier(n_estimators=31,max_depth=27,n_jobs=-1,random_state=99)\nclf3 = lgb.LGBMClassifier(num_leaves = 88 , max_depth  = 11 , learning_rate  = 0.244 , n_estimators = 46,random_state=42)\nclf4 = MLPClassifier(learning_rate='adaptive',max_iter=92,random_state=38,alpha=0.0001,learning_rate_init=0.001)\nclf5 = SVC(gamma='auto')\nclf6 = XGBClassifier()\n\neclf1 = VotingClassifier(estimators=[ ('rf', clf2), ('lgbm', clf3),\n                                     ('MLP', clf4),('SVC', clf5),('XGB', clf6)],weights=[1,1,1,1,1],flatten_transform=True, voting='hard')\n\neclf1 = eclf1.fit(X = X_train,y= y_train)","d87c3295":"prediction_rfc = eclf1.predict(X_test)\nprint(f1_score(prediction_rfc, y_test,average=\"micro\"))#notsamp micro 0.9413936317489617 ,avg  0.9516809965435226\n#samp micro 0.9619838872104733 ,avg  0.9620112023000488","2b0991c3":"normal_data_standard = np.copy(glassdata_new.drop(columns=[\"ID\",'y']))\nnormal_data_standard = stand.transform(normal_data_standard)    \n############t########################o#############i#####d######i##21##6-21-11\nnormal_test_standard = np.copy(test_df.drop(columns=[\"ID\"]))\nnormal_test_standard = stand.transform(normal_test_standard)","ccf60e03":"glassdata_new[\"VC_prediction\"] = eclf1.predict(normal_data_standard)\ntest_df[\"VC_prediction\"] = eclf1.predict(normal_test_standard)","1e8ce884":"# normalize the normal data\nfrom sklearn.preprocessing import StandardScaler\nstand_inhanced = StandardScaler()\n\nstandard_inhanced_df = np.copy(glassdata_new.drop(columns=[\"ID\",\"VC_prediction\",'y'])) ### use resampled_df -> drop y or train_df -> drop y and ID\n\nstand_inhanced.fit(standard_inhanced_df)\nstandard_inhanced_df = stand_inhanced.transform(standard_inhanced_df)               ","d187a037":"standard_inhanced_df= pd.DataFrame(standard_inhanced_df)\nstandard_inhanced_df[\"VC_prediction\"] =transform_to_num(glassdata_new[\"VC_prediction\"]).astype('int')","2f0fc3e0":"test_to_standard = np.copy(test_df.drop(columns=[\"ID\",'VC_prediction']))\ntest_to_standard = stand_inhanced.transform(test_to_standard)\ntest_to_standard= pd.DataFrame(test_to_standard)\ntest_to_standard[\"VC_prediction\"] =transform_to_num(test_df[\"VC_prediction\"]).astype('int')","71a186bc":"display(test_to_standard.head(5),standard_inhanced_df.head(5))","7c902077":"glassdata_new[\"y\"] = glassdata_new[\"y\"].astype('int') #at toka's transformation","40a489b7":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(standard_inhanced_df,glassdata_new[\"y\"], stratify=glassdata_new[\"y\"])\nprint(np.shape(X_train),np.shape(X_test),np.shape(y_train),np.shape(y_test))","1ffaff89":"clf1 = LogisticRegression(max_iter=41,multi_class='multinomial',n_jobs=-1)\nclf2 = RandomForestClassifier(n_estimators=31,max_depth=27,n_jobs=-1,random_state=99)\nclf3 = lgb.LGBMClassifier(num_leaves = 88 , max_depth  = 11 , learning_rate  = 0.244 , n_estimators = 46,random_state=42)\nclf4 = MLPClassifier(learning_rate='adaptive',max_iter=92,random_state=38,alpha=0.0001,learning_rate_init=0.001)\nclf5 = SVC(gamma='auto')\nclf6 = XGBClassifier()\n\nAOT_VOT = VotingClassifier(estimators=[ ('rf', clf2), ('lgbm', clf3),\n                                     ('MLP', clf4),('SVC', clf5),('XGB', clf6)],weights=[1,1,1,1,1],flatten_transform=True, voting='hard')\n\nAOT_VOT = AOT_VOT.fit(X = X_train,y= y_train)\n\n\n\n# clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n# clf3 = GaussianNB()\n# clf4 = MLPClassifier(learning_rate='adaptive',max_iter=200,random_state=0,alpha=0.0001,learning_rate_init=0.001)\n# clf5 = SVC(gamma='auto')\n# clf6 = XGBClassifier()\n\n# AOT_VOT = VotingClassifier(estimators=[ ('rf', clf2), ('gnb', clf3),\n#                                      ('MLP', clf4),('SVC', clf5),('XGB', clf6)],weights=[1,1,3,2,2],flatten_transform=True, voting='hard')\n\n# AOT_VOT = AOT_VOT.fit(X_train,y_train)","0906289c":"prediction_AOT_VOT = AOT_VOT.predict(X_test)\nprint(f1_score(prediction_AOT_VOT, y_test,average=None).mean())","51437565":"test_df.head()","e8e98ade":"X_test = np.copy(test_df.drop(columns=[\"ID\",'VC_prediction']))\nX_test = stand_inhanced.transform(X_test)\nX_test = pd.DataFrame(X_test)\nX_test[\"VC_prediction\"] =transform_to_num(test_df[\"VC_prediction\"]).astype('int')\n\n\ny_test_predicted = AOT_VOT.predict(X_test)\n\ntest_df['y'] = y_test_predicted\n\ntest_df = test_df.drop(\"VC_prediction\",axis=1)\n","e7bc6983":"test_df[\"y\"] = transform_to_cat(test_df[\"y\"])\n","f54a565f":"test_df.head(5)","fb450b41":"test_df[['ID', 'y']].to_csv('\/kaggle\/working\/submission.csv', index=False)","6dbe740c":"## A) Data preprocessing and Splitting for resampled_df","4d5968f8":"**Voting Classifier for resampled_df**","03611dfe":"## Submission File Generation","8a59ae95":"# AOT Idea","cdf0d8ec":"**1- add predoction column using (Voting Classifier for resampled_df) to normal data**","7986a4f0":"# remove outliers","b41091ef":"# Lazy predictor and Voting Classifier","f6e8ce68":"**splitting**","fce41320":"## Import the libraries","912e1d51":"## Exploratory Data Analysis","255bf560":"**2- make your prediction on new data**","2b9a9621":"**Standardization**","3c7643fc":"# Training oversampling","5c53d14f":"# Drop Corr columns"}}