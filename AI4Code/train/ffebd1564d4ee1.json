{"cell_type":{"e6d647e6":"code","f9cb3a45":"code","55e2d9b3":"code","3c236a7b":"code","09e3e9ad":"code","d8cfdebf":"code","6defb21a":"code","1501befb":"code","3dc57bd2":"code","b8c003d8":"code","c2110b7c":"code","1acb3b76":"code","3ce707a0":"code","685e5638":"code","0fa8c459":"code","7d21ef10":"code","0b0c563b":"code","8dfdb23d":"code","fea89cdb":"code","ea6be6ef":"code","f5d8472e":"code","b44587ff":"code","b3ed2c7e":"code","144a5b88":"code","cce16ae8":"code","2e008534":"code","0c4b6a5b":"code","a5a1aa03":"code","87e7e4d4":"code","26491d4c":"code","122bbe19":"code","44e0cbef":"code","c02940dc":"code","1508712c":"code","505c2065":"code","97900270":"code","a40c3c1f":"code","e6de4aff":"code","e1042b74":"code","0412bee6":"code","72336aa7":"code","dd60e559":"code","f0b7c40f":"code","e055e61a":"code","0f5e04b3":"code","eb7ef8ed":"code","49cd6d2b":"code","e2d50066":"code","55dd9aca":"code","dc59313c":"code","704ec2be":"code","6d0a6c2a":"code","025a5b0f":"code","d698ffa1":"code","4e0cce05":"code","cffe83ba":"code","976ffbd7":"code","2b8c6525":"code","3369670a":"code","21a2da10":"code","aa01373e":"code","0bef31b1":"code","0f43128f":"code","9d86e48a":"code","49294b2a":"code","fdef9220":"code","17e05b33":"code","7d108a2f":"code","705901a5":"code","94caeaa0":"code","e76cde7f":"code","8a36fbb9":"code","dff3ab2e":"code","70927e1d":"code","fbf79f55":"code","005f8b47":"markdown","abc2654e":"markdown","238d93b9":"markdown","e36069c3":"markdown","21674e2c":"markdown","a7033aa2":"markdown","cf7afa9d":"markdown","dd6a1412":"markdown","e65f5144":"markdown","8426f4ef":"markdown","66e28c1a":"markdown","1e0433d0":"markdown","1ca5501b":"markdown","764b4143":"markdown","4967a8da":"markdown","bfac4da1":"markdown","d074b10b":"markdown","361d5e72":"markdown","f3169e98":"markdown","54c1698f":"markdown","1af8dec7":"markdown","5ecc39e5":"markdown","59ae5ef5":"markdown","41b750a0":"markdown","f1e51b38":"markdown","c7307c3b":"markdown","9db79435":"markdown","a58ba417":"markdown","e8389ebc":"markdown","6c61ae9f":"markdown","8fbe9bd5":"markdown","f0a6039f":"markdown","a434be17":"markdown","5b312c16":"markdown","7c974217":"markdown"},"source":{"e6d647e6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f9cb3a45":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set_style('whitegrid')","55e2d9b3":"data = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')","3c236a7b":"# Attribute Information\n# 1) id: unique identifier\n# 2) gender: \"Male\", \"Female\" or \"Other\"\n# 3) age: age of the patient\n# 4) hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n# 5) heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n# 6) ever_married: \"No\" or \"Yes\"\n# 7) work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n# 8) Residence_type: \"Rural\" or \"Urban\"\n# 9) avg_glucose_level: average glucose level in blood\n# 10) bmi: body mass index\n# 11) smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n# 12) stroke: 1 if the patient had a stroke or 0 if not\n# *Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient","09e3e9ad":"data.head()","d8cfdebf":"data.shape","6defb21a":"data.isnull().sum()","1501befb":"data.info()","3dc57bd2":"print('------max values per columns-----')\nprint(data.apply(np.max))\nprint('------min values per columns-----')\nprint(data.apply(np.min))","b8c003d8":"data.gender.unique()","c2110b7c":"data[data.gender == 'Other']","1acb3b76":"data.drop(data[data.gender == 'Other'].index, inplace = True)\ndata.gender.unique()","3ce707a0":"data.stroke.value_counts().plot(kind='pie', autopct=\"%.2f\", figsize=(6,6))","685e5638":"data.describe(include='float64')","0fa8c459":"fig, ax = plt.subplots(1,3, figsize=(12,6))\nax1 = plt.subplot(1,3,1)\nsns.boxplot(y='age', data=data, ax=ax1).set(title = 'Age', ylabel='')\nax2 = plt.subplot(1,3,2)\nsns.boxplot(y='avg_glucose_level', data=data, ax=ax2).set(title = 'Glucose level', ylabel='')\nax3 = plt.subplot(1,3,3)\nsns.boxplot(y='bmi', data=data, ax=ax3).set(title = 'Bmi', ylabel='')\nplt.show()","7d21ef10":"pd.crosstab(data.work_type, data.stroke).plot(kind='bar')\nplt.show()","0b0c563b":"object_type_columns = data.select_dtypes(include='object')","8dfdb23d":"object_type_columns.head()","fea89cdb":"fig = plt.figure(figsize=(20,12))\nn = 1\nfor column in object_type_columns.columns:\n    ax = plt.subplot(2,3,n).set(title=column)\n    sns.countplot(x='stroke', hue=column, data=data)\n    n += 1\nplt.show()","ea6be6ef":"fig = plt.figure(figsize=(20,12))\nn = 1\nstroke_1_data = data[data.stroke == 1]\nfor column in object_type_columns.columns:\n    ax = plt.subplot(2,3,n).set(title=column)\n    sns.countplot(x='stroke', hue=column, data=stroke_1_data)\n    n += 1\nplt.show()","f5d8472e":"zero_data = data.fillna(0)","b44587ff":"zero_bmi = zero_data[zero_data.bmi == 0]\nzero_bmi","b3ed2c7e":"# Nice way to have a dictionary with all names of columns separate by types.\n\n# grouping_types = data.columns.to_series().groupby(data.dtypes).groups\n# grouping_types\n# d = {key.name: value for key, value in grouping_types.items()}\n# d['int64'], d['float64'], d['object']\n# for col_name in d['object']:\n#     ... # etc.","144a5b88":"obj_column_list = zero_bmi.select_dtypes(include=['object']).columns\nobj_column_list","cce16ae8":"fig = plt.figure(figsize=(20, 12))\nx = 1\nfor name_of_col in obj_column_list:\n    ax = plt.subplot(2, 3, x)\n    sns.countplot(x=name_of_col, data=zero_bmi)\n    x += 1\nplt.show()","2e008534":"male_bmi_with_cond = data[(data.gender == 'Male') & (data.ever_married == 'Yes') & (data.work_type == 'Private')].bmi.mean().round(2)\nprint(male_bmi_with_cond, '- this is our \"bmi\" with conditions.')\nprint(data[(data.gender == 'Male')].bmi.mean().round(2), '- this is \"bmi\" mean for male')","0c4b6a5b":"female_bmi_with_cond = data[(data.gender == 'Female') & (data.ever_married == 'Yes') & (data.work_type == 'Private')].bmi.mean().round(2)\nprint(female_bmi_with_cond, '- this is our \"bmi\" with conditions.')\nprint(data[(data.gender == 'Female')].bmi.mean().round(2), '- this is \"bmi\" mean for female')","a5a1aa03":"data = data.fillna(0)","87e7e4d4":"data[data.gender == 'Male'] = data[data.gender == 'Male'].replace({'bmi': {0: male_bmi_with_cond}})","26491d4c":"data[data.gender == 'Female'] = data[data.gender == 'Female'].replace({'bmi': {0: female_bmi_with_cond}})","122bbe19":"data[data.bmi == 0]","44e0cbef":"plt.figure(figsize=(10,9))\nsns.heatmap(data[['age', 'avg_glucose_level', 'bmi']].corr(), annot=True)\nplt.show()","c02940dc":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn import tree\n\nimport warnings\nwarnings.filterwarnings('ignore')","1508712c":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter","505c2065":"new_data = data","97900270":"new_data.head()","a40c3c1f":"le = LabelEncoder()\nfor title in object_type_columns:\n    new_data[title] = le.fit_transform(new_data[title])","e6de4aff":"new_data.head()","e1042b74":"print('absolut values for whole data: ', Counter(new_data.stroke), sep='\\n')\nprint()\nprint('in percents for whole data: ', new_data.stroke.value_counts(normalize=True).round(2) * 100, sep='\\n')","0412bee6":"# I will try to do prediction with unbalanced data and then do UnderSampling and OverSampling (SMOTE) to equal \"stroke = 1\" values in train data.\n# And compare the scores","72336aa7":"X = new_data.drop(['id', 'stroke'], axis=1)","dd60e559":"X.head()","f0b7c40f":"y = new_data['stroke']","e055e61a":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nprint('absolut values for train set: ', Counter(y_train), sep='\\n')\nprint()\nprint('in percents for train set: ', y_train.value_counts(normalize=True).round(2) * 100, sep='\\n')","0f5e04b3":"DT_clf = tree.DecisionTreeClassifier()\nDT_clf.fit(X_train, y_train)","eb7ef8ed":"DT_prediction = DT_clf.predict(X_test)\nDT_metrics = metrics.f1_score(y_test, DT_prediction).round(2)\nDT_report = metrics.classification_report(y_test, DT_prediction)","49cd6d2b":"metrics.plot_confusion_matrix(DT_clf, X_test, y_test, cmap='Blues');\nplt.grid(False)\nprint(DT_report)","e2d50066":"RF_clf = RandomForestClassifier()\nRF_clf.fit(X_train, y_train)","55dd9aca":"RF_prediction = RF_clf.predict(X_test)\nRF_metrics = metrics.f1_score(y_test, RF_prediction).round(2)\nRF_report = metrics.classification_report(y_test, RF_prediction)","dc59313c":"metrics.plot_confusion_matrix(RF_clf, X_test, y_test, cmap='Blues')\nplt.grid(False)\nprint(RF_report)","704ec2be":"from imblearn.under_sampling import NearMiss\nnm = NearMiss()","6d0a6c2a":"print(\"Before Undersampling, counts of label '1': {}\".format(sum(y_train == 1)))\nprint(\"Before Undersampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))\n\nX_train_under, y_train_under = nm.fit_resample(X_train, y_train)\n\nprint('After Undersampling, the shape of train_X: {}'.format(X_train_under.shape))\nprint('After Undersampling, the shape of train_y: {} \\n'.format(y_train_under.shape))\n  \nprint(\"After Undersampling, counts of label '1': {}\".format(sum(y_train_under == 1)))\nprint(\"After Undersampling, counts of label '0': {}\".format(sum(y_train_under == 0)))","025a5b0f":"under_DT_clf = tree.DecisionTreeClassifier()\nunder_DT_clf.fit(X_train_under, y_train_under)\nunder_DT_prediction = under_DT_clf.predict(X_test)\nunder_DT_report = metrics.classification_report(y_test, under_DT_prediction)","d698ffa1":"metrics.plot_confusion_matrix(under_DT_clf, X_test, y_test, cmap='Blues')\nplt.grid(False)\nprint(under_DT_report)","4e0cce05":"under_RF_clf = RandomForestClassifier()\nunder_RF_clf.fit(X_train_under, y_train_under)\nunder_RF_prediction = under_RF_clf.predict(X_test)\nunder_DT_report = metrics.classification_report(y_test, under_RF_prediction)","cffe83ba":"metrics.plot_confusion_matrix(under_RF_clf, X_test, y_test, cmap='Blues')\nplt.grid(False)\nprint(under_DT_report)","976ffbd7":"smote = SMOTE(random_state=42)","2b8c6525":"X_res_train, y_res_train = smote.fit_resample(X_train, y_train)\nprint('Original train dataset has %s' % Counter(y_train))\nprint('Resampled train dataset has %s' % Counter(y_res_train))","3369670a":"print('Original train dataset shape is', X_train.shape)\nprint('Resampled train dataset shape is', X_res_train.shape)\nprint()\nprint('in percents for train original set: ', y_train.value_counts(normalize=True).round(2) * 100, sep='\\n')\nprint()\nprint('in percents for train resampled set: ', y_res_train.value_counts(normalize=True).round(2) * 100, sep='\\n')\n","21a2da10":"smote_DT_clf = tree.DecisionTreeClassifier()\nsmote_DT_clf.fit(X_res_train, y_res_train)\nsmote_DT_prediction = smote_DT_clf.predict(X_test)\nsmote_DT_report = metrics.classification_report(y_test, smote_DT_prediction)","aa01373e":"metrics.plot_confusion_matrix(smote_DT_clf, X_test, y_test, cmap='Blues')\nplt.grid(False)\nprint(smote_DT_report)","0bef31b1":"smote_RF_clf = RandomForestClassifier()\nsmote_RF_clf.fit(X_res_train, y_res_train)\nsmote_RF_prediction = smote_RF_clf.predict(X_test)\nsmote_RF_report = metrics.classification_report(y_test, smote_RF_prediction)","0f43128f":"metrics.plot_confusion_matrix(smote_RF_clf, X_test, y_test, cmap='Blues')\nplt.grid(False)\nprint(smote_RF_report)","9d86e48a":"smote_LR_clf = LogisticRegression()\nsmote_LR_clf.fit(X_res_train, y_res_train)\nsmote_LR_prediction = smote_LR_clf.predict(X_test)\nsmote_LR_report = metrics.classification_report(y_test, smote_LR_prediction)","49294b2a":"metrics.plot_confusion_matrix(smote_LR_clf, X_test, y_test, cmap='Blues')\nplt.grid(False)\nprint(smote_LR_report)","fdef9220":"smote_KN_clf = KNeighborsClassifier()\nsmote_KN_clf.fit(X_res_train, y_res_train)\nsmote_KN_prediction = smote_KN_clf.predict(X_test)\nsmote_KN_report = metrics.classification_report(y_test, smote_KN_prediction)","17e05b33":"metrics.plot_confusion_matrix(smote_KN_clf, X_test, y_test, cmap='Blues')\nplt.grid(False)\nprint(smote_KN_report)","7d108a2f":"f1_score = [metrics.f1_score(y_test, smote_DT_prediction),\n            metrics.f1_score(y_test, smote_RF_prediction),\n            metrics.f1_score(y_test, smote_LR_prediction),\n            metrics.f1_score(y_test, smote_KN_prediction),\n            metrics.f1_score(y_test, DT_prediction),\n            metrics.f1_score(y_test, RF_prediction),\n            metrics.f1_score(y_test, under_DT_prediction),\n            metrics.f1_score(y_test, under_RF_prediction)]","705901a5":"roc_score = [metrics.roc_auc_score(y_test, smote_DT_prediction), \n            metrics.roc_auc_score(y_test, smote_RF_prediction),\n            metrics.roc_auc_score(y_test, smote_LR_prediction),\n            metrics.roc_auc_score(y_test, smote_KN_prediction),\n            metrics.roc_auc_score(y_test, DT_prediction),\n            metrics.roc_auc_score(y_test, RF_prediction),\n            metrics.roc_auc_score(y_test, under_DT_prediction),\n            metrics.roc_auc_score(y_test, under_RF_prediction)]","94caeaa0":"precision = [metrics.precision_score(y_test, smote_DT_prediction), \n            metrics.precision_score(y_test, smote_RF_prediction),\n            metrics.precision_score(y_test, smote_LR_prediction),\n            metrics.precision_score(y_test, smote_KN_prediction),\n            metrics.precision_score(y_test, DT_prediction),\n            metrics.precision_score(y_test, RF_prediction),\n            metrics.precision_score(y_test, under_DT_prediction),\n            metrics.precision_score(y_test, under_RF_prediction)]","e76cde7f":"recall = [metrics.recall_score(y_test, smote_DT_prediction), \n            metrics.recall_score(y_test, smote_RF_prediction),\n            metrics.recall_score(y_test, smote_LR_prediction),\n            metrics.recall_score(y_test, smote_KN_prediction), \n            metrics.recall_score(y_test, DT_prediction),\n            metrics.recall_score(y_test, RF_prediction),\n            metrics.recall_score(y_test, under_DT_prediction),\n            metrics.recall_score(y_test, under_RF_prediction)]","8a36fbb9":"df = pd.DataFrame({'label':['Decision Tree', 'Random Forest', 'Logistic Regression', 'KNeighbors', \n                            'Decision Tree (raw)', 'Random Forest(raw)', 'Decision Tree (undersample)', 'Random Forest (undersample)'], \n                   'f1_score': f1_score, 'roc_score': roc_score, 'precision_score': precision, 'recall_score': recall})","dff3ab2e":"df","70927e1d":"from sklearn import preprocessing\n\narray = df.iloc[:,1:].values #returns a numpy array\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(array)\nnormalize_results = pd.DataFrame(x_scaled, index=df.label, columns=df.iloc[:,1:].columns)","fbf79f55":"normalize_results.sort_values('f1_score', ascending=False)","005f8b47":"Only \"bmi = 0\" rows","abc2654e":"Good way to look for invalid values (like: 'space' or other symbols)","238d93b9":"**Summarize main metrics**","e36069c3":"Correlation between cont values aren't observed","21674e2c":"As we can see these meanings a bit different from just \"mean\". I will use them for main data","a7033aa2":"For prediction we have to convert object-type columns to numeric","cf7afa9d":"I would like to change NaN values in 'bmi' column separetly for genders. And look at some features in 'bmi = NaN'","dd6a1412":"**Summary:**\n* Visualized some aspects of data\n* Cleared missing values\n* Has been prepared data for Machine Learning\n* Looked on unbalanced data prediction, and did conclusion that this modeling is not valid\n* Did undersampling and oversampling (SMOTE)\n* Has been used different prediction models\n* Given some final observation for choosing model for further tuning","e65f5144":"Select \"data.dtypes == object\" for visual analise","8426f4ef":"We can see that larger group of people are \"ever_married\" is \"Yes\" and \"work_type\" is \"Private\". Let check the bmi.mean for different genders with this addition. And change missing values in bmi column","66e28c1a":"We have to convert all categorical columns to numeric data. I will use \"Label Encoder\" for it.","1e0433d0":"\"object_type_columns\" is list with title of object columns. I used it earlier","1ca5501b":"Ok, now it's good. Our data became bigger","764b4143":"Exrtact only object-type columns","4967a8da":"**Missing values**","bfac4da1":"It makes some difference","d074b10b":"A bit suspicious gender 'other'","361d5e72":"Normalize prediction results (the best result is \"1\", the worst - \"0\")","f3169e98":"Much more prettier","54c1698f":"**Prediction (original data, undersampling, oversampling)**","1af8dec7":"Very unbalanced data in column 'stroke', do this again but only for 'stroke = 1'","5ecc39e5":"Visualize object-type columns","59ae5ef5":"Drop useless 'id' column","41b750a0":"I think, it's enough. Now I will balance 0 and 1 levels in train data.","f1e51b38":"That's it.","c7307c3b":"1 - 'true positive'. Disaster xD ","9db79435":"It's interesting. We see that undersample model give most mark \"1\" (1282) but 1217 is false. In contrast with raw data, there was mostly \"0\" mark. Let's check OverSampling","a58ba417":"**Observation:**\n* Undersampled sets has best recall score, but terrible precision (even worse than raw data)\n* ROC AUC score is higher in Logistic Regression and KNeighbors\n* In my opinion, in the same tasks we need maximaze F1 score. Because it is serious medical task: predict srtoke for patient. We have to focus on recall (don't miss \"false positive\", - apparently ill man). And minimaze costs, if possible (don't expect any patient, - gain precision). \n* That's why \"KNeighbors\" fits better that others","e8389ebc":"**UNDERSAMPLING**","6c61ae9f":"Just 1 id, I'll drop it","8fbe9bd5":"Accuracy score doesn't matter in this case. This model predict \"true positive\" cases terribly. F1-score and recall for \"1\" cases are bad. \nMaybe Random Forest does it better?","f0a6039f":"Only bmi column has NaN values","a434be17":"We have some missing values in 'bmi' column","5b312c16":"Changed \"NaN\" values to \"0\"","7c974217":"Here I'm trying to make prediction with undersampling data. I will reduce train data, and have a look at our results"}}