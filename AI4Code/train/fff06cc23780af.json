{"cell_type":{"678b1fd6":"code","630a433b":"code","3a20eec9":"code","456bccd3":"code","119f213e":"code","92cf556b":"code","62a21a36":"code","64c2432e":"code","1f777b67":"code","4cfcdabc":"code","cd453838":"code","4f0351d2":"code","72ab277b":"code","7f21e999":"code","71c4d190":"code","e88cbedf":"code","417d4f6a":"code","f755f7f6":"code","60d8438c":"code","2b764147":"code","c4255350":"code","dc60f0a3":"code","7b8eb1f3":"code","80177f04":"code","da2abeff":"code","96e6a9a0":"code","49bc693b":"code","5b4a45a1":"code","c192e7b2":"code","46bb8e4e":"code","c462ab17":"code","868ce40d":"code","240688db":"code","df244712":"code","72abb2cb":"code","a39caf71":"code","04832928":"code","24522bd4":"code","8bae0bd7":"code","fbc48ffe":"code","17f9644d":"code","175cf557":"code","f57c4889":"code","d885d4f7":"code","797fb357":"code","5d317695":"code","2076f377":"code","c1743902":"code","f4959e02":"code","ce744392":"code","cc3604c5":"code","eb767c3b":"code","8a56fe1f":"code","f99cbd9b":"code","6abc57d0":"code","7cbb45db":"code","67fa293b":"code","4262fe8c":"code","c41bacda":"code","6ae9a880":"code","e312cb70":"code","8d8acbc2":"markdown","6294eebb":"markdown","36a33582":"markdown","cf1da96b":"markdown","d74c5de3":"markdown","ce9021a0":"markdown","1a4ecacf":"markdown","c676a6d0":"markdown","47368e1f":"markdown","454bd4ce":"markdown","87222582":"markdown","b15f65e7":"markdown","cb8fc7ee":"markdown","5a603cbf":"markdown","21cd30f2":"markdown","f8135651":"markdown","b61ba8ec":"markdown","e98b7e0d":"markdown","f31fa490":"markdown","d50a78be":"markdown"},"source":{"678b1fd6":"import pandas as pd ","630a433b":"df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","3a20eec9":"df.head()","456bccd3":"df['target'].value_counts()\r\n","119f213e":"df.isna().sum()","92cf556b":"df.fillna('',inplace=True)","62a21a36":"#library that contains punctuation\r\nimport string\r\nstring.punctuation","64c2432e":"\r\ndef remove_punctuation(text):\r\n    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\r\n    return punctuationfree\r\n","1f777b67":"df['text']= df['text'].apply(lambda x:remove_punctuation(x))","4cfcdabc":"df['text']= df['text'].apply(lambda x: x.lower())","cd453838":"\r\ndef tokenize(string):\r\n    '''\r\n    Tokenizes the string to a list of words\r\n    '''\r\n    tokens = string.split()\r\n    return tokens\r\n","4f0351d2":"df['text']= df['text'].apply(lambda x: tokenize(x))","72ab277b":"df.head()","7f21e999":"df['keyword']= df['keyword'].apply(lambda x: tokenize(x))","71c4d190":"df.tail()","e88cbedf":"df.drop(columns=['id'],inplace=True)","417d4f6a":"df.head()","f755f7f6":"#importing nlp library\r\nimport nltk\r\n#Stop words present in the library\r\nstopwords = nltk.corpus.stopwords.words('english')","60d8438c":"def remove_stopwords(text):\r\n    output= [i for i in text if i not in stopwords]\r\n    return output","2b764147":"df['text']= df['text'].apply(lambda x:remove_stopwords(x))","c4255350":"from nltk.stem.porter import PorterStemmer\r\nporter_stemmer = PorterStemmer()","dc60f0a3":"def stemming(text):\r\n    stem_text = [porter_stemmer.stem(word) for word in text]\r\n    return stem_text\r\ndf['text']=df['text'].apply(lambda x: stemming(x))","7b8eb1f3":"from nltk.stem import WordNetLemmatizer\r\n#defining the object for Lemmatization\r\nwordnet_lemmatizer = WordNetLemmatizer()","80177f04":"#defining the function for lemmatization\r\ndef lemmatizer(text):\r\n    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\r\n    return lemm_text","da2abeff":"nltk.download('wordnet')","96e6a9a0":"# df['text']=df['text'].apply(lambda x: lemmatizer(x))","49bc693b":"vocab = []\r\n\r\n'''\r\nWe add all the lists of tokenized strings to make one large list of words\r\n\r\nNote ['a','b'] + ['c'] = ['a','b','c']\r\n\r\n'''\r\n\r\nfor i in df['text'].values:\r\n    vocab = vocab + i\r\n\r\nprint(len(vocab))\r\n","5b4a45a1":"# We make a set of the vocab words to remove multiple occurences of a same word, implying only unique words stay in set.\r\n\r\nset_vocab = set(vocab)\r\nvocab = list(set_vocab)\r\n# we convert that set back to a list\r\nprint(len(vocab),type(vocab))","c192e7b2":"## Converting the tokens back to strings to feed it into Count Vectorizer\r\n\r\ndf['text_strings'] = df['text'].apply(lambda x: ' '.join([str(elem) for elem in x]))","46bb8e4e":"df['text_strings'].head()","c462ab17":"from sklearn.feature_extraction.text import CountVectorizer","868ce40d":"vectorizer = CountVectorizer()","240688db":"X = vectorizer.fit_transform(df['text_strings'])","df244712":"x_train = X.toarray()\r\n\r\n## The text is now vectorized","72abb2cb":"import numpy as np","a39caf71":"import numpy as nper\r\nx_train = np.array(x_train)\r\n\r\n# This x_train can be used directly to train a model","04832928":"y_train = df['target']","24522bd4":"x_train.shape","8bae0bd7":"y_train.shape","fbc48ffe":"from sklearn.linear_model import LogisticRegression","17f9644d":"clf = LogisticRegression(random_state=42)","175cf557":"clf.fit(x_train,y_train)","f57c4889":"pred = clf.predict(x_train)","d885d4f7":"from sklearn.metrics import accuracy_score","797fb357":"accuracy_score(y_train, pred)","5d317695":"df_test =pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","2076f377":"df_test.fillna('',inplace=True)","c1743902":"df_test.drop(columns=['id','keyword','location'],inplace=True)","f4959e02":"df_test['text']= df_test['text'].apply(lambda x:remove_punctuation(x))\r\ndf_test['text']= df_test['text'].apply(lambda x: tokenize(x))\r\ndf_test['text']= df_test['text'].apply(lambda x:remove_stopwords(x))\r\ndf_test['text']= df_test['text'].apply(lambda x: stemming(x))","ce744392":"df_test['text_strings'] = df_test['text'].apply(lambda x: ' '.join([str(elem) for elem in x]))","cc3604c5":"x_test = vectorizer.transform(df_test['text_strings'])","eb767c3b":"x_test = x_test.toarray()","8a56fe1f":"x_test = np.array(x_test)","f99cbd9b":"y_test_pred = clf.predict(x_test)","6abc57d0":"y_test_pred","7cbb45db":"import pandas as pd","67fa293b":"submission = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","4262fe8c":"submission['target'] = y_test_pred","c41bacda":"submission.head()","6ae9a880":"final_submission = submission[['id','target']]","e312cb70":"final_submission.to_csv('final_submission.csv')","8d8acbc2":"# Final Comments","6294eebb":"# Vectorizing the data","36a33582":"# Fitting a model","cf1da96b":"### This notebook contains binary classification of of text data by sentiment analysis. The data can be found [here](https:\/\/www.kaggle.com\/c\/nlp-getting-started\/data).\r\n### The idea behind this notebook is very simple, does not use fancy and complex models, cleans the data, encodes it in one hot vectors and trains a logistic regression model on it.\r\n","d74c5de3":"# Stemming","ce9021a0":"# Removing stop words","1a4ecacf":"# A beginners guide to sentiment nalysis","c676a6d0":"## Vectorizing the text data","47368e1f":"# Tokenizing ","454bd4ce":"# Lemmatization","87222582":"# Obtaining x_train and y_train","b15f65e7":"### The data is fairly balanced with 4342 examples of a fake disaster and 3271 examples of a real disaster","cb8fc7ee":"# Putting the predictions to test csv","5a603cbf":"# Removing punctuation","21cd30f2":"## Preprocessing the test set","f8135651":"# Lowering the case","b61ba8ec":"# Testing the model on test set","e98b7e0d":"## The aim of this notebook is to predict if a tweet is of a fake disaster or a distress signal of a real disaster.","f31fa490":"### Having created this notebook from scratch, starting afresh in the field of NLP,it would be invaluable to have you comment on this notebook to tell me what could have I done better, what I did right and what must never be done. Do comment ;)","d50a78be":"### Building the vocabulary\r\n### In another approach, we could have used the vocabulary, but in this approach, we do not use the vocabulary so if you want to reproduce this code, you can safely omit this section"}}