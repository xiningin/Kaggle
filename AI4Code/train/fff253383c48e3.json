{"cell_type":{"55bb2310":"code","5347c418":"code","aec54740":"code","5dc74d91":"code","45e3cd9b":"code","11e8f8bb":"code","d794736a":"code","3a8226aa":"code","859d136c":"code","920c743f":"code","f661f13f":"code","de7109b7":"code","bda98c0c":"code","ca558a25":"code","5093d758":"code","b596aa52":"code","efb12c23":"code","02e85f53":"code","e4083300":"code","259a812f":"code","e5f70ee0":"code","8f28a306":"code","52036d75":"code","1c8e66e0":"code","7986b9c9":"code","c9230040":"code","99e69cca":"code","3d2fb229":"code","fb6e4d1f":"code","4cd82aa9":"code","f24db4ef":"code","0ff6d57d":"code","98c7aadd":"code","2f2a8185":"code","e5b26b88":"code","d1b3c7b6":"code","d8ca181c":"code","5881478f":"code","19f3349a":"code","9e7c6874":"code","858dbc77":"code","68e1f8d9":"code","f6478f6c":"code","b4faebd9":"code","bd190220":"code","70a52015":"code","c978f2ca":"code","0c811bad":"code","eef0722d":"code","6477cc24":"code","578324e5":"code","f8f8e805":"code","264867fe":"code","c11f14a0":"code","9c924693":"code","76b4c700":"code","f85e00b0":"code","cb5629a0":"code","3d7a35d7":"code","2adc6f82":"code","7067350f":"code","20204967":"code","30b145a8":"code","f62904ac":"markdown","31f84cc1":"markdown","09dfb803":"markdown","87de5236":"markdown","108f7aae":"markdown","121c25a5":"markdown","543dbcee":"markdown","db4e5e41":"markdown","ec7fba5f":"markdown","5f74f30a":"markdown","b511f968":"markdown","7ce24046":"markdown","e8749791":"markdown","298ff8cf":"markdown","bbd0ffa4":"markdown","97cd814d":"markdown","0357a42f":"markdown","7b9fbcbf":"markdown","125c6743":"markdown","d09b9e11":"markdown","8be07dec":"markdown","db6ec6fc":"markdown","fad8ef4b":"markdown","02a27d50":"markdown","a6426d2a":"markdown","6874bdee":"markdown"},"source":{"55bb2310":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","5347c418":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","aec54740":"df=pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip', low_memory=False , sep=',') \nprint(df)\nprint(df.shape)","5dc74d91":"#List the fields in our dataframe\nprint(df.dtypes)","45e3cd9b":"# below line causes shuffling of indices, to avoid using train_test_split later\ndf = df.reindex(np.random.permutation(df.index))","11e8f8bb":"print(df)","d794736a":"comment = df['comment_text']\nprint(comment.head())\ncomment = comment.to_numpy()","3a8226aa":"label = df[['toxic', 'severe_toxic' , 'obscene' , 'threat' , 'insult' , 'identity_hate']]\nprint(label.head())\nlabel = label.to_numpy()","859d136c":"print(label)","920c743f":"# ct1 counts samples having atleast one label\n# ct2 counts samples having 2 or more than 2 labels\nct1,ct2 = 0,0\nfor i in range(label.shape[0]):\n    ct = np.count_nonzero(label[i])\n    if ct :\n        ct1 = ct1+1\n    if ct>1 :\n        ct2 = ct2+1\nprint(ct1)\nprint(ct2)","f661f13f":"x = [len(comment[i]) for i in range(comment.shape[0])]\n\nprint('average length of comment: {:.3f}'.format(sum(x)\/len(x)) )\nbins = [1,200,400,600,800,1000,1200]\nplt.hist(x, bins=bins)\nplt.xlabel('Length of comments')\nplt.ylabel('Number of comments')       \nplt.axis([0, 1200, 0, 90000])\nplt.grid(True)\nplt.show()","de7109b7":"y = np.zeros(label.shape)\nfor ix in range(comment.shape[0]):\n    l = len(comment[ix])\n    if label[ix][0] :\n        y[ix][0] = l\n    if label[ix][1] :\n        y[ix][1] = l\n    if label[ix][2] :\n        y[ix][2] = l\n    if label[ix][3] :\n        y[ix][3] = l\n    if label[ix][4] :\n        y[ix][4] = l\n    if label[ix][5] :\n        y[ix][5] = l\n\nlabelsplt = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\ncolor = ['red','green','blue','yellow','orange','chartreuse']        \nplt.hist(y,bins = bins,label = labelsplt,color = color)\nplt.axis([0, 1200, 0, 8000])\nplt.xlabel('Length of comments')\nplt.ylabel('Number of comments') \nplt.legend()\nplt.grid(True)\nplt.show()","bda98c0c":"comments = []\nlabels = []\n\nfor i in range(comment.shape[0]):\n    if len(comment[i])<=400:\n        comments.append(comment[i])\n        labels.append(label[i])\n\nlabels = np.asarray(labels)","ca558a25":"print(len(comments))","5093d758":"import string\nprint(string.punctuation)","b596aa52":"#Removing apostrophe character(') to prevent, words won't,don't.. to be coverted into wont,dont... and adding \"0123456789\". \npunctuation_edit = string.punctuation.replace('\\'','') +\"0123456789\"\nprint (punctuation_edit)","efb12c23":"# maketrans() returns a translation table that maps each character in the punctuation_edit into the character at the same position in the outtab string.\nouttab = \"                                         \"\ntrantab = str.maketrans(punctuation_edit, outtab)","02e85f53":"import nltk\nnltk.download('stopwords')","e4083300":"from nltk.corpus import stopwords\n# Initialize the stopwords\nstop_words = stopwords.words('english')\nprint(stop_words)","259a812f":"stop_words.append('')\nfor x in range(ord('b'), ord('z')+1):\n    stop_words.append(chr(x))\nprint(stop_words)","e5f70ee0":"from nltk.stem import PorterStemmer, WordNetLemmatizer","8f28a306":"#create objects for stemmer and lemmatizer\nlemmatiser = WordNetLemmatizer()\nstemmer = PorterStemmer()\n#download words from wordnet library\nnltk.download('wordnet')","52036d75":"for i in range(len(comments)):\n    comments[i] = comments[i].lower().translate(trantab)\n    l = []\n    for word in comments[i].split():\n        l.append(stemmer.stem(lemmatiser.lemmatize(word,pos=\"v\")))\n    comments[i] = \" \".join(l)","1c8e66e0":"type(comments), len(comments)","7986b9c9":"#import required library\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#create object supplying our custom stop words\ncount_vector = CountVectorizer(stop_words=stop_words)\n\ntf = count_vector.fit_transform(comments).toarray()","c9230040":"print(tf.shape)","99e69cca":"def shuffle(matrix, target, test_proportion):\n    ratio = int(matrix.shape[0]\/test_proportion)\n    X_train = matrix[ratio:,:]\n    X_test =  matrix[:ratio,:]\n    Y_train = target[ratio:,:]\n    Y_test =  target[:ratio,:]\n    return X_train, X_test, Y_train, Y_test\n\nX_train, X_test, Y_train, Y_test = shuffle(tf, labels,3)\n\nprint(X_test.shape)\nprint(X_train.shape)","3d2fb229":"from sklearn.metrics import hamming_loss\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\n\ndef evaluate_score(Y_test,predict): \n    loss = hamming_loss(Y_test,predict)\n    print(\"Hamming_loss : {}\".format(loss*100))\n    accuracy = accuracy_score(Y_test,predict)\n    print(\"Accuracy : {}\".format(accuracy*100))\n    try : \n        loss = log_loss(Y_test,predict)\n    except :\n        loss = log_loss(Y_test,predict.toarray())\n    print(\"Log_loss : {}\".format(loss))","fb6e4d1f":"pip install scikit-multilearn","4cd82aa9":"from sklearn.naive_bayes import MultinomialNB\n#clf will be the list of the classifiers for all the 6 labels\n# each classifier is fit with the training data and corresponding classifier\nclf = []\nfor ix in range(6):\n    clf.append(MultinomialNB())\n    clf[ix].fit(X_train,Y_train[:,ix])","f24db4ef":"# predict list contains the predictions, it is transposed later to get the proper shape\npredict = []\nfor ix in range(6):\n    predict.append(clf[ix].predict(X_test))\n\npredict = np.asarray(np.transpose(predict))\nprint(predict.shape)","0ff6d57d":"evaluate_score(Y_test,predict)","98c7aadd":"#create and fit classifier\nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.svm import SVC\nclassifier = BinaryRelevance(classifier = SVC(), require_dense = [False, True])\nclassifier.fit(X_train, Y_train)","2f2a8185":"#predictions\npredictions = classifier.predict(X_test)\n#calculate scores\nevaluate_score(Y_test,predictions)","e5b26b88":"#create and fit classifier\nclassifier = BinaryRelevance(classifier = MultinomialNB(), require_dense = [False, True])\nclassifier.fit(X_train, Y_train)","d1b3c7b6":"#predictions\npredictions = classifier.predict(X_test)\n#calculate scores\nevaluate_score(Y_test,predictions)","d8ca181c":"from sklearn.naive_bayes import GaussianNB\n#create and fit classifiers\nclf = []\nfor ix in range(6):\n    clf.append(GaussianNB())\n    clf[ix].fit(X_train,Y_train[:,ix])","5881478f":"#predictions\npredict = []\nfor ix in range(6):\n    predict.append(clf[ix].predict(X_test))","19f3349a":"#calculate scores\npredict = np.asarray(np.transpose(predict))\nevaluate_score(Y_test,predict)","9e7c6874":"#create and fit classifier\nfrom skmultilearn.problem_transform import ClassifierChain\nclassifier = ClassifierChain(MultinomialNB())\nclassifier.fit(X_train, Y_train)","858dbc77":"#predictions\npredictions = classifier.predict(X_test)\n#calculate scores\nevaluate_score(Y_test,predictions)","68e1f8d9":"#create and fit classifier\nfrom skmultilearn.problem_transform import LabelPowerset\nclassifier = LabelPowerset(MultinomialNB())\nclassifier.fit(X_train, Y_train)","f6478f6c":"#predictions\npredictions = classifier.predict(X_test)\nevaluate_score(Y_test,predictions)","b4faebd9":"#create and fit classifier\nfrom skmultilearn.adapt import MLkNN\nclassifier = MLkNN(k=2)\nclassifier.fit(X_train, Y_train)","bd190220":"#predictions\npredictions = classifier.predict(X_test)\n#calculate scores\nevaluate_score(Y_test,predictions)","70a52015":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout","c978f2ca":"#define model architecture\nmodel = Sequential()\nmodel.add(Dense(4, activation='relu', input_dim = X_train.shape[1]))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(6, activation='softmax'))\nmodel.summary()","0c811bad":"#compile model with all parameters set\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","eef0722d":"#Fit using check pointer\nfrom keras.callbacks import ModelCheckpoint  \n\ncheckpointer = ModelCheckpoint(filepath='saved_models\/weights.best.myneural.h5py', \n                               verbose=1, save_best_only=True)\nmodel.fit(X_train, Y_train, epochs=10, batch_size=32)","6477cc24":"#predictions\npredict = model.predict(X_test)","578324e5":"#calculate score\nloss = log_loss(Y_test,predict)\nprint(\"Log_loss : {}\".format(loss))\npredict = np.round(predict)\nloss = hamming_loss(Y_test,predict)\nprint(\"Hamming_loss : {}\".format(loss*100))\naccuracy = accuracy_score(Y_test,predict)\nprint(\"Accuracy : {}\".format(accuracy*100))","f8f8e805":"from sklearn.model_selection import GridSearchCV\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras import optimizers\n\n#define parameters for using in param grid\nnodes = [16, 32, 64] # number of nodes in the hidden layer\nlrs = [0.001, 0.002, 0.003] # learning rate, default = 0.001\nepochs = [10,20,30]\nbatch_size = 64","264867fe":"def create_model(nodes=10,lr=0.001):\n    model = Sequential()\n    model.add(Dense(nodes, activation='relu', input_dim = X_train.shape[1]))\n    model.add(Dropout(0.3))\n    model.add(Dense(6, activation='softmax'))\n    opt = optimizers.RMSprop(lr=lr)\n    model.compile(optimizer=opt,\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n    return model\n\nmodel = KerasClassifier(build_fn=create_model)","c11f14a0":"#start fitting process\nparam_grid = dict(epochs=epochs,nodes=nodes, lr=lrs)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1,refit=True,verbose=2)\ngrid_result = grid.fit(X_train, Y_train)","9c924693":"print(grid_result)","76b4c700":"print('Best estimator : {}'.format (grid.best_estimator_))\nprint('Best score : {}'.format(grid.best_score_))\nprint('Best params : {}'.format(grid.best_params_))","f85e00b0":"print(grid.cv_results_)","cb5629a0":"#predictions\npredictions = grid.predict(X_test)","3d7a35d7":"#predictions\npredict = grid.predict_proba(X_test)\nprint(predict.shape)","2adc6f82":"#calculate score\nloss = log_loss(Y_test,predict)\nprint(\"Log_loss : {}\".format(loss))\npredict = np.round(predict)\nloss = hamming_loss(Y_test,predict)\nprint(\"Hamming_loss : {}\".format(loss*100))\naccuracy = accuracy_score(Y_test,predict)\nprint(\"Accuracy : {}\".format(accuracy*100))","7067350f":"import matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.cm as cm\nimport itertools","20204967":"x = ['BR-MultNB','BR-GausNB','BR-SVC','CC-MultNB','LP-MultNB','BP-MLL-ini','BP-MLL-fin']\ny = [3.27,20.74,4.26,3.56,3.17,13.96,15.158]\ncolors = itertools.cycle(['b', 'g', 'r', 'c', 'm', 'y', 'k'])\nplt.ylabel('Hamming-Loss')\nplt.xlabel('Model-details')\nplt.xticks(rotation=90)\nfor i in range(len(y)):\n    plt.bar(x[i], y[i], color=next(colors))\nplt.show()","30b145a8":"x = ['BR-MultNB','BR-GausNB','BR-SVC','CC-MultNB','LP-MultNB','BP-MLL-ini','BP-MLL-fin']\ny = [1.92,1.422,0.46,1.5,1.47,0.36,0.35]\ncolors = itertools.cycle(['b', 'g', 'r', 'c', 'm', 'y', 'k'])\nplt.ylabel('Log-Loss')\nplt.xlabel('Model-details')\nplt.xticks(rotation=90)\nfor i in range(len(y)):\n    plt.bar(x[i], y[i], color=next(colors))\nplt.show()","f62904ac":"**8. BP-MLL Neural Networks (from scratch) (Back propagation Multi-label Neural Networks)**","31f84cc1":"**improving the BP-MLL model**","09dfb803":"**1. Binary Relevance (BR) Method with MultinomialNB classifiers**","87de5236":"**Applying Count Vectorizer\n\nHere we can finally convert our comments into a matrix of token counts, which signifies the number of times it occurs.**","108f7aae":"Find out the frequency of occurence of multilabelled data","121c25a5":"**Stemming and Lemmatizing**","543dbcee":"**2. BR Method with SVM classifier (from scikit-multilearn)**","db4e5e41":"**We can now, loop once through all the comments applying :**\n\n* punctuation removal\n* splitting the words by space\n* applying stemmer and lemmatizer\n* recombining the words again for further processing","ec7fba5f":"**Splitting dataset into training and testing**","5f74f30a":"**Log Loss**","b511f968":"**6. Label Powerset with MultinomialNB classifier (from scikit-multilearn)**","7ce24046":"Comments classified as toxic,severe_toxic,.. etc depending on numbers of comments and their lengths","e8749791":"**Finalising Evaluation Metric - Example based metrics**\n\n**1. Label based metrics**\n\nIt includes one-error, average precision, etc. These are calculated separately for each of the labels, and then averaged for all without taking into account any relation between the labels.\n\n**2. Example based metrics**\n\nIt include accuracy, hamming loss, etc.These are calculated for each example and then averaged across the test set.\n\n**defining the evaluation metrics**","298ff8cf":"**5. Classifier chain with MultinomialNB classifier (from scikit-multilearn)**","bbd0ffa4":"**Data Visualisations**\n\nAnalyse the no. of comments having lengths varying from 0 to 1200","97cd814d":"**4. BR Method with GausseanNB classifier (from scratch)**","0357a42f":"**Data Preprocessing**\n\nPreprocessing involves the following steps\n\n* Removing Punctuations and other special characters\n* Splitting the comments into individual words\n* Removing Stop Words\n* Stemming and Lemmatising\n* Applying Count Vectoriser\n* Splitting dataset into Training and Testing\n\n**Removing Punctuations**","7b9fbcbf":"**3. BR Method with Multinomial classifier (from scikit-multilearn)**","125c6743":"**Removing Stop Words**","d09b9e11":"**Applying algorithmic techniques to build a multi-label classifier**\n\n**1. Problem transformation methods** like binary relevance method, label power set, classifier chain and random k-label sets (RAKEL) algorithm \n\n**2. Adaptation algorithms** like the AdaBoost MH, AdaBoost MR, k-nearest neighbours, decision trees and back propagation-multi label neural networks(BP-MLL).\n\n**I. Problem Transformation Methods**","8be07dec":"**II. Adaptation Algorithms**\n\n**7. MLkNN with k=2 (from scikit-multilearn) (Multi label version of K-nearest neighbours)**","db6ec6fc":"**Visualisation**\n\nLet us have a plot showing the hamming-loss and log-loss of different models, which we selected.","fad8ef4b":"**Remove excessive length comments**\n\nThreshold = 400 words","02a27d50":"( Using scikit-multilearn library is used for implementing the various methods. eg.: Multinomial Naive Bayes, Gaussian Naive Bayes and SVC.)","a6426d2a":"**Separate the Comment field data and outcome lables**","6874bdee":"**Hamming Loss**"}}