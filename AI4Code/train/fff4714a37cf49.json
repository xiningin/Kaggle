{"cell_type":{"314dd4f1":"code","7823c3e4":"code","c338976a":"code","11866830":"code","1db5fc58":"code","ecfe3663":"code","edb43870":"code","72208211":"code","6bcf995b":"code","b9e2f6f7":"code","3b29acb0":"code","f2ae3490":"code","0c1bde24":"markdown","0dc33732":"markdown","4425d40b":"markdown","1dc5da63":"markdown","f8744de1":"markdown","effc5f17":"markdown","9527a079":"markdown","5edf1d2a":"markdown","1f7bbc00":"markdown"},"source":{"314dd4f1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n#from yellowbrick.text import TSNEVisualizer\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.feature_extraction.text import CountVectorizer #for term frequency\nfrom nltk import word_tokenize          \nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn.naive_bayes import MultinomialNB\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nclass LemmaTokenizer(object):\n    def __init__(self):\n        self.wnl = WordNetLemmatizer()\n    def __call__(self, articles):\n        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]","7823c3e4":"corpus = ['Python is a versatile programming language','R and Python are open source','Python Rocks','Python and R have text-processing capability','Prorgramming in Python is easy','Python and R have rich libraries']\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\nwnames= vectorizer.get_feature_names()\ndtm=X.toarray()\nnp.asarray(dtm)","c338976a":"def getWordNames(corpus,ind):\n    #  Takes the corpus as the input\n    #  ind allows to specify whether to use term frequency or term frequency inverse document frequency\n    if ind=='C':\n        vectorizer = CountVectorizer(tokenizer=LemmaTokenizer())\n    else:\n        vectorizer = TfidfVectorizer(tokenizer=LemmaTokenizer(),min_df=0.01, norm='l2', stop_words='english', use_idf=True, smooth_idf=True, sublinear_tf=False)\n    X = vectorizer.fit_transform(corpus)\n    wnames= vectorizer.get_feature_names()\n    return wnames\n\ndef getTermDocumentMatrix(corpus,ind):\n    #  Takes the corpus as the input\n    #  ind allows to specify whether to use term frequency or term frequency inverse document frequency\n    if ind=='C':\n        vectorizer = CountVectorizer(tokenizer=LemmaTokenizer())\n    else:\n        vectorizer = TfidfVectorizer(tokenizer=LemmaTokenizer(),min_df=0.01, norm='l2', stop_words='english', use_idf=True, smooth_idf=True, sublinear_tf=False)\n    X = vectorizer.fit_transform(corpus)\n    dtm=X.toarray()\n    np.asarray(dtm)\n    dtm=np.transpose(dtm)\n    return dtm","11866830":"def distmatrix (dtm):\n    # Takes the document term matrix as input and returns the distance matrix\n    n,p = dtm.shape\n    dist = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            x, y = dtm[i, :], dtm[j, :]\n            dist[i, j] = np.sqrt(np.sum((x - y)**2))\n    return dist","1db5fc58":"# Finding nearest word\ndef closest_word(w1,wnames,dist):\n    # Takes 3 parameters as input, word, name of all words or vocabulary, \n    ind= wnames.index(w1)\n    distv=dist[ind,:]\n    valid_idx = np.where(distv > 0)[0]\n    out = valid_idx[distv[valid_idx].argmin()]\n    print('nearest word is ' + wnames[out])","ecfe3663":"# Creating Corpus as a python list\ncorpus = ['Python is a versatile programming language','R and Python are open source','Python Rocks','Python and R have text-processing capability','Prorgramming in Python is easy','Python and R have rich libraries']\nwnames=getWordNames(corpus,'T')\nprint(wnames)\ndtm=getTermDocumentMatrix(corpus,'T')\nprint(dtm)\ndist=distmatrix(dtm)\nclosest_word('python',wnames,dist)","edb43870":"nRowsRead =1000\nfilepath=\"\/kaggle\/input\/brown-corpus\/brown.csv\"\ndf = pd.read_csv(filepath,encoding='iso-8859-1',nrows=nRowsRead)\ndf.head()","72208211":"corpus = df.tokenized_text\nwnames=getWordNames(corpus,'T')\nprint(wnames)\ndtm=getTermDocumentMatrix(corpus,'T')\ndist=distmatrix(dtm)\nclosest_word('wife',wnames,dist)","6bcf995b":"corpus = ['Chinese Beijing Chinese','Chinese Chinese Shanghai','Chinese Macao','Tokyo Japan Chinese','Chinese Chinese Chinese Tokyo Japan']\nlabels=['c','c','c','j']\nwnames=getWordNames(corpus,'C')\nprint(wnames)\ndtm=getTermDocumentMatrix(corpus,'C')\n# Transposing to get the documents in rows\ndtm=np.transpose(dtm)\ntrn=dtm[0:4,:]\nclf = MultinomialNB()\nclf.fit(trn, labels)\n# To make a two dimensional array out of the test set, will not be required if test set has multiple rows\ntst=dtm[4,:].reshape(1, -1)\nclf.predict(tst)","b9e2f6f7":"from nltk.corpus import reuters\nfrom nltk import bigrams, trigrams\nfrom collections import Counter, defaultdict\nmodel = defaultdict(lambda: defaultdict(lambda: 0))\n \nfor sentence in reuters.sents():\n    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n        model[(w1, w2)][w3] += 1\n\nfor w1_w2 in model:\n    total_count = float(sum(model[w1_w2].values()))\n    for w3 in model[w1_w2]:\n        model[w1_w2][w3] \/= total_count\n \nprint(model[\"what\", \"the\"][\"economists\"])\nprint(model[\"what\", \"the\"][\"nonexistingword\"])\nprint(model[None, None][\"The\"])","3b29acb0":"import gensim\nfrom gensim.models import Word2Vec\nfrom gensim.utils import simple_preprocess\nfrom gensim.models.keyedvectors import KeyedVectors\nfrom gensim.models import KeyedVectors\n# Load vectors directly from the file\nmodel = KeyedVectors.load_word2vec_format('\/kaggle\/input\/nlpword2vecembeddingspretrained\/GoogleNews-vectors-negative300.bin', binary=True)\n# Access vectors for specific words with a keyed lookup:\nvector = model['easy']\n# see the shape of the vector (300,)\nvector.shape","f2ae3490":"model.similarity('easy','simple')\nmodel.similarity('joy','sorry')\nmodel.most_similar('simple')\nmodel.get_vector('simple')","0c1bde24":"**Reprsenting words by occurence or term frequencey**","0dc33732":"**A Text Classification Example**","4425d40b":"**Trigrmas**","1dc5da63":"**Preparing the distance matrix**","f8744de1":"**Pre loading word embedding vector**","effc5f17":"**Finding the nearest word**","9527a079":"**Working with the Brown Corpus**","5edf1d2a":"**working with a simple corpus**","1f7bbc00":"**Evaluating the model**"}}