{"cell_type":{"04865907":"code","3e6e1627":"code","ef1a1f0c":"code","dea90512":"code","00ad5b7a":"code","0c976e24":"code","d30de682":"code","3b207c70":"code","7df2cb16":"code","de89dc34":"code","0cad3c63":"code","7ce2b948":"code","8ba5f349":"code","5732df3e":"code","0adc9288":"code","a5ae0425":"code","2b32f2f3":"code","f113261e":"code","beabc53a":"code","e639eb80":"code","29a1ea21":"code","3a3dd53f":"code","bccd1fcb":"code","323f0b85":"code","ac1e1d7f":"code","7912edbd":"code","19a20bdc":"markdown"},"source":{"04865907":"# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n# data visualisation and manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n \n#configure\n# sets matplotlib to inline and displays graphs below the corressponding cell.\n%matplotlib inline  \nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid',color_codes=True)\n\nfrom sklearn.metrics import confusion_matrix\nfrom fastai import *\nfrom fastai.vision import *\n\n# specifically for manipulating zipped images and getting numpy arrays of pixel values of images.\nimport cv2                  \nimport numpy as np  \nfrom tqdm import tqdm\nimport os                   \nfrom random import shuffle  \nfrom zipfile import ZipFile\nfrom PIL import Image\nfrom sklearn.utils import shuffle\n\nprint(os.listdir(\"..\/input\"))","3e6e1627":"# copy pretrained weights for resnet34 to the folder fastai will search by default\nPath('\/tmp\/.cache\/torch\/checkpoints\/').mkdir(exist_ok=True, parents=True)\n!cp '..\/input\/resnet34\/resnet34.pth' '\/tmp\/.cache\/torch\/checkpoints\/resnet34-333f7ec4.pth'","ef1a1f0c":"!mkdir ..\/data\n!mkdir ..\/data\/train","dea90512":"!mkdir ..\/data\/train\/0\n!mkdir ..\/data\/train\/1\n!mkdir ..\/data\/train\/2\n!mkdir ..\/data\/train\/3\n!mkdir ..\/data\/train\/4","00ad5b7a":"print(os.listdir(\"..\/data\/train\"))","0c976e24":"df_train = pd.read_csv('..\/input\/aptos2019-blindness-detection\/train.csv')\ndf_test = pd.read_csv('..\/input\/aptos2019-blindness-detection\/test.csv')\n\nx_train = df_train['id_code']\ny_train = df_train['diagnosis']","d30de682":"import subprocess\ndef move_img(x,y,kind):\n    for id_code ,diagnosis in tqdm(zip(x,y)):\n        if diagnosis == 0:\n            subprocess.call(['cp','..\/input\/aptos2019-blindness-detection\/{}_images\/{}.png'.format(kind,id_code),'..\/data\/{}\/0\/{}.png'.format(kind,id_code)])\n        if diagnosis == 1:\n            subprocess.call(['cp','..\/input\/aptos2019-blindness-detection\/{}_images\/{}.png'.format(kind,id_code),'..\/data\/{}\/1\/{}.png'.format(kind,id_code)])\n        if diagnosis == 2:\n            subprocess.call(['cp','..\/input\/aptos2019-blindness-detection\/{}_images\/{}.png'.format(kind,id_code),'..\/data\/{}\/2\/{}.png'.format(kind,id_code)])\n        if diagnosis == 3:\n            subprocess.call(['cp','..\/input\/aptos2019-blindness-detection\/{}_images\/{}.png'.format(kind,id_code),'..\/data\/{}\/3\/{}.png'.format(kind,id_code)])\n        if diagnosis == 4:\n            subprocess.call(['cp','..\/input\/aptos2019-blindness-detection\/{}_images\/{}.png'.format(kind,id_code),'..\/data\/{}\/4\/{}.png'.format(kind,id_code)])","3b207c70":"move_img(x_train,y_train,'train')","7df2cb16":"print(os.listdir(\"..\/data\/train\/\")) ","de89dc34":"# create image data bunch\ndata = ImageDataBunch.from_folder('..\/data\/', \n                                  train=\"..\/data\/train\", \n                                  valid_pct=0.2,\n                                  ds_tfms=get_transforms(flip_vert=True, max_warp=0),\n                                  size=224,\n                                  bs=64, \n                                  num_workers=0).normalize(imagenet_stats)","0cad3c63":"# check classes\nprint(f'Classes: \\n {data.classes}')","7ce2b948":"# show some sample images\ndata.show_batch(rows=3, figsize=(7,6))","8ba5f349":"# build model (use resnet34)\nlearn = create_cnn(data, models.resnet34, metrics=accuracy, model_dir=\"\/tmp\/model\/\")","5732df3e":"# first time learning\nlearn.fit_one_cycle(6,1e-2)","0adc9288":"# save stage\nlearn.save('stage-1')","a5ae0425":"# search appropriate learning rate\n#learn.unfreeze()\n#learn.lr_find()\n#learn.recorder.plot()","2b32f2f3":"# second time learning\n#learn.fit_one_cycle(4, max_lr=slice(1e-6,1e-5 ))","f113261e":"# save stage\n#learn.save('stage-2')","beabc53a":"learn.recorder.plot_losses()","e639eb80":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_top_losses(9, figsize=(15,11))","29a1ea21":"interp.plot_confusion_matrix(figsize=(8,8), dpi=60)","3a3dd53f":"sample_df = pd.read_csv('..\/input\/aptos2019-blindness-detection\/sample_submission.csv')\nsample_df.head()","bccd1fcb":"learn.data.add_test(ImageList.from_df(sample_df,'..\/input\/aptos2019-blindness-detection',folder='test_images',suffix='.png'))","323f0b85":"preds,y = learn.get_preds(DatasetType.Test)","ac1e1d7f":"sample_df.diagnosis = preds.argmax(1)\nsample_df.head()","7912edbd":"sample_df.to_csv('submission.csv',index=False)","19a20bdc":"**If you like it , please upvote :)**"}}