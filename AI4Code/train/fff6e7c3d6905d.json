{"cell_type":{"74623fce":"code","694239f5":"code","83bf5931":"code","bb48f3e3":"code","edc4301b":"code","566764f0":"code","8904c5be":"code","cbb702b1":"code","2e364ede":"code","e11f567c":"code","e05312bb":"code","5bf0d92e":"code","b650e5c4":"code","b82a140c":"code","d509f732":"code","7ceb4473":"code","22027409":"code","0c089de5":"code","36be4350":"code","44b9c3eb":"code","0c728091":"code","19593846":"code","38dc726b":"code","5c587d51":"code","391bfdbb":"code","fde1faf1":"code","102b7af8":"code","b4aba2dd":"code","0467a4ae":"code","08ecd9db":"code","9c7e829a":"code","f7387eb2":"code","1d998081":"code","85a146a0":"code","61c9363c":"code","21a09e30":"code","40ee770b":"code","59a817aa":"code","842061dc":"code","dfe261dd":"code","bee418bd":"code","95519d74":"code","cb63cb6f":"code","8e20b50f":"code","6cd54b25":"code","0f63e9a5":"code","6fdfc7ef":"code","ba8bbab2":"code","e308608f":"code","64d6f67b":"code","0f8d13a6":"code","ab6068df":"code","71faad68":"code","d23ac59f":"code","f0a04833":"code","ab83577b":"markdown","f69f10a2":"markdown","9e3f1290":"markdown","7d9a7268":"markdown","33824b08":"markdown","de2c4be2":"markdown","b0ffc01d":"markdown","0181d6f5":"markdown","b5118d09":"markdown","39e5b2f3":"markdown","1959c25d":"markdown","69919a93":"markdown","3944e2b6":"markdown","461ab2cc":"markdown","8f739b9b":"markdown","fbacf73c":"markdown","818d1039":"markdown"},"source":{"74623fce":"DATA_PATH = '..\/input'","694239f5":"import os\nimport warnings\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom IPython import display\n\n\n%matplotlib inline\nwarnings.filterwarnings('ignore')\nmpl.style.use('ggplot')\nsns.set_style('white')","83bf5931":"titanic = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\ndata = titanic.copy(deep=True)\ndata.info()","bb48f3e3":"data.head(10)","edc4301b":"n_missing = data.isnull().sum()\npercent_missing = (n_missing\/data.isnull().count())\nmissing_summary = pd.concat([n_missing, percent_missing], axis=1)\nmissing_summary","566764f0":"from sklearn.impute import SimpleImputer\n\n\nnumerical_imputer   = SimpleImputer(strategy='median')\ncategorical_imputer = SimpleImputer(strategy='most_frequent')\n\ndata['Age'] = numerical_imputer.fit_transform(data['Age'].values.reshape(-1, 1))\ndata['Fare'] = numerical_imputer.fit_transform(data['Fare'].values.reshape(-1, 1))\ndata['Embarked'] = categorical_imputer.fit_transform(data['Embarked'].values.reshape(-1, 1))\n\ndata.drop(['Cabin'], axis=1, inplace=True)\n\nn_missing = data.isnull().sum()\npercent_missing = (n_missing\/data.isnull().count())\nmissing_summary = pd.concat([n_missing, percent_missing], axis=1)\nmissing_summary","8904c5be":"data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n\ndata['IsAlone'] = 1\ndata['IsAlone'].loc[data['FamilySize'] > 1] = 0\n\ndata['Title'] = data['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n\ndata['FareBinned'] = pd.qcut(data['Fare'], 4)\n\ndata['AgeBinned'] = pd.qcut(data['Age'].astype(np.int64), 4)","cbb702b1":"stat_min = 10\ntitle_names = (data['Title'].value_counts() < stat_min)\ndata['Title'] = data['Title'].apply(lambda x: 'Misc' if title_names.loc[x] else x)","2e364ede":"dropping_cols = ['PassengerId', 'Name', 'Ticket', 'SibSp', 'Parch', 'Fare']\ndata.drop(dropping_cols, axis=1, inplace=True)","e11f567c":"data.head()","e05312bb":"from sklearn.preprocessing import LabelEncoder\n\n\nlabel_encoder = LabelEncoder()\ndata['SexCode']      = label_encoder.fit_transform(data['Sex'])\ndata['EmbarkedCode'] = label_encoder.fit_transform(data['Embarked'])\ndata['TitleCode']    = label_encoder.fit_transform(data['Title'])\ndata['AgeCode']      = label_encoder.fit_transform(data['AgeBinned'])\ndata['FareCode']     = label_encoder.fit_transform(data['FareBinned'])\n\ndata.head()","5bf0d92e":"target = ['Survived']\ndata_x = ['Sex', 'Pclass', 'Embarked', 'Title', 'Age', 'FamilySize', 'IsAlone']\ndata_x_calc = ['SexCode','Pclass', 'EmbarkedCode', 'TitleCode', 'Age']\ndata_xy =  target + data_x\n\ndata_x_bin = ['SexCode','Pclass', 'EmbarkedCode', 'TitleCode', 'FamilySize', 'AgeCode', 'FareCode']\ndata_xy_bin = target + data_x_bin\n\ndata_dummy = pd.get_dummies(data[data_x])\ndata_x_dummy = data_dummy.columns.tolist()\ndata_xy_dummy = target + data_x_dummy\n\ndata_dummy.head()","b650e5c4":"from sklearn.model_selection import train_test_split\n\n\nX_train, X_val, y_train, y_val = train_test_split(data[data_x_calc], data[target], random_state=0)\nX_train_bin, X_val_bin, y_train_bin, y_val_bin = train_test_split(data[data_x_bin], data[target], random_state=0)\nX_train_dummy, X_val_dummy, y_train_dummy, y_val_dummy = train_test_split(data_dummy[data_x_dummy], data[target], random_state=0)\n\nprint(\"Data Shape: {}\".format(data.shape))\nprint(\"Train Shape: {}\".format(X_train.shape))\nprint(\"Val Shape: {}\".format(X_val.shape))\n\nX_train_bin.head()","b82a140c":"plt.figure(figsize=(16, 12))\n\ndata['Fare'] = titanic['Fare']\n\nplt.subplot(231)\nplt.boxplot(x=data['Fare'], showmeans=True, meanline=True)\nplt.title('Fare Boxplot')\nplt.ylabel('Fare ($)')\n\nplt.subplot(232)\nplt.boxplot(data['Age'], showmeans=True, meanline=True)\nplt.title('Age Boxplot')\nplt.ylabel('Age (Years)')\n\nplt.subplot(233)\nplt.boxplot(data['FamilySize'], showmeans=True, meanline=True)\nplt.title('Family Size Boxplot')\nplt.ylabel('Family Size (#)')\n\nplt.subplot(234)\nplt.hist(x=[data[data['Survived']==1]['Fare'], data[data['Survived']==0]['Fare']], \n         stacked=True, color=['g', 'r'], label=['Survived', 'Dead'])\nplt.title('Fare Histogram by Survival')\nplt.xlabel('Fare ($)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\nplt.subplot(235)\nplt.hist(x=[data[data['Survived']==1]['Age'], data[data['Survived']==0]['Age']], \n         stacked=True, color=['g', 'r'], label=['Survived', 'Dead'])\nplt.title('Age Histogram by Survival')\nplt.xlabel('Age (Years)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\nplt.subplot(236)\nplt.hist(x=[data[data['Survived']==1]['FamilySize'], data[data['Survived']==0]['FamilySize']], \n         stacked=True, color=['g', 'r'], label=['Survived', 'Dead'])\nplt.title('Family Size Histogram by Survival')\nplt.xlabel('Family Size (#)')\nplt.ylabel('# of Passengers')\nplt.legend()","d509f732":"fig, saxis = plt.subplots(2, 3,figsize=(16,12))\n\nfor sax in saxis[0]:\n    sax.set_ylim(0, 1)\n    \nfor sax in saxis[1]:\n    sax.set_ylim(0, 1)\n\nsns.barplot(x='Embarked', y='Survived', data=data, ax=saxis[0, 0])\nsns.barplot(x='Pclass', y='Survived', order=[1, 2, 3], data=data, ax=saxis[0, 1])\nsns.barplot(x='IsAlone', y='Survived', order=[1, 0], data=data, ax=saxis[0, 2])\n\nsns.barplot(x='FareBinned', y='Survived', data=data, ax=saxis[1, 0])\nsns.barplot(x='AgeBinned', y='Survived', data=data, ax=saxis[1, 1])\nsns.barplot(x='FamilySize', y='Survived', data=data, ax=saxis[1, 2])","7ceb4473":"fig, (axis1,axis2,axis3) = plt.subplots(1, 3, figsize=(14,12))\n\nsns.boxplot(x='Pclass', y='Fare', hue='Survived', data=data, ax=axis1)\naxis1.set_title('Pclass vs Fare Survival Comparison')\n\nsns.violinplot(x='Pclass', y='Age', hue='Survived', data=data, split=True, ax=axis2)\naxis2.set_title('Pclass vs Age Survival Comparison')\n\nsns.boxplot(x='Pclass', y='FamilySize', hue='Survived', data=data, ax=axis3)\naxis3.set_title('Pclass vs Family Size Survival Comparison')","22027409":"fig, qaxis = plt.subplots(1,3,figsize=(14,12))\n\nsns.barplot(x='Sex', y='Survived', hue='Embarked', data=data, ax=qaxis[0])\naxis1.set_title('Sex vs Embarked Survival Comparison')\n\nsns.barplot(x='Sex', y='Survived', hue='Pclass', data=data, ax=qaxis[1])\naxis1.set_title('Sex vs Pclass Survival Comparison')\n\nsns.barplot(x='Sex', y='Survived', hue='IsAlone', data=data, ax=qaxis[2])\naxis1.set_title('Sex vs IsAlone Survival Comparison')","0c089de5":"e = sns.FacetGrid(data, col = 'Embarked')\ne.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\ne.add_legend()","36be4350":"a = sns.FacetGrid(data, hue='Survived', aspect=4)\na.map(sns.kdeplot, 'Age', shade=True)\na.set(xlim=(0 , data['Age'].max()))\na.add_legend()","44b9c3eb":"h = sns.FacetGrid(data, row='Sex', col='Pclass', hue='Survived')\nh.map(plt.hist, 'Age', alpha=.75)\nh.add_legend()","0c728091":"def correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(data)","19593846":"from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.linear_model import LogisticRegressionCV, PassiveAggressiveClassifier, RidgeClassifierCV, SGDClassifier, Perceptron\nfrom sklearn.naive_bayes import BernoulliNB, GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, NuSVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom xgboost import XGBClassifier\n\n\nMLA = [\n    #Ensemble Methods\n    AdaBoostClassifier(),\n    BaggingClassifier(),\n    ExtraTreesClassifier(),\n    GradientBoostingClassifier(),\n    RandomForestClassifier(),\n\n    #Gaussian Processes\n    GaussianProcessClassifier(),\n    \n    #GLM\n    LogisticRegressionCV(),\n    PassiveAggressiveClassifier(),\n    RidgeClassifierCV(),\n    SGDClassifier(),\n    Perceptron(),\n    \n    #Navies Bayes\n    BernoulliNB(),\n    GaussianNB(),\n    \n    #Nearest Neighbor\n    KNeighborsClassifier(),\n    \n    #SVM\n    SVC(probability=True),\n    NuSVC(probability=True),\n    LinearSVC(),\n    \n    #Trees    \n    DecisionTreeClassifier(),\n    ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    XGBClassifier()    \n]","38dc726b":"from sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import cross_validate\n\n\ncv_split = StratifiedShuffleSplit(n_splits=10, test_size=.3, train_size=.6, random_state=0)\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns=MLA_columns)\n\nMLA_predict = data[target]\n\nfor row_index, alg in enumerate(MLA):\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    cv_results = cross_validate(alg, data[data_x_bin], data[target], cv=cv_split)\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    #MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3\n    alg.fit(data[data_x_bin], data[target])\n    MLA_predict[MLA_name] = alg.predict(data[data_x_bin])\n    \nMLA_compare.sort_values(by=['MLA Test Accuracy Mean'], ascending=False, inplace=True)\nMLA_compare","5c587d51":"sns.barplot(x='MLA Test Accuracy Mean', y='MLA Name', data=MLA_compare, color='m')\n\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')\n","391bfdbb":"import random\nfrom sklearn.metrics import accuracy_score\n\n\nfor index, row in data.iterrows():\n    data.set_value(index, 'RandomPredict', 1 if random.random() > .5 else 0)\n    \ndata['RandomScore'] = 0\ndata.loc[(data['Survived'] == data['RandomPredict']), 'RandomScore'] = 1\nprint('Coin Flip Model Accuracy: {:.2f}%'.format(data['RandomScore'].mean()*100))\nprint('Coin Flip Model Accuracy w\/ Scikit-Learn: {:.2f}%'.format(accuracy_score(data['Survived'], data['RandomPredict'])*100))","fde1faf1":"pivot_female = data[data.Sex=='female'].groupby(['Sex','Pclass', 'Embarked','FareBinned'])['Survived'].mean()\nprint('Survival Decision Tree w\/Female Node: \\n',pivot_female)\n\npivot_male = data[data.Sex=='male'].groupby(['Sex','Title'])['Survived'].mean()\nprint('\\n\\nSurvival Decision Tree w\/Male Node: \\n',pivot_male)","102b7af8":"def mytree(df):\n    \n    #initialize table to store predictions\n    Model = pd.DataFrame(data = {'Predict':[]})\n    male_title = ['Master'] #survived titles\n\n    for index, row in df.iterrows():\n\n        #Question 1: Were you on the Titanic; majority died\n        Model.loc[index, 'Predict'] = 0\n\n        #Question 2: Are you female; majority survived\n        if (df.loc[index, 'Sex'] == 'female'):\n                  Model.loc[index, 'Predict'] = 1\n\n        #Question 3A Female - Class and Question 4 Embarked gain minimum information\n\n        #Question 5B Female - FareBin; set anything less than .5 in female node decision tree back to 0       \n        if ((df.loc[index, 'Sex'] == 'female') & \n            (df.loc[index, 'Pclass'] == 3) & \n            (df.loc[index, 'Embarked'] == 'S')  &\n            (df.loc[index, 'Fare'] > 8)\n\n           ):\n                  Model.loc[index, 'Predict'] = 0\n\n        #Question 3B Male: Title; set anything greater than .5 to 1 for majority survived\n        if ((df.loc[index, 'Sex'] == 'male') &\n            (df.loc[index, 'Title'] in male_title)\n            ):\n            Model.loc[index, 'Predict'] = 1\n        \n        \n    return Model\n\n\n#model data\nTree_Predict = mytree(data)\nprint('Decision Tree Model Accuracy\/Precision Score: {:.2f}%\\n'.format(accuracy_score(data['Survived'], Tree_Predict)*100))","b4aba2dd":"import itertools\nfrom  sklearn.metrics import confusion_matrix\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n    \n# Compute confusion matrix\ncnf_matrix = confusion_matrix(data['Survived'], Tree_Predict)\nnp.set_printoptions(precision=2)\n\nclass_names = ['Dead', 'Survived']\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names,\n                      title='Confusion matrix, without normalization')\n\n# Plot normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, \n                      title='Normalized confusion matrix')","0467a4ae":"data.head()","08ecd9db":"X_train_dummy.head()","9c7e829a":"X_train_bin.head()","f7387eb2":"X_train.head()","1d998081":"X_train = pd.concat([pd.concat([X_train_bin, X_train_dummy], axis=1), pd.concat([X_val_bin, X_val_dummy], axis=1)])","85a146a0":"try:\n    X_train.drop(['SexCode', 'EmbarkedCode', 'TitleCode', 'Age', 'FamilySize', 'Pclass'], axis=1, inplace=True)\nexcept KeyError:\n    pass\nX_train['FamilySize'] = pd.concat([X_train_bin['FamilySize'], X_val_bin['FamilySize']])\nX_train['Pclass'] = pd.concat([X_train_bin['Pclass'], X_val_bin['Pclass']])\nX_train.head()","61c9363c":"X_train['IsAlone'].value_counts()","21a09e30":"y_train = pd.concat([y_train_bin, y_val_bin], axis=0)\ny_train.head()","40ee770b":"y_train['Survived'].value_counts()","59a817aa":"from sklearn.model_selection import GridSearchCV\n\n\nparam_grid = [\n    {\n        'kernel': ['rbf'],\n        'nu': [i\/10 for i in range(2, 8)],\n        'gamma': [i\/1000 for i in range(12, 20)],\n    },\n]\nnusv_clf = NuSVC()\ngrid_search = GridSearchCV(nusv_clf, param_grid=param_grid, cv=4)\ngrid_search.fit(X_train, y_train)\n\nbest_nusv_clf = grid_search.best_estimator_\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)","842061dc":"test_data = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\nids = test_data['PassengerId']","dfe261dd":"test_data.head()","bee418bd":"n_missing = test_data.isnull().sum()\npercent_missing = (n_missing\/test_data.isnull().count())\nmissing_summary = pd.concat([n_missing, percent_missing], axis=1)\nmissing_summary","95519d74":"numerical_imputer   = SimpleImputer(strategy='median')\ncategorical_imputer = SimpleImputer(strategy='most_frequent')\n\ntest_data['Age']      = numerical_imputer.fit_transform(test_data['Age'].values.reshape(-1, 1))\ntest_data['Fare']     = numerical_imputer.fit_transform(test_data['Fare'].values.reshape(-1, 1))\ntest_data['Embarked'] = categorical_imputer.fit_transform(test_data['Embarked'].values.reshape(-1, 1))\n\ntest_data.drop(['Cabin'], axis=1, inplace=True)\n\nn_missing = test_data.isnull().sum()\npercent_missing = (n_missing\/test_data.isnull().count())\nmissing_summary = pd.concat([n_missing, percent_missing], axis=1)\nmissing_summary","cb63cb6f":"test_data['FamilySize'] = test_data['SibSp'] + test_data['Parch'] + 1\n\ntest_data['IsAlone'] = 1\ntest_data['IsAlone'].loc[test_data['FamilySize'] > 1] = 0\n\ntest_data['Title'] = test_data['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n\ntest_data['FareBinned'] = pd.qcut(test_data['Fare'], 4)\n\ntest_data['AgeBinned'] = pd.qcut(test_data['Age'].astype(np.int64), 4)","8e20b50f":"stat_min = 10\ntitle_names = (test_data['Title'].value_counts() < stat_min)\ntest_data['Title'] = test_data['Title'].apply(lambda x: 'Misc' if title_names.loc[x] else x)","6cd54b25":"dropping_cols = ['PassengerId', 'Name', 'Ticket', 'SibSp', 'Parch', 'Fare']\ntest_data.drop(dropping_cols, axis=1, inplace=True)","0f63e9a5":"from sklearn.preprocessing import LabelEncoder\n\n\nlabel_encoder = LabelEncoder()\ntest_data['SexCode']      = label_encoder.fit_transform(test_data['Sex'])\ntest_data['EmbarkedCode'] = label_encoder.fit_transform(test_data['Embarked'])\ntest_data['TitleCode']    = label_encoder.fit_transform(test_data['Title'])\ntest_data['AgeCode']      = label_encoder.fit_transform(test_data['AgeBinned'])\ntest_data['FareCode']     = label_encoder.fit_transform(test_data['FareBinned'])\n\ntest_data.head()","6fdfc7ef":"target = ['Survived']\ndata_x = ['Sex', 'Pclass', 'Embarked', 'Title', 'Age', 'FamilySize', 'IsAlone']\ndata_x_calc = ['SexCode','Pclass', 'EmbarkedCode', 'TitleCode', 'Age']\ndata_xy =  target + data_x\n\ndata_x_bin = ['SexCode','Pclass', 'EmbarkedCode', 'TitleCode', 'FamilySize', 'AgeCode', 'FareCode']\ndata_xy_bin = target + data_x_bin\n\ndata_dummy = pd.get_dummies(test_data[data_x])\ndata_x_dummy = data_dummy.columns.tolist()\ndata_xy_dummy = target + data_x_dummy\n\ndata_dummy.head()","ba8bbab2":"test_data.describe()","e308608f":"X_train.head()","64d6f67b":"X_test = pd.concat([\n    test_data['AgeCode'],\n    test_data['FareCode'],\n    test_data['IsAlone'],\n    data_dummy['Sex_female'],\n    data_dummy['Sex_male'],\n    data_dummy['Embarked_C'],\n    data_dummy['Embarked_Q'],\n    data_dummy['Embarked_S'],\n    data_dummy['Title_Master'],\n    data_dummy['Title_Misc'],\n    data_dummy['Title_Miss'],\n    data_dummy['Title_Mr'],\n    data_dummy['Title_Mrs'],\n    test_data['FamilySize'],\n    test_data['Pclass']\n], axis=1)","0f8d13a6":"X_test.head()","ab6068df":"submit_data = pd.DataFrame()\nsubmit_data['PassengerId'] = ids\nsubmit_data.head()","71faad68":"submit_data['Survived'] = best_nusv_clf.predict(X_test)\n\nprint('Validation Data Distribution: \\n', submit_data['Survived'].value_counts(normalize=True))","d23ac59f":"submit_data.head()","f0a04833":"submit_data.to_csv('submission.csv', index=False)","ab83577b":"### Train and Validation Data","f69f10a2":"### Baseline Accuracy\n\nOkay, so with no information about the dataset, we can always get 50% with a binary problem. But we have information about the dataset, so we should be able to do better. We know that 1,502\/2,224 or 67.5% of people died. Therefore, if we just predict the most frequent occurrence, that 100% of people died, then we would be right 67.5% of the time. So, let's set 68% as bad model performance, because again, anything lower than that, then why do I need you, when I can just predict using the most frequent occurrence.","9e3f1290":"### 21 Questions Game\n\nRemember, the name of the game is to create subgroups using a decision tree model to get survived\/1 in one bucket and dead\/0 in another bucket. Our rule of thumb will be the majority rules. Meaning, if the majority or 50% or more survived, then everybody in our subgroup survived\/1, but if 50% or less survived then if everybody in our subgroup died\/0. Also, we will stop if the subgroup is less than 10 and\/or our model accuracy plateaus or decreases. Got it? Let's go!\n\n1. **Were you on Titanic?** If Yes, then majority (62%) died. Note our sample survival is different than our population of 68%. Nonetheless, if we assumed everybody died, our sample accuracy is 62%.\n2. **Are you male or female?** Male, majority (81%) died. Female, majority (74%) survived. Giving us an accuracy of 79%.\n3. *(going down the female branch with count = 314)* **Are you in class 1, 2, or 3?** Class 1, majority (97%) survived and Class 2, majority (92%) survived. Since the dead subgroup is less than 10, we will stop going down this branch. Class 3, is even at a 50-50 split. No new information to improve our model is gained.\n4. *(going down the female class 3 branch with count = 144)* **Did you embark from port C, Q, or S?** We gain a little information. C and Q, the majority still survived, so no change. Also, the dead subgroup is less than 10, so we will stop. S, the majority (63%) died. So, we will change females, class 3, embarked S from assuming they survived, to assuming they died. Our model accuracy increases to 81%.\n\n5. *(going down the female class 3 embarked S branch with count = 88)* So far, it looks like we made good decisions. Adding another level does not seem to gain much more information. This subgroup 55 died and 33 survived, since majority died we need to find a signal to identify the 33 or a subgroup to change them from dead to survived and improve our model accuracy. We can play with our features. One I found was fare 0-8, majority survived. It's a small sample size 11-9, but one often used in statistics. We slightly improve our accuracy, but not much to move us past 82%. So, we'll stop here.\n\n6. *(going down the male branch with count = 577)* Going back to question 2, we know the majority of males died. So, we are looking for a feature that identifies a subgroup that majority survived. Surprisingly, class or even embarked didn't matter like it did for females, but title does and gets us to 82%. Guess and checking other features, none seem to push us past 82%. So, we'll stop here for now.\n\nYou did it, with very little information, we get to 82% accuracy. On a worst, bad, good, better, and best scale, we'll set 82% to good, since it's a simple model that yields us decent results. But the question still remains, can we do better than our handmade model?","7d9a7268":"### Convert Formats","33824b08":"### Import Libraries","de2c4be2":"# Submission","b0ffc01d":"Conclusions:\n1. `'Survived'` is the target variable. It has an integer value `0` (died) or `1` (survived).\n2. `Name`, `Sex`, `Ticket`, `Cabin`, and `Embarked` are categorical variables.\n3. `PassengerId`, `PClass`, `Age`, `SibSp`, `Parch`, and `Fare` are numerical variables.\n4. `Age`, `Cabin`, and `Embarked` have missing values (714, 204, and 879 out of 891 respectively).\n5. `PassengerId` and `Ticket` are unique identifiers of a passenger that don't influence the chance of survival.\n6. `PClass` is a proxy for the socio-economic status. Passengers of the 1st class probably had more chances to survive, than passengers of the 2nd or the 3rd class.\n7. `Cabin` could be used for modelling of passengers' distribution aboard. However, it has too many missing values, and thus will be excluded.\n8. `SibSp` and `Parch` represent the number of related family members aboard. They can be used for feature engineering and yield a single variable of family size.","0181d6f5":"# Step 2: Data Acquisition","b5118d09":"### The 4 C's of Data Cleaning: Correcting, Completing, Creating, Converting","39e5b2f3":"# Step 5: Modelling","1959c25d":"# Step 4: EDA","69919a93":"Develop a classifier for determining whether a Titanic's passenger survived or not.","3944e2b6":"### Clean Data","461ab2cc":"### Get to Know Data\n\nWhat does it look like (datatype and values), what makes it tick (independent \/ feature variables), what are its goals (dependent \/ target variables).","8f739b9b":"Now, when the data is cleaned, we will explore it with descriptive and graphical statistics.","fbacf73c":"# Step 3: Data Cleaning","818d1039":"# Step 1: Define the Problem"}}