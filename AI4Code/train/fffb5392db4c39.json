{"cell_type":{"99489bb1":"code","18d2e846":"code","c7b09bff":"code","d041bb0e":"code","7a8d2a6d":"code","7476cbae":"code","8e737a82":"code","4bd3a648":"code","853b9028":"code","00848917":"code","0811e40c":"code","f270214f":"code","24ee7b88":"code","e218a3e4":"code","cdfa99dc":"code","cbf9660b":"code","48c64337":"code","0ae9964e":"code","278fef52":"code","75fc6e00":"code","fb7e8739":"code","ca6f502d":"code","24e7710f":"code","d7a15bb0":"code","f0668516":"code","947bd94e":"code","aec28d8d":"code","4284e923":"code","ca22dcb5":"code","21343bdc":"code","b832f81d":"code","3cbda4aa":"code","84e08864":"code","3e81c6a9":"code","1bad15f7":"code","eec598d7":"code","9a5c3026":"code","cc2c0e0b":"code","7bda17a1":"code","4e66f591":"code","e6469e7f":"code","70847009":"code","b47c24f3":"code","71caec32":"code","99ff1e93":"code","36a1b9a5":"code","384eaa57":"code","bd255383":"code","9989a743":"code","36ff070c":"code","04323d3a":"code","7fd23ed0":"code","42bf5a02":"code","73c09dfb":"code","1bb8bf2b":"code","13e50ac7":"code","e77462e9":"code","66335faa":"code","ccf6676e":"code","0e5b4789":"code","efe7db19":"code","9224c63e":"code","0d461859":"code","d528c4ca":"code","b1f25645":"code","c9950211":"code","42c85b13":"code","d0011c53":"code","2c1e9d7a":"code","02a4ba6a":"code","2ace9c24":"code","fa586f88":"code","f2c1fbd8":"code","34631f34":"code","97029023":"code","b8e4b98e":"code","e8062abf":"code","05ab0b1b":"code","37937d34":"code","f8f9f07f":"code","4082a103":"code","7c69b793":"code","7a15ae89":"code","5583853e":"code","e9d9893c":"code","84682663":"code","a0580ed0":"code","ebd33964":"code","444a8f7a":"code","742aaad8":"code","2aab6432":"code","a19cb007":"code","28bc73b2":"code","68964ca4":"code","e93f4c66":"code","cc542961":"code","86c8ff81":"code","249dcd14":"code","a13b1147":"code","89054cc3":"code","9c694657":"code","903c0f2f":"code","59950436":"code","5576d887":"code","91ed3732":"code","c5e0d05a":"code","4da1f12a":"code","02848bba":"code","73e59b4f":"code","a478bdf3":"code","78b3b459":"code","2f87caf5":"code","f3a2f62f":"code","38176a26":"code","581e5ae8":"code","5c2a590b":"code","fe970671":"code","3a256067":"code","5390248d":"code","ce528260":"code","2e39a0bb":"code","944ab741":"code","6c213a94":"code","8bf23c48":"code","62953a96":"code","ca599f80":"code","c7ce9451":"code","6ce29c1b":"code","d667f330":"code","283c487b":"code","82ab4b86":"code","07297703":"code","0617e13e":"code","063dae41":"code","9ec76d28":"code","e1ab5c88":"code","9df1d7d1":"code","a35e9417":"code","da537da0":"code","c9651354":"code","3691399f":"code","426b4c4e":"code","4a699ce1":"code","00c13be7":"code","799c3535":"code","04116de1":"code","0f0fb743":"code","c1c06ee9":"code","9782c761":"code","cdbeec15":"code","1ff59579":"code","c5ef9b3c":"code","034f192a":"code","53328a79":"code","44c51638":"code","b6fdb537":"code","f2be9c00":"code","8ff1fa4a":"code","56f60892":"code","fdb70877":"code","6673ef99":"code","66521b4e":"code","7e8f5e80":"code","d7d49f25":"code","970a4ecc":"code","b881c82a":"code","af92febf":"code","f0fe2806":"code","2e299e2a":"code","98c04015":"code","24c14730":"code","dc7d15ec":"code","ea96daea":"code","d61422eb":"code","e6e18205":"code","226aea7c":"code","9e8004f9":"code","04a7990b":"code","8d7e6467":"code","ffededb1":"code","4356c0cd":"code","224b77fe":"code","4c4b3778":"code","1fd1d3d3":"code","1d2cb450":"code","c8fd9c5f":"code","6a510bc6":"code","d3ba09d3":"code","62b70f91":"code","7d82db54":"code","80f251ba":"code","9b1f4026":"code","0630a872":"code","91e7129e":"code","9568d79a":"code","3a4043bd":"code","dd08e356":"code","a18481ee":"code","c296350f":"code","086bb811":"code","4497d09b":"code","3330a637":"code","31125397":"code","9f5399b6":"code","a8041791":"code","d0d8309c":"code","29b8b523":"code","fbdc2468":"code","3eac9cb5":"code","45f9cec2":"code","c6eee788":"code","5648d886":"code","be76dedb":"code","a47bd906":"code","139fb2bf":"code","378eb28c":"code","3ab55aef":"code","b505a2be":"code","2673d823":"code","2d226a18":"code","eeeb9f2d":"code","203cccc7":"code","44fb6714":"code","1f340c36":"code","e5e7c0c9":"code","018ed747":"code","a4f37131":"code","fb51ecb9":"code","f29879d5":"code","d3e883ea":"code","1cebe9d2":"code","717b97b1":"code","ca4f297f":"code","bb8d299b":"code","d2dd43fa":"code","49919b55":"code","015061c9":"code","abe8f375":"code","73d3c7dd":"code","fb1202df":"code","37674cf5":"code","d8ab9208":"code","edea173f":"code","89a9d439":"code","cb718d2b":"code","344467f1":"code","37c3aca0":"code","58d36414":"code","62e32363":"code","32426d50":"code","1b67b703":"code","9c6c5210":"code","d92bea41":"code","24a36b88":"code","217ef53a":"code","5015df27":"code","7b6223eb":"code","f8f6d9ff":"code","0412862c":"code","f4499bc3":"code","d87bb6fd":"code","3bb4f544":"code","865036bf":"code","ff70695b":"code","ffb201e7":"code","d16cc747":"code","0a77e7f5":"code","bc0303e9":"code","046e1410":"code","fd1d0fdb":"code","e49199dc":"code","8da159dc":"code","15173fbd":"code","e65f20f7":"code","da9c095d":"code","9cada28b":"code","6bdf7500":"code","c565ae4d":"code","6e2afbc0":"code","1bab9ed9":"code","97883500":"code","ba7b4dff":"code","906f3564":"code","04954b42":"code","07b97a8f":"code","c4e444e5":"code","c533f03e":"code","95eb151d":"code","9c66cf58":"code","d5d4b4fa":"code","cfce2890":"code","bb3edf46":"code","8651ec43":"code","1f9c9d77":"code","ff084d9c":"code","689b7504":"code","3c56d2ae":"code","1c986561":"code","6a86c66d":"code","49664a86":"code","3a18ee5b":"code","68f2a094":"code","b78a97f5":"code","826c1250":"code","874a02fb":"code","34cc3162":"code","bbdd4cdb":"code","5628ce1b":"code","d3a65cb2":"code","d43f33e8":"code","d5474d95":"code","243be090":"code","85125444":"code","d744dbbb":"code","e9eb3dc4":"code","056b6050":"code","8706beb0":"code","ed0b5151":"code","b2981481":"code","87c45b1a":"code","30ecaab6":"code","d5d4171e":"code","129961c0":"code","9924213c":"code","a743e175":"code","b0cc37ad":"code","7de57fe1":"code","aecb37ac":"code","1a354b23":"markdown","cf540fc7":"markdown","dc7a1cb0":"markdown","bd065539":"markdown","04bb7b60":"markdown","16ca6a72":"markdown","b006d956":"markdown","9f80bcec":"markdown","854131f3":"markdown","bbace86f":"markdown","75db7ff9":"markdown","18dc7056":"markdown","c6672bfe":"markdown","d34d8db2":"markdown","934306ab":"markdown","e7aaf426":"markdown","53cffdcc":"markdown","bca3d087":"markdown","969ff6d6":"markdown","daccafe1":"markdown","55bac078":"markdown","f1845aa4":"markdown","4eab99e9":"markdown","798c710d":"markdown","e0b0a784":"markdown","7f6ed994":"markdown","115c5a55":"markdown","10b73e83":"markdown","5a53c86b":"markdown","0034a179":"markdown","dc32ab76":"markdown","ccef8a60":"markdown","9b96227f":"markdown","00e4d304":"markdown","d2744177":"markdown","cc534984":"markdown","93379e8e":"markdown","9b9fe274":"markdown","c4503552":"markdown","70287b00":"markdown","88972839":"markdown","5a6cb017":"markdown","151dd9e1":"markdown","4dad6b6b":"markdown","f2ff3f10":"markdown","de9d22db":"markdown","9a4cbdf6":"markdown","01e68647":"markdown","245a88cf":"markdown","092024d0":"markdown","6fdac76c":"markdown","77e92c31":"markdown","bd1b2a00":"markdown","ff33db3e":"markdown","895b8f99":"markdown","07e04995":"markdown","fcc2e330":"markdown","d1efa131":"markdown","87e5b59a":"markdown","06a4ec7d":"markdown","b6d81a8a":"markdown","55b962fb":"markdown","2885a3c7":"markdown","d3d1b0ad":"markdown","1b6b7060":"markdown","4586d20a":"markdown","a6e531d0":"markdown","50bf5dca":"markdown","eaf31eec":"markdown","733e5036":"markdown","67e21ae2":"markdown","8f4b017c":"markdown","bd1006f3":"markdown","d24d638f":"markdown","7d22ae56":"markdown","f37427c0":"markdown","6d5f5a16":"markdown","6bfe38dc":"markdown","72b952ce":"markdown","1c8717e7":"markdown","677d4166":"markdown","dc2d0d5c":"markdown","19450776":"markdown","9530ee78":"markdown","7199c7a9":"markdown","bafe6a41":"markdown","25eba208":"markdown","e5991a9f":"markdown","dc4d9be2":"markdown","3bb1e68b":"markdown","368a0a06":"markdown","c972fbc4":"markdown","47819fd0":"markdown","9790e2d0":"markdown","254bb01e":"markdown","a77f9e2d":"markdown","886bb985":"markdown","e336c8e3":"markdown","9e43b5c3":"markdown","d63ccfed":"markdown","ff85dae2":"markdown","c6ae3f62":"markdown","19f57cc2":"markdown","c6c3ef24":"markdown","efb6c975":"markdown","f5e48d0e":"markdown","c3d8ee4d":"markdown","22243ae4":"markdown","3d1b0e34":"markdown","6661eed3":"markdown","c22c6c81":"markdown","583b05fe":"markdown","d32b1967":"markdown","05dc909b":"markdown","95e612ad":"markdown","e0c96da6":"markdown","9ea49bfc":"markdown","64c8b918":"markdown","6778251e":"markdown","29d128c7":"markdown","fb54b04e":"markdown","5ad888e2":"markdown","ae66cd2e":"markdown","64140e57":"markdown","b331c54c":"markdown","e361d8a2":"markdown","e87a086c":"markdown","92c0a237":"markdown","62bed012":"markdown","5bee1526":"markdown","7cd49966":"markdown","804ed73d":"markdown","ab6b594a":"markdown","76ed1167":"markdown","c78cca7e":"markdown","3ebc139c":"markdown","d69c30aa":"markdown","c2491983":"markdown","e98d08e0":"markdown","695ac5f0":"markdown","f5923777":"markdown","d0b7b3e9":"markdown","56040716":"markdown","43212702":"markdown","0b831890":"markdown","f1b6384a":"markdown","22346982":"markdown","23922153":"markdown","64567de8":"markdown","681fba71":"markdown","8f21f7ac":"markdown","390d4cb0":"markdown","5c1812e4":"markdown","1d494986":"markdown","d3f55dbd":"markdown","1ae07be1":"markdown"},"source":{"99489bb1":"# !pip install pyforest\n# 1-Import Libraies\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport datetime as dt\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport missingno as msno \nimport plotly.express as px\nimport plotly.graph_objects as go\nimport datetime\n\nfrom sklearn.compose import make_column_transformer\n\n# Scaling\nfrom sklearn.preprocessing import scale \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures \nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import PowerTransformer \nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import RobustScaler\n\n# Modelling\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\n# Importing plotly and cufflinks in offline mode\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\nimport plotly.graph_objects as go\n\n# Ignore Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.warn(\"this will not show\")\n\n# Figure&Display options\nplt.rcParams[\"figure.figsize\"] = (16, 9)\npd.set_option('max_colwidth',200)\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 200)\npd.set_option('display.float_format', lambda x: '%.2f' % x)\n\n# !pip install termcolor\nimport colorama\nfrom colorama import Fore, Style  # makes strings colored\nfrom termcolor import colored\nfrom termcolor import cprint\n\nimport ipywidgets\nfrom ipywidgets import interact\n\n# !pip install -U pandas-profiling --user\nimport pandas_profiling\nfrom pandas_profiling.report.presentation.flavours.html.templates import create_html_assets\n\n# !pip install wordcloud\nfrom wordcloud import WordCloud\n\n# !pip install squarify\nimport squarify as sq","18d2e846":"## Some Useful User-Defined-Functions\n\n###############################################################################\n\ndef missing_values(df):\n    missing_number = df.isnull().sum().sort_values(ascending=False)\n    missing_percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n    return missing_values[missing_values['Missing_Number']>0]\n\n###############################################################################\n\ndef first_looking(df):\n    print(colored(\"Shape:\", attrs=['bold']), df.shape,'\\n', \n          colored('*'*100, 'red', attrs=['bold']),\n          colored(\"\\nInfo:\\n\", attrs=['bold']), sep='')\n    print(df.info(), '\\n', \n          colored('*'*100, 'red', attrs=['bold']), sep='')\n    print(colored(\"Number of Uniques:\\n\", attrs=['bold']), df.nunique(),'\\n',\n          colored('*'*100, 'red', attrs=['bold']), sep='')\n    print(colored(\"Missing Values:\\n\", attrs=['bold']), missing_values(df),'\\n', \n          colored('*'*100, 'red', attrs=['bold']), sep='')\n    print(colored(\"All Columns:\", attrs=['bold']), list(df.columns),'\\n', \n          colored('*'*100, 'red', attrs=['bold']), sep='')\n\n    df.columns= df.columns.str.lower().str.replace('&', '_').str.replace(' ', '_')\n    print(colored(\"Columns after rename:\", attrs=['bold']), list(df.columns),'\\n',\n          colored('*'*100, 'red', attrs=['bold']), sep='')\n    print(colored(\"Descriptive Statistics \\n\", attrs=['bold']), df.describe().round(2),'\\n',\n          colored('*'*100, 'red', attrs=['bold']), sep='') # Gives a statstical breakdown of the data.\n    print(colored(\"Descriptive Statistics (Categorical Columns) \\n\", attrs=['bold']), df.describe(include=object).T,'\\n',\n          colored('*'*100, 'red', attrs=['bold']), sep='') # Gives a statstical breakdown of the data.\n    \n###############################################################################\n\n# To view summary information about the columns\n\ndef first_look(col):\n    print(\"column name    : \", col)\n    print(\"--------------------------------\")\n    print(\"per_of_nulls   : \", \"%\", round(df[col].isnull().sum()\/df.shape[0]*100, 2))\n    print(\"num_of_nulls   : \", df[col].isnull().sum())\n    print(\"num_of_uniques : \", df[col].nunique())\n    print(\"Value counts   : \\n\", df[col].value_counts(dropna = False))    \n\n###############################################################################\n    \ndef multicolinearity_control(df):\n    feature =[]\n    collinear=[]\n    for col in df.corr().columns:\n        for i in df.corr().index:\n            if (abs(df.corr()[col][i])> .9 and abs(df.corr()[col][i]) < 1):\n                    feature.append(col)\n                    collinear.append(i)\n                    print(colored(f\"Multicolinearity alert in between:{col} - {i}\", \n                                  \"red\", attrs=['bold']), df.shape,'\\n',\n                                  colored('*'*100, 'red', attrs=['bold']), sep='')\n\n###############################################################################\n\ndef duplicate_values(df):\n    print(colored(\"Duplicate check...\", attrs=['bold']), sep='')\n    duplicate_values = df.duplicated(subset=None, keep='first').sum()\n    if duplicate_values > 0:\n        df.drop_duplicates(keep='first', inplace=True)\n        print(duplicate_values, colored(\" Duplicates were dropped!\"),'\\n',\n              colored('*'*100, 'red', attrs=['bold']), sep='')\n    else:\n        print(colored(\"There are no duplicates\"),'\\n',\n              colored('*'*100, 'red', attrs=['bold']), sep='')     \n\n###############################################################################\n        \ndef drop_columns(df, drop_columns):\n    if drop_columns !=[]:\n        df.drop(drop_columns, axis=1, inplace=True)\n        print(drop_columns, 'were dropped')\n    else:\n        print(colored('We will now check the missing values and if necessary, the realted columns will be dropped!', attrs=['bold']),'\\n',\n              colored('*'*100, 'red', attrs=['bold']), sep='')\n\n###############################################################################\n\ndef drop_null(df, limit):\n    print('Shape:', df.shape)\n    for i in df.isnull().sum().index:\n        if (df.isnull().sum()[i]\/df.shape[0]*100)>limit:\n            print(df.isnull().sum()[i], 'percent of', i ,'null and were dropped')\n            df.drop(i, axis=1, inplace=True)\n            print('new shape:', df.shape)       \n    print(colored(\"New shape after missing value control:\"),'\\n', df.shape)\n\n###############################################################################\n\ndef fill_most(df, group_col, col_name):\n    '''Fills the missing values with the most existing value (mode) in the relevant column according to single-stage grouping'''\n    for group in list(df[group_col].unique()):\n        cond = df[group_col]==group\n        mode = list(df[cond][col_name].mode())\n        if mode != []:\n            df.loc[cond, col_name] = df.loc[cond, col_name].fillna(df[cond][col_name].mode()[0])\n        else:\n            df.loc[cond, col_name] = df.loc[cond, col_name].fillna(df[col_name].mode()[0])\n    print(\"Number of NaN : \",df[col_name].isnull().sum())\n    print(\"------------------\")\n    print(df[col_name].value_counts(dropna=False))\n    \n###############################################################################","c7b09bff":"def explore(x):\n    divider = \"*_*\"\n    print(\"\\n {} \\n\".format((divider*20))) #creates a divider between each method output breaking at each end.\n    print(\"Dataframe Makeup \\n\") # title for output.\n    x.info() # Explains what the data and values the data is madeup from.\n    print(\"\\n {} \\n\".format((divider*20))) # creates a dvider between each method output breaking at each end.\n    print(\"Descriptive Statistics \\n\\n\", x.describe().round(2)) # Gives a statstical breakdown of the data.\n    print(\"\\n {} \\n\".format((divider*20))) # creates a divider between each method output breaking at each end.\n    print(\"Shape of dataframe: {}\".format(x.shape)) # Gives the shape of the data.\n    print(\"\\n {} \\n\".format((divider*20))) # creates a dvider between each method output breaking at each end.\n    return","d041bb0e":"df0 = pd.read_csv('..\/input\/onlineretail\/OnlineRetail.csv', encoding='latin1')\ndf = df0.copy()\ndf.head() ","7a8d2a6d":"# df.profile_report()","7476cbae":"first_looking(df)\nduplicate_values(df)\ndrop_columns(df, [])\ndrop_null(df, 90)","8e737a82":"df.columns","4bd3a648":"df.describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu')","853b9028":"df[df['unitprice'] < 0].shape","00848917":"df[[\"invoiceno\", \"quantity\", \"unitprice\"]].describe(include=object).T","0811e40c":"df['total_price'] = df['quantity'] * df['unitprice']\ndf.head(3)","f270214f":"df['invoiceno'].sample(10)","24ee7b88":"cprint(\"Have a First Look at 'invoice' Column\", 'blue')\nfirst_look('invoiceno')","e218a3e4":"cprint(\"Total number of invoices by country :\",'blue')\ndf.groupby('country')['invoiceno'].count().sort_values(ascending=False)","cdfa99dc":"fig = px.histogram(df, \n                   x = 'country', \n                   title = 'The Number of Invoices by Country', \n                   color='country').update_xaxes(categoryorder=\"total descending\")\nfig.show()","cbf9660b":"fig = px.histogram(df, \n                   x = df.groupby('country')['invoiceno'].nunique().index,\n                   y = df.groupby('country')['invoiceno'].nunique().values, \n                   title = 'The Unique Number of Invoices by Country', \n                   labels = dict(x = \"Countries\", y =\"Invoice\")).update_xaxes(categoryorder=\"total descending\")\nfig.show();","48c64337":"cprint(\"The Top Five 'invoiceno' By 'total_price':\", 'blue')\ndf.groupby(['invoiceno', 'customerid', 'country'])['total_price'].sum().sort_values(ascending=False).head()","0ae9964e":"cprint(\"The Bottom Five 'invoiceno' By 'total_price':\", 'blue')\ndf.groupby(['invoiceno','customerid', 'country'])['total_price'].sum().sort_values().head()","278fef52":"cprint(\"The Top Five 'invoiceno' By Quantity :\", 'blue')\ndf.groupby(['invoiceno','customerid', 'country'])['quantity'].sum().sort_values(ascending=False).head()","75fc6e00":"cprint(\"invoiceno Starts with A:\",'blue')\ndf['invoiceno'].str.startswith('A').value_counts()","fb7e8739":"cprint(\"invoiceno Starts with C:\",'blue')\ndf['invoiceno'].str.startswith('C').value_counts()","ca6f502d":"cprint(\"invoiceno Starts with A or C:\",'blue')\n(df['invoiceno'].str.startswith('C') | df['invoiceno'].str.startswith('A')).value_counts()","24e7710f":"CA_values = (df['invoiceno'].str.startswith('C') | df['invoiceno'].str.startswith('A')).value_counts()\nCA_values = pd.DataFrame(CA_values)\nCA_values.rename(index={False: 'Invoices Without C & A', True: 'Invoices With C & A'}, inplace=True)\nCA_values","d7a15bb0":"fig = px.pie(CA_values, \n             values = CA_values['invoiceno'], \n             names = CA_values.index, \n             title = \"The Percentage of 'invoiceno' Starts With A or C\")\n\nfig.show()","f0668516":"cprint(\"Have a First Look at 'stockcode' Column\",'blue')\nfirst_look('stockcode')","947bd94e":"cprint(\"Have a First Look at 'description' Column\",'blue')\nfirst_look('description')","aec28d8d":"cprint(\"Number of Missing Values\",'blue')\ndf[['stockcode', 'description']].isnull().sum()","4284e923":"cprint(\"Croos check\", 'blue')\ndf[(df['stockcode'].notnull()) & (df['description'].isnull())][['stockcode','description']]","ca22dcb5":"cprint(\"Croos check\", 'blue')\ndf[(df['description'].notnull()) & (df['stockcode'].isnull())][['stockcode','description']]","21343bdc":"cprint(\"Croos check\", 'blue')\ndf[df['stockcode']=='22139.0']['description'].value_counts(dropna=False)","b832f81d":"df[df['stockcode']=='22139.0'][['invoiceno','description']].value_counts()[:5]","3cbda4aa":"cprint(\"Have a First Look to 'stockcode' Column\",'blue')\nfirst_look('quantity')","84e08864":"cprint(\"The Number of Observations in the 'Quantity' Column Which are Below 0\",'blue')\n(df['quantity']<0).value_counts()","3e81c6a9":"cprint(\"The Number of Observations in the 'Quantity' Column Which are Above 0\",'blue')\ndf[df['quantity'] > 0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']]","1bad15f7":"cprint(\"The Number of Observations in the 'Quantity' Column Which Equal to 0\",'blue')\ndf[df['quantity'] == 0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']]","eec598d7":"cprint(\"The Number of Observations in the 'Quantity' Column Which are Below 0\",'blue')\ndf[df['quantity'] < 0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']]","9a5c3026":"(df['quantity'] < 0).value_counts()","cc2c0e0b":"quantity_values = (df['quantity'] < 0).value_counts()\nquantity_values = pd.DataFrame(quantity_values)\nquantity_values.rename(index={False: 'Invoices Smaller Than 0', True: 'Invoices Bigger Than 0'}, inplace=True)\nquantity_values","7bda17a1":"fig = px.pie(quantity_values, \n             values = quantity_values['quantity'], \n             names = quantity_values.index, \n             title = 'Quantity Smaller\/Bigger')\n\nfig.show()","4e66f591":"cprint(\"Have a First Look at 'unitprice' Column\",'blue')\nfirst_look('unitprice')","e6469e7f":"cprint(\"The Number of Items in the 'unitprice' Column Which are Above 0\",'blue')\ndf[df['unitprice'] > 0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']].head()","70847009":"cprint(\"The Number of Observations in the 'unitprice' Column Which Equal to 0\",'blue')\ndf[df['unitprice']==0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']].head()","b47c24f3":"cprint(\"The Number of Observations in the 'unitprice' Column Which are Below 0\",'blue')\ndf[df['unitprice']<0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']].head()","71caec32":"cprint(\"Have a First Look at 'customerid' Column\",'blue')\nfirst_look('customerid')","99ff1e93":"cprint(\"Customer Number by Orders (Top 5)\",'blue')\ndf.groupby(['customerid', 'country'])['invoiceno'].count().sort_values(ascending=False).head()","36a1b9a5":"cprint(\"Customer Number by Orders (Bottom 5)\",'blue')\ndf.groupby(['customerid', 'country'])['invoiceno'].count().sort_values().head()","384eaa57":"cprint(\"Customer Status According to the Number of Items in the Orders (Top 5)\",'blue')\ndf.groupby(['customerid', 'country'])['quantity'].sum().sort_values(ascending=False).head()","bd255383":"cprint(\"Customer Status According to the Number of Items in the Orders (Bottom 5)\",'blue')\ndf.groupby(['customerid', 'country'])['quantity'].sum().sort_values().head()","9989a743":"cprint(\"Customer Status According to the Spendings (Top 5)\",'blue')\ndf.groupby(['customerid', 'country'])['total_price'].sum().sort_values(ascending=False).head()","36ff070c":"cprint(\"Customer Status According to the Spendings (Bottom 5)\",'blue')\ndf.groupby(['customerid', 'country'])['total_price'].sum().sort_values().head()","04323d3a":"fig = px.histogram(df, x = df.groupby('country')['customerid'].nunique().index, \n                   y = df.groupby('country')['customerid'].nunique().values, \n                   title = 'The Number of Customers By Country',\n                   labels = dict(x = \"Countries\", y =\"Customer\")).update_xaxes(categoryorder=\"total descending\")\nfig.show()","7fd23ed0":"df_wo_UK = df.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False).iloc[1:]\n\nfig = px.histogram(df, x = df_wo_UK.index, \n                   y = df_wo_UK.values, \n                   title = 'The Number of Customers By Country Without UK',\n                   labels = dict(x = \"Countries\", y =\"Customer\")).update_xaxes(categoryorder=\"total descending\")\nfig.show()","42bf5a02":"cprint(\"Have a First Look at 'invoicedate' Column\",'blue')\nfirst_look('invoicedate')","73c09dfb":"df.invoicedate.max()","1bb8bf2b":"df.invoicedate.min()","13e50ac7":"# df[\"invoicedate\"] = pd.to_datetime(df[\"invoicedate\"])\n# df.groupby('customerid')['invoicedate'].max()","e77462e9":"df[['invoiceno', 'quantity', 'unitprice']].head(5)","66335faa":"df[[\"invoiceno\", \"quantity\", \"unitprice\"]].describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu')","ccf6676e":"df[[\"invoiceno\", \"quantity\", \"unitprice\"]].describe(include=object).T","0e5b4789":"df[df['quantity'] < 0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']]","efe7db19":"df[df['quantity'] > 0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']].head(1)","9224c63e":"df[df['quantity'] == 0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']]","0d461859":"df[df['unitprice'] < 0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']]","d528c4ca":"df[df['unitprice'] > 0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']]","b1f25645":"df[df['unitprice'] == 0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']]","c9950211":"df.columns","42c85b13":"df[['unitprice', 'quantity']].describe().T.style.background_gradient(subset = ['mean','std','50%','count'], cmap='RdPu')","d0011c53":"# This is our dataset which was performed some functions by first_looking() Def at the begining of study.\n\ndf[\"invoiceno\"].str.startswith('C').value_counts()","2c1e9d7a":"df[\"invoiceno\"].str.startswith('C').value_counts(normalize=True)*100","02a4ba6a":"df[\"invoiceno\"].str.startswith('C').sum()","2ace9c24":"# This is our raw datatset which we have at the begining of study.\n\ndf0[\"InvoiceNo\"].str.startswith('C').value_counts()","fa586f88":"df0[\"InvoiceNo\"].str.startswith('C').value_counts(normalize=True)*100","f2c1fbd8":"df[df[\"invoiceno\"].str.startswith('C')]['customerid'].nunique() \/ df['customerid'].nunique()","34631f34":"df[df[\"invoiceno\"].str.startswith('C')][[\"invoiceno\", \"quantity\", \"unitprice\"]]","97029023":"df[df[\"invoiceno\"].str.startswith('C')][[\"invoiceno\", \"quantity\", \"unitprice\"]].describe()","b8e4b98e":"df[['unitprice', 'quantity']].describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu')","e8062abf":"cprint(\"Cancelled Orders\",'blue')\ndf[df['quantity'] < 0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']]","05ab0b1b":"cancelled_orders = (df['invoiceno'].str.startswith('C').value_counts())\ncancelled_orders = pd.DataFrame(cancelled_orders)\ncancelled_orders.rename(index={False: 'Non-Cancelled Orders', True: 'Cancelled Orders'}, inplace=True)\ncancelled_orders","37937d34":"fig = px.pie(cancelled_orders, \n             values = cancelled_orders['invoiceno'], \n             names = cancelled_orders.index, \n             title = 'The Proportion of Canceled Orders')\n\nfig.show()","f8f9f07f":"cprint(\"Non-Cancelled Orders\",'blue')\ndf[df['quantity'] > 0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']]","4082a103":"df[df['unitprice'] < 0]","7c69b793":"df[df['unitprice'] == 0]","7a15ae89":"df.shape","5583853e":"missing_values(df)","e9d9893c":"df.isnull().melt(value_name=\"missing\")","84682663":"plt.figure(figsize = (10, 5))\n\nsns.displot(\n    data = df.isnull().melt(value_name = \"missing\"),\n    y = \"variable\",\n    hue = \"missing\",\n    multiple = \"fill\",\n    height = 9.25)\n\nplt.axvline(0.3, color = \"r\");","a0580ed0":"df = df.dropna(subset=[\"customerid\"])\ndf.shape","ebd33964":"missing_values(df)","444a8f7a":"plt.figure(figsize = (10, 5))\n\nsns.displot(\n    data = df.isnull().melt(value_name = \"missing\"),\n    y = \"variable\",\n    hue = \"missing\",\n    multiple = \"fill\",\n    height = 9.25)\n\nplt.axvline(0.3, color = \"r\");","742aaad8":"df.sample(10)","2aab6432":"df.shape","a19cb007":"missing_values(df)","28bc73b2":"df.sample(10)","68964ca4":"df = df[(df['unitprice'] > 0) & (df['quantity'] > 0)]","e93f4c66":"df.sample(10)","cc542961":"df.shape","86c8ff81":"print(\"There are\", df.duplicated(subset=None, keep='first').sum(), \"duplicated observations in the dataset.\")\nprint(df.duplicated(subset=None, keep='first').sum(), \"Duplicated observations are dropped!\")\ndf.drop_duplicates(keep='first', inplace=True)","249dcd14":"df.groupby(\"customerid\")[\"invoiceno\"].nunique()","a13b1147":"df.head()","89054cc3":"df['customerid'].nunique()","9c694657":"df[\"stockcode\"].nunique()","903c0f2f":"df[\"description\"].nunique() ","59950436":"df[\"description\"].value_counts()","5576d887":"cprint(\"The Average Number of Unqiue Items By Order\",'blue')\ndf.groupby([\"invoiceno\", \"stockcode\", \"description\"])[\"quantity\"].mean()","91ed3732":"cprint(\"The Average Number of Unqiue Items By Customer\",'blue')\ndf.groupby([\"customerid\", \"stockcode\", \"description\"])[\"quantity\"].mean()","c5e0d05a":"df.groupby(['customerid', 'invoiceno', 'stockcode', \"description\"])['quantity'].mean()","4da1f12a":"df.groupby(\"customerid\")[[\"stockcode\"]].nunique().sort_values(by=\"stockcode\", ascending=False)","02848bba":"df.head(3)","73e59b4f":"df['total_price'] = df['quantity'] * df['unitprice']\ndf.head(3)","a478bdf3":"df.groupby(\"country\")[['total_price']].sum().sort_values(by='total_price', ascending=False)","78b3b459":"total_revenue = df.groupby(\"country\")[\"total_price\"].sum().sort_values(ascending=False)\ntotal_revenue","2f87caf5":"customer_num = df.groupby(\"country\")['customerid'].nunique().sort_values(ascending=False)\ncustomer_num","f3a2f62f":"customer_num.sum()","38176a26":"df.groupby(\"customerid\")['country'].nunique().sum()","581e5ae8":"df['customerid'].nunique()","5c2a590b":"df.groupby('customerid')['country'].nunique().value_counts()","fe970671":"dfg1 = df.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False)\nfig = px.bar(x=dfg1.index, \n             y=dfg1, \n             title=\"Customers By Countries\", \n             labels=dict(x=\"Countries\", y=\"Total Number of Customers\"))\n\n# also works with graph_objects:\n# fig = go.Figure(go.Bar(x=dfg.index, y=dfg))\nfig.show()","3a256067":"dfg1_w_oUK = df.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False).iloc[1:]\nfig = px.bar(x=dfg1_w_oUK.index, \n             y=dfg1_w_oUK, \n             title=\"Customers By Countries Without The UK\", \n             labels=dict(x=\"Countries\", y=\"Total Number of Customers Without The UK\"))\n\n# also works with graph_objects:\n# fig = go.Figure(go.Bar(x=dfg.index, y=dfg))\nfig.show()","5390248d":"fig = px.treemap(dfg1, path=[dfg1.index], values='customerid', width=950, height=600)\nfig.update_layout(title_text='Customers By Countries',\n                  title_x=0.5, title_font=dict(size=20)\n                  )\nfig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\nfig.show()","ce528260":"dfg1","2e39a0bb":"dfg1 = pd.DataFrame(dfg1).reset_index()\ndfg1","944ab741":"dfg1[\"country\"] = dfg1[\"country\"].str.split(\" \").str.join(\"_\")\ndfg1","6c213a94":"dfg1_list = dict(zip(dfg1['country'].tolist(), dfg1['customerid'].tolist()))\ndfg1_list","8bf23c48":"#Define a list of stop words\nstopwords = ['country', 'customerid', 'total_price']\n\n#A function to generate the word cloud from text\ndef generate_wordcloud_frequencies(data, title):\n    cloud = WordCloud(width=400,\n                      height=200,\n                      background_color=\"#32fcbc\",\n                      max_words=150,\n                      colormap='seismic',\n                      stopwords=stopwords,\n                      collocations=True).generate_from_frequencies(data)\n    plt.figure(figsize=(13, 13))\n    plt.imshow(cloud)\n    plt.axis('off')\n    plt.title(title, fontsize=13)\n    plt.show()\n    \n#Use the function to generate the wordcloud by fequencies\ngenerate_wordcloud_frequencies(dfg1_list, 'Customers By Countries')","62953a96":"from IPython.display import display\nfrom PIL import Image\nimport numpy as np\n\n# Create an array from the image you want to use as a mask\n## Your file path will look different\n\npath=\"..\/input\/uk-map\/united-kingdom-map-vector-silhouette.jpg\"\ndisplay(Image.open(path))\nUK_mask = np.array(Image.open(path))\nUK_mask","ca599f80":"# A similar function, but using the mask\n\n#Define a list of stop words\nstopwords = ['country', 'customerid', 'total_price']\n\ndef generate_wordcloud_mask(data, title, mask=None):\n    cloud = WordCloud(#scale=3,\n                      width=350,\n                      height=400,\n                      #max_words=150,\n                      colormap='gist_heat',\n                      mask=mask,\n                      background_color='#bbfce8',\n                      stopwords=stopwords,\n                      collocations=True).generate_from_text(data)\n    plt.figure(figsize=(10, 8))\n    plt.imshow(cloud)\n    plt.axis('off')\n    plt.title(title)\n    plt.show()\n    \n# Use the function with the UK_mask and our mask to create wordcloud     \ngenerate_wordcloud_mask(str(dfg1), 'Customers By Countries', mask=UK_mask)","c7ce9451":"import plotly.graph_objects as go\n\ndfg2 = df.groupby('country')[\"total_price\"].sum().sort_values(ascending=False)\nfig = px.bar(x=dfg2.index, \n             y=dfg2, \n             title=\"Total Cost (\u00a3) By Countries\", \n             labels=dict(x=\"Countries\", y=\"Total Cost (\u00a3)\"))\n\n# also works with graph_objects:\n# fig = go.Figure(go.Bar(x=dfg.index, y=dfg))\nfig.show()","6ce29c1b":"dfg2_w_oUK = df.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False).iloc[1:]\nfig = px.bar(x=dfg2_w_oUK.index, \n             y=dfg2_w_oUK, \n             title=\"Customers By Countries Without The UK\", \n             labels=dict(x=\"Countries\", y=\"Total Number of Customers Without The UK\"))\n\n# also works with graph_objects:\n# fig = go.Figure(go.Bar(x=dfg.index, y=dfg))\nfig.show()","d667f330":"fig = px.treemap(dfg2, path=[dfg2.index], values='total_price', width=950, height=600)\nfig.update_layout(title_text='Total Cost (\u00a3) By Countries',\n                  title_x=0.5, title_font=dict(size=20)\n                  )\nfig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\nfig.show()","283c487b":"dfg2 = pd.DataFrame(dfg2).reset_index()\ndfg2[\"country\"] = dfg2[\"country\"].str.split(\" \").str.join(\"_\")\ndfg2","82ab4b86":"dfg2_list = dict(zip(dfg1['country'].tolist(), dfg2['total_price'].tolist()))\ndfg2_list","07297703":"#Use the function to generate the wordcloud by frequencies\n\ngenerate_wordcloud_frequencies(dfg2_list, 'Total Cost (\u00a3) By Countries')","0617e13e":"# Use the function with the UK_mask and our mask to create wordcloud     \n\ngenerate_wordcloud_mask(str(dfg1), 'Total Cost (\u00a3) By Countries', mask=UK_mask)","063dae41":"df_uk = df[df[\"country\"]==\"United Kingdom\"]\ndf_uk.head(3)","9ec76d28":"df_uk.shape","e1ab5c88":"df_uk.groupby(\"stockcode\")[\"quantity\"].sum().sort_values(ascending=False).head(1)","9df1d7d1":"df_uk.groupby([\"stockcode\", \"description\"])[[\"quantity\"]].sum().sort_values(by=\"quantity\", ascending=False)","a35e9417":"import datetime as dt\nfrom datetime import datetime\nfrom datetime import timedelta","da537da0":"df_uk.head()","c9651354":"first_looking(df_uk)\nduplicate_values(df_uk)","3691399f":"# Top 15 most purchased products.\n\ndf_uk[\"stockcode\"].value_counts().head(15).plot(kind=\"bar\", width=0.5, color='pink', edgecolor='purple', figsize=(14, 6))\nplt.xticks(rotation=45);","426b4c4e":"cprint(\"Top 10 Demanded Products By Quantity\",'blue')\ndf_uk.groupby(\"stockcode\")['quantity'].sum().sort_values(ascending=False)[:10].iplot(kind=\"bar\", width=0.5, color='cyan');","4a699ce1":"cprint(\"Top 10 Demanded Products By Total_Price\",'blue')\ndf_uk.groupby(\"stockcode\")['total_price'].sum().sort_values(ascending=False)[:10].iplot(kind=\"bar\", width=0.5, color='#46ADF5');","00c13be7":"min_invoice_date = min(df_uk['invoicedate'])\nmin_invoice_date","799c3535":"max_invoice_date = max(df_uk['invoicedate'])\nmax_invoice_date","04116de1":"df_uk.info()","0f0fb743":"df_uk['last_purchase_date'] = df_uk.groupby('customerid')['invoicedate'].transform(max)","c1c06ee9":"df_uk['last_purchase_date'] = pd.to_datetime(df_uk['last_purchase_date']).dt.date","9782c761":"df_uk.head()","cdbeec15":"df_uk.sample(10)","1ff59579":"df_uk.info()","c5ef9b3c":"df_uk['last_purchase_date'] = pd.to_datetime(df_uk['last_purchase_date'])\ndf_uk['invoicedate'] = pd.to_datetime(df_uk['invoicedate'])","034f192a":"df_uk.info()","53328a79":"# alternative code\n# ref_date = datetime(2011, 12, 16)\n# ref_date = df_uk['invoicedate'].max()\n\ndf_uk['ref_date'] = df_uk['invoicedate'].max() + timedelta(days=7)","44c51638":"df_uk.info()","b6fdb537":"df_uk['ref_date'] = df_uk['ref_date'].dt.date","f2be9c00":"df_uk.sample(5)","8ff1fa4a":"df_uk['ref_date'] = pd.to_datetime(df_uk['ref_date'])","56f60892":"df_uk.info()","fdb70877":"df_uk['date'] = pd.to_datetime(df_uk['invoicedate'])","6673ef99":"df_uk['date'] = df_uk['date'].dt.date","66521b4e":"df_uk.sample(10)","7e8f5e80":"df_uk.info()","d7d49f25":"customer_recency = pd.DataFrame(df_uk.groupby('customerid', as_index=False).date.max())\ncustomer_recency.head()","970a4ecc":"df_uk[df_uk[\"customerid\"] == 12346.00][[\"customerid\", 'last_purchase_date']]","b881c82a":"customer_recency.info()","af92febf":"df_uk.head(3)","f0fe2806":"df_uk[\"customer_recency\"] = df_uk[\"ref_date\"] - df_uk[\"last_purchase_date\"]\ndf_uk[[\"customerid\", 'last_purchase_date', \"ref_date\", \"customer_recency\"]]","2e299e2a":"df_uk['recency2'] = pd.to_numeric(df_uk['customer_recency'].dt.days.astype('int64'))\ndf_uk[[\"customerid\", 'last_purchase_date', \"ref_date\", \"customer_recency\", 'recency2']]\n\n# alternative code\n# df_uk[\"recency\"] = df_uk.groupby('customerid')['last_purchase_date'].apply(lambda x: ref_date - x)\n# df_uk['recency'] = pd.to_numeric(df_uk['recency'].dt.days, downcast='integer')","98c04015":"customer_recency = df_uk.groupby('customerid', as_index=False)['recency2'].mean()\ncustomer_recency.rename(columns={'recency2':'Recency'}, inplace=True)\ncustomer_recency.sort_values(by='Recency', ascending=False).head()","24c14730":"df_uk.drop(['last_purchase_date'], axis = 1, inplace=True)\ndf_uk.head(3)","dc7d15ec":"%matplotlib inline\n\nplt.figure(figsize=(10, 8))\ngraph = sns.scatterplot(data=df_uk, x=\"customerid\", y=\"recency2\")\ngraph.axhline(100, color=\"blue\")\nplt.show();","ea96daea":"fig = px.scatter(df_uk, x=\"customerid\", y=\"recency2\")\nfig.show()","d61422eb":"fig = px.histogram(df_uk, x=\"recency2\", nbins=70)\nfig.show()","e6e18205":"dfUK_copy = df_uk.copy()\ndfUK_copy.head(3)","226aea7c":"print(\"There are\", dfUK_copy.duplicated(subset=None, keep='first').sum(), \"duplicated observations in the dataset.\")\nprint(dfUK_copy.duplicated(subset=None, keep='first').sum(), \"Duplicated observations are dropped!\")\ndfUK_copy.drop_duplicates(keep='first', inplace=True)","9e8004f9":"customer_frequency = dfUK_copy.groupby('customerid', as_index=False)['invoiceno'].nunique()\ncustomer_frequency.rename(columns={'invoiceno':'Frequency'}, inplace=True)\ncustomer_frequency.sort_values(by='Frequency', ascending=False)","04a7990b":"customer_frequency.nlargest(5, \"Frequency\")","8d7e6467":"fig = px.scatter(customer_frequency, x=\"customerid\", y=\"Frequency\")\nfig.show()","ffededb1":"fig = px.histogram(customer_frequency, x=\"customerid\", y=\"Frequency\", nbins=3920)\nfig.show()","4356c0cd":"plt.figure(figsize=(12, 6))\nsns.distplot(customer_frequency['Frequency'], kde=False, bins=50);","224b77fe":"customer_monetary = dfUK_copy.groupby('customerid', as_index=False)['total_price'].sum()\ncustomer_monetary.rename(columns={'total_price':'Monetary'}, inplace=True)\ncustomer_monetary.sort_values(by='Monetary', ascending=False).head()","4c4b3778":"customer_monetary.nlargest(5, \"Monetary\")","1fd1d3d3":"plt.figure(figsize=(14, 7))\nsns.scatterplot(data=customer_monetary.Monetary);","1d2cb450":"fig = px.scatter(customer_monetary, x=\"customerid\", y=\"Monetary\")\nfig.show()","c8fd9c5f":"fig = px.histogram(customer_monetary, x=\"Monetary\", nbins=50)\nfig.show()","6a510bc6":"dfUK_copy.head(3)","d3ba09d3":"customer_rfm = pd.merge(pd.merge(customer_recency, customer_frequency, on='customerid'), customer_monetary, on='customerid')\ncustomer_rfm.head()","62b70f91":"customer_rfm.info()","7d82db54":"customer_rfm.describe()","80f251ba":"quantiles = customer_rfm.quantile(q = [0.25, 0.50, 0.75])\nquantiles","9b1f4026":"def recency_scoring(rfm):\n    if rfm.Recency <= 24.0:\n        recency_score = 4\n    elif rfm.Recency <= 57.0:\n        recency_score = 3\n    elif rfm.Recency <= 149.0:\n        recency_score = 2\n    else:\n        recency_score = 1\n    return recency_score\n\ncustomer_rfm['Recency_Score'] = customer_rfm.apply(recency_scoring, axis=1)\ncustomer_rfm.sample(10)","0630a872":"fig = px.pie(df, values = customer_rfm['Recency_Score'].value_counts(), \n             names = (customer_rfm[\"Recency_Score\"].value_counts()).index, \n             title = 'Recency Score Distribution')\nfig.show()","91e7129e":"def frequency_scoring(rfm):\n    if rfm.Frequency >= 10.0:\n        frequency_score = 4\n    elif rfm.Frequency >= 5.0:\n        frequency_score = 3\n    elif rfm.Frequency >= 2.0:\n        frequency_score = 2\n    else:\n        frequency_score = 1\n    return frequency_score\n\ncustomer_rfm['Frequency_Score'] = customer_rfm.apply(frequency_scoring, axis=1)\ncustomer_rfm.sample(10)","9568d79a":"fig = px.pie(df, values = customer_rfm['Frequency_Score'].value_counts(), \n             names = (customer_rfm[\"Frequency_Score\"].value_counts()).index, \n             title = 'Frequency Score Distribution')\nfig.show()","3a4043bd":"def monetary_scoring(rfm):\n    if rfm.Monetary >= 1571.0:\n        monetary_score = 4\n    elif rfm.Monetary >= 645.0:\n        monetary_score = 3\n    elif rfm.Monetary >= 298.0:\n        monetary_score = 2\n    else:\n        monetary_score = 1\n    return monetary_score\n\ncustomer_rfm['Monetary_Score'] = customer_rfm.apply(monetary_scoring, axis=1)\ncustomer_rfm.sample(10)","dd08e356":"fig = px.pie(df, values = customer_rfm['Monetary_Score'].value_counts(), \n             names = (customer_rfm[\"Monetary_Score\"].value_counts()).index, \n             title = 'Monetary Score Distribution')\nfig.show()","a18481ee":"customer_rfm.info()","c296350f":"def rfm_scoring(customer):\n    return str(int(customer['Recency_Score'])) + str(int(customer['Frequency_Score'])) + str(int(customer['Monetary_Score']))\n\n\ncustomer_rfm['Customer_RFM_Score'] = customer_rfm.apply(rfm_scoring, axis=1)\ncustomer_rfm.sample(8)","086bb811":"fig = px.histogram(customer_rfm, x = customer_rfm['Customer_RFM_Score'].value_counts().index, \n                   y = customer_rfm['Customer_RFM_Score'].value_counts().values, \n                   title = 'Customer RFM Score Distribution',\n                   labels = dict(x = \"Customer_RFM_Score\", y =\"counts\"))\nfig.show()","4497d09b":"customer_rfm['RFM_Label'] = customer_rfm['Recency_Score'] + customer_rfm['Frequency_Score'] + customer_rfm['Monetary_Score']\n\ncustomer_rfm.sample(8)","3330a637":"customer_rfm.info()","31125397":"customer_rfm.groupby(['Customer_RFM_Score']).size().sort_values(ascending=False)[:]","9f5399b6":"customer_rfm[customer_rfm['Customer_RFM_Score']=='111'].head(3)","a8041791":"customer_rfm[customer_rfm['Customer_RFM_Score']=='123'].head(3)","d0d8309c":"customer_rfm[customer_rfm['Customer_RFM_Score']=='214'].head(3)","29b8b523":"customer_rfm[customer_rfm['Customer_RFM_Score']=='444'].head(3)","fbdc2468":"fig = px.pie(df, values = customer_rfm['RFM_Label'].value_counts(), \n             names = (customer_rfm[\"RFM_Label\"].value_counts()).index, \n             title = 'RFM Label Distribution')\nfig.show()","3eac9cb5":"customer_rfm['RFM_Label'].min()","45f9cec2":"customer_rfm['RFM_Label'].max()","c6eee788":"np.sort(customer_rfm['RFM_Label'].unique())","5648d886":"segments = {'Customer_Segment':['Champion', \n                                'Top Loyal Customer', \n                                'Loyal Customer', \n                                'Top Recent Customer', \n                                'Recent Customer', \n                                'Top Customer Needed Attention', \n                                'Customer Needed Attention', \n                                'Top Lost Customer', \n                                'Lost Customer'],\n            'RFM':['(2|3|4)-(4)-(4)', \n                   '(3)-(1|2|3|4)-(3|4)', \n                   '(3)-(1|2|3|4)-(1|2)', \n                   '(4)-(1|2|3|4)-(3|4)', \n                   '(4)-(1|2|3|4)-(1|2)',\n                   '(2|3)-(1|2|3|4)-(3|4)', \n                   '(2|3)-(1|2|3|4)-(1|2)',\n                   '(1)-(1|2|3|4)-(3|4)', \n                   '(1)-(1|2|3|4)-(1|2)',]}\n\npd.DataFrame(segments)","be76dedb":"def categorizer(rfm):\n    \n    if (rfm[0] in ['2', '3', '4']) & (rfm[1] in ['4']) & (rfm[2] in ['4']):\n        rfm = 'Champion'\n        \n    elif (rfm[0] in ['3']) & (rfm[1] in ['1', '2', '3', '4']) & (rfm[2] in ['3', '4']):\n        rfm = 'Top Loyal Customer'\n        \n    elif (rfm[0] in ['3']) & (rfm[1] in ['1', '2', '3', '4']) & (rfm[2] in ['1', '2']):\n        rfm = 'Loyal Customer'\n    \n    elif (rfm[0] in ['4']) & (rfm[1] in ['1', '2', '3', '4']) & (rfm[2] in ['3', '4']):\n        rfm = 'Top Recent Customer'\n    \n    elif (rfm[0] in ['4']) & (rfm[1] in ['1', '2', '3', '4']) & (rfm[2] in ['1', '2']):\n        rfm = 'Recent Customer'\n    \n    elif (rfm[0] in ['2', '3']) & (rfm[1] in ['1', '2', '3', '4']) & (rfm[2] in ['3', '4']):\n        rfm = 'Top Customer Needed Attention'    \n   \n    elif (rfm[0] in ['2', '3']) & (rfm[1] in ['1', '2', '3', '4']) & (rfm[2] in ['1', '2']):\n        rfm = 'Customer Needed Attention'\n    \n    elif (rfm[0] in ['1']) & (rfm[1] in ['1', '2', '3', '4']) & (rfm[2] in ['3', '4']):\n        rfm = 'Top Lost Customer'\n                \n    elif (rfm[0] in ['1']) & (rfm[1] in ['1', '2', '3', '4']) & (rfm[2] in ['1', '2']):\n        rfm = 'Lost Customer'\n    \n    return rfm ","a47bd906":"customer_rfm['Customer_Category'] = customer_rfm[\"Customer_RFM_Score\"].apply(categorizer)\ncustomer_rfm","139fb2bf":"customer_rfm.groupby('Customer_Category').RFM_Label.mean()","378eb28c":"customer_rfm['Customer_Category'].value_counts(dropna=False, normalize=True)*100","3ab55aef":"customer_rfm[customer_rfm['Customer_Category'] == \"Recent Customer\"].sample(8)","b505a2be":"customer_rfm[customer_rfm['Customer_Category'] == \"Top Recent Customer\"].sample(8)","2673d823":"customer_rfm[customer_rfm['Customer_Category'] == \"Champion\"].sample(8)","2d226a18":"customer_rfm[customer_rfm['Customer_Category'] == \"Top Loyal Customer\"].sample(8)","eeeb9f2d":"customer_rfm[customer_rfm['Customer_Category'] == \"Lost Customer\"].sample(8)","203cccc7":"fig = px.histogram(customer_rfm, \n                   x = customer_rfm['Customer_Category'].value_counts().index, \n                   y = customer_rfm['Customer_Category'].value_counts().values, \n                   title = 'Customer Category Distribution',\n                   labels = dict(x = \"Customer_Category\", y =\"counts\"))\nfig.show()","44fb6714":"fig = px.pie(df, \n             values = customer_rfm['Customer_Category'].value_counts(), \n             names = (customer_rfm[\"Customer_Category\"].value_counts()).index, \n             title = 'Customer Category Distribution')\nfig.show()","1f340c36":"customer_rfm","e5e7c0c9":"Avg_RFM_Label = customer_rfm.groupby('Customer_Category').RFM_Label.mean()\nSize_RFM_Label = customer_rfm['Customer_Category'].value_counts()\ndf_customer_segmentation = pd.concat([Avg_RFM_Label, Size_RFM_Label], axis=1).rename(columns={'RFM_Label':'Avg_RFM_Label',\n                                                                           'Customer_Category':'Size_RFM_Label'})\ndf_customer_segmentation","018ed747":"fig = px.histogram(customer_rfm, \n                   x = customer_rfm.groupby('Customer_Category').RFM_Label.mean().sort_values(ascending=False).index, \n                   y = customer_rfm.groupby('Customer_Category').RFM_Label.mean().sort_values(ascending=False).values, \n                   title = 'The Average of RFM Label',\n                   labels = dict(x = \"Customer Segments (Categories)\", y =\"RFM Label Mean Values\"))\nfig.show()","a4f37131":"fig = px.treemap(df_customer_segmentation, \n                 path=[df_customer_segmentation.index], \n                 values='Avg_RFM_Label', \n                 width=950, height=600)\n\nfig.update_layout(title_text='The Average of Each Customer Segments',\n                  title_x=0.5, title_font=dict(size=20)\n                  )\nfig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\nfig.show()","fb51ecb9":"fig = px.histogram(customer_rfm, \n                   x = customer_rfm['Customer_Category'].value_counts().index, \n                   y = customer_rfm['Customer_Category'].value_counts().values, \n                   title = 'The Size of RFM Label',\n                   labels = dict(x = \"Customer Segments (Categories)\", y =\"RFM Label Mean Values\"))\nfig.show()","f29879d5":"fig = px.treemap(df_customer_segmentation, \n                 path=[df_customer_segmentation.index], \n                 values='Size_RFM_Label', \n                 width=950, height=600)\n\nfig.update_layout(title_text='Customer Segmentation',\n                  title_x=0.5, title_font=dict(size=20)\n                  )\nfig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\nfig.show()","d3e883ea":"customer_rfm.head(3)","1cebe9d2":"segmentation = pd.DataFrame(customer_rfm.Customer_Category.value_counts(dropna=False).sort_values(ascending=False))\nsegmentation.reset_index(inplace=True)\nsegmentation.rename(columns={'index':'Customer Category', 'Customer_Category':'The Number Of Customer'}, inplace=True)\nsegmentation","717b97b1":"plt.figure(figsize=(19, 8))\n\nsns.barplot(data=segmentation, x='Customer Category', y='The Number Of Customer', palette='Oranges_r');","ca4f297f":"plt.figure(figsize=(14, 7))\nsns.lineplot(x=df_customer_segmentation.index, y=df_customer_segmentation.Avg_RFM_Label)\nplt.xticks(rotation=30, fontsize=14);","bb8d299b":"fig = px.bar(segmentation, x='Customer Category', y='The Number Of Customer')\nfig.show()","d2dd43fa":"segmentation","49919b55":"#import squarify as sq\n\nfig = plt.gcf()\nax = fig.add_subplot()\nfig.set_size_inches(13, 8)\nsq.plot(sizes=segmentation['The Number Of Customer'], \n                      label=['Lost Customer', \n                            'Customer Needed Attention', \n                            'Top Recent Customer', \n                            'Top Loyal Customer', \n                            'Top Customer Needed Attention', \n                            'Loyal Customer', \n                            'Champion', \n                            'Recent Customer', \n                            'Top Lost Customer'], \n                            alpha=0.8, \n                            color=[\"red\", \"#48BCF5\", \"#DD6AE1\", \"blue\", \"cyan\", \"magenta\", '#B20CB7', \"#A4E919\"])\nplt.title(\"RFM Segments\", fontsize=18, fontweight=\"bold\")\nplt.axis('off')\nplt.show()","015061c9":"# import plotly.express as px\n\nfig = px.treemap(segmentation,\n                 path=[segmentation['Customer Category']], \n                 values='The Number Of Customer', \n                 width=900, \n                 height=600)\nfig.update_layout(title=\"RFM Segments\",\n                  title_x = 0.5, title_font = dict(size=18),\n                 )\nfig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\nfig.show()","abe8f375":"segment_text = segmentation[\"Customer Category\"].str.split(\" \").str.join(\"_\")\nall_segments = \" \".join(segment_text)\n\nwc = WordCloud(background_color=\"orange\", \n               #max_words=250, \n               max_font_size=256, \n               random_state=42,\n               width=800, height=400)\nwc.generate(all_segments)\nplt.figure(figsize = (16, 15))\nplt.imshow(wc)\nplt.title(\"RFM Segments\", fontsize=18, fontweight=\"bold\")\nplt.axis('off')\nplt.show()","73d3c7dd":"customer_rfm.sample(5)","fb1202df":"customer_rfm.groupby('RFM_Label')[['Recency_Score', 'Frequency_Score', 'Monetary_Score']].describe().style.background_gradient(cmap='RdPu')","37674cf5":"customer_rfm.corr()","d8ab9208":"df_temp = customer_rfm.corr()\n\nfeature =[]\ncollinear=[]\n\nfor col in df_temp.columns:\n    for i in df_temp.index:\n        if (df_temp[col][i]> .85 and df_temp[col][i] < 1) or (df_temp[col][i]< -.85 and df_temp[col][i] > -1) :\n                feature.append(col)\n                collinear.append(i)\n                print(Fore.RED + f\"\\033[1mmulticolinearity alert\\033[0m between {col} - {i}\")\n        else:\n            print(f\"For {col} and {i}, there is \\033[1mNO multicollinearity problem\\033[0m\") \n\nunique_list = list(set(feature+collinear))\n\nprint(colored('*'*80, 'cyan', attrs=['bold']))\nprint(\"\\033[1mThe total number of strong corelated features:\\033[0m\", len(unique_list)) ","edea173f":"sns.set_style(\"whitegrid\")\nplt.figure(figsize=(14, 10))\n\n# Getting the Upper Triangle of the co-relation matrix\nmatrix = np.triu(customer_rfm.corr())\n\n# using the upper triangle matrix as mask \nsns.heatmap(customer_rfm.corr(), annot=True, cmap = sns.cubehelix_palette(8), mask=matrix)\n\nplt.xticks(rotation=45);","89a9d439":"customer_rfm.head(3)","cb718d2b":"g = sns.scatterplot(data=customer_rfm, x=\"RFM_Label\", y=\"Recency_Score\", hue=\"Customer_Category\")\ng.legend(loc='center left', bbox_to_anchor=(1, 0.85), ncol=1);","344467f1":"g = sns.scatterplot(data=customer_rfm, x=\"RFM_Label\", y=\"Monetary_Score\", hue=\"Customer_Category\")\ng.legend(loc='center left', bbox_to_anchor=(1, 0.85), ncol=1);","37c3aca0":"g = sns.scatterplot(data=customer_rfm, x=\"RFM_Label\", y=\"Frequency_Score\", hue=\"Customer_Category\")\ng.legend(loc='center left', bbox_to_anchor=(1, 0.85), ncol=1);","58d36414":"plt.figure(figsize=(20, 20))\nsns.pairplot(customer_rfm[['Recency', 'Frequency', 'Monetary','Customer_Category']], hue='Customer_Category');","62e32363":"fig = px.scatter_matrix(customer_rfm, \n                        dimensions=['Recency', 'Frequency', 'Monetary'], \n                        color=\"Customer_Category\",\n                        width=1000, height=800)\n\nfig.show()","32426d50":"customer_rfm[['Recency', 'Frequency', 'Monetary']].iplot(kind='histogram', subplots=True, bins=50)","1b67b703":"# plot the distribution of RFM values\n\nf, ax = plt.subplots(figsize=(10, 12))\nplt.subplot(3, 1, 1); sns.distplot(customer_rfm.Recency, label = 'Recency')\nplt.subplot(3, 1, 2); sns.distplot(customer_rfm.Frequency, label = 'Frequency')\nplt.subplot(3, 1, 3); sns.distplot(customer_rfm.Monetary, label = 'Monetary')\nplt.style.use('fivethirtyeight')\nplt.tight_layout()\nplt.show()","9c6c5210":"customer_rfm[['Recency', 'Frequency', 'Monetary']].sample(10)","d92bea41":"matrix = np.triu(customer_rfm[['Recency','Frequency','Monetary']].corr())\nfig, ax = plt.subplots(figsize=(11, 6)) \nsns.heatmap (customer_rfm[['Recency','Frequency','Monetary']].corr(), \n             annot=True, \n             fmt= '.2f', \n             vmin=-1, \n             vmax=1, \n             center=0, \n             cmap='coolwarm',\n             mask=matrix, \n             ax=ax);","24a36b88":"customer_rfm.set_index(\"customerid\", inplace=True)","217ef53a":"# **Handling with Skewness - np.log**\n\nskew_limit = 0.75 # This is our threshold-limit to evaluate skewness. Overall below abs(1) seems acceptable for the linear models. \nskew_vals = customer_rfm[['Recency', 'Frequency', 'Monetary']].skew()\nskew_cols = skew_vals[abs(skew_vals)> skew_limit].sort_values(ascending=False)\nskew_cols","5015df27":"rfm_log = customer_rfm[skew_cols.index].copy()\n\nfor col in skew_cols.index.values:\n    rfm_log[col] = rfm_log[col].apply(np.log1p)\n\n    print(rfm_log.skew())\nprint()\n\nrfm_log.iplot(kind='histogram', subplots=True, bins=50);","7b6223eb":"# Interpreting Skewness \n\nfor skew in rfm_log.skew():\n    if -0.75 < skew < 0.75:\n        print (\"A skewness value of\", '\\033[1m', Fore.GREEN, skew, '\\033[0m', \"means that the distribution is approx.\", '\\033[1m', Fore.GREEN, \"symmetric\", '\\033[0m')\n    elif  -0.75 < skew < -1.0 or 0.75 < skew < 1.0:\n        print (\"A skewness value of\", '\\033[1m', Fore.YELLOW, skew, '\\033[0m', \"means that the distribution is approx.\", '\\033[1m', Fore.YELLOW, \"moderately skewed\", '\\033[0m')\n    else:\n        print (\"A skewness value of\", '\\033[1m', Fore.RED, skew, '\\033[0m', \"means that the distribution is approx.\", '\\033[1m', Fore.RED, \"highly skewed\", '\\033[0m')","f8f6d9ff":"rfm_log.head(10)","0412862c":"# **Handling with Skewness - Power Transformer**\n\nrfm_before_trans = customer_rfm[skew_cols.index].copy()\npt = PowerTransformer(method='yeo-johnson')\ntrans= pt.fit_transform(rfm_before_trans)\nrfm_trans = pd.DataFrame(trans, columns =skew_cols.index )\n\nprint(rfm_trans.skew())\nprint()","f4499bc3":"# Interpreting Skewness \n\nfor skew in rfm_trans.skew():\n    if -0.75 < skew < 0.75:\n        print (\"A skewness value of\", '\\033[1m', Fore.GREEN, skew, '\\033[0m', \"means that the distribution is approx.\", '\\033[1m', Fore.GREEN, \"symmetric\", '\\033[0m')\n    elif  -0.75 < skew < -1.0 or 0.75 < skew < 1.0:\n        print (\"A skewness value of\", '\\033[1m', Fore.YELLOW, skew, '\\033[0m', \"means that the distribution is approx.\", '\\033[1m', Fore.YELLOW, \"moderately skewed\", '\\033[0m')\n    else:\n        print (\"A skewness value of\", '\\033[1m', Fore.RED, skew, '\\033[0m', \"means that the distribution is approx.\", '\\033[1m', Fore.RED, \"highly skewed\", '\\033[0m')\n","d87bb6fd":"rfm_log.iplot(kind='histogram',subplots=True,bins=50);","3bb4f544":"rfm_trans.iplot(kind='histogram', subplots=True, bins=50);","865036bf":"f, ax = plt.subplots(figsize=(10, 12))\nplt.subplot(3, 1, 1); sns.distplot(rfm_trans.Recency, label = 'Recency')\nplt.subplot(3, 1, 2); sns.distplot(rfm_trans.Frequency, label = 'Frequency')\nplt.subplot(3, 1, 3); sns.distplot(rfm_trans.Monetary, label = 'Monetary')\nplt.style.use('default')\nplt.tight_layout()\nplt.show()","ff70695b":"# we will continue to the analyses with power tranformed data\n\nplt.figure(figsize = (20,20));\nsns.pairplot(rfm_trans);","ffb201e7":"plt.figure(figsize = (15, 8))\nsns.scatterplot(data = rfm_trans);","d16cc747":"plt.figure(figsize = (15, 8))\nsns.boxplot(data = rfm_trans);","0a77e7f5":"rfm_trans.head()","bc0303e9":"fig = px.scatter_3d(rfm_trans, \n                    x='Recency',\n                    y='Frequency',\n                    z='Monetary',\n                    color='Frequency')\nfig.show();","046e1410":"!pip install pyclustertend","fd1d0fdb":"# !pip install pyclustertend\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom pyclustertend import hopkins\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics.cluster import adjusted_rand_score\nfrom sklearn.metrics import silhouette_samples,silhouette_score\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nfrom yellowbrick.cluster import KElbowVisualizer","e49199dc":"# Normalize the variables with StandardScaler\n# from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(rfm_trans)\n\n#Store it separately for clustering\nrfm_scaled = scaler.transform(rfm_trans)","8da159dc":"# hopkins(rfm_log, rfm_log.shape[0])\n\nhopkins(rfm_scaled, rfm_scaled.shape[0])","15173fbd":"#First : Get the Best KMeans \nks = range(1, 12)\ninertias=[]\nfor k in ks :\n    # Create a KMeans clusters\n    kc = KMeans(n_clusters=k, random_state=42)\n    kc.fit(rfm_scaled)\n    inertias.append(kc.inertia_)\n\n# Plot ks vs inertias\nf, ax = plt.subplots(figsize=(15, 8))\nplt.plot(ks, inertias, '-o')\nplt.xlabel('Number of clusters, k')\nplt.ylabel('Inertia')\nplt.xticks(ks)\nplt.style.use('ggplot')\nplt.title('What is the Best Number for KMeans?')\nplt.show()","e65f20f7":"# we will use rfm_scaled data from mow instead of rfm_log\n\nplt.rcParams['figure.facecolor'] = 'white'\nmodel = KMeans()\nvisualizer = KElbowVisualizer(model, k=(1, 12))\n\nplt.figure(figsize=(14, 8))\nvisualizer.fit(rfm_scaled)        # Fit the data to the visualizer\nvisualizer.show() ","da9c095d":"ssd =[]\n\nK = range(2, 12)\n\nfor k in K:\n    model = KMeans(n_clusters=k)\n    model.fit(rfm_scaled)\n    ssd.append(model.inertia_)\n    print(f'Silhouette Score for {k} clusters: {silhouette_score(rfm_scaled, model.labels_)}')","9cada28b":"from sklearn.cluster import KMeans\n\nfrom yellowbrick.cluster import SilhouetteVisualizer\n\nmodel_3 = KMeans(n_clusters=3, random_state=42)\nvisualizer = SilhouetteVisualizer(model_3)\n\nvisualizer.fit(rfm_scaled)    # Fit the data to the visualizer\nvisualizer.poof();","6bdf7500":"from yellowbrick.cluster import InterclusterDistance\nplt.rcParams[\"figure.figsize\"] = (10, 7)\n\n# Instantiate the clustering model and visualizer\nmodel = KMeans(3)\nvisualizer = InterclusterDistance(model_3)\n\nvisualizer.fit(rfm_scaled)  # Fit the data to the visualizer\nvisualizer.show()  # Finalize and render the figure","c565ae4d":"# prediction was added\n\nkmeans = KMeans(n_clusters = 3).fit(rfm_scaled)\nkmeans.fit_predict(rfm_scaled)\nlabels = kmeans.labels_\nrfm_trans['ClusterID']=labels\nrfm_trans","6e2afbc0":"rfm_trans.ClusterID.value_counts()","1bab9ed9":"fig = px.pie(df, values = rfm_trans['ClusterID'].value_counts(), \n             names = (rfm_trans['ClusterID'].value_counts()).index, \n             title = 'Predicted Clusters Distribution')\nfig.show()","97883500":"customer_rfm['ClusterID'] = labels\ncustomer_rfm.sample(20)","ba7b4dff":"rfm_trans.sample(8)","906f3564":"f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=(14, 6)) # sharey=True ile y eksen labels lari ortak kullanirlar.\n\nax1.set_title('Recency-Frequency')\nax1.set_xlabel('Recency')\nax1.set_ylabel('Frequency')\nax1.scatter(rfm_trans.iloc[:, 0], rfm_trans.iloc[:, 1], c=kmeans.labels_, cmap=\"rainbow\")\nax1.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, alpha=0.9, label = 'Centroids')\n\nax2.set_title(\"Frequency-Monetary\")\nax2.set_xlabel('Frequency')\nax2.set_ylabel('Monetary')\nax2.scatter(rfm_trans.iloc[:, 1], rfm_trans.iloc[:, 2], c=kmeans.labels_,cmap=\"rainbow\")\nax2.scatter(kmeans.cluster_centers_[:, 1], kmeans.cluster_centers_[:, 2], s=300, alpha=0.9, label = 'Centroids')\n\nax3.set_title(\"Recency-Monetary\")\nax3.set_xlabel('Recency')\nax3.set_ylabel('Monetary')\nax3.scatter(rfm_trans.iloc[:, 0], rfm_trans.iloc[:, 2], c=kmeans.labels_,cmap=\"rainbow\")\nax3.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 2], s=300, alpha=0.9, label = 'Centroids');","04954b42":"plt.figure(figsize=(15, 8))\nplt.scatter(rfm_trans.iloc[:, 0], rfm_log.iloc[:, 1], c = labels, s = 50, cmap = \"viridis\")\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red',alpha=0.5, label = 'Centroids');","07b97a8f":"rfm_trans.sample(8)","c4e444e5":"plt.figure(figsize=(15,6))\n\nplt.subplot(1, 3, 1)\nsns.boxplot(rfm_trans['ClusterID'], rfm_trans['Recency'])\n\nplt.subplot(1, 3, 2)\nsns.boxplot(rfm_trans['ClusterID'], rfm_trans['Frequency'])\n\nplt.subplot(1, 3, 3)\nsns.boxplot(rfm_trans['ClusterID'], rfm_trans['Monetary'])\nplt.show()","c533f03e":"for i in rfm_trans.drop(\"ClusterID\", axis=1):\n    rfm_trans[i].iplot(kind=\"box\", title=i, boxpoints=\"all\", color='lightseagreen')","95eb151d":"rfm_trans['Labels'] = rfm_trans['ClusterID'].map({0:'Lost\/Needed Attention Customers', 1:'Current Custemer', 2:'Top\/Best Customer'})\nrfm_trans.sample(8) ","9c66cf58":"rfm_trans[rfm_trans['Labels'] == \"Lost\/Needed Attention Customers\"]","d5d4b4fa":"rfm_trans[rfm_trans['Labels'] == \"Top\/Best Customer\"]","cfce2890":"rfm_trans[rfm_trans['Labels'] == \"Current Custemer\"]","bb3edf46":"customer_rfm.sample(10)","8651ec43":"customer_rfm[customer_rfm['ClusterID'] == 0].sample(10)","1f9c9d77":"customer_rfm[customer_rfm['ClusterID'] == 1].sample(10)","ff084d9c":"customer_rfm[customer_rfm['ClusterID'] == 2].sample(10)","689b7504":"rfm_trans.shape","3c56d2ae":"customer_rfm.shape","1c986561":"# column to added from first dataframe to second\nextracted_col = rfm_trans['Labels']\n\n# Second dataframe after adding column from first dataframe\ncustomer_rfm.insert(10, \"K-Means Predicted Cluster\", extracted_col)\n# customer_rfm = customer_rfm.join(extracted_col)\ncustomer_rfm['K-Means Predicted Cluster'] = rfm_trans['Labels'].values\ncustomer_rfm.sample(8)","6a86c66d":"customer_rfm['Customer_Category'].value_counts()","49664a86":"RFM_Customer_Segment = pd.DataFrame(customer_rfm['Customer_Category'].value_counts(dropna=False).sort_values(ascending=False))\nRFM_Customer_Segment.reset_index(inplace=True)\nRFM_Customer_Segment.rename(columns={'index':'RFM Customer Segment', 'Customer_Category':'The Number Of Customer'}, inplace=True)\nRFM_Customer_Segment","3a18ee5b":"customer_rfm['K-Means Predicted Cluster'].value_counts()","68f2a094":"KMeans_Predicted_Clusters = pd.DataFrame(customer_rfm['K-Means Predicted Cluster'].value_counts(dropna=False).sort_values(ascending=False))\nKMeans_Predicted_Clusters.reset_index(inplace=True)\nKMeans_Predicted_Clusters.rename(columns={'index':'K-Means Predicted Cluster', 'K-Means Predicted Cluster':'The Number Of Customer'}, inplace=True)\nKMeans_Predicted_Clusters","b78a97f5":"fig = go.Figure()\nfig.add_trace(go.Bar(\n    x=customer_rfm['K-Means Predicted Cluster'].values,\n    y=customer_rfm['K-Means Predicted Cluster'].index,\n    name='Predicted-Clusters By K-Means',\n    marker_color='indianred'\n))\nfig.add_trace(go.Bar(\n    x=customer_rfm['Customer_Category'].values,\n    y=customer_rfm['Customer_Category'].index,\n    name='Customer-Segment By RFM Analysis',\n    marker_color='lightsalmon'\n))\n\n# Here we modify the tickangle of the xaxis, resulting in rotated labels.\nfig.update_layout(barmode='relative', \n                  xaxis_tickangle=-45, \n                  title=\"The Comparison of Predicted-Clusters By K-Means vs Customer-Segment By RFM Analysis\")\nfig.update_traces(dict(marker_line_width=0))\nfig.show()","826c1250":"cohort_df = df\ncohort_df.sample(8)","874a02fb":"def first_of_month(date):    \n    formatted_date = dt.datetime.strptime(str(date), \"%m\/%d\/%Y %H:%M\")\n    return dt.datetime(formatted_date.year, formatted_date.month, 1)","34cc3162":"cohort_df['invoicemonth'] = cohort_df['invoicedate'].apply(first_of_month)\ncohort_df.head(3)","bbdd4cdb":"cohort_df['cohortmonth'] = cohort_df.groupby('customerid')['invoicemonth'].transform('min')\ncohort_df.sample(10)","5628ce1b":"def get_month_int(df, column):\n    year = df[column].dt.year\n    month = df[column].dt.month\n    day = df[column].dt.day\n    return year, month, day","d3a65cb2":"invoice_year, invoice_month, invoice_day = get_month_int(cohort_df, 'invoicemonth')\ncohort_year, cohort_month, cohort_day = get_month_int(cohort_df, 'cohortmonth')","d43f33e8":"cohort_year","d5474d95":"year_diff = invoice_year - cohort_year \nmonth_diff = invoice_month - cohort_month \n\ncohort_df['cohortindex'] = year_diff * 12 + month_diff + 1 ","243be090":"cohort_df.sample(10)","85125444":"#Count monthly active customers from each cohort\n\nfirst_cohort = cohort_df.groupby(['cohortmonth', 'cohortindex'])['customerid'].nunique().reset_index()\nfirst_cohort","d744dbbb":"cohort_df.info()","e9eb3dc4":"cohort_min = cohort_df['cohortmonth'].min()\ncohort_min","056b6050":"cohort_max = cohort_df['cohortmonth'].max()\ncohort_max","8706beb0":"cohort_max - cohort_min","ed0b5151":"first_pivot = first_cohort.pivot(index='cohortmonth', columns='cohortindex', values='customerid').round(1)\nfirst_pivot","b2981481":"sizes = first_pivot.iloc[:, 0]\nretention = first_pivot.divide(sizes, axis=0).round(2)  #axis=0 to ensure the divide along the row axis\nretention","87c45b1a":"sizes","30ecaab6":"plt.figure(figsize=(15, 8))\nplt.title('Retention rates')\nsns.heatmap(data=retention, annot=True, fmt='.0%', vmin=0.0, vmax=0.5, cmap=sns.cubehelix_palette(8))\nplt.show()","d5d4171e":"plt.figure(figsize=(10, 4))\nsns.lineplot(data=first_pivot[1]);","129961c0":"# Average quantity for each cohort\n\nsecond_cohort = cohort_df.groupby(['cohortmonth', 'cohortindex'])['quantity'].mean().reset_index()\nsecond_cohort","9924213c":"second_pivot = second_cohort.pivot(index='cohortmonth', columns='cohortindex', values='quantity').round(1)\nsecond_pivot","a743e175":"plt.figure(figsize=(12, 8))\nplt.title('Average Quantity')\nsns.heatmap(data=second_pivot, annot=True, cmap='Greens', fmt='g');","b0cc37ad":"third_cohort = cohort_df.groupby(['cohortmonth', 'cohortindex'])['total_price'].mean().reset_index()\nthird_cohort","7de57fe1":"third_pivot = third_cohort.pivot(index='cohortmonth', columns='cohortindex', values='total_price').round(2)\nthird_pivot","aecb37ac":"plt.figure(figsize=(12, 8))\nplt.title('Average Revenue')\nsns.heatmap(data=third_pivot, annot=True, cmap='Paired');","1a354b23":"<a id=\"4.2.b\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>4.2.b What does the letter \"C\" in the InvoiceNo column mean?<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","cf540fc7":"<a id=\"5.1.b\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>5.1.b Reviewing df_uk DataFrame<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","dc7a1cb0":"**As we remember from the the previous section, if the invoice number starts with the letter \"C\", it represents the order which was cancelled or customers who abandon their order.**","bd065539":"<a id=\"8.2.b\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>8.2.b Visualizing Analysis of Cohort-1 Using Seaborn & Matplotlib Modules<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","04bb7b60":"**Segment Visualization with WorldCloud**","16ca6a72":"<a id=\"1.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:left; border-radius:10px 10px;\">1.2 About The Features<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**The features in the given dataset are:**\n\n**InvoiceNo**: Invoice number. *Nominal*, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation. \n<br>\n**StockCode**: Product (item) code. *Nominal*, a 5-digit integral number uniquely assigned to each distinct product.\n<br>\n**Description**: Product (item) name. *Nominal*. \n<br>\n**Quantity**: The quantities of each product (item) per transaction. *Numeric*.\n<br>\n**InvoiceDate**: Invoice Date and time. *Numeric*, the day and time when each transaction was generated.\n<br>\n**UnitPrice**: Unit price. *Numeric*, Product price per unit in sterling.\n<br>\n**CustomerID**: Customer number. *Nominal*, a 5-digit integral number uniquely assigned to each customer.\n<br>\n**Country**: Country name. *Nominal*, the name of the country where each customer resides.","b006d956":"<a id=\"8.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.2 - Create 1st Cohort: User number & Retention Rate<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","9f80bcec":"<a id=\"8.1.b\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>8.1.b Calculating Time Offset in Months; i.e. Cohort Index<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**Calculating time offset for each transaction will allows us to report the metrics for each cohort in a comparable fashion.\nFirst, you will create 4 variables that capture the integer value of years, months for Invoice and Cohort Date using the get_date_int() function which you'll create it below.**","854131f3":"**Let's check out duplicated values if exits.**","bbace86f":"#### The UK not only has the most sales revenue, but also the most customers. Since the majority of this dataset contains orders from the UK, we can explore the UK market further analysis by finding out what products the customers buy together and any other buying behaviors to improve our sales and targeting strategy.","75db7ff9":"**1. Choose a date as a point of reference to evaluate how many days ago was the customer's last purchase.**","18dc7056":"**Now let's handle with cancellations and negative values in the given dataset.**","c6672bfe":"**Annotation:**\n\nLimitations of K-means clustering:\n\n1. There is no assurance that it will lead to the global best solution.\n2. Can't deal with different shapes(not circular) and consider one point's probability of belonging to more than one cluster.\n\nThese disadvantages of K-means show that for many datasets (especially low-dimensional datasets), it may not perform as well as you might hope.","d34d8db2":"**1. Dividing the customer_rfm into quarters**","934306ab":"**1. Make a copy of df_uk and drop duplicates**","e7aaf426":"**- 9288 or 1.7% (Original dataset) \/ 8872 or 2% (Tortured dataset) orders were cancelled.**  \n**- 36% --> cancelled and at the same time [\"customerid\"].nunique() ratio.**<br> \n**- Looking deeper into why these orders were cancelled may prevent future cancellations. Now let's find out what a negative UnitPrice means.**","53cffdcc":"**1. Create your plot and resize it.**","bca3d087":"Using customer segmentation categories found [here](http:\/\/www.blastam.com\/blog\/rfm-analysis-boosts-sales) we can formulate different marketing strategies and approaches for customer engagement for each type of customer.\n\nNote: The author in the article scores 1 as the highest and 4 as the lowest","969ff6d6":"<a id=\"5.1.a\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>5.1.a Importing Related Libraries<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","daccafe1":"<a id=\"7.2.b\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.2.b Model Fitting<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\nFit the K-Means Algorithm with the optimal number of clusters you decided and save the model to disk.","55bac078":"**1. What's the total revenue per country?**","f1845aa4":"<a id=\"7.2.d\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.2.d Assigning the Label<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","4eab99e9":"<a id=\"7.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">7.1 - Data Pre-Processing & Exploring<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","798c710d":"**- Before jumping into the tasks, let's try to get some information by examining the columns in the dataset one by one.** ","e0b0a784":"**In the visualizer above, the KElbowVisualizer fits the model for a range of RFM clusters values from 1 to 11, which is set by the parameter k=(1, 12). When the model is fit with 3 clusters we can see an \"elbow\" in the graph, which in this case we know to be the optimal number since we created our synthetic dataset with 3 clusters of points.**","7f6ed994":"**It seems that there have been strong correlations between RFM_Label and the features including RFM scores; however, it should be remembered that RFM_Label were derived from these related RFM scores. So the strong relationship among them is inevitable and acceptable and does NOT create any problem for our analysis.**","115c5a55":"**2. What are the most popular products that are bought in the UK?**","10b73e83":"<a id=\"4.2.a\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>4.2.a Taking a look at relationships between InvoiceNo, Quantity and UnitPrice columns overall.<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","5a53c86b":"**2. Visualize number of customer per country**","0034a179":"**WATCH OUT:** Be careful when determining the number of \"n_clusters\" and the expressions corresponding to the \"labels\" or \"decision\" columns, as my cluster analysis gives different results each time I run my notebook. Please consider the analysis of your own results as I did.","dc32ab76":"<a id=\"1\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">1 - DATA<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n\nUsing the [Online Retail dataset](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+Retail) from the UCI Machine Learning Repository for exploratory data analysis, ***Customer Segmentation***, ***RFM Analysis***, ***K-Means Clustering*** and ***Cohort Analysis***.\n\nThis is a transnational data set which contains all the transactions occurring between 01\/12\/2010 and 09\/12\/2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n\n**For a better understanding and more information, please refer to [UCI Machine Learning Repository](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+Retail) and [Kaggle Website](https:\/\/www.kaggle.com\/vijayuv\/onlineretail)**","ccef8a60":"**5. Drop Last_Purchase_Date since we don't need it anymore**","9b96227f":"**2. Plot normalized data with scatter matrix or pairplot. Also evaluate results.**","00e4d304":"<a id=\"8.3\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.3 - Create the 2nd Cohort: Average Quantity Sold<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","d2744177":"**- Before going further Exploratory data Analysis, let's first create a 'total_price' column to use it for some analyses since the dataset has 'unitprice' and 'quantity' columns.**","cc534984":"In the age of the internet and e-commerce, companies that do not expand their businesses online or utilize digital tools to reach their customers will run into issues like scalability and a lack of digital precsence. An important marketing strategy e-commerce businesses use for analyzing and predicting customer value is customer segmentation. Customer data is used to sort customers into group based on their behaviors and preferences.\n\n**[RFM](https:\/\/www.putler.com\/rfm-analysis\/) (Recency, Frequency, Monetary) Analysis** is a customer segmentation technique for analyzing customer value based on past buying behavior. RFM analysis was first used by the direct mail industry more than four decades ago, yet it is still an effective way to optimize your marketing.\n<br>\n<br>\n**Our goal in this Notebook is to cluster the customers in our data set to:**<br>\n - Recognize who are our most valuable customers\n - Increase revenue\n - Increase customer retention\n - Learn more about the trends and behaviors of our customers\n - Define customers that are at risk\n\nWe will start with **RFM Analysis** and then compliment our findings with predictive analysis using **K-Means Clustering Algorithms.**\n\n**- RECENCY (R):** Time since last purchase<br>\n**- FREQUENCY (F):** Total number of purchases<br>\n**- MONETARY VALUE (M):** Total monetary value\n\n\n\n\n**Benefits of RFM Analysis**\n\n- Increased customer retention\n- Increased response rate\n- Increased conversion rate\n- Increased revenue\n\n**RFM Analysis answers the following questions:**<br>\n - Who are our best customers?\n - Who has the potential to be converted into more profitable customers?\n - Which customers do we need to retain?\n - Which group of customers is most likely to respond to our marketing campaign?\n ","93379e8e":"<a id=\"5.1.c\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>5.1.c Calculating Recency<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**Recency: Days since last purchase**\n\nTo calculate the recency values, follow these steps in order:\n\n**1.** To calculate recency, we need to choose a date as a point of reference to evaluate how many days ago was the customer's last purchase.<br>\n**2.** Create a new column called Date which contains the invoice date without the timestamp.<br>\n**3.** Group by CustomerID and check the last date of purchase.<br>\n**4.** Calculate the days since last purchase.<br>\n**5.** Drop Last_Purchase_Date since we don't need it anymore.<br>\n**6.** Plot RFM distributions.<br>","9b9fe274":"**2. What's the average number of unique items per order or per customer?**","c4503552":"**Rather than applying log1p method for dealing with skewness, using PowerTransformer(method='yeo-johnson') for normalization process gave better results. So we will continue with rfm_trans for further analysis.**  ","70287b00":"<a id=\"6.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.2 - Plotting RFM Segments<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","88972839":"**- With respect to normalization process, 2 different methods werw used; log1p method and PowerTransformer(method='yeo-johnson'). Both revealed different results. Since the results given by PowerTransformer(method='yeo-johnson') are better than the one by log1p, the dataframe normalized by PowerTransformer(method='yeo-johnson') method will be prefered to use for the further analysis.** ","5a6cb017":"<a id=\"5.1.f\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>5.1.f Creating RFM Table<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**For creating the RFM Table, we need to merge the recency, frequency and monetary dataframes.**","151dd9e1":"**1. Find the unique number of InvoiceNo  per customer**","4dad6b6b":"**2. Calculate the frequency of purchases**","f2ff3f10":"[Silhouette Coefficient](http:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_silhouette_analysis.html)","de9d22db":"**0.75 is our threshold-limit to evaluate skewness. Overall below abs(1) seems acceptable for the linear models.**","9a4cbdf6":"<a id=\"8.4.b\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>8.4.b Visualizing Analysis of Cohort-3 Using Seaborn & Matplotlib Modules<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","01e68647":"**3. Let's see how this compares to the number of unique products per customer.**","245a88cf":"[Cohort Analysis](https:\/\/medium.com\/swlh\/cohort-analysis-using-python-and-pandas-d2a60f4d0a4d) is specifically useful in analyzing user growth patterns for products. In terms of a product, a cohort can be a group of people with the same sign-up date, the same usage starts month\/date, or the same traffic source.\nCohort analysis is an analytics method by which these groups can be tracked over time for finding key insights. This analysis can further be used to do customer segmentation and track metrics like retention, churn, and lifetime value.\n\nFor e-commerce organizations, cohort analysis is a unique opportunity to find out which clients are the most valuable to their business. by performing Cohort analysis you can get the following answers to the following questions:\n\n- How much effective was a marketing campaign held in a particular time period?\n- Did the strategy employ to improve the conversion rates of Customers worked?\n- Should I focus more on retention rather than acquiring new customers?\n- Are my customer nurturing strategies effective?\n- Which marketing channels bring me the best results?\n- Is there a seasonality pattern in Customer behavior?\n- Along with various performance measures\/metrics for your organization.","092024d0":"<a id=\"6.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.1 - Calculating RFM Scoring<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\nThe simplest way to create customer segments from an RFM model is by using **Quartiles**. We will assign a score from 1 to 4 to each category (Recency, Frequency, and Monetary) with 4 being the highest\/best value. The final RFM score is calculated by combining all RFM values. For Customer Segmentation, you will use the customer_rfm data set resulting from the RFM analysis.\n<br>\n<br>\n**Note**: Data can be assigned into more groups for better granularity, but we will use 4 in this case.","6fdac76c":"<a id=\"5\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">5) RFM ANALYSIS<p>\n    \n![image.png](attachment:image.png)\n\n**Image credit:** [Retail Automata Analytics](https:\/\/www.retailreco.com\/blog\/rfm-analysis-for-customer-segmentation-in-ecommerce\/\/)    ","77e92c31":"<a id=\"5.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.1 - The Tasks in This Section<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n    \n**1.** Importing Related Libraries.<br>\n**2.** Reviewing \"df_uk\" DataFrame.<br>\n**3.** Calculating Recency.<br>\n**4.** Calculating Frequency.<br>\n**5.** Calculating Monetary Values.<br>\n**6.** Creating RFM Table.<br>","bd1b2a00":"**- Let's check the first item out on the \"stockcode\" column above.**","ff33db3e":"- Having being examined it was clear that the unique value numbers of the stockcode and description columns were different.\n- Moreover, as seen above, there are 2 different descriptions for the item with stockcode 22139.0.\n- So it can be concluded that the difference between the unique value numbers results from the reason that some items have more than one description and the missing values in the description column.\n- In other words, while there should be a unique description for each unique stockcode, some items have more descriptions than one. ","895b8f99":"**As seen above, the items in \"invoiceno\" with negative values start with 'C'. The letter 'C' seems to represents the abbrevation of word 'Cancelled'.**","07e04995":"You will use this function to extract the integer values for Invoice as well as Cohort Date in 3 seperate series for each of the two columns","fcc2e330":"**- Unique Number of 'stockcode' & 'description' columns with a value of 4070 & 1454 respectively are different. Let's take a close look at them why?**","d1efa131":"For e-commerce organisations, cohort analysis is a unique opportunity to find out which clients are the most valuable to their business. by performing Cohort analysis you can get answers to following questions:\n\n- How much effective was a marketing campaign held in a particular time period?\n- Did the strategy employed to improve the conversion rates of Customers worked?\n- Should I focus more on retention rather than acquiring new customers?\n- Are my customer nurturing strategies effective?\n- Which marketing channels bring me the best results?\n- Is there a seasoanlity pattern in Customer behahiour?","87e5b59a":"<a id=\"11\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">11 - FURTHER READINGS<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","06a4ec7d":"<a id=\"8.2.a\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>8.2.a Pivoting Cohort & Cohort Retention<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","b6d81a8a":"**Now we will use the function created above to convert all the invoice dates into respective month date format.**","55b962fb":"<a id=\"1.4\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:left; border-radius:10px 10px;\">1.4 Project Structure<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**- Data Cleaning & Exploratory Data Analysis**<br>\n**- RFM Analysis**<br>\n**- Customer Segmentation**<br>\n**- Applying K-Means Clustering**<br>\n**- Create Cohort and Conduct Cohort Analysis**<br>","2885a3c7":"**5. Calculate average values for each RFM_Label, and return a size of each segment.** ","d3d1b0ad":"<a id=\"7\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">7 - APPLYING K-MEANS CLUSTERING<\/p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","1b6b7060":"### invoicedate","4586d20a":"The Silhouette Coefficient computes the density of clusters computed by the model. The score is computed by averaging the silhouette coefficient for each sample, computed as the difference between the average intra-cluster distance and the mean nearest-cluster distance for each sample, normalized by the maximum value. This produces a score between 1 and -1, where 1 is highly dense clusters and -1 is completely incorrect clustering.\n\nIn SilhouetteVisualizer plots, clusters with higher scores have wider silhouettes, but clusters that are less cohesive will fall short of the average score across all clusters, which is plotted as a vertical dotted red line. In other words, the vertical red-dotted line on the plot indicates the average silhouette score for all observations.\n\n**Notice that our graph contains heterogeneous and short silhouettes.** ","a6e531d0":"### description","50bf5dca":"**6. Plot RFM distributions**","eaf31eec":"**1. Create a scatter plot and select cluster centers.**","733e5036":"**3. Visualize total cost per country**","67e21ae2":"**What is cohort index?**<br>\nCohort period \/ Cohort Index:\u200aA integer representation a customer\u2019s stage in its \u201clifetime\u201d. **The number represents the number of months passed since the first purchase.**","8f4b017c":"**How we want to continue this analysis depends on how the business plans to use the results and the level of granularity the business stakeholders want to see in the clusters. We can also ask what range of customer behavior from high to low value customers are the stakeholders interested in exploring. From those answers, various methods of clustering can be used and applied on RFM variable or directly on the transaction data set.**","bd1006f3":"**2. Visualize Cluster Id vs Recency, Cluster Id vs Frequency and Cluster Id vs Monetary using Box plot. Also evaluate the results.** ","d24d638f":"**Note That : The min for unit price = 0 and the min for Quantity with negative values.**","7d22ae56":"[The Elbow Method](https:\/\/en.wikipedia.org\/wiki\/Elbow_method_(clustering)) ","f37427c0":"<a id=\"7.2.f\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.2.f Discussion<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**Discuss your final results. Compare your own labels from the Customer Segmentation with the labels found by K-Means.**","6d5f5a16":"<a id=\"8.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.1 - Feature Engineering<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n    \n**Feature engineering is the process of using domain knowledge to extract features (characteristics, properties, attributes) from raw data.**\n    \n**Usefull Links for a better understanding and more information, please refer to;**<br> \n[Link 1](https:\/\/en.wikipedia.org\/wiki\/Feature_engineering#:~:text=Feature%20engineering%20is%20the%20process,competitions%20and%20machine%20learning%20projects.), [Link 2](https:\/\/towardsdatascience.com\/feature-engineering-for-machine-learning-3a5e293a5114), [Link 3](https:\/\/www.kdnuggets.com\/2018\/12\/feature-engineering-explained.html), [Link 4](https:\/\/machinelearningmastery.com\/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it\/) & [Link 5](https:\/\/www.analyticsvidhya.com\/blog\/2021\/03\/step-by-step-process-of-feature-engineering-for-machine-learning-algorithms-in-data-science\/)","6bfe38dc":"<a id=\"7.1.c\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.1.c Data Normalization<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","72b952ce":"# <p style=\"background-color:#9452a5;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\">CUSTOMER SEGMENTATION & RFM ANALYSIS<\/p>","1c8717e7":"**- When we have examined the 'invoiceno' column, it is seen that some invoiceno values start with A & C. They most probably represent cancelled orders. They will be handled with in a separate section.** ","677d4166":"<a id=\"4.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.2 - The Tasks in This Section<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","dc2d0d5c":"<a id=\"1.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:left; border-radius:10px 10px;\">1.1 Context<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**\"Predict behavior to retain customers. You can analyze all relevant customer data and develop focused customer retention programs.\" [IBM Sample Data Sets]**\n\n**Each row represents a customer, each column contains customer\u2019s attributes described on the column Metadata.**\n\n**The data set includes information about:**\n\n- **Customers who left within the last month \u2013 the column is called Churn**\n- **Services that each customer has signed up for \u2013 phone, multiple lines, internet, online security, online backup, device -protection, tech support, and streaming TV and movies**\n- **Customer account information \u2013 how long they\u2019ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges**\n- **Demographic info about customers \u2013 gender, age range, and if they have partners and dependents**\n\n**To explore this type of models and learn more about the subject.**\n\n**[New version](https:\/\/community.ibm.com\/community\/user\/businessanalytics\/blogs\/steven-macko\/2019\/07\/11\/telco-customer-churn-1113) from IBM.**","19450776":"**- The number of rows in the query result is the same as the missing value of the 'description' column.**","9530ee78":"Now that we have our customers segmented into 6 different categories, we can gain further insight into customer behavior by using predictive models in conjuction with out RFM model.\nPossible algorithms include **Logistic Regression**, **K-means Clustering**, and **K-nearest Neighbor**. We will go with [K-Means](https:\/\/towardsdatascience.com\/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1) since we already have our distinct groups determined. K-means has also been widely used for market segmentation and has the advantage of being simple to implement.","7199c7a9":"Businesses have this ever-lasting urge to understand their customers. The better you understand the customer, the better you serve them, and the higher the financial gain you receive from that customer. Since the dawn of trade, this process of understanding customers for a strategic gain has been there practiced and this task is known majorly as [Customer Segmentation](https:\/\/clevertap.com\/blog\/rfm-analysis\/).\nWell as the name suggests, Customer Segmentation could segment customers according to their precise needs. Some of the common ways of segmenting customers are based on their Recency-Frequency-Monatory values, their demographics like gender, region, country, etc, and some of their business-crafted scores. You will use Recency-Frequency-Monatory values for this case.\n\nIn this section, you will create an RFM Segmentation Table where you segment your customers by using the RFM table. For example, you can label the best customer as \"Big Spenders\" and the lost customer as \"Lost Customer\".","bafe6a41":"### stockcode","25eba208":"**1. Calculate sum total cost by customers and named \"Monetary\"**","e5991a9f":"- Kline, R.B. (2011). Principles and practice of structural equation modeling (5th ed., pp. 3-427). New York:The Guilford Press.\n- Edwards, A. (1976). An introduction to linear regression and correlation. W. H. Freeman\n- Everitt, B. S.; Skrondal, A. (2010), The Cambridge Dictionary of Statistics, Cambridge University Press.\n- https:\/\/www.amazon.com\/Python-Feature-Engineering-Cookbook-transforming\/dp\/1789806313\/ref=sr_1_1?dchild=1&keywords=feature+engineering+cookbook&qid=1627628487&s=books&sr=1-1\n- https:\/\/www.amazon.com\/Feature-Engineering-Made-Easy-Identify-ebook\/dp\/B077N6MK5W\n- https:\/\/www.amazon.com\/Feature-Engineering-Selection-Chapman-Science\/dp\/1032090855\/ref=sr_1_1?crid=19T9G95E1W7VJ&dchild=1&keywords=feature+engineering+and+selection+kuhn&qid=1628050948&sprefix=feature+engineering+and+%2Cdigital-text%2C293&sr=8-1\n- https:\/\/www.amazon.com\/Introduction-Machine-Learning-Python-Scientists\/dp\/1449369413\n- Neural Networks from Scratch in Python (by Kinsley \u00a7 Kukiela) [external link text](https:\/\/nnfs.io\/)\n- Practical Statistics for Data Scientists (by Bruce & Gedeck) [external link text](https:\/\/www.amazon.com\/Practical-Statistics-Data-Scientists-Essential\/dp\/149207294X\/ref=sr_1_1?dchild=1&keywords=Practical+Statistics+for+Data+Scientists&qid=1627662007&sr=8-1)\n- Applications of Deep Neural Networks(by Jeff Heaton) [external link text](https:\/\/arxiv.org\/abs\/2009.05673)\n- Applied Predictive Modeling (by Kuhn & Johnson) [external link text](https:\/\/www.amazon.com\/Applied-Predictive-Modeling-Max-Kuhn\/dp\/1461468485\/ref=pd_sbs_3\/141-4288971-3747365?pd_rd_w=AOIS7&pf_rd_p=3676f086-9496-4fd7-8490-77cf7f43f846&pf_rd_r=MCCHJXWK39VD6VW7RVAR&pd_rd_r=4ffcd1ea-44b9-4f33-b9b3-dc02ee159662&pd_rd_wg=nU1Ex&pd_rd_i=1461468485&psc=1:)\n- Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (by Aur\u00e9lien G\u00e9ron) [external link text](https:\/\/www.amazon.com\/Hands-Machine-Learning-Scikit-Learn-TensorFlow\/dp\/1492032646\/ref=sr_1_1?crid=2GV554Q2EKD1E&dchild=1&keywords=hands-on+machine+learning+with+scikit-learn%2C+keras%2C+and+tensorflow&qid=1627628294&s=books&sprefix=hands%2Cstripbooks-intl-ship%2C309&sr=1-1)\n- Master Machine Learning Algorithms (by Brownlee, ML algorithms are very well explained ) [external link text](https:\/\/machinelearningmastery.com\/master-machine-learning-algorithms\/)\n- Python Feature Engineering Cookbook (by Galli) [external link text](https:\/\/www.amazon.com\/Python-Feature-Engineering-Cookbook-transforming\/dp\/1789806313\/ref=sr_1_1?dchild=1&keywords=feature+engineering+cookbook&qid=1627628487&s=books&sr=1-1)\n- Feature Engineering Made Easy (by Ozdemir & Susarla) [external link text](https:\/\/www.amazon.com\/Feature-Engineering-Made-Easy-Identify-ebook\/dp\/B077N6MK5W)\n- Feature Engineering and Selection (by Kuhn & Johnson) [external link text](https:\/\/www.amazon.com\/Feature-Engineering-Selection-Chapman-Science\/dp\/1032090855\/ref=sr_1_1?crid=19T9G95E1W7VJ&dchild=1&keywords=feature+engineering+and+selection+kuhn&qid=1628050948&sprefix=feature+engineering+and+%2Cdigital-text%2C293&sr=8-1)\n- Imbalanced Classification with Python(by Brownlee) [external link text](https:\/\/machinelearningmastery.com\/imbalanced-classification-with-python\/)\n- https:\/\/www.kaggle.com\/kaanboke\/the-most-common-evaluation-metrics-a-gentle-intro","dc4d9be2":"<a id=\"0\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">PREFACE<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","3bb1e68b":"**2. Plot RFM distributions**","368a0a06":"**3. Plot RFM distributions**","c972fbc4":"<a id=\"6.1.a\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>6.1.a Creating the RFM Segmentation Table<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","47819fd0":"<a id=\"4.2.e\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>4.2.e Exploring the Orders<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","9790e2d0":"Use the variables created above to calcualte the difference in days and store them in cohort Index column.","254bb01e":"**- There are some values below zero in the \"quantity\" and \"unitprice\" columns. So they need to be examined closely to find out why.**","a77f9e2d":"<a id=\"8\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">9 - CONCLUSION<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","886bb985":"<a id=\"4.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.1 - A General Look at the Data<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n    \n**Pandas profiling is an open source Python module with which we can quickly do an exploratory data analysis with just a few lines of code. Similarly, for EDA, profile_report() is One-Line Magical Code creating reports in the interactive HTML format which is quite easy to understand and analyze the data. In short, at the first hand, what pandas profiling does is to save us all the work of visualizing and understanding the distribution of each variable.**\n\n**For a better understanding and more information, please refer to [Source 1](https:\/\/www.analyticsvidhya.com\/blog\/2021\/06\/generate-reports-using-pandas-profiling-deploy-using-streamlit\/) & [Source 2](https:\/\/towardsdatascience.com\/exploratory-data-analysis-with-pandas-profiling-de3aae2ddff3)**","e336c8e3":"<a id=\"7.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">7.2 - K-Means Implementation<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\nFor k-means, you have to set k to the number of clusters you want, but figuring out how many clusters is not obvious from the beginning. We will try different cluster numbers and check their [silhouette coefficient](http:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_silhouette_analysis.html). The silhouette coefficient for a data point measures how similar it is to its assigned cluster from -1 (dissimilar) to 1 (similar). \n<br>\n<br>\n**Note**: K-means is sensitive to initializations because they are critical to qualifty of optima found. Thus, we will use smart initialization called \"Elbow Method\".","9e43b5c3":"The Hopkins statistic is a way of measuring the cluster tendency of a data set. A value close to 1 tends to indicate the data is highly clustered, random data will tend to result in values around 0.5, and uniformly distributed data will tend to result in values close to 0. The rule of the thumb is that if the Hopkins statistic is **above 0.5,** then the dataset is **not clusterable.**\n\nIf the value of Hopkins is **close to zero,** then we can reject null hypothesis and deduce that dataset contains **meaningful clusters.** Usually, we can believe in the existence of clusters when the hopkins score is bellow 0.25. Thus, for clusterable datasets Hopkins statistic should be close to 0. **In our study Hopkins test score demonstrates a clusterable dataset with a value of 0.08696686895678812.**","d63ccfed":"<a id=\"4.2.f\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>4.2.f Exploring Customers by Country<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","ff85dae2":"**Note That:** Customer retention is a very useful metric to understand how many of the all customers are still active. Retention gives you the percentage of active customers compared to the total number of customers.","c6ae3f62":"**Segment Visualization with Plotly Matplotlib Squarify**","19f57cc2":"- Kline, R.B. (2011). Principles and practice of structural equation modeling (5th ed., pp. 3-427). New York:The Guilford Press.\n- Edwards, A. (1976). An introduction to linear regression and correlation. W. H. Freeman\n- Everitt, B. S.; Skrondal, A. (2010), The Cambridge Dictionary of Statistics, Cambridge University Press.\n- https:\/\/www.amazon.com\/Python-Feature-Engineering-Cookbook-transforming\/dp\/1789806313\/ref=sr_1_1?dchild=1&keywords=feature+engineering+cookbook&qid=1627628487&s=books&sr=1-1\n- https:\/\/www.amazon.com\/Feature-Engineering-Made-Easy-Identify-ebook\/dp\/B077N6MK5W\n- https:\/\/www.amazon.com\/Feature-Engineering-Selection-Chapman-Science\/dp\/1032090855\/ref=sr_1_1?crid=19T9G95E1W7VJ&dchild=1&keywords=feature+engineering+and+selection+kuhn&qid=1628050948&sprefix=feature+engineering+and+%2Cdigital-text%2C293&sr=8-1\n- https:\/\/www.amazon.com\/Introduction-Machine-Learning-Python-Scientists\/dp\/1449369413\n- Neural Networks from Scratch in Python (by Kinsley \u00a7 Kukiela) [external link text](https:\/\/nnfs.io\/)\n- Practical Statistics for Data Scientists (by Bruce & Gedeck) [external link text](https:\/\/www.amazon.com\/Practical-Statistics-Data-Scientists-Essential\/dp\/149207294X\/ref=sr_1_1?dchild=1&keywords=Practical+Statistics+for+Data+Scientists&qid=1627662007&sr=8-1)\n- Applications of Deep Neural Networks(by Jeff Heaton) [external link text](https:\/\/arxiv.org\/abs\/2009.05673)\n- Applied Predictive Modeling (by Kuhn & Johnson) [external link text](https:\/\/www.amazon.com\/Applied-Predictive-Modeling-Max-Kuhn\/dp\/1461468485\/ref=pd_sbs_3\/141-4288971-3747365?pd_rd_w=AOIS7&pf_rd_p=3676f086-9496-4fd7-8490-77cf7f43f846&pf_rd_r=MCCHJXWK39VD6VW7RVAR&pd_rd_r=4ffcd1ea-44b9-4f33-b9b3-dc02ee159662&pd_rd_wg=nU1Ex&pd_rd_i=1461468485&psc=1:)\n- Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (by Aur\u00e9lien G\u00e9ron) [external link text](https:\/\/www.amazon.com\/Hands-Machine-Learning-Scikit-Learn-TensorFlow\/dp\/1492032646\/ref=sr_1_1?crid=2GV554Q2EKD1E&dchild=1&keywords=hands-on+machine+learning+with+scikit-learn%2C+keras%2C+and+tensorflow&qid=1627628294&s=books&sprefix=hands%2Cstripbooks-intl-ship%2C309&sr=1-1)\n- Master Machine Learning Algorithms (by Brownlee, ML algorithms are very well explained ) [external link text](https:\/\/machinelearningmastery.com\/master-machine-learning-algorithms\/)\n- Python Feature Engineering Cookbook (by Galli) [external link text](https:\/\/www.amazon.com\/Python-Feature-Engineering-Cookbook-transforming\/dp\/1789806313\/ref=sr_1_1?dchild=1&keywords=feature+engineering+cookbook&qid=1627628487&s=books&sr=1-1)\n- Feature Engineering Made Easy (by Ozdemir & Susarla) [external link text](https:\/\/www.amazon.com\/Feature-Engineering-Made-Easy-Identify-ebook\/dp\/B077N6MK5W)\n- Feature Engineering and Selection (by Kuhn & Johnson) [external link text](https:\/\/www.amazon.com\/Feature-Engineering-Selection-Chapman-Science\/dp\/1032090855\/ref=sr_1_1?crid=19T9G95E1W7VJ&dchild=1&keywords=feature+engineering+and+selection+kuhn&qid=1628050948&sprefix=feature+engineering+and+%2Cdigital-text%2C293&sr=8-1)\n- Imbalanced Classification with Python(by Brownlee) [external link text](https:\/\/machinelearningmastery.com\/imbalanced-classification-with-python\/)\n- https:\/\/www.analyticsvidhya.com\/blog\/2021\/06\/generate-reports-using-pandas-profiling-deploy-using-streamlit\/\n- https:\/\/towardsdatascience.com\/exploratory-data-analysis-with-pandas-profiling-de3aae2ddff3\n- https:\/\/analyticsindiamag.com\/why-data-scaling-is-important-in-machine-learning-how-to-effectively-do-it\/\n- https:\/\/www.analyticsvidhya.com\/blog\/2021\/01\/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning\/\n- https:\/\/www.analyticsvidhya.com\/blog\/2021\/01\/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning\/\n- https:\/\/www.kaggle.com\/kadirduran\/military-power-clustering\n- https:\/\/www.analyticsvidhya.com\/blog\/2020\/04\/feature-scaling-machine-learning-normalization-standardization\/\n- https:\/\/machinelearningmastery.com\/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling\/\n- https:\/\/machinelearningmastery.com\/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping\/#:~:text=Early%20stopping%20is%20a%20method,deep%20learning%20neural%20network%20models.\n- https:\/\/www.sciencedirect.com\/science\/article\/pii\/S1556086415306043\n- https:\/\/medium.com\/machine-learning-t%C3%BCrkiye\/crm-analizi-rfm-analizi-ve-cltv-m%C3%BC%C5%9Fteri-ya%C5%9Fam-boyu-de%C4%9Feri-36e5c3a232b1\n- https:\/\/medium.com\/capillary-data-science\/rfm-analysis-an-effective-customer-segmentation-technique-using-python-58804480d232\n- https:\/\/towardsdatascience.com\/rfm-analysis-using-bigquery-ml-bfaa51b83086\n- https:\/\/www.ticimax.com\/blog\/kohort-analizi-hakkinda-bilmeniz-gerekenler\n- https:\/\/www.ibm.com\/docs\/en\/spss-modeler\/18.2.2?topic=node-rfm-analysis-settings\n- https:\/\/www.kaggle.com\/vijayuv\/onlineretail\n- https:\/\/www.putler.com\/rfm-analysis\/\n- https:\/\/clevertap.com\/blog\/rfm-analysis\/\n- https:\/\/towardsdatascience.com\/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1\n- https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_silhouette_analysis.html\n- https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_silhouette_analysis.html\n- https:\/\/medium.com\/swlh\/cohort-analysis-using-python-and-pandas-d2a60f4d0a4d\n- https:\/\/en.wikipedia.org\/wiki\/Feature_engineering#:~:text=Feature%20engineering%20is%20the%20process,competitions%20and%20machine%20learning%20projects.\n- https:\/\/towardsdatascience.com\/feature-engineering-for-machine-learning-3a5e293a5114\n- https:\/\/www.kdnuggets.com\/2018\/12\/feature-engineering-explained.html\n- https:\/\/machinelearningmastery.com\/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it\/\n- https:\/\/www.analyticsvidhya.com\/blog\/2021\/03\/step-by-step-process-of-feature-engineering-for-machine-learning-algorithms-in-data-science\/\n- https:\/\/clevertap.com\/blog\/cohort-analysis\/","c6c3ef24":"<a id=\"2\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">2) LIBRARIES NEEDED IN THE STUDY<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","efb6c975":"<a id=\"8.1.a\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>8.1.a Extracting the Month of the Purchase<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**First we will create a function, which takes any date and returns the formatted date with day value as 1st of the same month and Year.**","f5e48d0e":"**Let's check if there exits a multicollinearity problem among the features.**","c3d8ee4d":"<a id=\"7.2.c\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.2.c Visualizing the Clusters<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","22243ae4":"<a id=\"3\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">3) ANALYSIS<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","3d1b0e34":"**4. Calculate the days since last purchase**","6661eed3":"<a id=\"5.1.d\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>5.1.d Calculating Frequency<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**Frequency: Number of purchases**<br>To calculate how many times a customer purchased something, we need to count how many invoices each customer has. Therefore, to calculate the frequency values, follow these steps in order:","c22c6c81":"<a id=\"3.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">3.1 Reading the Data<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**How to read and assign the dataset as df: You can [Visit Here](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_csv.html) (You can define it as what you want instead of df)**","583b05fe":"### We will continue analyzing the UK transactions with customer segmentation.","d32b1967":"<a id=\"4\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">4) DATA CLEANING & EXPLORATORY DATA ANALYSIS (EDA)<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","05dc909b":"**2. How many customers do we have in each segment?**","95e612ad":"<a id=\"10\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">10 - REFERENCES<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","e0c96da6":"Since we will be performing Cohort Analysis based on transaction records of customers, the columns we will be dealing with mainly:\n- Invoice Data\n- CustomerID\n- Price\n- Quantity\n\nThe following steps will performed to generate the Cohort Chart of Retention Rate:\n- Month Extraction from InvioceDate column\n- Assigning Cohort to Each Transaction\n- Assigning Cohort Index to each transaction\n- Calculating number of unique customers in each Group of (ChortDate,Index)\n- Creating Cohort Table for Retention Rate\n- Creating the Cohort Chart using the Cohort Table\n\nThe Detailed information about each step is given below:","9ea49bfc":"<a id=\"4.2.c\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>4.2.c Handling with Missing Values<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","64c8b918":"In this project, three different types of analyses were applied independently, including RFM Segmentation, K-Means Clustering and Cohort Analysis. All three analysis methods offer some specific advantages and also have limitations. In this project, we were able to see in which area and to what extent we should use these methods.\n\nFirst, RFM Customer Segmentation was performed, which is the technique of diving customers into groups based on their purchase patterns to identify who are the most profitable groups. In segmenting customers, various criteria can also be used depending on the market such as geographic, demographic characteristics or behavior bases. This technique assumes that groups with different features require different approaches to marketing and wants to figure out the groups who can boost their profitability the most. In this sense, the customers in our dataset were divided into insightful clusters. What makes RFM analysis attractive is the flexibility it offers so the segmentation could be specified in terms of business needs and an analyist could create customer groups based on their purchase history \u2013 how recently, with what Frequency, and what value they bought. On the other hand, it is susceptible to user-induced biases.\n\nSecond, K-Means clustering as an unsupervised machine learning algorithm was applied to determine underlying clusters. Since the dataset in the given study did NOT provide a significant tendency to cluster when compared to user defined segmentations in RFM modelling, K-Means clustering could NOT produce a satisfactory and insightful clustering.\n\nFinally, Cohort Analysis is used to gain new insights from the data related to time-based cohorts and is a kind of behavioral analytics that breaks the data in a data set into related groups before analysis. These groups, or cohorts, usually share common characteristics or experiences within a defined time-span. Cohort analysis allows the analysist to ask more specific, targeted questions and make informed product decisions that will reduce churn and drastically increase revenue. You could also call it customer churn analysis. Therefore, the strength of cohort analysis is its elasticity since it provides specific analysis based on business needs. As such, business owners can develop effective strategies and marketing campaigns to retent their customers, prevent churn rate. ","6778251e":"**1. You can use the logarithm method to normalize the values in a column.**","29d128c7":"### invoiceno","fb54b04e":"<a id=\"5.1.e\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>5.1.e Calculating Monetary Values<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**Monetary: Total amount of money spent**<br>The monetary value is calculated by adding together the cost of the customers' purchases.","5ad888e2":"<a id=\"toc\"><\/a>\n\n## <h3 style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\" class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tablist\" aria-controls=\"home\">TABLE OF CONTENTS<\/h3>\n\n* [   PREFACE](#0)\n* [1) DATA](#1)\n    * [1.1 Context](#1.1)\n    * [1.2 About the Features](#1.2) \n    * [1.3 What the Problem is](#1.3) \n    * [1.4 Project Structure](#1.4) \n* [2) LIBRARIES NEEDED IN THE STUDY](#2)\n    * [2.1 User Defined Functions](#2.1)\n* [3) ANALYSIS](#3)\n    * [3.1) Reading the Data](#3)\n* [4) EXPLORATORY DATA ANALYSIS (EDA) & VISUALIZATION](#4)\n    * [4.1 - A General Look at the Data](#4.1)\n    * [4.2 - The Tasks in This Section](#4.2)\n        * [4.2.a Taking a look at relationships between \"InvoiceNo\", \"Quantity\" and \"UnitPrice columns\"](#4.2.a)\n        * [4.2.b What does the letter \"C\" in the \"invoiceno\" column mean?](#4.2.b)\n        * [4.2.c Handling with Missing Values](#4.2.c)\n        * [4.2.d Cleaning the Data from the Noise and Missing Values](#4.2.d)\n        * [4.2.e Exploring the Orders](#4.2.e)\n        * [4.2.f Exploring Customers by Country](#4.2.f)\n        * [4.2.g Exploring the UK Market](#4.2.g)\n* [5) RFM ANALYSIS](#5)     \n    * [5.1 - The Tasks in This Section](#5.1)\n        * [5.1.a Importing Related Libraries](#5.1.a)\n        * [5.1.b Reviewing \"df_uk\" DataFrame](#5.1.b)\n        * [5.1.c Calculating Recency](#5.1.c)\n        * [5.1.d Calculating Frequency](#5.1.d)\n        * [5.1.e Calculating Monetary Values](#5.1.e)\n        * [5.1.f Creating RFM Table](#5.1.f)\n* [6 - CUSTOMER SEGMENTATION WITH RFM SCORES](#6)        \n    * [6.1 Calculating RFM Scoring](#6.1)\n        * [6.1.a Creating the RFM Segmentation Table](#6.1.a)\n    * [6.2 Plotting RFM Segments](#6.2)\n* [7 - APPLYING K-MEANS CLUSTERING](#7)\n    * [7.1 Data Pre-Processing & Exploring](#7.1)\n        * [7.1.a Defining & Plotting Feature Correlations](#7.1.a)\n        * [7.1.b Visualizing Feature Distributions](#7.1.b)\n        * [7.1.c Data Normalization](#7.1.c)\n    * [7.2 K-Means Implementation](#7.2)\n        * [7.2.a Defining Optimal Cluster Number (K) by using \"Elbow Method\" & \"Silhouette Analysis\"](#7.2.a)\n        * [7.2.b Model Fitting](#7.2.b)\n        * [7.2.c Visualizing the Clusters](#7.2.c)\n        * [7.2.d Assigning the Label](#7.2.d)\n        * [7.2.e Conclusion](#7.2.e)    \n        * [7.2.f Discussion](#7.2.f)    \n* [8 - CREATING COHORT & CONDUCTING COHORT ANALYSIS](#8)    \n    * [8.1 Future Engineering](#8.1)\n        * [8.1.a Extracting the Month of the Purchase](#8.1.a)\n        * [8.1.b Calculating time offset in Months i.e. Cohort Index](#8.1.b)    \n    * [8.2 Creating 1st Cohort: User Number & Retention Rate](#8.2) \n        * [8.2.a Pivoting Cohort & Cohort Retention](#8.2.a)\n        * [8.2.b Visualizing Analysis of Cohort-1 Using Seaborn & Matplotlib](#8.2.b)        \n    * [8.3 Creating 2nd Cohort: Average Quantity Sold](#8.3) \n        * [8.3.a Pivoting Cohort and Cohort Retention](#8.3.a)\n        * [8.3.b Visualizing Analysis of Cohort-2 Using Seaborn & Matplotlib](#8.3.b)      \n    * [8.4 Creating 3rd Cohort: Average Sales](#8.4)     \n        * [8.4.a Pivoting Cohort & Cohort Retention](#8.4.a)\n        * [8.4.b Visualizing Analysis of Cohort-2 Using Seaborn & Matplotlib](#8.4.b)        \n**Note: There may be some additional sub-tasks associated with each task, you will see them in order during the course of the work.*    \n* [9) CONLUSION](#9)\n* [10) REFERENCES](#10)\n* [11) FURTHER READINGS](#11)","ae66cd2e":"**As seen above, the items in \"invoiceno\" with negative values start with 'A'. The letter 'A' seems to represents the abbrevation of word 'Abandoned'.**\n\n**Similarly we also see that there are some negative values in the Quantity and UnitPrice columns. These are possibly canceled and\/or returned orders. Let's check them out.**","64140e57":"### unitprice","b331c54c":"<a id=\"7.1.b\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.1.b Visualizing Feature Distributions<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**To get a better understanding of the dataset, you can costruct a scatter matrix of each of the three features in the RFM data.**","e361d8a2":"<a id=\"8\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">8 - CREATING COHORT & CONDUCTING COHORT ANALYSIS<\/p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","e87a086c":"Welcome to \"RFM Customer Segmentation & Cohort Analysis Project\". This is the first project of the Capstone Project Series, which consists of 4 different project that contain different scenarios.\n\nThis is a project which you will learn what is RFM? And how to apply RFM Analysis and Customer Segmentation using K-Means Clustering. Also you will improve your Data Cleaning, Data Visualization and Exploratory Data Analysis capabilities. On the other hand you will create Cohort and Conduct Cohort Analysis. \n\nBefore diving into the project, please take a look at the determines and project structure.\n\n- **NOTE:** This tutorial assumes that you already know the basics of coding in Python and are familiar with the theory behind K-Means Clustering.\n\n**Usefull Links for a better understanding and more information, please refer to;**<br> \n[Link 1](https:\/\/medium.com\/machine-learning-t%C3%BCrkiye\/crm-analizi-rfm-analizi-ve-cltv-m%C3%BC%C5%9Fteri-ya%C5%9Fam-boyu-de%C4%9Feri-36e5c3a232b1), [Link 2](https:\/\/medium.com\/capillary-data-science\/rfm-analysis-an-effective-customer-segmentation-technique-using-python-58804480d232), [Link 3](https:\/\/towardsdatascience.com\/rfm-analysis-using-bigquery-ml-bfaa51b83086), [Link 4](https:\/\/www.ticimax.com\/blog\/kohort-analizi-hakkinda-bilmeniz-gerekenler) & [Link 5](https:\/\/www.ibm.com\/docs\/en\/spss-modeler\/18.2.2?topic=node-rfm-analysis-settings)\n","92c0a237":"<a id=\"8.4.a\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>8.4.a Pivoting Cohort & Cohort Retention<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","62bed012":"### quantity","5bee1526":"**4. Define rfm_level function that tags customers by using RFM_Scrores and Create a new variable RFM_Label.**","7cd49966":"**Intercluster distance maps display an embedding of the cluster centers in 2 dimensions with the distance to other centers preserved, e.g. the closer two centers are in the visualization, the closer they are in the original feature space.**","804ed73d":"**1. Create df_uk DataFrame**","ab6b594a":"**1.a. Create two functions, one for Recency and one for Frequency and Monetary. For Recency, customers in the first quarter should be scored as 4, this represents the highest Recency value. Conversely, for Frequency and Monetary, customers in the last quarter should be scored as 4, representing the highest Frequency and Monetary values.**\n\n**1.b. Score customers from 1 to 4 by applying the functions you have created. Also create separate score column for each value.** ","76ed1167":"<a id=\"8.3.a\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>8.3.a Pivoting Cohort & Cohort Retention<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","c78cca7e":"<a id=\"7.2.a\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.2.a Defining the Optimal Number of Clusters<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","3ebc139c":"<a id=\"1.3\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:left; border-radius:10px 10px;\">1.3 What The Problem Is<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\nFirst of all, to observe the structure of the data and missing values, you can use exploratory data analysis and data visualization techniques.\n\nYou must do descriptive analysis. Because you must understand the relationship of the features to each other and clear the noise and missing values in the data. After that, the data set will be ready for RFM analysis.\n\nBefore starting the RFM Analysis, you will be asked to do some analysis regarding the distribution of *Orders*, *Customers* and *Countries*. These analyzes will help the company develop its sales policies and contribute to the correct use of resources.\n\nYou will notice that the UK not only has the most sales revenue, but also the most customers. So you will continue to analyze only UK transactions in the next RFM Analysis, Customer Segmentation and K-Means Clustering topics.\n\nNext, you will begin RFM Analysis, a customer segmentation technique based on customers' past purchasing behavior. \n\nBy using RFM Analysis, you can enable companies to develop different approaches to different customer segments so that they can get to know their customers better, observe trends better, and increase customer retention and sales revenues.\n\nYou will calculate the Recency, Frequency and Monetary values of the customers in the RFM Analysis you will make using the data consisting of UK transactions. Ultimately, you have to create an RFM table containing these values.\n\nIn the Customer Segmentation section, you will create an RFM Segmentation Table where you segment your customers by using the RFM table. For example, you can label the best customer as \"Big Spenders\" and the lost customer as \"Lost Customer\".\n\nWe will segment the customers ourselves based on their recency, frequency, and monetary values. But can an **unsupervised learning** model do this better for us? You will use the K-Means algorithm to find the answer to this question. Then you will compare the classification made by the algorithm with the classification you have made yourself.\n\nBefore applying K-Means Clustering, you should do data pre-processing. In this context, it will be useful to examine feature correlations and distributions. In addition, the data you apply for K-Means should be normalized.\n\nOn the other hand, you should inform the K-means algorithm about the number of clusters it will predict. You will also try the *** Elbow method *** and *** Silhouette Analysis *** to find the optimum number of clusters.\n\nAfter the above operations, you will have made cluster estimation with K-Means. You should visualize the cluster distribution by using a scatter plot. You can observe the properties of the resulting clusters with the help of the boxplot. Thus you will be able to tag clusters and interpret results.\n\nFinally, you will do Cohort Analysis with the data you used at the beginning, regardless of the analysis you have done before. Cohort analysis is a subset of behavioral analytics that takes the user data and breaks them into related groups for analysis. This analysis can further be used to do customer segmentation and track metrics like retention, churn, and lifetime value.","d69c30aa":"<a id=\"6\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">6 - CUSTOMER SEGMENTATION WITH RFM SCORES<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","c2491983":"**We have 4338 unique \"customerid\". But while we are grouping them by using by \"country\" column; we notice that our unique number of customers is increasing. <br>It means that a total of 8 customerids are included in multiple countries. For ex: *'12431.0'***","e98d08e0":"<a id=\"2.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">2.1 User Defined Functions<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n    \n**We have defined some useful user defined functions.**","695ac5f0":"<a id=\"8.4\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.4 - Create the 3rd Cohort: Average Sales<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","f5923777":"**Let's assume that the orders which do NOT have customer ID's were not made by the customers already in the dataset because the customers who in fact made some purchases already have ID's.** \n\n**So we don't want to assign these orders to those customers because this would alter the insights we draw from the data.** ","d0b7b3e9":"### stockcode & description","56040716":"<a id=\"7.2.e\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.2.e Conclusion<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**- Cluster 0 :** The first cluster is more related to the **\"Lost\/Needed Attention Customers\"** who used to visit and shopping at different frequencies or who are still customer but haven\u2019t been visiting recently. The store\/department should bring them back with relevant personalized promotions, and run surveys to find out what went wrong and avoid losing them to a competitor.\n\n**- Cluster 1 :** The second cluster belongs to the **\"Current Custemer\"** who have been shopping at different frequencies and spending in varying amounts. The store\/department should offer membership or loyalty programs or recommend related products to upsell them and help them become its Loyalists or Champions, and offer renewals and helpful products to encourage another purchase.\n\n**- Cluster 2 :** The third cluster can be interpreted as **\"Top\/Best Customers\"** who have been shopping in average frequency or most recently, but spent a good amount. The store\/department should reward or offer membership or loyalty programs them to become its Loyalists or Champions.","43212702":"**Segment Visualization with Plotly TreeMap**","0b831890":"<a id=\"7.1.a\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>7.1.a Defining and Plot Feature Correlations<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**Create Heatmap and evaluate the results.**","f1b6384a":"**So it can be concluded that according to YellowBrick Elbow method above we are recommended to apply 3 n_clusters for modelling. In addition we will check Classic Elbow Method for a better decision-making.**","22346982":"<a id=\"8.3.b\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>8.3.b Visualizing Analysis of Cohort-2 Using Seaborn & Matplotlib Modules<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","23922153":"**Two main benefits of reading the above cohort table, are:**\n\n- **product lifetime** (as depicted vertically down in the table) \u2013 comparing different cohorts at the same stage in their life cycle \u2013 it can be seen what % of people in a cohort are coming back to app after 3 months and so on. The early lifetime months can be linked to the quality of your onboarding experience and the performance of customer success team, and\n\n\n- **user lifetime** (as depicted horizontally to the right of the table) \u2013 seeing the long term relationship with people in any cohort \u2013 to ascertain how long people are coming back and how strong or how valuable that cohort is. This can be presumably linked to something like the quality of the product, operations, and customer support.\n\n**For a better understanding and more information, please refer to [Source 1](https:\/\/clevertap.com\/blog\/cohort-analysis\/) ","64567de8":"<a id=\"4.2.d\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>4.2.d Cleaning the Data from the Noise & Missing Values<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","681fba71":"**3. Now that scored each customer, you'll combine the scores for segmentation.**","8f21f7ac":"**The number of \"stockcode\" & \"description\" should be the same; however, there is a difference.**","390d4cb0":"**When we filter either canceled orders by Quantity > 0 or non-canceled orders by Quantity < 0 as returns, this confirms that negative values mean that the order was canceled. So let's find out how many orders were cancelled?**","5c1812e4":"**2. Create a new column called Date which contains the invoice date without the timestamp**","1d494986":"<a id=\"4.2.g\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>4.2.g Exploring the UK Market<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","d3f55dbd":"**3. Group by CustomerID and check the last date of purchase**","1ae07be1":"### customerid"}}